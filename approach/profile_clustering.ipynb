{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import KMeans\n",
    "from kneed import KneeLocator\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "import numpy as np\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.cluster import AffinityPropagation\n",
    "from sklearn.cluster import MeanShift, estimate_bandwidth\n",
    "from sklearn.cluster import SpectralClustering\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.cluster import OPTICS\n",
    "from sklearn.cluster import Birch\n",
    "import matplotlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def blockPrint():\n",
    "    sys.__stdout__ = sys.stdout\n",
    "    sys.stdout = open(os.devnull, 'w')\n",
    "\n",
    "# Restore\n",
    "def enablePrint():\n",
    "    sys.stdout = sys.__stdout__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_max_normalization(df_col):\n",
    "    data_min, data_max = numpy.min(df_col),numpy.max(df_col)\n",
    "    normalized = [(item - data_min)/(data_max - data_min) for item in df_col]\n",
    "    return normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_HierarchicalClustering(df, linkage, k=None):\n",
    "    blockPrint()\n",
    "    #d_array = numpy.array(data_list)\n",
    "    # Find K\n",
    "    if not k:\n",
    "        #km_kwargs = {\"init\":\"random\", \"n_init\":10,\"max_iter\": 3000000, \"random_state\" : 50}\n",
    "        silhouette = []\n",
    "        for k in range(2, 10):\n",
    "            hc = AgglomerativeClustering(n_clusters=k, linkage = linkage)\n",
    "            hc.fit(df)\n",
    "            silhouette.append(silhouette_score(df,hc.labels_))\n",
    "\n",
    "        plt.style.use(\"fivethirtyeight\")\n",
    "        plt.plot(range(2, 10), silhouette)\n",
    "        plt.xticks(range(2, 10))\n",
    "        plt.xlabel(\"Number of Clusters\")\n",
    "        plt.ylabel(\"silhouette coefficient\")\n",
    "        plt.show()  \n",
    "        #Find K\n",
    "        kl = KneeLocator(range(2, 10), silhouette, curve=\"convex\", direction=\"decreasing\")\n",
    "        k = kl.elbow\n",
    "        print(f\"elbow point = {k}\")\n",
    "\n",
    "    d_hc = AgglomerativeClustering(n_clusters=k, linkage = linkage)\n",
    "    d_hc.fit(df)\n",
    "    labels = list(d_hc.labels_)\n",
    "    enablePrint()\n",
    "    \n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_SpectralClustering(df, k=None):\n",
    "    #d_array = numpy.array(data_list)\n",
    "    # Find K\n",
    "    blockPrint()\n",
    "    if not k:\n",
    "        #km_kwargs = {\"init\":\"random\", \"n_init\":10,\"max_iter\": 3000000, \"random_state\" : 50}\n",
    "        silhouette = []\n",
    "        for k in range(2, 10):\n",
    "            sc = SpectralClustering(n_clusters=k, assign_labels='cluster_qr')\n",
    "            sc.fit(df)\n",
    "            silhouette.append(silhouette_score(df,sc.labels_))\n",
    "\n",
    "        plt.style.use(\"fivethirtyeight\")\n",
    "        plt.plot(range(2, 10), silhouette)\n",
    "        plt.xticks(range(2, 10))\n",
    "        plt.xlabel(\"Number of Clusters\")\n",
    "        plt.ylabel(\"silhouette coefficient\")\n",
    "        plt.show()  \n",
    "        #Find K\n",
    "        kl = KneeLocator(range(2, 10), silhouette, curve=\"convex\", direction=\"decreasing\")\n",
    "        k = kl.elbow\n",
    "        print(f\"elbow point = {k}\")\n",
    "\n",
    "    d_sc = SpectralClustering(n_clusters=k, assign_labels='discretize')\n",
    "    d_sc.fit(df)\n",
    "    labels = list(d_sc.labels_)\n",
    "    enablePrint()\n",
    "    \n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_kmeans(df, k=None):\n",
    "    #d_array = numpy.array(data_list)\n",
    "    # Find K\n",
    "    blockPrint()\n",
    "    if not k:\n",
    "        km_kwargs = {\"init\":\"random\", \"n_init\":10,\"max_iter\": 3000000, \"random_state\" : 50}\n",
    "        cost = []\n",
    "\n",
    "        for k in range(1, 11):\n",
    "            km = KMeans(n_clusters=k, **km_kwargs)\n",
    "            km.fit(df)\n",
    "            cost.append(km.inertia_)\n",
    "\n",
    "        plt.style.use(\"fivethirtyeight\")\n",
    "        plt.plot(range(1, 11), cost)\n",
    "        plt.xticks(range(1, 11))\n",
    "        plt.xlabel(\"Number of Clusters\")\n",
    "        plt.ylabel(\"Cost\")\n",
    "        plt.show()  \n",
    "        #Find K\n",
    "        kl = KneeLocator(range(1, 11), cost, curve=\"convex\", direction=\"decreasing\")\n",
    "        k = kl.elbow\n",
    "        print(f\"elbow point = {k}\")\n",
    "\n",
    "    d_km = KMeans( n_clusters=k, init=\"random\", n_init=10, verbose=1, random_state = 100)\n",
    "    d_km.fit(df)\n",
    "    labels = list(d_km.labels_)\n",
    "    enablePrint()\n",
    "    \n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "basedir = os.path.join(\"..\",\"data\")\n",
    "developer_data = pd.read_csv(os.path.join(basedir, \"contributor_features.csv\"), index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_data1 = developer_data[[\"worktime\", \"code contribution density\", \"languages\", \"total commits\"]]\n",
    "\n",
    "for fea in [\"code contribution density\", \"languages\", \"total commits\"]:\n",
    "    q1,q2,q3 = developer_data[fea].quantile([.5,.75,.9])\n",
    "    cluster_data1.loc[developer_data[fea] < q1, fea] = 0\n",
    "    cluster_data1.loc[(developer_data[fea] >= q1) & (developer_data[fea] < q2), fea] = 1\n",
    "    cluster_data1.loc[(developer_data[fea] >= q2) & (developer_data[fea] < q3), fea] = 2\n",
    "    cluster_data1.loc[(developer_data[fea] >= q3) , fea] = 3\n",
    "\n",
    "cluster_data1.loc[ (developer_data[\"worktime\"]>=1)&(developer_data[\"worktime\"]<=7),\"worktime\"] = 3\n",
    "cluster_data1.loc[ (developer_data[\"worktime\"]>=8)&(developer_data[\"worktime\"]<=18),\"worktime\"] = 0\n",
    "cluster_data1.loc[ (developer_data[\"worktime\"]>=19)&(developer_data[\"worktime\"]<=23),\"worktime\"] = 3\n",
    "cluster_data1.loc[ developer_data[\"worktime\"]==0,\"worktime\"] = 3\n",
    "\n",
    "cluster_data1[\"code contribution density\"] = cluster_data1[\"code contribution density\"].astype(int)\n",
    "cluster_result1 = developer_data[[\"name\", \"project\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "km_labels = my_kmeans(cluster_data1)\n",
    "cluster_result1[\"kmeans\"] = km_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_result1[\"kmeans\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "silhouette_score(cluster_data1,km_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### affinity propogation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(100):\n",
    "    af = AffinityPropagation(preference=i*(-1), random_state=0).fit(cluster_data1)\n",
    "    cluster_centers_indices = af.cluster_centers_indices_\n",
    "    af_labels = af.labels_\n",
    "\n",
    "    n_clusters_ = len(cluster_centers_indices)\n",
    "    if n_clusters_<= 1:\n",
    "        print(i*(-1), \"Estimated number of clusters: %d\" % n_clusters_)\n",
    "    else:\n",
    "        print(i*(-1), \"Estimated number of clusters: %d\" % n_clusters_, silhouette_score(cluster_data1,af_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "af = AffinityPropagation(preference=-6, random_state=0).fit(cluster_data1)\n",
    "cluster_centers_indices = af.cluster_centers_indices_\n",
    "af_labels = af.labels_\n",
    "\n",
    "n_clusters_ = len(cluster_centers_indices)\n",
    "cluster_result1[\"af\"] = af_labels\n",
    "print(\"Estimated number of clusters: %d\" % n_clusters_,silhouette_score(cluster_data1,af_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(davies_bouldin_score(cluster_data1,cluster_result1['af']),calinski_harabasz_score(cluster_data1,cluster_result1['af']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### means shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [x*0.1 for x in range(1,11)]:\n",
    "    bandwidth = estimate_bandwidth(cluster_data1, quantile=i, n_samples=500)\n",
    "    \n",
    "    ms = MeanShift(bandwidth=bandwidth, bin_seeding=True)\n",
    "    ms.fit(cluster_data1)\n",
    "    ms_labels = ms.labels_\n",
    "    cluster_centers = ms.cluster_centers_\n",
    "    \n",
    "    labels_unique = np.unique(ms_labels)\n",
    "    n_clusters_ = len(labels_unique)\n",
    "    \n",
    "    if n_clusters_<= 1:\n",
    "        print(i, \"Estimated number of clusters: %d\" % n_clusters_)\n",
    "    else:\n",
    "        print(i, \"Estimated number of clusters: %d\" % n_clusters_, silhouette_score(cluster_data1,ms_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [1000,800, 500, 100, 90, 80, 60, 50, 40, 30, 20]:\n",
    "    bandwidth = estimate_bandwidth(cluster_data1, quantile=0.3, n_samples=i)\n",
    "    \n",
    "    ms = MeanShift(bandwidth=bandwidth, bin_seeding=True)\n",
    "    ms.fit(cluster_data1)\n",
    "    ms_labels = ms.labels_\n",
    "    cluster_centers = ms.cluster_centers_\n",
    "    \n",
    "    labels_unique = np.unique(ms_labels)\n",
    "    n_clusters_ = len(labels_unique)\n",
    "    \n",
    "    if n_clusters_<= 1:\n",
    "        print(i, \"Estimated number of clusters: %d\" % n_clusters_)\n",
    "    else:\n",
    "        print(i, \"Estimated number of clusters: %d\" % n_clusters_, silhouette_score(cluster_data1,ms_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bandwidth = estimate_bandwidth(cluster_data1, quantile=0.3, n_samples=100)\n",
    "ms = MeanShift(bandwidth=bandwidth, bin_seeding=True)\n",
    "ms.fit(cluster_data1)\n",
    "ms_labels = ms.labels_\n",
    "cluster_result1['ms'] = ms_labels\n",
    "silhouette_score(cluster_data1,ms_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### spectral clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc_labels = my_SpectralClustering(cluster_data1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SpectralClustering(n_clusters=4,assign_labels='cluster_qr',random_state=0).fit(cluster_data1)\n",
    "sc_labels = sc.labels_\n",
    "cluster_result1[\"sc\"] = sc_labels\n",
    "silhouette_score(cluster_data1,sc_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### hierarchical clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hc_labels = my_HierarchicalClustering(cluster_data1,\"ward\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hc = AgglomerativeClustering(n_clusters=3, linkage=\"ward\").fit(cluster_data1)\n",
    "hc_labels = hc.labels_\n",
    "labels_unique = np.unique(hc_labels)\n",
    "n_clusters_ = len(labels_unique)\n",
    "\n",
    "cluster_result1[\"hc\"] = hc_labels\n",
    "print(\"number of estimated clusters : %d\" % n_clusters_, silhouette_score(cluster_data1,hc_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dbscan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [0.1*x for x in range(1,10)]:\n",
    "    dbscan = DBSCAN(eps=i,min_samples=50).fit(cluster_data1)\n",
    "    db_labels = dbscan.labels_\n",
    "\n",
    "    # Number of clusters in labels, ignoring noise if present.\n",
    "    n_clusters_ = len(set(db_labels)) - (1 if -1 in db_labels else 0)\n",
    "    n_noise_ = list(db_labels).count(-1)\n",
    "\n",
    "    print(i,\"Estimated number of clusters: %d\" % n_clusters_)\n",
    "    print(i,\"Estimated number of noise points: %d\" % n_noise_)\n",
    "    print(silhouette_score(cluster_data1,db_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in range(1,500,10):   \n",
    "    dbscan = DBSCAN(eps=0.1,min_samples=n).fit(cluster_data1)\n",
    "    db_labels = dbscan.labels_\n",
    "\n",
    "    # Number of clusters in labels, ignoring noise if present.\n",
    "    n_clusters_ = len(set(db_labels)) - (1 if -1 in db_labels else 0)\n",
    "    n_noise_ = list(db_labels).count(-1)\n",
    "    #if n_clusters_ < 10:\n",
    "    print(n)\n",
    "    print(n,\"Estimated number of clusters: %d\" % n_clusters_)\n",
    "    print(n, \"Estimated number of noise points: %d\" % n_noise_)\n",
    "    print(silhouette_score(cluster_data1,db_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbscan = DBSCAN(eps=0.1,min_samples=171).fit(cluster_data1)\n",
    "db_labels = dbscan.labels_\n",
    "\n",
    "# Number of clusters in labels, ignoring noise if present.\n",
    "n_clusters_ = len(set(db_labels)) - (1 if -1 in db_labels else 0)\n",
    "n_noise_ = list(db_labels).count(-1)\n",
    "#if n_clusters_ < 10:\n",
    "print(\"Estimated number of clusters: %d\" % n_clusters_)\n",
    "print(\"Estimated number of noise points: %d\" % n_noise_)\n",
    "print(silhouette_score(cluster_data1,db_labels))\n",
    "cluster_result1[\"dbscan\"] = db_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### optics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [5000, 4000, 3000, 2000, 1000, 900, 800, 700, 600, 500, 400, 300, 200, 150, 100, 90, 80, 60, 50, 40, 30, 20, 10,9,8,7,6,5,4,3,2]:\n",
    "    optics = OPTICS(eps=0.1,min_samples=i).fit(cluster_data1)\n",
    "    optics_labels = optics.labels_\n",
    "    \n",
    "    labels_unique = np.unique(optics_labels)\n",
    "    n_clusters_ = len(labels_unique)\n",
    "    if n_clusters_ > 1:\n",
    "        print(i,\"number of estimated clusters : %d\" % n_clusters_,silhouette_score(cluster_data1,optics_labels))\n",
    "    else:\n",
    "        print(i,\"number of estimated clusters : %d\" % n_clusters_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [x*0.1 for x in range(1,11)]:\n",
    "    optics = OPTICS(eps=i,min_samples=4000).fit(cluster_data1)\n",
    "    optics_labels = optics.labels_\n",
    "    \n",
    "    labels_unique = np.unique(optics_labels)\n",
    "    n_clusters_ = len(labels_unique)\n",
    "    \n",
    "    if n_clusters_ > 1:\n",
    "        print(i,\"number of estimated clusters : %d\" % n_clusters_,silhouette_score(cluster_data1,optics_labels))\n",
    "    else:\n",
    "        print(i,\"number of estimated clusters : %d\" % n_clusters_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optics = OPTICS(eps=0.1,min_samples=4000).fit(cluster_data1)\n",
    "optics_labels = optics.labels_\n",
    "labels_unique = np.unique(optics_labels)\n",
    "n_clusters_ = len(labels_unique)\n",
    "cluster_result1[\"optics\"] = optics_labels\n",
    "silhouette_score(cluster_data1,optics_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BIRCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for i in [5000, 4000, 3000, 2000, 1000, 900, 800, 700, 600, 500, 400, 300, 200, 150, 100, 90, 80, 60, 50, 40, 30, 20, 10,9,8,7,6,5,4,3,2]:\n",
    "for k in range(2,11):\n",
    "    brc = Birch(threshold=1.0, branching_factor=500, n_clusters=k).fit(np.ascontiguousarray(cluster_data1))\n",
    "    brc_labels = brc.labels_\n",
    "    \n",
    "    labels_unique = np.unique(brc_labels)\n",
    "    n_clusters_ = len(labels_unique)\n",
    "    \n",
    "    if n_clusters_ > 1:\n",
    "        print(k,\"number of estimated clusters : %d\" % n_clusters_,silhouette_score(cluster_data1,brc_labels))\n",
    "    else:\n",
    "        print(k,\"number of estimated clusters : %d\" % n_clusters_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brc = Birch(threshold=1.0, branching_factor=500, n_clusters=4).fit(np.ascontiguousarray(cluster_data1))\n",
    "brc_labels = brc.labels_\n",
    "\n",
    "labels_unique = np.unique(brc_labels)\n",
    "n_clusters_ = len(labels_unique)\n",
    "\n",
    "print(i, \"number of estimated clusters : %d\" % n_clusters_)\n",
    "cluster_result1[\"birch\"] = brc_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_result1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for alg in ['kmeans',\t'ms',\t'sc',\t'hc',\t'dbscan',\t'optics',\t'birch']:\n",
    "    print(alg, len(cluster_result1[alg].value_counts()), \n",
    "          silhouette_score(cluster_data1,cluster_result1[alg]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering Results - Profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_result1.loc[cluster_result1['sc']==0, 'profile'] = 'ca'\n",
    "cluster_result1.loc[cluster_result1['sc']==1, 'profile'] = 'cw'\n",
    "cluster_result1.loc[cluster_result1['sc']==2, 'profile'] = 'pa'\n",
    "cluster_result1.loc[cluster_result1['sc']==3, 'profile'] = 'pw'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "developer_data['profile'] = cluster_result1['profile']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_result1['profile'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add a column 'profile' to contributor_features.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "developer_data.to_csv(os.path.join(basedir, \"contributor_features.csv\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
