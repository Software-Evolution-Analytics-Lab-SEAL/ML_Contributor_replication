{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import statsmodels\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from rpy2.robjects.packages import importr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def date_time_handler(date_time_str):\n",
    "    return datetime.datetime.strptime(date_time_str, '%Y-%m-%d %H:%M:%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "basedir = os.path.join(\"..\",\"data\")\n",
    "developer_data = pd.read_csv(os.path.join(basedir, 'contributor_features.csv'), index_col = 0)\n",
    "filtered_developer_period_df = pd.read_csv(os.path.join(basedir,'contributor_period_activity.csv'), index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "basedir = os.path.join(os.getcwd(), '..','data')\n",
    "proj_names = ['tensorflow_tensorflow',\n",
    "              'pytorch_pytorch',\n",
    "                'scikit-learn_scikit-learn',\n",
    "                'keras-team_keras',\n",
    "                'apache_mxnet',\n",
    "                'theano_aesara',\n",
    "                'onnx_onnx',\n",
    "                'deeplearning4j_deeplearning4j']\n",
    "\n",
    "repo_dirs = [os.path.join(basedir,item) for item in proj_names]\n",
    "projects = ['tensorflow', 'pytorch', 'scikit-learn', 'keras', 'mxnet', 'theano_aesara', 'onnx', 'deeplearning4j']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# studied period\n",
    "start = datetime.datetime(2008,1,1)\n",
    "end = datetime.datetime(2024,1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read project fork history\n",
    "project_fork_df = []\n",
    "for path in repo_dirs:\n",
    "    fork_df = pd.read_csv(os.path.join(path, 'fork_history.csv'), index_col=0)\n",
    "    fork_df['time'] = fork_df['time'].apply(lambda x:date_time_handler(x.replace('+00:00','')))\n",
    "    fork_df = fork_df.loc[(fork_df['time']>=start)&(fork_df['time']<end)]\n",
    "    project_fork_df.append(fork_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read project star history\n",
    "project_star_df = []\n",
    "for path in repo_dirs:\n",
    "    stargazer_df = pd.read_csv(os.path.join(path, 'star_history.csv'), index_col=0)\n",
    "    #stargazer_df['starredAt'] = stargazer_df['starredAt'].apply(lambda x:datetime.datetime.strptime(x, '%Y-%m-%dT%H:%M:%SZ'))\n",
    "    stargazer_df['starredAt'] = stargazer_df['starredAt'].apply(lambda x: datetime.datetime.strptime(x, '%Y-%m-%d %H:%M:%S'))\n",
    "    stargazer_df = stargazer_df.loc[(stargazer_df['starredAt']>=start)&(stargazer_df['starredAt']<end)]\n",
    "    project_star_df.append(stargazer_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read project commit data\n",
    "project_commit_df = []\n",
    "\n",
    "for proj_dir in repo_dirs:\n",
    "    commit_df = pd.read_csv(os.path.join(proj_dir,'commit_main.csv'), index_col=0)\n",
    "\n",
    "    commit_df['Time'] = pd.to_datetime(commit_df['Time'])\n",
    "    commit_df['Changed Files'] = commit_df['Changed Files'].apply(lambda x: x.split() if isinstance(x, str) else [])\n",
    "    commit_df['Parents'] = commit_df['Parents'].apply(lambda x: x.split() if isinstance(x, str) else [])\n",
    "    project_commit_df.append(commit_df)\n",
    "for df in project_commit_df:\n",
    "    df.set_index('Commit#', inplace=True)\n",
    "project_commit_dict = [df.to_dict(orient='index') for df in project_commit_df ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read project prs and issues\n",
    "project_pr_df = []\n",
    "for proj_dir in repo_dirs:    \n",
    "    pr_df = pd.read_csv(os.path.join(proj_dir,'pull_request_main.csv'), index_col=0)\n",
    "    pr_df['Opened time'] = pd.to_datetime(pr_df['Opened time'])\n",
    "    pr_df['Closed time'] = pd.to_datetime(pr_df['Closed time'])\n",
    "    pr_df['Labels'] = pr_df['Labels'].apply(lambda x: x.split() if isinstance(x, str) else [])\n",
    "    pr_df['Assignees'] = pr_df['Assignees'].apply(lambda x: x.split() if isinstance(x, str) else [])\n",
    "    pr_df['Reviewers'] = pr_df['Reviewers'].apply(lambda x: x.split() if isinstance(x, str) else [])\n",
    "    pr_df['Participants'] = pr_df['Participants'].apply(lambda x: x.split() if isinstance(x, str) else [])\n",
    "    pr_df['Commits'] = pr_df['Commits'].apply(lambda x: x.split() if isinstance(x, str) else [])\n",
    "    project_pr_df.append(pr_df)\n",
    "project_pr_dict = [df.to_dict(orient='index') for df in project_pr_df ]\n",
    "\n",
    "project_issue_df = []\n",
    "for proj_dir in repo_dirs: \n",
    "    issue_df = pd.read_csv(os.path.join(proj_dir,'issue_main.csv'), index_col=0)\n",
    "    issue_df['Opened time'] = pd.to_datetime(issue_df['Opened time'])\n",
    "    issue_df['Closed time'] = pd.to_datetime(issue_df['Closed time'])\n",
    "    issue_df['Labels'] = issue_df['Labels'].apply(lambda x: x.split() if isinstance(x, str) else [])\n",
    "    issue_df['Assignees'] = issue_df['Assignees'].apply(lambda x: x.split() if isinstance(x, str) else [])\n",
    "    issue_df['Participants'] = issue_df['Participants'].apply(lambda x: x.split() if isinstance(x, str) else [])\n",
    "    project_issue_df.append(issue_df)\n",
    "project_issue_dict = [df.to_dict(orient='index') for df in project_issue_df ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare dependent variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get 90-days periods for each project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_period_stars(i,x):\n",
    "    return len(project_star_df[i].loc[(project_star_df[i]['starredAt']>= x['start'])&(project_star_df[i]['starredAt']< x['end'])])\n",
    "    \n",
    "def get_period_forks(i,x):\n",
    "    return len(project_fork_df[i].loc[(project_fork_df[i]['time']>= x['start'])&(project_fork_df[i]['time']< x['end'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_duration = []\n",
    "for i in range(len(projects)):\n",
    "    project_duration.append({'start':min(project_commit_df[i]['Time'].min(),project_issue_df[i]['Opened time'].min(),\n",
    "                                         project_pr_df[i]['Opened time'].min()),\n",
    "                            'end':end})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split project lifespan into 90 days periods\n",
    "num_days = 90\n",
    "project_stage_df = []\n",
    "project_metric = ['start', 'end','star','fork']\n",
    "for i in range(len(projects)):\n",
    "    df = pd.DataFrame(columns=project_metric)\n",
    "    bin_dates = [[],[]]\n",
    "    cur = project_duration[i]['start']\n",
    "    delta = datetime.timedelta(days=num_days)\n",
    "    while cur+delta < project_duration[i]['end']:\n",
    "        bin_dates[0].append(cur)\n",
    "        bin_dates[1].append(cur+delta)\n",
    "        cur += delta\n",
    "    df['start'] = bin_dates[0]\n",
    "    df['end'] = bin_dates[1]\n",
    "    project_stage_df.append(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bin the number of forks and stars into 90 days periods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(projects)):\n",
    "    project_stage_df[i]['project'] = projects[i]\n",
    "    project_stage_df[i]['period'] = project_stage_df[i].index\n",
    "    project_stage_df[i]['star'] = project_stage_df[i].apply(lambda x: get_period_stars(i,x), axis=1)\n",
    "    project_stage_df[i]['fork'] = project_stage_df[i].apply(lambda x: get_period_forks(i,x), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the initial periods for pytorch and theano where the projects might not be public yet (fork=0)\n",
    "df = project_stage_df[projects.index('pytorch')]\n",
    "df = df.loc[~df['period'].between(0, 19)]\n",
    "df['period'] = df['period'].apply(lambda x:x-20)\n",
    "project_stage_df[projects.index('pytorch')] = df\n",
    "\n",
    "df = project_stage_df[projects.index('theano_aesara')]\n",
    "df = df.loc[~df['period'].between(0, 13)]\n",
    "df['period'] = df['period'].apply(lambda x:x-14)\n",
    "project_stage_df[projects.index('theano_aesara')] = df\n",
    "\n",
    "df = project_stage_df[projects.index('scikit-learn')]\n",
    "df = df.loc[~df['period'].between(0, 1)]\n",
    "df['period'] = df['period'].apply(lambda x:x-2)\n",
    "project_stage_df[projects.index('scikit-learn')] = df\n",
    "\n",
    "df = project_stage_df[projects.index('mxnet')]\n",
    "df = df.loc[~df['period']==0]\n",
    "df['period'] = df['period'].apply(lambda x:x-1)\n",
    "project_stage_df[projects.index('mxnet')] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge all projects and obtain the dependent variables\n",
    "lemr_data = pd.concat(project_stage_df,ignore_index = True)\n",
    "lemr_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare indenpendent variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemr_data['project_code'] = lemr_data['project'].apply(lambda x:projects.index(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### workload composition related variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(projects)):\n",
    "    periods = lemr_data.loc[lemr_data['project'] == projects[i]]['period'].unique()\n",
    "    for p in periods:\n",
    "        #print(projects[i],p)\n",
    "        try:\n",
    "            pavtivity = filtered_developer_period_df.loc[(filtered_developer_period_df['project'] == projects[i])&(filtered_developer_period_df['period'] == p)]\n",
    "            lemr_data.loc[(lemr_data['project'] == projects[i])&(lemr_data['period'] == p), 'pattern1_ratio'] = len(pavtivity.loc[pavtivity['wcp_code']==1])/len(pavtivity) if len(pavtivity) > 0 else 0\n",
    "            lemr_data.loc[(lemr_data['project'] == projects[i])&(lemr_data['period'] == p), 'pattern2_ratio'] = len(pavtivity.loc[pavtivity['wcp_code']==2])/len(pavtivity) if len(pavtivity) > 0 else 0\n",
    "            lemr_data.loc[(lemr_data['project'] == projects[i])&(lemr_data['period'] == p), 'pattern3_ratio'] = len(pavtivity.loc[pavtivity['wcp_code']==3])/len(pavtivity) if len(pavtivity) > 0 else 0\n",
    "            lemr_data.loc[(lemr_data['project'] == projects[i])&(lemr_data['period'] == p), 'pattern4_ratio'] = len(pavtivity.loc[pavtivity['wcp_code']==4])/len(pavtivity) if len(pavtivity) > 0 else 0\n",
    "            lemr_data.loc[(lemr_data['project'] == projects[i])&(lemr_data['period'] == p), 'pattern5_ratio'] = len(pavtivity.loc[pavtivity['wcp_code']==5])/len(pavtivity) if len(pavtivity) > 0 else 0\n",
    "            \n",
    "\n",
    "            lemr_data.loc[(lemr_data['project'] == projects[i])&(lemr_data['period'] == p), 'pattern1_commit_ratio'] = pavtivity.loc[pavtivity['wcp_code']==1]['commit'].sum()/pavtivity['commit'].sum()\n",
    "            lemr_data.loc[(lemr_data['project'] == projects[i])&(lemr_data['period'] == p), 'pattern1_issue_ratio'] = pavtivity.loc[pavtivity['wcp_code']==1]['issue'].sum()/pavtivity['issue'].sum()\n",
    "            lemr_data.loc[(lemr_data['project'] == projects[i])&(lemr_data['period'] == p), 'pattern1_issue_comment_ratio'] = pavtivity.loc[pavtivity['wcp_code']==1]['issue comment'].sum()/pavtivity['issue comment'].sum()\n",
    "            lemr_data.loc[(lemr_data['project'] == projects[i])&(lemr_data['period'] == p), 'pattern1_pr_comment_ratio'] = pavtivity.loc[pavtivity['wcp_code']==1]['pr comment'].sum()/pavtivity['pr comment'].sum()\n",
    "            lemr_data.loc[(lemr_data['project'] == projects[i])&(lemr_data['period'] == p), 'pattern1_review_ratio'] = pavtivity.loc[pavtivity['wcp_code']==1]['review'].sum()/pavtivity['review'].sum()\n",
    "\n",
    "            lemr_data.loc[(lemr_data['project'] == projects[i])&(lemr_data['period'] == p), 'pattern2_commit_ratio'] = pavtivity.loc[pavtivity['wcp_code']==2]['commit'].sum()/pavtivity['commit'].sum()\n",
    "            lemr_data.loc[(lemr_data['project'] == projects[i])&(lemr_data['period'] == p), 'pattern2_issue_ratio'] = pavtivity.loc[pavtivity['wcp_code']==2]['issue'].sum()/pavtivity['issue'].sum()\n",
    "            lemr_data.loc[(lemr_data['project'] == projects[i])&(lemr_data['period'] == p), 'pattern2_issue_comment_ratio'] = pavtivity.loc[pavtivity['wcp_code']==2]['issue comment'].sum()/pavtivity['issue comment'].sum()\n",
    "            lemr_data.loc[(lemr_data['project'] == projects[i])&(lemr_data['period'] == p), 'pattern2_pr_comment_ratio'] = pavtivity.loc[pavtivity['wcp_code']==2]['pr comment'].sum()/pavtivity['pr comment'].sum()\n",
    "            lemr_data.loc[(lemr_data['project'] == projects[i])&(lemr_data['period'] == p), 'pattern2_review_ratio'] = pavtivity.loc[pavtivity['wcp_code']==2]['review'].sum()/pavtivity['review'].sum()\n",
    "\n",
    "            lemr_data.loc[(lemr_data['project'] == projects[i])&(lemr_data['period'] == p), 'pattern3_commit_ratio'] = pavtivity.loc[pavtivity['wcp_code']==3]['commit'].sum()/pavtivity['commit'].sum()\n",
    "            lemr_data.loc[(lemr_data['project'] == projects[i])&(lemr_data['period'] == p), 'pattern3_issue_ratio'] = pavtivity.loc[pavtivity['wcp_code']==3]['issue'].sum()/pavtivity['issue'].sum()\n",
    "            lemr_data.loc[(lemr_data['project'] == projects[i])&(lemr_data['period'] == p), 'pattern3_issue_comment_ratio'] = pavtivity.loc[pavtivity['wcp_code']==3]['issue comment'].sum()/pavtivity['issue comment'].sum()\n",
    "            lemr_data.loc[(lemr_data['project'] == projects[i])&(lemr_data['period'] == p), 'pattern3_pr_comment_ratio'] = pavtivity.loc[pavtivity['wcp_code']==3]['pr comment'].sum()/pavtivity['pr comment'].sum()\n",
    "            lemr_data.loc[(lemr_data['project'] == projects[i])&(lemr_data['period'] == p), 'pattern3_review_ratio'] = pavtivity.loc[pavtivity['wcp_code']==3]['review'].sum()/pavtivity['review'].sum()\n",
    "\n",
    "            lemr_data.loc[(lemr_data['project'] == projects[i])&(lemr_data['period'] == p), 'pattern4_commit_ratio'] = pavtivity.loc[pavtivity['wcp_code']==4]['commit'].sum()/pavtivity['commit'].sum()\n",
    "            lemr_data.loc[(lemr_data['project'] == projects[i])&(lemr_data['period'] == p), 'pattern4_issue_ratio'] = pavtivity.loc[pavtivity['wcp_code']==4]['issue'].sum()/pavtivity['issue'].sum()\n",
    "            lemr_data.loc[(lemr_data['project'] == projects[i])&(lemr_data['period'] == p), 'pattern4_issue_comment_ratio'] = pavtivity.loc[pavtivity['wcp_code']==4]['issue comment'].sum()/pavtivity['issue comment'].sum()\n",
    "            lemr_data.loc[(lemr_data['project'] == projects[i])&(lemr_data['period'] == p), 'pattern4_pr_comment_ratio'] = pavtivity.loc[pavtivity['wcp_code']==4]['pr comment'].sum()/pavtivity['pr comment'].sum()\n",
    "            lemr_data.loc[(lemr_data['project'] == projects[i])&(lemr_data['period'] == p), 'pattern4_review_ratio'] = pavtivity.loc[pavtivity['wcp_code']==4]['review'].sum()/pavtivity['review'].sum()\n",
    "\n",
    "            lemr_data.loc[(lemr_data['project'] == projects[i])&(lemr_data['period'] == p), 'pattern5_commit_ratio'] = pavtivity.loc[pavtivity['wcp_code']==5]['commit'].sum()/pavtivity['commit'].sum()\n",
    "            lemr_data.loc[(lemr_data['project'] == projects[i])&(lemr_data['period'] == p), 'pattern5_issue_ratio'] = pavtivity.loc[pavtivity['wcp_code']==5]['issue'].sum()/pavtivity['issue'].sum()\n",
    "            lemr_data.loc[(lemr_data['project'] == projects[i])&(lemr_data['period'] == p), 'pattern5_issue_comment_ratio'] = pavtivity.loc[pavtivity['wcp_code']==5]['issue comment'].sum()/pavtivity['issue comment'].sum()\n",
    "            lemr_data.loc[(lemr_data['project'] == projects[i])&(lemr_data['period'] == p), 'pattern5_pr_comment_ratio'] = pavtivity.loc[pavtivity['wcp_code']==5]['pr comment'].sum()/pavtivity['pr comment'].sum()\n",
    "            lemr_data.loc[(lemr_data['project'] == projects[i])&(lemr_data['period'] == p), 'pattern5_review_ratio'] = pavtivity.loc[pavtivity['wcp_code']==5]['review'].sum()/pavtivity['review'].sum()\n",
    "        except Exception as e:\n",
    "            print(projects[i],p)\n",
    "            print(e)\n",
    "            raise\n",
    "lemr_data.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify correlated variables\n",
    "workload_variables = [\n",
    "\n",
    "       'pattern1_ratio', 'pattern2_ratio', 'pattern3_ratio', 'pattern4_ratio', 'pattern5_ratio', \n",
    "       \n",
    "       'pattern1_commit_ratio', 'pattern1_issue_ratio',\n",
    "       'pattern1_issue_comment_ratio', 'pattern1_pr_comment_ratio',\n",
    "       'pattern1_review_ratio', 'pattern2_commit_ratio',\n",
    "       'pattern2_issue_ratio', 'pattern2_issue_comment_ratio',\n",
    "       'pattern2_pr_comment_ratio', 'pattern2_review_ratio',\n",
    "       'pattern3_commit_ratio', 'pattern3_issue_ratio',\n",
    "       'pattern3_issue_comment_ratio', 'pattern3_pr_comment_ratio',\n",
    "       'pattern3_review_ratio', 'pattern4_commit_ratio',\n",
    "       'pattern4_issue_ratio', 'pattern4_issue_comment_ratio',\n",
    "       'pattern4_pr_comment_ratio', 'pattern4_review_ratio',\n",
    "       'pattern5_commit_ratio', 'pattern5_issue_ratio',\n",
    "       'pattern5_issue_comment_ratio', 'pattern5_pr_comment_ratio',\n",
    "       'pattern5_review_ratio'\n",
    "]\n",
    "correlated_features = ['pattern1_commit_ratio','pattern1_issue_comment_ratio','pattern1_pr_comment_ratio','pattern1_review_ratio',\n",
    "                       'pattern2_pr_comment_ratio','pattern2_commit_ratio','pattern2_issue_ratio',\n",
    "                       'pattern3_commit_ratio','pattern3_issue_comment_ratio','pattern3_pr_comment_ratio',\n",
    "                       'pattern4_ratio','pattern4_issue_comment_ratio','pattern4_pr_comment_ratio',\n",
    "                       'pattern5_ratio','pattern5_commit_ratio','pattern5_issue_comment_ratio','pattern5_pr_comment_ratio']\n",
    "# in old paper\n",
    "# correlated_features = [ 'pattern1_commit_ratio', 'pattern1_issue_comment_ratio','pattern1_pr_comment_ratio',\n",
    "#                        'pattern2_pr_comment_ratio', 'pattern2_commit_ratio', \n",
    "#                        'pattern3_commit_ratio', 'pattern3_issue_comment_ratio', 'pattern3_pr_comment_ratio',\n",
    "#                        'pattern4_ratio', 'pattern4_issue_comment_ratio', 'pattern4_pr_comment_ratio',\n",
    "#                        'pattern5_ratio','pattern5_commit_ratio','pattern5_issue_ratio', 'pattern5_issue_comment_ratio', 'pattern5_pr_comment_ratio'\n",
    "                       \n",
    "#     ]\n",
    "workload_variables_noncor = [item for item in workload_variables if not item in correlated_features]\n",
    "correlation_matrix = lemr_data[workload_variables_noncor].corr(method='spearman')\n",
    "cutoff = (abs(correlation_matrix) > 0.5)\n",
    "correlated = [(col, row) for col in correlation_matrix.columns for row in correlation_matrix.index if cutoff.at[row, col] and not row==col]\n",
    "correlated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### work preference related variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(projects)):\n",
    "    periods = lemr_data.loc[lemr_data['project'] == projects[i]]['period'].unique()\n",
    "    for p in periods:\n",
    "        try:\n",
    "            pavtivity = filtered_developer_period_df.loc[(filtered_developer_period_df['project'] == projects[i])&(filtered_developer_period_df['period'] == p)]\n",
    "            if len(pavtivity) <= 0:\n",
    "                continue\n",
    "            lemr_data.loc[(lemr_data['project'] == projects[i])&(lemr_data['period'] == p), 'binned_entropy'] = pavtivity['binned_entropy'].median()\n",
    "            lemr_data.loc[(lemr_data['project'] == projects[i])&(lemr_data['period'] == p), 'c3(1)'] = pavtivity['c3(1)'].median()\n",
    "            lemr_data.loc[(lemr_data['project'] == projects[i])&(lemr_data['period'] == p), 'c3(2)'] = pavtivity['c3(2)'].median()\n",
    "            lemr_data.loc[(lemr_data['project'] == projects[i])&(lemr_data['period'] == p), 'c3(3)'] = pavtivity['c3(3)'].median()\n",
    "            lemr_data.loc[(lemr_data['project'] == projects[i])&(lemr_data['period'] == p), 'number_cwt_peaks'] = pavtivity['number_cwt_peaks'].median()\n",
    "            lemr_data.loc[(lemr_data['project'] == projects[i])&(lemr_data['period'] == p), 'longest_strike_above_mean'] = pavtivity['longest_strike_above_mean'].median()\n",
    "            lemr_data.loc[(lemr_data['project'] == projects[i])&(lemr_data['period'] == p), 'longest_strike_below_mean'] = pavtivity['longest_strike_below_mean'].median()\n",
    "            lemr_data.loc[(lemr_data['project'] == projects[i])&(lemr_data['period'] == p), 'balance'] = pavtivity['balance'].median()\n",
    "            lemr_data.loc[(lemr_data['project'] == projects[i])&(lemr_data['period'] == p), 'diverse'] = pavtivity['diverse'].median()\n",
    "         \n",
    "            lemr_data.loc[(lemr_data['project'] == projects[i])&(lemr_data['period'] == p), 'commit'] = pavtivity['commit'].mean()\n",
    "            lemr_data.loc[(lemr_data['project'] == projects[i])&(lemr_data['period'] == p), 'issue'] = pavtivity['issue'].mean()\n",
    "            lemr_data.loc[(lemr_data['project'] == projects[i])&(lemr_data['period'] == p), 'issue_comment'] = pavtivity['issue comment'].mean()\n",
    "            lemr_data.loc[(lemr_data['project'] == projects[i])&(lemr_data['period'] == p), 'pr_comment'] = pavtivity['pr comment'].mean()\n",
    "            lemr_data.loc[(lemr_data['project'] == projects[i])&(lemr_data['period'] == p), 'review'] = pavtivity['review'].mean()\n",
    "        except Exception as e:\n",
    "            print(projects[i],p)\n",
    "            print(e)\n",
    "            raise\n",
    "lemr_data.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify correlated variables\n",
    "work_preference_variables = ['period', 'binned_entropy', 'c3(1)', 'c3(2)', 'c3(3)', 'number_cwt_peaks', 'longest_strike_above_mean', \n",
    "                             'longest_strike_below_mean', 'diverse', 'balance', 'commit', 'issue', 'issue_comment', 'pr_comment', \n",
    "                             'review']\n",
    "#correlated_variables = ['binned_entropy', 'longest_strike_below_mean', 'c3(2)', 'c3(3)']\n",
    "correlated_variables =['longest_strike_above_mean', 'longest_strike_below_mean','number_cwt_peaks', 'c3(2)', 'c3(3)']\n",
    "# In old paper\n",
    "# correlated_variables = ['number_cwt_peaks','diverse', 'longest_strike_above_mean', 'c3(1)', 'c3(2)', 'c3(3)', 'pr_comment']\n",
    "work_preference_variables_noncor = [item for item in work_preference_variables if not item in correlated_variables]\n",
    "correlation_matrix = lemr_data[work_preference_variables_noncor].corr(method='spearman')\n",
    "cutoff = (abs(correlation_matrix) > 0.7)\n",
    "correlated = [(col, row) for col in correlation_matrix.columns for row in correlation_matrix.index if cutoff.at[row, col] and not row==col]\n",
    "correlated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemr_data['c3_1'] = lemr_data['c3(1)']\n",
    "work_preference_variables_noncor = [item.replace('(','_').replace(')','') for item in work_preference_variables_noncor]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mixed effect model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install this package if you have not install it, otherwise donot run this cell\n",
    "utils = importr('utils')\n",
    "base = importr('base')\n",
    "utils.chooseCRANmirror(ind=1)\n",
    "utils.install_packages('lme4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arrow(num):\n",
    "    return '↗' if num > 0 else '↘'\n",
    "def signf_code(chisq):\n",
    "    if chisq <= 0.001:\n",
    "        return '***'\n",
    "    elif chisq <= 0.01:\n",
    "        return '**'\n",
    "    elif chisq <= 0.05:\n",
    "        return '*'\n",
    "    elif chisq <= 0.1:\n",
    "        return '.'\n",
    "    else:\n",
    "        return ' '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_max_normalize(column):\n",
    "    col_min = column.min()\n",
    "    col_max = column.max()\n",
    "    if col_min == col_max:\n",
    "        return 0 if col_min == 0 else column / col_min\n",
    "    else:\n",
    "        return (column - col_min) / (col_max - col_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_mixedlm(formula, df, groups, reformula=None):\n",
    "    model = smf.mixedlm(formula, df, groups=groups, re_formula = reformula)\n",
    "    rslt = model.fit(method=[\"lbfgs\"])\n",
    "    pred = rslt.predict()\n",
    "    var_resid = rslt.scale\n",
    "    var_random_effect = float(rslt.cov_re.iloc[0][0])\n",
    "    var_fixed_effect = pred.var()\n",
    "    total_var = var_fixed_effect + var_random_effect + var_resid\n",
    "    marginal_r2 = var_fixed_effect / total_var\n",
    "    conditional_r2 = (var_fixed_effect + var_random_effect) / total_var\n",
    "    return marginal_r2, conditional_r2, rslt "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model: work preference - star"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "formula = \"star ~ \" + ' + '.join(work_preference_variables_noncor)\n",
    "print(formula)\n",
    "# Remove the initial period\n",
    "df = lemr_data.loc[lemr_data['period']!=0]\n",
    "for i in range(len(projects)):\n",
    "    df.loc[df['project']==projects[i],'star'] = min_max_normalize(df.loc[df['project']==projects[i]]['star'])\n",
    "    df.loc[df['project']==projects[i],'fork'] = min_max_normalize(df.loc[df['project']==projects[i]]['fork'])\n",
    "df[['project', 'star','fork']]\n",
    "mr2, cr2, rslt = fit_mixedlm(formula, df, df['project_code'])\n",
    "print('conditional r2', cr2)\n",
    "print('marginal r2', mr2)\n",
    "print(rslt.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rslt_summary = rslt.pvalues.apply(signf_code).to_frame(name='Signif.')\n",
    "rslt_summary['Rel.'] = rslt.params.apply(arrow)\n",
    "rslt_summary.head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model: work preference - fork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "formula = \"fork ~ \" + ' + '.join(work_preference_variables_noncor)\n",
    "print(formula)\n",
    "df = lemr_data.loc[lemr_data['period']!=0]\n",
    "for i in range(len(projects)):\n",
    "    df.loc[df['project']==projects[i],'star'] = min_max_normalize(df.loc[df['project']==projects[i]]['star'])\n",
    "    df.loc[df['project']==projects[i],'fork'] = min_max_normalize(df.loc[df['project']==projects[i]]['fork'])\n",
    "df[['project', 'star','fork']]\n",
    "mr2, cr2, rslt = fit_mixedlm(formula, df, df['project_code'])\n",
    "print('conditional r2', cr2)\n",
    "print('marginal r2', mr2)\n",
    "print(rslt.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rslt_summary = rslt.pvalues.apply(signf_code).to_frame(name='Signif.')\n",
    "rslt_summary['Rel.'] = rslt.params.apply(arrow)\n",
    "rslt_summary.head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model: workload composition - star"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "formula = \"star ~ \" + ' + '.join(workload_variables_noncor)\n",
    "print(formula)\n",
    "df = lemr_data.loc[lemr_data['period']!=0]\n",
    "for i in range(len(projects)):\n",
    "    df.loc[df['project']==projects[i],'star'] = min_max_normalize(df.loc[df['project']==projects[i]]['star'])\n",
    "    df.loc[df['project']==projects[i],'fork'] = min_max_normalize(df.loc[df['project']==projects[i]]['fork'])\n",
    "df[['project', 'star','fork']]\n",
    "mr2, cr2, rslt = fit_mixedlm(formula, df, df['project_code'])\n",
    "print('conditional r2', cr2)\n",
    "print('marginal r2', mr2)\n",
    "print(rslt.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rslt_summary = rslt.pvalues.apply(signf_code).to_frame(name='Signif.')\n",
    "rslt_summary['Rel.'] = rslt.params.apply(arrow)\n",
    "rslt_summary.head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model: workload composition - fork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "formula = \"fork ~ \" + ' + '.join(workload_variables_noncor)\n",
    "print(formula)\n",
    "df = lemr_data.loc[lemr_data['period']!=0]\n",
    "for i in range(len(projects)):\n",
    "    df.loc[df['project']==projects[i],'star'] = min_max_normalize(df.loc[df['project']==projects[i]]['star'])\n",
    "    df.loc[df['project']==projects[i],'fork'] = min_max_normalize(df.loc[df['project']==projects[i]]['fork'])\n",
    "df[['project', 'star','fork']]\n",
    "mr2, cr2, rslt = fit_mixedlm(formula, df, df['project_code'])\n",
    "print('conditional r2', cr2)\n",
    "print('marginal r2', mr2)\n",
    "print(rslt.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rslt_summary = rslt.pvalues.apply(signf_code).to_frame(name='Signif.')\n",
    "rslt_summary['Rel.'] = rslt.params.apply(arrow)\n",
    "rslt_summary.head(50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
