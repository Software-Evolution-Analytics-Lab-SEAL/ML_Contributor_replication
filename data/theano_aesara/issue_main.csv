,Issue#,Title,Owner,Description,Labels,Assignees,Opened time,Closed time,Events,Participants,Number of Comments
0,42,Fix/remove confusing __iter__ implementation in theano.tensor.var.TensorVariable,ghost,"I'm trying to translate [Statistical Rethinking](http://xcelab.net/rm/statistical-rethinking/) from R and RStan to Python and PyMC3.

On page 304, there's a simple logistic regression example. Data being used (publicly available):

```
	dept	applicant.gender	admit	reject	applications	is_male
1	A	male	512	313	825	1
2	A	female	89	19	108	0
3	B	male	353	207	560	1
4	B	female	17	8	25	0
5	C	male	120	205	325	1
6	C	female	202	391	593	0
7	D	male	138	279	417	1
8	D	female	131	244	375	0
9	E	male	53	138	191	1
10	E	female	94	299	393	0
11	F	male	22	351	373	1
12	F	female	24	317	341	0
```

Data is stored in a pandas DataFrame object. When I try and fit the model using:

```
with pm.Model() as m106:
    
    alpha = pm.Normal('alpha', 0, 10)
    beta_m = pm.Normal('beta_m', 0, 10)
    
    lin = alpha + beta_m * data['is_male']
    p = np.exp(lin) / (1 + np.exp(lin))
    
    admit = pm.Binomial('admit', n=data['applications'], p=p, observed=data['admit'])
    
    m106_map = pm.find_MAP()
    m106_traces = pm.sample(1000, start=m106_map)
```

I get the following error (which seems similar to pymc-devs/pymc3#918): 

```
---------------------------------------------------------------------------
KeyError                                  Traceback (most recent call last)
/Users/horatiu/anaconda/lib/python3.5/site-packages/theano/tensor/type.py in dtype_specs(self)
    266                 'complex64': (complex, 'theano_complex64', 'NPY_COMPLEX64')
--> 267             }[self.dtype]
    268         except KeyError:

KeyError: 'object'

During handling of the above exception, another exception occurred:

TypeError                                 Traceback (most recent call last)
/Users/horatiu/anaconda/lib/python3.5/site-packages/theano/tensor/basic.py in constant_or_value(x, rtype, name, ndim, dtype)
    407             rval = rtype(
--> 408                 TensorType(dtype=x_.dtype, broadcastable=bcastable),
    409                 x_.copy(),

/Users/horatiu/anaconda/lib/python3.5/site-packages/theano/tensor/type.py in __init__(self, dtype, broadcastable, name, sparse_grad)
     49         self.broadcastable = tuple(bool(b) for b in broadcastable)
---> 50         self.dtype_specs()  # error checking is done there
     51         self.name = name

/Users/horatiu/anaconda/lib/python3.5/site-packages/theano/tensor/type.py in dtype_specs(self)
    269             raise TypeError(""Unsupported dtype for %s: %s""
--> 270                             % (self.__class__.__name__, self.dtype))
    271 

TypeError: Unsupported dtype for TensorType: object

During handling of the above exception, another exception occurred:

TypeError                                 Traceback (most recent call last)
/Users/horatiu/anaconda/lib/python3.5/site-packages/theano/tensor/basic.py in as_tensor_variable(x, name, ndim)
    201     try:
--> 202         return constant(x, name=name, ndim=ndim)
    203     except TypeError:

/Users/horatiu/anaconda/lib/python3.5/site-packages/theano/tensor/basic.py in constant(x, name, ndim, dtype)
    421     ret = constant_or_value(x, rtype=TensorConstant, name=name, ndim=ndim,
--> 422                             dtype=dtype)
    423 

/Users/horatiu/anaconda/lib/python3.5/site-packages/theano/tensor/basic.py in constant_or_value(x, rtype, name, ndim, dtype)
    416     except Exception:
--> 417         raise TypeError(""Could not convert %s to TensorType"" % x, type(x))
    418 

TypeError: ('Could not convert 1     Elemwise{mul,no_inplace}.0\\n2     Elemwise{mul,no_inplace}.0\\n3     Elemwise{mul,no_inplace}.0\\n4     Elemwise{mul,no_inplace}.0\\n5     Elemwise{mul,no_inplace}.0\\n6     Elemwise{mul,no_inplace}.0\\n7     Elemwise{mul,no_inplace}.0\\n8     Elemwise{mul,no_inplace}.0\\n9     Elemwise{mul,no_inplace}.0\\n10    Elemwise{mul,no_inplace}.0\\n11    Elemwise{mul,no_inplace}.0\\n12    Elemwise{mul,no_inplace}.0\\nName: applications, dtype: object to TensorType', <class 'pandas.core.series.Series'>)

During handling of the above exception, another exception occurred:

AsTensorError                             Traceback (most recent call last)
<ipython-input-144-fb615dfa2e93> in <module>()
      7     p = np.exp(lin) / (1 + np.exp(lin))
      8 
----> 9     admit = pm.Binomial('admit', n=data['applications'], p=p, observed=data['admit'])
     10 
     11     m106_map = pm.find_MAP()

/Users/horatiu/anaconda/lib/python3.5/site-packages/pymc3/distributions/distribution.py in __new__(cls, name, *args, **kwargs)
     24         if isinstance(name, string_types):
     25             data = kwargs.pop('observed', None)
---> 26             dist = cls.dist(*args, **kwargs)
     27             return model.Var(name, dist, data)
     28         elif name is None:

/Users/horatiu/anaconda/lib/python3.5/site-packages/pymc3/distributions/distribution.py in dist(cls, *args, **kwargs)
     37     def dist(cls, *args, **kwargs):
     38         dist = object.__new__(cls)
---> 39         dist.__init__(*args, **kwargs)
     40         return dist
     41 

/Users/horatiu/anaconda/lib/python3.5/site-packages/pymc3/distributions/discrete.py in __init__(self, n, p, *args, **kwargs)
     43         self.n = n
     44         self.p = p
---> 45         self.mode = tt.cast(tt.round(n * p), self.dtype)
     46 
     47     def random(self, point=None, size=None, repeat=None):

/Users/horatiu/anaconda/lib/python3.5/site-packages/theano/tensor/basic.py in round(a, mode)
   2052     """"""round_mode(a) with mode in [half_away_from_zero, half_to_even]""""""
   2053     if mode == ""half_away_from_zero"":
-> 2054         return round_half_away_from_zero(a)
   2055     elif mode == ""half_to_even"":
   2056         return round_half_to_even(a)

/Users/horatiu/anaconda/lib/python3.5/site-packages/theano/gof/op.py in __call__(self, *inputs, **kwargs)
    609         """"""
    610         return_list = kwargs.pop('return_list', False)
--> 611         node = self.make_node(*inputs, **kwargs)
    612 
    613         if config.compute_test_value != 'off':

/Users/horatiu/anaconda/lib/python3.5/site-packages/theano/tensor/elemwise.py in make_node(self, *inputs)
    541         using DimShuffle.
    542         """"""
--> 543         inputs = list(map(as_tensor_variable, inputs))
    544         shadow = self.scalar_op.make_node(
    545             *[get_scalar_type(dtype=i.type.dtype).make_variable()

/Users/horatiu/anaconda/lib/python3.5/site-packages/theano/tensor/basic.py in as_tensor_variable(x, name, ndim)
    206         except Exception:
    207             str_x = repr(x)
--> 208         raise AsTensorError(""Cannot convert %s to TensorType"" % str_x, type(x))
    209 
    210 # this has a different name, because _as_tensor_variable is the

AsTensorError: ('Cannot convert 1     Elemwise{mul,no_inplace}.0\\n2     Elemwise{mul,no_inplace}.0\\n3     Elemwise{mul,no_inplace}.0\\n4     Elemwise{mul,no_inplace}.0\\n5     Elemwise{mul,no_inplace}.0\\n6     Elemwise{mul,no_inplace}.0\\n7     Elemwise{mul,no_inplace}.0\\n8     Elemwise{mul,no_inplace}.0\\n9     Elemwise{mul,no_inplace}.0\\n10    Elemwise{mul,no_inplace}.0\\n11    Elemwise{mul,no_inplace}.0\\n12    Elemwise{mul,no_inplace}.0\\nName: applications, dtype: object to TensorType', <class 'pandas.core.series.Series'>)
```
The error is not very informative, and doesn't seem to point to my code. 
But when using:

```
with pm.Model() as m106:
    
    alpha = pm.Normal('alpha', 0, 10)
    beta_m = pm.Normal('beta_m', 0, 10)
    
    lin = alpha + beta_m * data['is_male']
    p = np.exp(lin) / (1 + np.exp(lin))
    
    admit = pm.Binomial('admit', n=data['applications'].values, p=p, observed=data['admit'])
    
    m106_map = pm.find_MAP()
    m106_traces = pm.sample(1000, start=m106_map)
```

So explicitly passing in the numpy array rather than the pandas Series:

```
admit = pm.Binomial('admit', n=data['applications'].values, p=p, observed=data['admit'])
```

Everything works as expected. I'm just trying to figure out why that is? Is there a reference in the documentation for this behavior? What is the lesson I should take away from this? :)

Using pandas 0.19, numpy 1.11, pymc3.0rc2
",bug,,2016-11-01 11:34:47,2021-05-19 22:51:05,"Horatiu mentioned 2016-11-01 12:30:06,Horatiu subscribed 2016-11-01 12:30:06,springcoil mentioned 2016-11-01 12:32:58,springcoil subscribed 2016-11-01 12:32:58,aloctavodia closed 2018-01-13 14:08:17,twiecki reopened 2020-08-17 08:05:28,brandonwillard mentioned 2020-08-17 08:05:54,brandonwillard subscribed 2020-08-17 08:05:54,brandonwillard mentioned 2020-08-17 13:18:59,brandonwillard subscribed 2020-08-17 13:18:59,brandonwillard mentioned 2020-08-17 14:05:20,brandonwillard subscribed 2020-08-17 14:05:20,canyon289 transferred 2020-09-18 14:14:37,brandonwillard renamed 2020-11-02 04:44:56,brandonwillard labeled 2020-11-02 04:45:09,brandonwillard milestoned 2020-11-02 04:45:12,michaelosthege demilestoned 2020-12-15 19:10:05,michaelosthege milestoned 2020-12-15 19:10:05,brandonwillard closed 2021-05-19 22:51:05,brandonwillard marked_as_duplicate 2021-05-19 23:05:23",tolex3 springcoil Horatiu brandonwillard aloctavodia ghost canyon289 michaelosthege twiecki,18
1,1,Fix links to repo,canyon289,Various links in README's point to the original repo. Change them over to this repo,good first issue help wanted,,2020-02-07 15:19:37,2020-10-31 13:06:37,"brandonwillard labeled 2020-07-08 14:43:14,brandonwillard labeled 2020-09-28 05:17:47,brandonwillard labeled 2020-09-28 05:17:47,brandonwillard milestoned 2020-09-29 16:24:50,eigenfoo closed 2020-10-31 13:06:37",canyon289 eigenfoo brandonwillard,1
5,5,Update automatic formatting and linting,brandonwillard,We should set this up to work like our other projects (e.g. use `black` and similar linting options/checks).,,,2020-03-18 19:41:39,2020-07-08 14:32:04,"brandonwillard connected 2020-07-06 16:20:38,brandonwillard closed 2020-07-08 14:32:04,brandonwillard referenced 2020-07-08 14:32:13",brandonwillard,0
6,6,Refactor Travis Build Script,brandonwillard,"It looks like some/all [Travis build failures](https://travis-ci.org/github/pymc-devs/Theano-PyMC/jobs/664091602#L692) are due to dependency issues involving old versions of Python (e.g. 2.7, 3.4).  We should probably remove all tests involving Python <= 3.5 and cover 3.6-8 instead.

Additionally, the `.travis.yml` is unnecessarily complex/redundant and has confusing, unexplained combinations of versions, settings, and tests (e.g. why are we only sometimes performing some tests with different Python and NumPy versions?).  We should try to simplify it as much as possible.",bug enhancement testing,,2020-03-18 20:57:06,2020-04-10 08:24:50,"brandonwillard labeled 2020-03-18 20:57:06,brandonwillard labeled 2020-03-18 20:57:06,twiecki closed 2020-04-10 08:24:50,brandonwillard labeled 2020-07-08 14:41:16",twiecki brandonwillard,0
10,10,"Use Numba, Cython, and JAX for Python-implemented Ops",brandonwillard,"[Here's](https://gist.github.com/nouiz/5492778) an old example of a Numba-enabled Theano `Op`.  We can most likely do something similar for Cython and JAX, as well.

As an extension to that example, it would be nice to have the compiled function be the `Op.perform` method itself; that way, we could attempt to compile existing `Op`s with little-to-no changes.",enhancement,,2020-06-07 22:19:28,2020-09-27 08:54:42,"brandonwillard labeled 2020-06-07 22:19:41,brandonwillard renamed 2020-06-08 18:06:41,nouiz mentioned 2020-06-08 18:08:46,nouiz subscribed 2020-06-08 18:08:46,dfm mentioned 2020-06-08 18:08:46,dfm subscribed 2020-06-08 18:08:46,nouiz mentioned 2020-07-27 14:57:48,nouiz subscribed 2020-07-27 14:57:48,brandonwillard connected 2020-07-31 22:19:41,twiecki mentioned 2020-08-05 11:36:25,twiecki subscribed 2020-08-05 11:36:25,twiecki mentioned 2020-08-05 22:05:58,twiecki subscribed 2020-08-05 22:05:58,junpenglao mentioned 2020-08-05 22:08:44,junpenglao subscribed 2020-08-05 22:08:44,twiecki closed 2020-09-27 08:54:42,twiecki referenced 2020-09-27 08:54:49",junpenglao nouiz brandonwillard dfm twiecki,23
11,11,"Add testing integration, PyPI release, and conda-forge package",twiecki,"In order to release we should have integration testing (I think theano used travis), as well as an upload to PyPI and a conda-forge package.",testing,brandonwillard,2020-07-03 14:31:14,2020-09-26 17:17:47,"dfm mentioned 2020-07-03 14:34:32,dfm subscribed 2020-07-03 14:34:33,brandonwillard pinned 2020-07-03 15:08:19,brandonwillard assigned 2020-07-06 08:39:28,brandonwillard labeled 2020-07-08 14:40:37,brandonwillard labeled 2020-07-08 14:43:31,twiecki closed 2020-09-26 17:17:47,brandonwillard unpinned 2020-09-28 05:17:56",canyon289 dfm twiecki brandonwillard,7
15,15,Add code coverage,brandonwillard,This project needs code coverage as part of its CI.,testing,,2020-07-07 00:37:00,2020-08-02 18:21:57,"brandonwillard labeled 2020-07-08 14:40:37,brandonwillard connected 2020-07-31 23:13:57,brandonwillard closed 2020-08-02 18:21:57",brandonwillard,0
16,17,"Add test, build, and coverage badges to the README",brandonwillard,"Badges are a simple and nice thing to have, so why not set it up?",enhancement,,2020-07-08 16:16:57,2020-08-02 18:21:57,"brandonwillard labeled 2020-07-08 16:16:57,brandonwillard connected 2020-08-02 18:13:57,brandonwillard closed 2020-08-02 18:21:57",brandonwillard,0
18,19,Add new Scan Op utils,brandonwillard,"Let's consider adding the `Scan` utilities from https://github.com/pymc-devs/symbolic-pymc/pull/114 (e.g. the improved `ScanArgs` class and `convert_outer_out_to_in` function).

These would also help with a much needed general refactoring and simplification of `Scan`.",enhancement,,2020-07-21 18:09:04,2021-07-29 23:18:05,"brandonwillard labeled 2020-07-21 18:09:41,brandonwillard closed 2021-07-29 23:18:05",ricardoV94 brandonwillard,2
22,23,Some tests take a long time,brandonwillard,"There are 920 tests generated and run in `test_sort.py`, so it takes a while to finish.  Are this many tests necessary?",question testing,,2020-08-01 04:14:18,2021-01-02 20:39:53,"brandonwillard renamed 2020-08-01 05:13:54,brandonwillard labeled 2020-08-01 05:14:20,brandonwillard labeled 2020-08-01 05:14:20,rpgoldman referenced 2020-12-19 21:51:46,rpgoldman referenced 2020-12-19 22:10:55,rpgoldman referenced 2020-12-20 14:48:47,rpgoldman referenced 2020-12-20 14:56:58,rpgoldman referenced 2020-12-20 15:03:45,rpgoldman referenced 2020-12-28 20:14:35,michaelosthege referenced 2020-12-31 15:05:42,rpgoldman referenced 2021-01-02 17:51:10,brandonwillard referenced 2021-01-02 19:09:55,brandonwillard closed 2021-01-02 20:39:53",rpgoldman michaelosthege brandonwillard,4
23,24,Finish removing old Python 2.x compatibility code,brandonwillard,"There's still a lot of unnecessary code for Python 2.x compatibility that needs to be removed.  For instances, it seems like the entire `theano.compat` module could be removed.",enhancement good first issue help wanted,eigenfoo,2020-08-02 18:15:35,2020-11-02 01:38:16,"brandonwillard labeled 2020-08-02 19:05:02,brandonwillard labeled 2020-09-28 05:17:11,brandonwillard labeled 2020-09-28 05:17:11,brandonwillard milestoned 2020-09-29 16:24:50,brandonwillard mentioned 2020-10-31 13:45:42,brandonwillard subscribed 2020-10-31 13:45:42,eigenfoo assigned 2020-11-02 01:18:26,brandonwillard closed 2020-11-02 01:38:16",eigenfoo brandonwillard,2
24,25,Adding support of FEniCS models,IvanYashchuk,"I've made a small package for embedding [FEniCS](https://fenicsproject.org/) PDE solvers in Theano.
https://github.com/IvanYashchuk/fenics-pymc3
The current API is just one function `create_fenics_theano_op` which turns a normal Python function, which expects FEniCS inputs and outputs a solution to the problem, into a differentiable Theano Op that can be directly used in a PyMC3 model.
This function can be used as a decorator (with `@create_fenics_theano_op` syntax) as well.
@junpenglao suggested that it could of interest to port it here. What do other people think about this?
I'm not well familiar with the code structure here, what would be a good place for this FEniCS integration functionality?
Also if we get to the point of having FEniCS support here, we should also make it work with [Firedrake](https://firedrakeproject.org/), which has more or less the same Python interface as FEniCS, but a different backend.
",,,2020-08-15 18:19:14,2020-08-15 21:44:40,"junpenglao mentioned 2020-08-15 18:19:14,junpenglao subscribed 2020-08-15 18:19:14,brandonwillard closed 2020-08-15 21:44:40,brandonwillard mentioned 2020-08-17 09:23:20,brandonwillard subscribed 2020-08-17 09:23:20",twiecki junpenglao brandonwillard IvanYashchuk,2
26,29,Beta often diverges when logodds is close to 19.0,bridgeland,"## Description 

This simple model never diverges in NUTS:

```python
import numpy as np
import pymc3 as pm

with pm.Model() as good_model:
    good_beta = pm.Beta('good_beta', alpha=0.25, beta=3.0, shape=40)

with good_model:
    good_trace = pm.sample(draws=600, chains=2, tune=500) 
```

But this very similar model---with alpha and beta swapped---diverges on about half the samples:


```python
import numpy as np
import pymc3 as pm

with pm.Model() as bad_model:
    bad_beta = pm.Beta('bad_beta', alpha=3.0, beta=0.25, shape=40)

with bad_model:
    bad_trace = pm.sample(draws=600, chains=2, tune=500)
```

```python
Sampling 2 chains for 500 tune and 600 draw iterations (1_000 + 1_200 draws total) took 17 seconds.
There were 328 divergences after tuning. Increase `target_accept` or reparameterize.
There were 282 divergences after tuning. Increase `target_accept` or reparameterize.
The acceptance probability does not match the target. It is 0.8848443240050202, but should be close to 0.8. Try to increase the number of tuning steps.
The estimated number of effective samples is smaller than 200 for some parameters.
```

The problem appears to be that in the diverging samples, one element of **bad_beta** jumps to a logodds that is close to (but less than) the value 19.0

```python
def max_bad_beta(warning):
    return np.amax(warning.extra['bad_beta_logodds__'])

def logodds_and_beta(value):
    return value, pm.transforms.logodds.backward(value).eval()[()]

def maxs_from_warnings(trace, limit=50):
    warnings = trace.report._warnings[:limit]
    return [logodds_and_beta(max_bad_beta(warning)) for warning in warnings]

maxs_from_warnings(bad_trace)
[(18.522285926243683, 0.9999999909661387),
 (18.026951801202443, 0.9999999851750137),
 (18.273787751452954, 0.9999999884176999),
 (15.811521213093858, 0.9999998641237577),
 (18.788894703456123, 0.999999993080309),
 (17.840933189508505, 0.9999999821441242),
 (18.99068873589819, 0.9999999943447908),
 (18.650396555754487, 0.9999999920524058),
 (18.886465782831156, 0.9999999937235782),
 (18.614120566380194, 0.9999999917588059),
 (18.748619470023584, 0.9999999927959284),
 (18.96328594127111, 0.9999999941876794),
 (18.43516905323891, 0.9999999901438387),
 (18.7824011307429, 0.9999999930352292),
 (18.61426914065103, 0.9999999917600301),
 (18.61426914065103, 0.9999999917600301),
 (18.959200871340606, 0.9999999941638871),
 (18.310579256420056, 0.9999999888360865),
 (18.875771090438388, 0.9999999936560935),
 (18.419609630862556, 0.9999999899892832),
 (18.743196196992155, 0.9999999927567527), 
...
]
```

What is special about 19? Note that **good_beta** generates logodds close to -19.0 as often as **bad_beta** generates logodds close to 19.0, but **good_beta** never diverges.

## Versions and main components

* PyMC3 Version: 3.9.1
* Theano Version: 1.0.4
* Python Version: 3.8.3
* Operating system: macOS 10.15.16
* How did you install PyMC3: conda
",bug,,2020-08-23 12:58:26,2020-09-16 08:45:23,"twiecki mentioned 2020-08-25 08:10:11,twiecki subscribed 2020-08-25 08:10:11,brandonwillard mentioned 2020-08-25 08:10:11,brandonwillard subscribed 2020-08-25 08:10:11,aseyboldt mentioned 2020-08-25 08:10:11,aseyboldt subscribed 2020-08-25 08:10:11,bridgeland mentioned 2020-08-25 08:10:11,bridgeland subscribed 2020-08-25 08:10:11,AlexAndorra mentioned 2020-08-25 08:39:06,AlexAndorra subscribed 2020-08-25 08:39:06,AlexAndorra transferred 2020-08-25 08:42:06,AlexAndorra labeled 2020-08-25 08:42:41,brandonwillard mentioned 2020-08-25 12:05:17,brandonwillard subscribed 2020-08-25 12:05:18,aseyboldt mentioned 2020-08-25 13:13:23,aseyboldt subscribed 2020-08-25 13:13:23,AlexAndorra mentioned 2020-08-25 13:13:23,AlexAndorra subscribed 2020-08-25 13:13:23,aseyboldt mentioned 2020-08-26 11:21:10,aseyboldt subscribed 2020-08-26 11:21:10,aseyboldt mentioned 2020-08-27 15:00:10,aseyboldt subscribed 2020-08-27 15:00:10,aseyboldt mentioned 2020-08-27 16:35:59,aseyboldt subscribed 2020-08-27 16:35:59,twiecki mentioned 2020-08-28 10:57:52,twiecki subscribed 2020-08-28 10:57:52,brandonwillard mentioned 2020-08-28 10:57:52,brandonwillard subscribed 2020-08-28 10:57:52,aseyboldt mentioned 2020-08-28 10:57:52,aseyboldt subscribed 2020-08-28 10:57:52,AlexAndorra mentioned 2020-08-28 10:57:52,AlexAndorra subscribed 2020-08-28 10:57:53,bridgeland mentioned 2020-08-28 15:30:09,bridgeland subscribed 2020-08-28 15:30:09,brandonwillard mentioned 2020-08-28 20:08:45,brandonwillard subscribed 2020-08-28 20:08:45,brandonwillard connected 2020-08-28 20:35:51,twiecki closed 2020-09-16 08:45:23",brandonwillard aseyboldt bridgeland twiecki AlexAndorra,20
31,33,Add grad of GammaInc,twiecki,https://github.com/pymc-devs/pymc3/pull/2688#issuecomment-687196112,,,2020-09-04 18:22:33,2020-11-07 22:49:56,eigenfoo closed 2020-11-07 22:49:57,eigenfoo twiecki,1
32,34,Fix constant folding optimization error caused by AdvancedBooleanSubtensor,brandonwillard,"Theano's boolean indexing does not match NumPy's.

Consider the following:

```python
import numpy as np

import theano
import theano.tensor as tt


theano.config.cxx = """"


test_array = np.array([[np.inf],
                       [10],
                       [np.inf]])

test_array[~np.isinf(test_array)]
```

```python
[10.]
```

The Theano analog is as follows:

```python
test_array_tt = tt.as_tensor_variable(test_array)

test_array_tt[~tt.isinf(test_array_tt)]
```

```python
/tmp/user/1000/babel-Hkkg0F/python-ZsmiGy in <module>
      1 test_array_tt = tt.as_tensor_variable(test_array)
      2
----> 3 test_array_tt[~tt.isinf(test_array_tt)]

~/projects/code/python/Theano/theano/tensor/var.py in __getitem__(self, args)
    579
    580         if advanced_boolean:
--> 581             return theano.tensor.subtensor.advanced_boolean_subtensor(self, *args)
    582         elif advanced:
    583             if (

~/projects/code/python/Theano/theano/gof/op.py in __call__(self, *inputs, **kwargs)
    611         """"""
    612         return_list = kwargs.pop(""return_list"", False)
--> 613         node = self.make_node(*inputs, **kwargs)
    614
    615         if config.compute_test_value != ""off"":

~/projects/code/python/Theano/theano/tensor/subtensor.py in make_node(self, x, *index)
   2305
   2306         index = tuple(map(as_index_variable, index))
-> 2307         bcast = adv_index_broadcastable_pattern(x, index)
   2308         return gof.Apply(
   2309             self,

~/projects/code/python/Theano/theano/tensor/subtensor.py in adv_index_broadcastable_pattern(a, idx)
   2239     # 2 - True = 1; 2 - False = 2
   2240     fakeshape = [2 - bc for bc in a.broadcastable]
-> 2241     retshape = np.empty(fakeshape)[newidx].shape
   2242     return tuple([dim == 1 for dim in retshape])
   2243

IndexError: boolean index did not match indexed array along dimension 1; dimension is 1 but corresponding boolean dimension is 2


```

The code can be made to work after `tt.squeeze`-ing the boolean index array, but this is a poor solution.
",bug graph rewriting,brandonwillard,2020-09-06 23:13:26,2020-10-14 06:07:11,"brandonwillard labeled 2020-09-06 23:13:33,brandonwillard connected 2020-09-26 00:30:58,brandonwillard disconnected 2020-10-05 04:43:26,brandonwillard renamed 2020-10-14 04:43:07,brandonwillard labeled 2020-10-14 04:43:49,brandonwillard assigned 2020-10-14 04:43:51,brandonwillard connected 2020-10-14 05:10:46,brandonwillard closed 2020-10-14 06:07:11",brandonwillard,1
34,36,Implement NumPy's broadcast_to,brandonwillard,"Theano could really benefit from an implementation of NumPy's [`broadcast_to`](https://numpy.org/doc/stable/reference/generated/numpy.broadcast_to.html)&mdash;one that uses views and doesn't create an entirely new array.  With an implementation of this, it seems like most of the other `broadcast_*` functions could be implemented, as well.",enhancement,brandonwillard,2020-09-06 23:56:59,2020-11-04 06:06:55,"brandonwillard labeled 2020-09-06 23:57:07,brandonwillard labeled 2020-09-29 16:18:09,brandonwillard assigned 2020-11-04 04:40:31,brandonwillard closed 2020-11-04 06:06:55",brandonwillard,0
35,37,Reconsider the graph object model,brandonwillard,"~The constant caching severely complicates graph manipulation/""optimization"" by&mdash;for instance&mdash;requiring that such constants be cloned in order to be used within more than one `FunctionGraph`.  This cloning requirement ultimately extends to the graphs that contain these cached constants and the result is an overly complicated and wasteful process of object cloning and clone-to-original map management (and maps to and from further clones, in some cases).~

~First, how much does this caching actually save?  Was this ever measured or was it just applied?  It's implied that this caching improves the performance of the `MergeOptimizer`; is that the only thing it helps?  Is it simply attempting to save resources by only performing an `is` check instead of a potentially costly descent into subgraphs for further comparisons?  What problem is the `FunctionGraph` trying to avoid by not allowing cached variables, and is that problem only avoided by completely cloning graphs?~

All this touches upon some very fundamental graph object model choices/inconsistencies revolving around object identity and equality.  Theano graph objects do not consistently implement `__eq__` (well, most simply don't), so there are numerous ""local"" work-arounds throughout the codebase to determine object equality to varying degrees&mdash;including the `MergeOptimizer` itself.  ~Constant caching looks like just another example, since it's effectively assigning a singleton-like property to a subset of graph objects.~

This&mdash;and many other issues and complexities&mdash;could be better addressed with a more consistent object model, and one that uses the built-in Python OOP features to implement it.  (Especially since they were arguably designed for exactly these kinds of things.)

For instance, a base implementation of `__eq__` is simple to provide, since&mdash;as usual&mdash;the core graph types can be mapped directly to S-expressions (see the Theano meta graph support in [`symbolic-pymc`](https://github.com/pymc-devs/symbolic-pymc)).  Furthermore, if immutability is added to the model, hash generation and equality comparisons can be cached (and locally, by the the objects themselves).  

Alternatively, unique IDs/names can be used to simplify identity and equality comparisons, with some severe limitations, though.

Anyway, we should start considering changes like this now, because, after they're in place, I believe we could start to make vast improvements in performance and general usability.  Without them, we could end up facing the same problems over and over again in superficially different forms.",enhancement question,,2020-09-11 04:07:14,2021-04-16 19:11:53,"brandonwillard labeled 2020-09-11 04:09:43,brandonwillard labeled 2020-09-11 04:09:43,brandonwillard renamed 2020-10-15 00:37:24,michaelosthege milestoned 2020-12-15 19:16:22,brandonwillard closed 2021-04-16 19:11:53,aesara-devs locked 2021-04-16 19:11:54",michaelosthege aesara-devs brandonwillard,0
39,41,Uninformative Theano Error (Copied from PyMC3),canyon289,"Moving as part of the issue triage. See discussion below
https://github.com/pymc-devs/pymc3/issues/1492",,,2020-09-18 14:14:02,2020-09-18 14:14:40,canyon289 closed 2020-09-18 14:14:40,canyon289,1
41,45,"Refactor sub-packages, modules, and imports",brandonwillard,"The current package/module structure leads to numerous circular dependencies and an overall unwieldy import process.  We need to refactor the imports and module/subpackage layouts entirely. 

For instance, it should be possible to import the core graph types (e.g. `Node`, `Apply`, `Variable`) without importing any `Op`s, `Linkers`, and other compilation-based classes.  Furthermore, there's rampant use of module references like `theano.*` to access objects within subpackages.  This obfuscates the dependency structure when casually reading source file headers and balloons the cross-dependencies when use of `theano.*` alone perform numerous automatic imports.",good first issue help wanted important,,2020-09-26 00:29:05,2021-01-27 20:25:08,"brandonwillard labeled 2020-09-26 00:29:05,brandonwillard labeled 2020-09-26 00:29:05,brandonwillard pinned 2020-09-29 14:35:16,brandonwillard labeled 2020-09-29 14:35:33,brandonwillard milestoned 2020-09-29 16:24:50,twiecki unpinned 2020-11-08 20:37:40,michaelosthege demilestoned 2020-12-15 19:18:51,michaelosthege milestoned 2020-12-15 19:18:51,brandonwillard closed 2021-01-27 20:25:08",michaelosthege twiecki brandonwillard,4
42,46,Implement JAX conversion for CGemv Op,aloctavodia,"This model runs 

```Python
data = np.random.normal(0, 1, 10)
with pm.Model() as model:
    a = pm.Normal(""a"", 0., 1.)
    b = pm.Normal(""b"", 0, 1.)
    y = pm.Normal(""y"", a, b, observed=data)
    trace = pm.sample_smc(100, chains=1, parallel=False)
```

This other model 

```Python
def two_gaussians(x):
    log_like1 = - 0.5 * n * tt.log(2 * np.pi) \\
                - 0.5 * tt.log(dsigma) \\
                - 0.5 * (x - mu1).T.dot(isigma).dot(x - mu1)
    log_like2 = - 0.5 * n * tt.log(2 * np.pi) \\
                - 0.5 * tt.log(dsigma) \\
                - 0.5 * (x - mu2).T.dot(isigma).dot(x - mu2)
    return pm.math.logsumexp([tt.log(w1) + log_like1, tt.log(w2) + log_like2])


with pm.Model() as model:
    X = pm.Uniform('X',
                   shape=n,
                   lower=-2. * np.ones_like(mu1),
                   upper=2. * np.ones_like(mu1),
                   testval=-1. * np.ones_like(mu1))
    llk = pm.Potential('llk', two_gaussians(X))
    trace = pm.sample_smc(2000, chains=1, parallel=False)
```
throws `AttributeError: 'ScalarSigmoid' object has no attribute 'nfunc_spec'`

It seems this could be fixing adding  `nfunc_spec = ('scipy.special.expit', 1, 1)` to the class `ScalarSigmoid` (not sure this is correct) by doing so 
I then get the error `'ScalarSoftplus' object has no attribute 'nfunc_spec'` which I guess should have a similar solution but not sure exactly what. 

If I just add something just to avoid the error I then get `NotImplementedError: No JAX conversion for the given `Op`: CGemv{inplace}` 



",enhancement JAX,brandonwillard,2020-09-26 15:40:36,2020-09-27 05:08:48,"twiecki labeled 2020-09-26 17:21:34,brandonwillard renamed 2020-09-27 00:03:57,brandonwillard assigned 2020-09-27 00:03:58,brandonwillard labeled 2020-09-27 00:04:04,aloctavodia mentioned 2020-09-27 03:53:24,aloctavodia subscribed 2020-09-27 03:53:24,brandonwillard closed 2020-09-27 05:08:48",aloctavodia twiecki brandonwillard,3
44,49,Missing Op: ExtractDiag,twiecki,"Trying to run a GP model:

```python
---------------------------------------------------------------------------
NotImplementedError                       Traceback (most recent call last)
<ipython-input-19-7a2d0b5601fb> in <module>
      1 with model:
----> 2     tr_jax = sample_tfp_nuts(chains=4, target_accept=0.95)

<ipython-input-9-54c111d89074> in sample_tfp_nuts(draws, tune, chains, target_accept, seed, model)
     15 
     16     fgraph = theano.gof.FunctionGraph(model.free_RVs, [model.logpt])
---> 17     fns = theano.sandbox.jaxify.jax_funcify(fgraph)
     18     logp_fn_jax = fns[0]
     19 

~/miniconda3/envs/pymc3theano/lib/python3.8/functools.py in wrapper(*args, **kw)
    873                             '1 positional argument')
    874 
--> 875         return dispatch(args[0].__class__)(*args, **kw)
    876 
    877     funcname = getattr(func, '__name__', 'singledispatch function')

~/projects/Theano-PyMC/theano/sandbox/jaxify.py in jax_funcify_FunctionGraph(fgraph)
    522 
    523     out_nodes = [r.owner for r in fgraph.outputs if r.owner is not None]
--> 524     jax_funcs = [compose_jax_funcs(o, fgraph.inputs) for o in out_nodes]
    525 
    526     return jax_funcs

~/projects/Theano-PyMC/theano/sandbox/jaxify.py in <listcomp>(.0)
    522 
    523     out_nodes = [r.owner for r in fgraph.outputs if r.owner is not None]
--> 524     jax_funcs = [compose_jax_funcs(o, fgraph.inputs) for o in out_nodes]
    525 
    526     return jax_funcs

~/projects/Theano-PyMC/theano/sandbox/jaxify.py in compose_jax_funcs(out_node, fgraph_inputs, memo)
    109             input_f = jax_data_func
    110         else:
--> 111             input_f = compose_jax_funcs(i.owner, fgraph_inputs, memo)
    112 
    113         input_funcs.append(input_f)

~/projects/Theano-PyMC/theano/sandbox/jaxify.py in compose_jax_funcs(out_node, fgraph_inputs, memo)
    109             input_f = jax_data_func
    110         else:
--> 111             input_f = compose_jax_funcs(i.owner, fgraph_inputs, memo)
    112 
    113         input_funcs.append(input_f)

~/projects/Theano-PyMC/theano/sandbox/jaxify.py in compose_jax_funcs(out_node, fgraph_inputs, memo)
    109             input_f = jax_data_func
    110         else:
--> 111             input_f = compose_jax_funcs(i.owner, fgraph_inputs, memo)
    112 
    113         input_funcs.append(input_f)

~/projects/Theano-PyMC/theano/sandbox/jaxify.py in compose_jax_funcs(out_node, fgraph_inputs, memo)
    109             input_f = jax_data_func
    110         else:
--> 111             input_f = compose_jax_funcs(i.owner, fgraph_inputs, memo)
    112 
    113         input_funcs.append(input_f)

~/projects/Theano-PyMC/theano/sandbox/jaxify.py in compose_jax_funcs(out_node, fgraph_inputs, memo)
    109             input_f = jax_data_func
    110         else:
--> 111             input_f = compose_jax_funcs(i.owner, fgraph_inputs, memo)
    112 
    113         input_funcs.append(input_f)

~/projects/Theano-PyMC/theano/sandbox/jaxify.py in compose_jax_funcs(out_node, fgraph_inputs, memo)
    109             input_f = jax_data_func
    110         else:
--> 111             input_f = compose_jax_funcs(i.owner, fgraph_inputs, memo)
    112 
    113         input_funcs.append(input_f)

~/projects/Theano-PyMC/theano/sandbox/jaxify.py in compose_jax_funcs(out_node, fgraph_inputs, memo)
    109             input_f = jax_data_func
    110         else:
--> 111             input_f = compose_jax_funcs(i.owner, fgraph_inputs, memo)
    112 
    113         input_funcs.append(input_f)

~/projects/Theano-PyMC/theano/sandbox/jaxify.py in compose_jax_funcs(out_node, fgraph_inputs, memo)
    109             input_f = jax_data_func
    110         else:
--> 111             input_f = compose_jax_funcs(i.owner, fgraph_inputs, memo)
    112 
    113         input_funcs.append(input_f)

~/projects/Theano-PyMC/theano/sandbox/jaxify.py in compose_jax_funcs(out_node, fgraph_inputs, memo)
    109             input_f = jax_data_func
    110         else:
--> 111             input_f = compose_jax_funcs(i.owner, fgraph_inputs, memo)
    112 
    113         input_funcs.append(input_f)

~/projects/Theano-PyMC/theano/sandbox/jaxify.py in compose_jax_funcs(out_node, fgraph_inputs, memo)
     90         return memo[out_node]
     91 
---> 92     jax_return_func = jax_funcify(out_node.op)
     93 
     94     input_funcs = []

~/miniconda3/envs/pymc3theano/lib/python3.8/functools.py in wrapper(*args, **kw)
    873                             '1 positional argument')
    874 
--> 875         return dispatch(args[0].__class__)(*args, **kw)
    876 
    877     funcname = getattr(func, '__name__', 'singledispatch function')

~/projects/Theano-PyMC/theano/sandbox/jaxify.py in jax_funcify(op)
    136 def jax_funcify(op):
    137     """"""Create a JAX ""perform"" function for a Theano `Variable` and its `Op`.""""""
--> 138     raise NotImplementedError(""No JAX conversion for the given `Op`: {}"".format(op))
    139 
    140 

NotImplementedError: No JAX conversion for the given `Op`: ExtractDiag{offset=0, axis1=0, axis2=1, view=False}
```",JAX,,2020-09-27 11:48:38,2020-10-01 21:22:04,"twiecki labeled 2020-09-27 11:48:38,brandonwillard connected 2020-10-01 21:21:12,brandonwillard closed 2020-10-01 21:22:04",twiecki brandonwillard,1
46,52,Remove theano.dot,brandonwillard,"The `__init__.py`-level `theano.dot` function is awkward and&mdash;as far as I can tell&mdash;completely unnecessary.  What it appears to be doing is supposed to be done in class-level implementations of `__dot__` and `__rdot__` (e.g. an interface/mixin class that checks both objects for the appropriate methods).

This function should be removed and the classes that implement `__dot__` and `__rdot__` (e.g. `theano.tensor.var._tensor_py_operators`) should handle this.",enhancement good first issue,eigenfoo,2020-09-27 22:37:07,2020-11-10 00:03:02,"brandonwillard labeled 2020-09-27 22:37:07,brandonwillard labeled 2020-09-27 22:37:07,brandonwillard milestoned 2020-09-29 16:24:51,eigenfoo assigned 2020-11-02 02:07:09,brandonwillard closed 2020-11-10 00:03:02",eigenfoo twiecki brandonwillard,5
47,53,Move Theano Type instance definitions,brandonwillard,"The modules `theano.tensor.basic` and `theano.scalar.basic` contain numerous type definitions (e.g. `TensorType(""float32"", (True, False))`).  These are more appropriately defined in their respective `type.py` modules (e.g. where `TensorType` itself is defined).  Currently, they're just creating unnecessary dependencies between said types and those very large `basic.py` modules.",enhancement help wanted,,2020-09-27 23:04:16,2021-01-27 20:25:24,"brandonwillard labeled 2020-09-27 23:04:16,brandonwillard labeled 2020-09-27 23:04:16,brandonwillard labeled 2020-09-27 23:04:16,brandonwillard milestoned 2020-09-29 16:24:51,canyon289 assigned 2020-11-07 21:22:17,brandonwillard renamed 2020-11-16 01:16:35,canyon289 unassigned 2020-11-22 03:55:19,brandonwillard unlabeled 2020-11-22 04:10:04,michaelosthege demilestoned 2020-12-15 19:21:57,michaelosthege milestoned 2020-12-15 19:21:57,brandonwillard closed 2021-01-27 20:25:24",canyon289 michaelosthege brandonwillard,4
49,55,Rename scalar and tensor maximum and minimum Ops,brandonwillard,There are distinct `maximum` and `minimum` `Op`s in both `theano.scalar.basic` and `theano.tensor.basic`.  It seems like we could give these different names and avoid the confusion and potential for bugs.,enhancement good first issue help wanted,michaelosthege,2020-09-28 03:55:10,2021-01-03 00:13:04,"brandonwillard labeled 2020-09-28 03:55:10,brandonwillard labeled 2020-09-28 03:55:10,brandonwillard labeled 2020-09-28 03:55:10,brandonwillard renamed 2020-09-28 03:55:22,brandonwillard milestoned 2020-09-29 16:24:51,michaelosthege demilestoned 2020-12-15 19:22:22,michaelosthege milestoned 2020-12-15 19:22:22,michaelosthege assigned 2020-12-15 21:03:58,michaelosthege referenced 2020-12-16 12:42:01,brandonwillard referenced 2021-01-02 21:03:36,brandonwillard referenced 2021-01-02 23:23:19,brandonwillard closed 2021-01-03 00:13:05",michaelosthege brandonwillard,2
50,56,pymc3jax: AttributeError: partially initialized module 'theano' has no attribute 'compile' (most likely due to a circular import),fbartolic,"# Description of your problem
I was very excited to try out the new JAX sampler on my models but I ran into an error when trying to run the example notebook:
https://gist.github.com/twiecki/f0a28dd06620aa86142931c1f10b5434

I created a new conda environment and installed all the dependencies mentioned in the notebook but I get the following error when I run it:

```Python
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
<ipython-input-1-f103cf5131d4> in <module>
      3 import numpy as np
      4 import pandas as pd
----> 5 import pymc3 as pm
      6 import theano
      7 import pymc3.sampling_jax

~/anaconda3/envs/pymc3jax/lib/python3.8/importlib/_bootstrap.py in _find_and_load(name, import_)

~/anaconda3/envs/pymc3jax/lib/python3.8/importlib/_bootstrap.py in _find_and_load_unlocked(name, import_)

~/anaconda3/envs/pymc3jax/lib/python3.8/importlib/_bootstrap.py in _load_unlocked(spec)

~/anaconda3/envs/pymc3jax/lib/python3.8/importlib/_bootstrap.py in _load_backward_compatible(spec)

<frozen zipimport> in load_module(self, fullname)

~/anaconda3/envs/pymc3jax/lib/python3.8/site-packages/pymc3-3.9.3-py3.8.egg/pymc3/__init__.py in <module>
     37 
     38 
---> 39 __set_compiler_flags()
     40 
     41 from .blocking import *

~/anaconda3/envs/pymc3jax/lib/python3.8/site-packages/pymc3-3.9.3-py3.8.egg/pymc3/__init__.py in __set_compiler_flags()
     31 def __set_compiler_flags():
     32     # Workarounds for Theano compiler problems on various platforms
---> 33     import theano
     34 
     35     current = theano.config.gcc.cxxflags

~/anaconda3/envs/pymc3jax/lib/python3.8/site-packages/Theano_PyMC-1.0.5+28.gcbb4f83ee-py3.8.egg/theano/__init__.py in <module>
     92 
     93 
---> 94 from theano.configdefaults import config
     95 from theano.configparser import change_flags
     96 

~/anaconda3/envs/pymc3jax/lib/python3.8/site-packages/Theano_PyMC-1.0.5+28.gcbb4f83ee-py3.8.egg/theano/configdefaults.py in <module>
    620 
    621 
--> 622 AddConfigVar(
    623     ""mode"", ""Default compilation mode"", ConfigParam(""Mode"", filter_mode), in_c_key=False
    624 )

~/anaconda3/envs/pymc3jax/lib/python3.8/site-packages/Theano_PyMC-1.0.5+28.gcbb4f83ee-py3.8.egg/theano/configparser.py in AddConfigVar(name, doc, configparam, root, in_c_key)
    299         # This allow to filter wrong value from the user.
    300         if not callable(configparam.default):
--> 301             configparam.__get__(root, type(root), delete_key=True)
    302         else:
    303             # We do not want to evaluate now the default value

~/anaconda3/envs/pymc3jax/lib/python3.8/site-packages/Theano_PyMC-1.0.5+28.gcbb4f83ee-py3.8.egg/theano/configparser.py in __get__(self, cls, type_, delete_key)
    345                 else:
    346                     val_str = self.default
--> 347             self.__set__(cls, val_str)
    348         # print ""RVAL"", self.val
    349         return self.val

~/anaconda3/envs/pymc3jax/lib/python3.8/site-packages/Theano_PyMC-1.0.5+28.gcbb4f83ee-py3.8.egg/theano/configparser.py in __set__(self, cls, val)
    357         # print ""SETTING PARAM"", self.fullname,(cls), val
    358         if self.filter:
--> 359             self.val = self.filter(val)
    360         else:
    361             self.val = val

~/anaconda3/envs/pymc3jax/lib/python3.8/site-packages/Theano_PyMC-1.0.5+28.gcbb4f83ee-py3.8.egg/theano/configdefaults.py in filter_mode(val)
    605             ""DEBUG_MODE"",
    606         ]
--> 607         or val in theano.compile.mode.predefined_modes
    608     ):
    609         return val

AttributeError: partially initialized module 'theano' has no attribute 'compile' (most likely due to a circular import)
```

## Versions and main components
* PyMC3 Version:  `pymc3jax` branch
* Theano Version:  `Theano-Pymc` master branch
* Python Version: 3.8
* Operating system: Mac OS Catalina
* How did you install PyMC3: manual installation ",bug,,2020-09-28 21:03:51,2020-09-29 16:36:09,"brandonwillard transferred 2020-09-28 21:18:17,brandonwillard labeled 2020-09-28 21:18:31,brandonwillard connected 2020-09-28 21:21:55,fbartolic mentioned 2020-09-28 21:22:31,fbartolic subscribed 2020-09-28 21:22:31,brandonwillard disconnected 2020-09-29 07:05:09,brandonwillard connected 2020-09-29 07:05:09,brandonwillard closed 2020-09-29 16:36:09",brandonwillard fbartolic,2
51,57,Turn theano.compat into a module,brandonwillard,"Currently, `theano.compat` is a sub-package containing only an `__init__.py` file.  The `__init__.py` file should be renamed to `compat.py`, moved into the `theano` directory/package, and the old `compat` directory should be deleted.",enhancement good first issue help wanted,Rashmi-K-A,2020-09-28 21:37:17,2020-10-01 01:25:53,"brandonwillard labeled 2020-09-28 21:37:17,brandonwillard labeled 2020-09-28 21:37:17,brandonwillard labeled 2020-09-28 21:37:18,brandonwillard mentioned 2020-09-29 05:21:43,brandonwillard subscribed 2020-09-29 05:21:43,Rashmi-K-A assigned 2020-09-29 05:54:48,Rashmi-K-A mentioned 2020-09-29 09:24:49,Rashmi-K-A subscribed 2020-09-29 09:24:49,brandonwillard milestoned 2020-09-29 16:24:51,brandonwillard connected 2020-10-01 01:24:38,brandonwillard closed 2020-10-01 01:25:53",Rashmi-K-A twiecki brandonwillard,2
53,60,pymc3jax: AttributeError: 'Identity' object has no attribute 'nfunc_spec',mschmidt87,"## Description of your problem

I ran into another problem with the experimental JAX-based sampler on the `pymc3jax` branch:

I am playing around with a hierarchical model where I am simulating something like a hierarchical Gaussian mixture process, i.e. I have 3 clusters with associated cluster_means and std around them and then I simulate a number of instantiations for each cluster, where each instance has its own mean and a fixed std.

This is the code to simulate the data

```python
import pandas as pd
import numpy as np
import pymc3 as pm
import arviz as az
import pylab as pl

np.random.seed(123)

N_clusters = 3  # Number of clusters
N_samples = [10, 5, 0]  # Number of samples per cluster
total_samples = sum(N_samples)
N = 100 # Number of samples per sample
cluster_means = [1., 2., 3.]  # Mean of means within cluster
cluster_means_std = [0.1, 0.1, 0.1]  # Std of means within cluster
std = 0.5

data = []
true_means = []
for i in range(N_clusters):
    if N_samples[i] > 0:
        means = np.random.normal(loc=cluster_means[i], scale=cluster_means_std[i], size=N_samples[i])
        true_means = np.append(true_means, means)
        data.append(np.array([np.random.normal(means[j], std, N) for j in range(N_samples[i])]))
data = np.vstack(data)
clusters = []
for i in range(N_clusters):
    clusters += [i] * N_samples[i]
data = data.reshape(-1)

c = np.repeat(clusters, N).reshape(-1)
sample = np.repeat(np.arange(sum(N_samples)), N)
```

Using these data, I am creating this model:
```python
with pm.Model() as model:
    a = pm.Normal('a', mu= 0., sigma=3., shape=N_clusters)
    sigma_a = pm.Exponential('sigma_a', 1., shape=N_clusters)
    
    mu_tilde = pm.Normal('mu_t', mu=0., sigma=1., shape=total_samples)
    mu = pm.Deterministic('mu', mu_tilde * sigma_a[clusters] + a[clusters])
    
    sigma = pm.Exponential('sigma', 1., shape=total_samples)
    
    data_obs = pm.Normal('data', mu=mu[sample], sigma=sigma[sample], observed=data)
```

and then I want to use the sampler to do inference:


```python
import pymc3.sampling_jax


with model:
    trace_jax = pm.sampling_jax.sample_numpyro_nuts(
            2000, tune=2000, target_accept=.9)
```

**Please provide the full traceback.**
Running this code, I am getting this error:

```python
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
<ipython-input-6-7bb1ee15df7f> in <module>
      3 
      4 with model:
----> 5     trace_jax = pm.sampling_jax.sample_numpyro_nuts(
      6             2000, tune=2000, target_accept=.9)
      7     idata = trace_jax

/path/to/pymc3/pymc3/sampling_jax.py in sample_numpyro_nuts(draws, tune, chains, target_accept, random_seed, model, progress_bar)
    114 
    115     fgraph = theano.gof.FunctionGraph(model.free_RVs, [model.logpt])
--> 116     fns = theano.sandbox.jaxify.jax_funcify(fgraph)
    117     logp_fn_jax = fns[0]
    118 

/path/to/lib/python3.8/functools.py in wrapper(*args, **kw)
    873                             '1 positional argument')
    874 
--> 875         return dispatch(args[0].__class__)(*args, **kw)
    876 
    877     funcname = getattr(func, '__name__', 'singledispatch function')

/path/to/Theano-PyMC/theano/sandbox/jaxify.py in jax_funcify_FunctionGraph(fgraph)
    523 
    524     out_nodes = [r.owner for r in fgraph.outputs if r.owner is not None]
--> 525     jax_funcs = [compose_jax_funcs(o, fgraph.inputs) for o in out_nodes]
    526 
    527     return jax_funcs

/path/to/Theano-PyMC/theano/sandbox/jaxify.py in <listcomp>(.0)
    523 
    524     out_nodes = [r.owner for r in fgraph.outputs if r.owner is not None]
--> 525     jax_funcs = [compose_jax_funcs(o, fgraph.inputs) for o in out_nodes]
    526 
    527     return jax_funcs

/path/to/Theano-PyMC/theano/sandbox/jaxify.py in compose_jax_funcs(out_node, fgraph_inputs, memo)
    109             input_f = jax_data_func
    110         else:
--> 111             input_f = compose_jax_funcs(i.owner, fgraph_inputs, memo)
    112 
    113         input_funcs.append(input_f)

/path/to/Theano-PyMC/theano/sandbox/jaxify.py in compose_jax_funcs(out_node, fgraph_inputs, memo)
    109             input_f = jax_data_func
    110         else:
--> 111             input_f = compose_jax_funcs(i.owner, fgraph_inputs, memo)
    112 
    113         input_funcs.append(input_f)

/path/to/Theano-PyMC/theano/sandbox/jaxify.py in compose_jax_funcs(out_node, fgraph_inputs, memo)
    109             input_f = jax_data_func
    110         else:
--> 111             input_f = compose_jax_funcs(i.owner, fgraph_inputs, memo)
    112 
    113         input_funcs.append(input_f)

/path/to/Theano-PyMC/theano/sandbox/jaxify.py in compose_jax_funcs(out_node, fgraph_inputs, memo)
    109             input_f = jax_data_func
    110         else:
--> 111             input_f = compose_jax_funcs(i.owner, fgraph_inputs, memo)
    112 
    113         input_funcs.append(input_f)

/path/to/Theano-PyMC/theano/sandbox/jaxify.py in compose_jax_funcs(out_node, fgraph_inputs, memo)
    109             input_f = jax_data_func
    110         else:
--> 111             input_f = compose_jax_funcs(i.owner, fgraph_inputs, memo)
    112 
    113         input_funcs.append(input_f)

/path/to/Theano-PyMC/theano/sandbox/jaxify.py in compose_jax_funcs(out_node, fgraph_inputs, memo)
    109             input_f = jax_data_func
    110         else:
--> 111             input_f = compose_jax_funcs(i.owner, fgraph_inputs, memo)
    112 
    113         input_funcs.append(input_f)

/path/to/Theano-PyMC/theano/sandbox/jaxify.py in compose_jax_funcs(out_node, fgraph_inputs, memo)
    109             input_f = jax_data_func
    110         else:
--> 111             input_f = compose_jax_funcs(i.owner, fgraph_inputs, memo)
    112 
    113         input_funcs.append(input_f)

/path/to/Theano-PyMC/theano/sandbox/jaxify.py in compose_jax_funcs(out_node, fgraph_inputs, memo)
    109             input_f = jax_data_func
    110         else:
--> 111             input_f = compose_jax_funcs(i.owner, fgraph_inputs, memo)
    112 
    113         input_funcs.append(input_f)

/path/to/Theano-PyMC/theano/sandbox/jaxify.py in compose_jax_funcs(out_node, fgraph_inputs, memo)
    109             input_f = jax_data_func
    110         else:
--> 111             input_f = compose_jax_funcs(i.owner, fgraph_inputs, memo)
    112 
    113         input_funcs.append(input_f)

/path/to/Theano-PyMC/theano/sandbox/jaxify.py in compose_jax_funcs(out_node, fgraph_inputs, memo)
    109             input_f = jax_data_func
    110         else:
--> 111             input_f = compose_jax_funcs(i.owner, fgraph_inputs, memo)
    112 
    113         input_funcs.append(input_f)

/path/to/Theano-PyMC/theano/sandbox/jaxify.py in compose_jax_funcs(out_node, fgraph_inputs, memo)
    109             input_f = jax_data_func
    110         else:
--> 111             input_f = compose_jax_funcs(i.owner, fgraph_inputs, memo)
    112 
    113         input_funcs.append(input_f)

/path/to/Theano-PyMC/theano/sandbox/jaxify.py in compose_jax_funcs(out_node, fgraph_inputs, memo)
     90         return memo[out_node]
     91 
---> 92     jax_return_func = jax_funcify(out_node.op)
     93 
     94     input_funcs = []

/path/to/lib/python3.8/functools.py in wrapper(*args, **kw)
    873                             '1 positional argument')
    874 
--> 875         return dispatch(args[0].__class__)(*args, **kw)
    876 
    877     funcname = getattr(func, '__name__', 'singledispatch function')

/path/to/Theano-PyMC/theano/sandbox/jaxify.py in jax_funcify_Elemwise(op)
    320 def jax_funcify_Elemwise(op):
    321     scalar_op = op.scalar_op
--> 322     return jax_funcify(scalar_op)
    323 
    324 

/path/to/lib/python3.8/functools.py in wrapper(*args, **kw)
    873                             '1 positional argument')
    874 
--> 875         return dispatch(args[0].__class__)(*args, **kw)
    876 
    877     funcname = getattr(func, '__name__', 'singledispatch function')

/path/to/Theano-PyMC/theano/sandbox/jaxify.py in jax_funcify_ScalarOp(op)
    142 def jax_funcify_ScalarOp(op):
    143     print(op)
--> 144     func_name = op.nfunc_spec[0]
    145 
    146     if ""."" in func_name:

AttributeError: 'Identity' object has no attribute 'nfunc_spec'

```



## Versions and main components

- PyMC3 Version: checkout of pymc3jax branch
- Theano Version: checkout of Theano-Pymc master branch
- Python Version: 3.8
- Operating system: Mac OS
- How did you install PyMC3: manual installation of the branch
",,,2020-09-29 08:11:18,2020-09-29 14:04:21,"junpenglao closed 2020-09-29 08:52:27,brandonwillard transferred 2020-09-29 14:04:22",mschmidt87 junpenglao brandonwillard,3
56,63,Use GitHub Actions,brandonwillard,"If I'm not mistaken, we can run more jobs in parallel using GitHub Actions and speed up these tests.  For that reason&mdash;and some others (e.g. we can easily perform our deployment automations in Actions, as well), let's switch from Travis to GitHub Actions.",enhancement help wanted important CI,,2020-09-29 16:21:57,2020-10-04 20:17:29,"brandonwillard labeled 2020-09-29 16:21:57,brandonwillard labeled 2020-09-29 16:21:57,brandonwillard labeled 2020-09-29 16:21:58,brandonwillard pinned 2020-09-29 16:22:02,dfm mentioned 2020-09-29 16:44:00,dfm subscribed 2020-09-29 16:44:00,brandonwillard labeled 2020-10-02 00:52:54,brandonwillard closed 2020-10-04 20:17:29,brandonwillard unpinned 2020-10-04 20:19:20",twiecki dfm brandonwillard,2
57,64,ImportError with PyMC3,michaelosthege,"With latest PyMC3 master and Theano-PyMC master, I still get an import error:

![image](https://user-images.githubusercontent.com/5894642/94588791-ae693080-0284-11eb-83b5-f9417c4bc808.png)

Just `import theano` is fine..
Should it be a PyMC3 ticket?",bug,,2020-09-29 16:52:01,2020-09-29 16:54:28,"michaelosthege labeled 2020-09-29 16:52:01,michaelosthege closed 2020-09-29 16:54:28",michaelosthege brandonwillard,2
59,66,Merge RandomVariable Ops,brandonwillard,"The Symbolic PyMC project contains a [base `RandomVariable` `Op`](https://github.com/pymc-devs/symbolic-pymc/blob/master/symbolic_pymc/theano/ops.py#L75) with advanced shape-handling logic and a more optimization-friendly form, as well as implementations for [many common random variable types](https://github.com/pymc-devs/symbolic-pymc/blob/master/symbolic_pymc/theano/random_variables.py).  We should merge/replace the current [`RandomFunction` `Op`](https://github.com/pymc-devs/Theano-PyMC/blob/master/theano/tensor/raw_random.py#L107) with `RandomVariable` (and the related [`rng_mrg`](https://github.com/pymc-devs/Theano-PyMC/blob/master/theano/sandbox/rng_mrg.py implementations).

There's also [`RandomStreams`](https://github.com/pymc-devs/Theano-PyMC/blob/master/theano/tensor/shared_randomstreams.py) class that should be considered/made to work with `RandomVariable`. ",enhancement,brandonwillard,2020-10-01 21:49:38,2020-12-15 21:52:06,"brandonwillard labeled 2020-10-01 21:49:38,brandonwillard labeled 2020-10-02 00:52:25,brandonwillard connected 2020-12-15 21:51:59,brandonwillard closed 2020-12-15 21:52:06,brandonwillard assigned 2020-12-15 21:52:26",michaelosthege twiecki brandonwillard,2
60,67,Replace unification framework,brandonwillard,"We should replace the custom Theano [unification implementation](https://github.com/pymc-devs/Theano-PyMC/blob/master/theano/gof/unify.py) with use of an external&mdash;and more robust&mdash;library, as demonstrated in [Symbolic PyMC](https://github.com/pymc-devs/symbolic-pymc).  Since there are already at least two Python unification libraries  that can be easily made to work with Theano (e.g. [`unification`](https://github.com/mrocklin/unification) and the fork used by Symbolic PyMC, [`logical-unification`](https://github.com/pythological/unification)), there's no point in maintaining an independent implementation here, especially since those capabilities aren't core to this library's offerings.",enhancement graph rewriting,,2020-10-01 22:04:45,2021-12-08 00:15:03,"brandonwillard labeled 2020-10-01 22:04:45,brandonwillard labeled 2020-10-02 00:52:11,michaelosthege milestoned 2020-12-15 19:26:24,brandonwillard closed 2021-12-08 00:15:03",michaelosthege brandonwillard,0
61,69,Implement JAX conversion for Scan Op,brandonwillard,"The Theano [`Scan` `Op`](https://github.com/pymc-devs/Theano-PyMC/blob/1a9b04bfe480b758ddfa54ba49c88bee3bec419c/theano/scan/op.py#L80) currently has an incomplete implementation.  We need to finish that, especially since this `Op` is an important bridge to a lot of valuable JAX functionality and future performance enhancements.

There's already [a (skipped) test for this conversion](https://github.com/pymc-devs/Theano-PyMC/blob/1a9b04bfe480b758ddfa54ba49c88bee3bec419c/tests/sandbox/test_jax.py#L267) that builds a `Scan` `Op` and its equivalent `jax.lax.scan`.  This should provide a good start for anyone wanting to get involved.  Plus, the [current partial implementation](https://github.com/pymc-devs/Theano-PyMC/blob/1a9b04bfe480b758ddfa54ba49c88bee3bec419c/theano/sandbox/jaxify.py#L385) already provides the necessary framework for the conversion.  All that's needed is for someone to finish bridging the inputs and outputs.",enhancement JAX important,junpenglao,2020-10-01 22:42:31,2020-11-20 03:48:36,"brandonwillard labeled 2020-10-01 22:42:31,brandonwillard labeled 2020-10-01 22:42:31,brandonwillard labeled 2020-10-01 22:42:31,brandonwillard pinned 2020-10-02 00:51:29,twiecki unpinned 2020-11-08 20:37:43,junpenglao assigned 2020-11-14 22:52:40,junpenglao mentioned 2020-11-14 23:02:18,junpenglao subscribed 2020-11-14 23:02:18,eigenfoo mentioned 2020-11-14 23:02:19,eigenfoo subscribed 2020-11-14 23:02:19,brandonwillard closed 2020-11-20 03:48:36",junpenglao eigenfoo twiecki brandonwillard,3
62,70,Consider making the `Scan` inner-graph an input to the `Op`,brandonwillard,"`Scan` has a member/field that holds the inner-graph (i.e a graph representing the computation in the `Scan`'s body).  This design choice effectively hides the inner-graph from a standard graph traversal, and&mdash;as a result&mdash;forces the need for special `Scan`-specific logic changes in generic routines in order to work with these `Op`s.

We should consider changing this.",enhancement question graph rewriting refactor request discussion Scan,,2020-10-02 01:04:02,2022-04-11 21:55:52,"brandonwillard labeled 2020-10-02 01:04:02,brandonwillard labeled 2020-10-02 01:04:02,brandonwillard labeled 2021-09-12 22:34:31,brandonwillard labeled 2022-01-21 17:26:38,brandonwillard labeled 2022-01-21 17:26:38,brandonwillard labeled 2022-01-21 17:26:38,brandonwillard renamed 2022-01-21 17:26:54,brandonwillard renamed 2022-01-21 17:27:13,aesara-devs locked 2022-04-11 21:55:52,brandonwillard converted_to_discussion 2022-04-11 21:55:52",aesara-devs brandonwillard,0
64,72,Base core graph classes on attrs or dataclasses,brandonwillard,"In line with #37, we should consider basing the core graph objects (e.g. `theano.gof.graph.[Node, Variable, Apply]`, `theano.gof.op.[PureOp, Op]`, and all their subclasses) on some form of flexible, (relatively) immutable, easily convertible data type.  The packages [`attrs`](https://www.attrs.org/en/stable/overview.html) and the newly built-in [`dataclasses`](https://docs.python.org/3/library/dataclasses.html) provide much&mdash;if not all&mdash;of what we need.

Specifically, both packages provide automatic recursive conversion into `tuple`s and `dict`s and easily configurable defaults for things like equality, ordering, `repr`, hashing, etc.  The `attrs` goes a little further and can perform automatic input validation and conversion, as well as use `__slots__` for a relative performance gain.

More importantly, both provide convenient mutation functions (i.e. [`dataclasses.replace`](https://docs.python.org/3/library/dataclasses.html#dataclasses.replace) and [`attrs.evolve`](https://www.attrs.org/en/stable/api.html#attr.evolve)) that fit well with the existing object identity model and the pervasive requirement for object cloning.  In this setting, graph object mutation could be turned into a considerably easier&mdash;and universal&mdash;process!",enhancement,,2020-10-04 03:55:06,2021-05-19 22:49:41,"brandonwillard labeled 2020-10-04 03:55:06,michaelosthege milestoned 2020-12-15 19:27:46,brandonwillard demilestoned 2021-05-19 22:49:27,brandonwillard closed 2021-05-19 22:49:41,aesara-devs locked 2021-05-19 22:49:42",majidaldo michaelosthege aesara-devs brandonwillard,4
65,73,Support theano.shared in jax_funcify,twiecki,"Currently using a theano.shared leads to a `MissingInputError` , see https://github.com/pymc-devs/pymc3/issues/4142.",enhancement question JAX,,2020-10-04 09:53:15,2020-10-08 01:23:50,"twiecki labeled 2020-10-04 09:53:25,twiecki labeled 2020-10-04 09:53:25,twiecki labeled 2020-10-04 09:53:33,junpenglao mentioned 2020-10-04 16:20:59,junpenglao subscribed 2020-10-04 16:20:59,twiecki mentioned 2020-10-04 16:45:12,twiecki subscribed 2020-10-04 16:45:12,brandonwillard unlabeled 2020-10-04 16:59:26,brandonwillard unlabeled 2020-10-04 16:59:35,brandonwillard labeled 2020-10-04 17:06:12,brandonwillard labeled 2020-10-04 17:06:16,brandonwillard renamed 2020-10-04 18:17:09,brandonwillard closed 2020-10-08 01:23:50",junpenglao twiecki brandonwillard,4
66,74,Implement grad for GammaInc,MrinankSharma,"The Gamma `logcdf` does not have a gradient function implemented. 

Example:

```
self.GI_mean = pm.Normal('GI_mean', gi_mean_mean, gi_mean_sd)
self.GI_sd = pm.Normal('GI_sd', gi_sd_mean, gi_sd_sd)

gi_beta = self.GI_mean / self.GI_sd ** 2
gi_alpha = self.GI_mean ** 2 / self.GI_sd ** 2

GI_dist = pm.Gamma.dist(alpha=gi_alpha, beta=gi_beta)

bins = np.zeros(gi_truncation + 1)
bins[1:] = np.arange(gi_truncation)
bins[2:] += 0.5
bins[:2] += 1e-5

cdf_vals = T.exp(GI_dist.logcdf(bins))
pmf = cdf_vals[1:] - cdf_vals[:-1]
GI_rev = T.repeat(T.reshape(pmf[::-1] / T.sum(pmf), (1, 1, gi_truncation)), 2, axis=0)
```

Trying to sample from this raises an error:

`MethodNotDefined: ('grad', <class 'theano.scalar.basic_scipy.GammaInc'>, 'GammaInc')`


Note: the `logcdf` derivative should be available as a function of the `logpdf`. It would be great to have this implemented. For example, this would mean my use case above (generating a discretised distribution) can be implemented. ",,,2020-10-04 12:02:30,2021-09-19 11:27:18,"brandonwillard renamed 2020-10-12 02:48:16,brandonwillard renamed 2020-10-12 02:50:10,twiecki closed 2021-09-19 11:27:18",MrinankSharma twiecki brandonwillard,3
69,77,Publish Conda and PyPi builds through GitHub Actions,brandonwillard,We need GitHub Actions workflows for publishing Conda and PyPi builds.,help wanted important CI,,2020-10-04 20:26:14,2020-10-08 01:18:51,"brandonwillard labeled 2020-10-04 20:26:14,brandonwillard labeled 2020-10-04 20:26:14,brandonwillard labeled 2020-10-04 20:26:14,brandonwillard pinned 2020-10-04 20:26:20,twiecki mentioned 2020-10-04 21:08:58,twiecki subscribed 2020-10-04 21:08:58,twiecki mentioned 2020-10-05 16:53:04,twiecki subscribed 2020-10-05 16:53:04,twiecki mentioned 2020-10-05 17:07:00,twiecki subscribed 2020-10-05 17:07:00,brandonwillard connected 2020-10-08 01:14:42,brandonwillard closed 2020-10-08 01:18:51,brandonwillard unpinned 2020-10-08 01:19:21",twiecki dfm brandonwillard,6
71,79,No JAX conversion for CumOp,fbartolic,"I'm sure you guys are working on it but just for reference, the following code raises an error due to  missing `CumOp`:
```Python
import numpy as np
import pymc3 as pm
import pymc3.sampling_jax

with pm.Model() as model:
    x = pm.Dirichlet(""x"", a=np.ones(10))

with model:
    trace_jax = pm.sampling_jax.sample_numpyro_nuts(
        500, tune=500
    )
```

Traceback:
```Python
/Users/fb90/anaconda3/envs/pymc3jax/lib/python3.8/site-packages/Theano_PyMC-1.0.5+40.g5de033879-py3.8.egg/theano/gof/cc.py:1029: UserWarning: Your g++ compiler fails to compile OpenMP code. We know this happen with some version of the EPD mingw compiler and LLVM compiler on Mac OS X. We disable openmp everywhere in Theano. To remove this warning set the theano flags `openmp` to False.
  ret += x.c_compile_args()
---------------------------------------------------------------------------
NotImplementedError                       Traceback (most recent call last)
<ipython-input-1-b482cf1c4aad> in <module>
      7 
      8 with model:
----> 9     trace_jax = pm.sampling_jax.sample_numpyro_nuts(
     10         500, tune=500
     11     )

~/anaconda3/envs/pymc3jax/lib/python3.8/site-packages/pymc3-3.9.3-py3.8.egg/pymc3/sampling_jax.py in sample_numpyro_nuts(draws, tune, chains, target_accept, random_seed, model, progress_bar)
    115 
    116     fgraph = theano.gof.FunctionGraph(model.free_RVs, [model.logpt])
--> 117     fns = theano.sandbox.jaxify.jax_funcify(fgraph)
    118     logp_fn_jax = fns[0]
    119 

~/anaconda3/envs/pymc3jax/lib/python3.8/functools.py in wrapper(*args, **kw)
    873                             '1 positional argument')
    874 
--> 875         return dispatch(args[0].__class__)(*args, **kw)
    876 
    877     funcname = getattr(func, '__name__', 'singledispatch function')

~/anaconda3/envs/pymc3jax/lib/python3.8/site-packages/Theano_PyMC-1.0.5+40.g5de033879-py3.8.egg/theano/sandbox/jaxify.py in jax_funcify_FunctionGraph(fgraph)
    550 
    551     out_nodes = [r.owner for r in fgraph.outputs if r.owner is not None]
--> 552     jax_funcs = [compose_jax_funcs(o, fgraph.inputs) for o in out_nodes]
    553 
    554     return jax_funcs

~/anaconda3/envs/pymc3jax/lib/python3.8/site-packages/Theano_PyMC-1.0.5+40.g5de033879-py3.8.egg/theano/sandbox/jaxify.py in <listcomp>(.0)
    550 
    551     out_nodes = [r.owner for r in fgraph.outputs if r.owner is not None]
--> 552     jax_funcs = [compose_jax_funcs(o, fgraph.inputs) for o in out_nodes]
    553 
    554     return jax_funcs

~/anaconda3/envs/pymc3jax/lib/python3.8/site-packages/Theano_PyMC-1.0.5+40.g5de033879-py3.8.egg/theano/sandbox/jaxify.py in compose_jax_funcs(out_node, fgraph_inputs, memo)
    129             input_f = jax_data_func
    130         else:
--> 131             input_f = compose_jax_funcs(i.owner, fgraph_inputs, memo)
    132 
    133         input_funcs.append(input_f)

~/anaconda3/envs/pymc3jax/lib/python3.8/site-packages/Theano_PyMC-1.0.5+40.g5de033879-py3.8.egg/theano/sandbox/jaxify.py in compose_jax_funcs(out_node, fgraph_inputs, memo)
    129             input_f = jax_data_func
    130         else:
--> 131             input_f = compose_jax_funcs(i.owner, fgraph_inputs, memo)
    132 
    133         input_funcs.append(input_f)

~/anaconda3/envs/pymc3jax/lib/python3.8/site-packages/Theano_PyMC-1.0.5+40.g5de033879-py3.8.egg/theano/sandbox/jaxify.py in compose_jax_funcs(out_node, fgraph_inputs, memo)
    129             input_f = jax_data_func
    130         else:
--> 131             input_f = compose_jax_funcs(i.owner, fgraph_inputs, memo)
    132 
    133         input_funcs.append(input_f)

~/anaconda3/envs/pymc3jax/lib/python3.8/site-packages/Theano_PyMC-1.0.5+40.g5de033879-py3.8.egg/theano/sandbox/jaxify.py in compose_jax_funcs(out_node, fgraph_inputs, memo)
    129             input_f = jax_data_func
    130         else:
--> 131             input_f = compose_jax_funcs(i.owner, fgraph_inputs, memo)
    132 
    133         input_funcs.append(input_f)

~/anaconda3/envs/pymc3jax/lib/python3.8/site-packages/Theano_PyMC-1.0.5+40.g5de033879-py3.8.egg/theano/sandbox/jaxify.py in compose_jax_funcs(out_node, fgraph_inputs, memo)
    129             input_f = jax_data_func
    130         else:
--> 131             input_f = compose_jax_funcs(i.owner, fgraph_inputs, memo)
    132 
    133         input_funcs.append(input_f)

~/anaconda3/envs/pymc3jax/lib/python3.8/site-packages/Theano_PyMC-1.0.5+40.g5de033879-py3.8.egg/theano/sandbox/jaxify.py in compose_jax_funcs(out_node, fgraph_inputs, memo)
    129             input_f = jax_data_func
    130         else:
--> 131             input_f = compose_jax_funcs(i.owner, fgraph_inputs, memo)
    132 
    133         input_funcs.append(input_f)

~/anaconda3/envs/pymc3jax/lib/python3.8/site-packages/Theano_PyMC-1.0.5+40.g5de033879-py3.8.egg/theano/sandbox/jaxify.py in compose_jax_funcs(out_node, fgraph_inputs, memo)
    129             input_f = jax_data_func
    130         else:
--> 131             input_f = compose_jax_funcs(i.owner, fgraph_inputs, memo)
    132 
    133         input_funcs.append(input_f)

~/anaconda3/envs/pymc3jax/lib/python3.8/site-packages/Theano_PyMC-1.0.5+40.g5de033879-py3.8.egg/theano/sandbox/jaxify.py in compose_jax_funcs(out_node, fgraph_inputs, memo)
    129             input_f = jax_data_func
    130         else:
--> 131             input_f = compose_jax_funcs(i.owner, fgraph_inputs, memo)
    132 
    133         input_funcs.append(input_f)

~/anaconda3/envs/pymc3jax/lib/python3.8/site-packages/Theano_PyMC-1.0.5+40.g5de033879-py3.8.egg/theano/sandbox/jaxify.py in compose_jax_funcs(out_node, fgraph_inputs, memo)
    129             input_f = jax_data_func
    130         else:
--> 131             input_f = compose_jax_funcs(i.owner, fgraph_inputs, memo)
    132 
    133         input_funcs.append(input_f)

~/anaconda3/envs/pymc3jax/lib/python3.8/site-packages/Theano_PyMC-1.0.5+40.g5de033879-py3.8.egg/theano/sandbox/jaxify.py in compose_jax_funcs(out_node, fgraph_inputs, memo)
    129             input_f = jax_data_func
    130         else:
--> 131             input_f = compose_jax_funcs(i.owner, fgraph_inputs, memo)
    132 
    133         input_funcs.append(input_f)

~/anaconda3/envs/pymc3jax/lib/python3.8/site-packages/Theano_PyMC-1.0.5+40.g5de033879-py3.8.egg/theano/sandbox/jaxify.py in compose_jax_funcs(out_node, fgraph_inputs, memo)
    129             input_f = jax_data_func
    130         else:
--> 131             input_f = compose_jax_funcs(i.owner, fgraph_inputs, memo)
    132 
    133         input_funcs.append(input_f)

~/anaconda3/envs/pymc3jax/lib/python3.8/site-packages/Theano_PyMC-1.0.5+40.g5de033879-py3.8.egg/theano/sandbox/jaxify.py in compose_jax_funcs(out_node, fgraph_inputs, memo)
    129             input_f = jax_data_func
    130         else:
--> 131             input_f = compose_jax_funcs(i.owner, fgraph_inputs, memo)
    132 
    133         input_funcs.append(input_f)

~/anaconda3/envs/pymc3jax/lib/python3.8/site-packages/Theano_PyMC-1.0.5+40.g5de033879-py3.8.egg/theano/sandbox/jaxify.py in compose_jax_funcs(out_node, fgraph_inputs, memo)
    129             input_f = jax_data_func
    130         else:
--> 131             input_f = compose_jax_funcs(i.owner, fgraph_inputs, memo)
    132 
    133         input_funcs.append(input_f)

~/anaconda3/envs/pymc3jax/lib/python3.8/site-packages/Theano_PyMC-1.0.5+40.g5de033879-py3.8.egg/theano/sandbox/jaxify.py in compose_jax_funcs(out_node, fgraph_inputs, memo)
    129             input_f = jax_data_func
    130         else:
--> 131             input_f = compose_jax_funcs(i.owner, fgraph_inputs, memo)
    132 
    133         input_funcs.append(input_f)

~/anaconda3/envs/pymc3jax/lib/python3.8/site-packages/Theano_PyMC-1.0.5+40.g5de033879-py3.8.egg/theano/sandbox/jaxify.py in compose_jax_funcs(out_node, fgraph_inputs, memo)
    129             input_f = jax_data_func
    130         else:
--> 131             input_f = compose_jax_funcs(i.owner, fgraph_inputs, memo)
    132 
    133         input_funcs.append(input_f)

~/anaconda3/envs/pymc3jax/lib/python3.8/site-packages/Theano_PyMC-1.0.5+40.g5de033879-py3.8.egg/theano/sandbox/jaxify.py in compose_jax_funcs(out_node, fgraph_inputs, memo)
    110         return memo[out_node]
    111 
--> 112     jax_return_func = jax_funcify(out_node.op)
    113 
    114     input_funcs = []

~/anaconda3/envs/pymc3jax/lib/python3.8/functools.py in wrapper(*args, **kw)
    873                             '1 positional argument')
    874 
--> 875         return dispatch(args[0].__class__)(*args, **kw)
    876 
    877     funcname = getattr(func, '__name__', 'singledispatch function')

~/anaconda3/envs/pymc3jax/lib/python3.8/site-packages/Theano_PyMC-1.0.5+40.g5de033879-py3.8.egg/theano/sandbox/jaxify.py in jax_funcify(op)
    156 def jax_funcify(op):
    157     """"""Create a JAX ""perform"" function for a Theano `Variable` and its `Op`.""""""
--> 158     raise NotImplementedError(""No JAX conversion for the given `Op`: {}"".format(op))
    159 
    160 

NotImplementedError: No JAX conversion for the given `Op`: CumOp{0, mul}

```
## Versions and main components
* PyMC3 Version:  `pymc3jax` branch
* Theano Version:  `Theano-Pymc` master branch
* Python Version: 3.8
* Operating system: Mac OS Catalina
",JAX,brandonwillard,2020-10-05 17:22:58,2020-10-06 00:26:24,"brandonwillard labeled 2020-10-05 17:32:05,brandonwillard assigned 2020-10-05 18:40:03,brandonwillard connected 2020-10-05 23:11:47,brandonwillard closed 2020-10-06 00:26:24",brandonwillard fbartolic,0
73,81,"Coveralls build error ""No build matching CI build number...""",brandonwillard,"[This build](https://github.com/pymc-devs/Theano-PyMC/pull/80/checks?check_run_id=1212125829) had the following Coveralls error:
```
> Run exoplanet-dev/coveralls-python-action@develop
/usr/bin/docker run --name e5c3570f78f745ad14bcd878b0e92176cdb26_634b8d --label 1e5c35 --workdir /github/workspace --rm -e INPUT_PARALLEL-FINISHED -e INPUT_GITHUB-TOKEN -e INPUT_PARALLEL -e INPUT_DEBUG -e HOME -e GITHUB_JOB -e GITHUB_REF -e GITHUB_SHA -e GITHUB_REPOSITORY -e GITHUB_REPOSITORY_OWNER -e GITHUB_RUN_ID -e GITHUB_RUN_NUMBER -e GITHUB_RETENTION_DAYS -e GITHUB_ACTOR -e GITHUB_WORKFLOW -e GITHUB_HEAD_REF -e GITHUB_BASE_REF -e GITHUB_EVENT_NAME -e GITHUB_SERVER_URL -e GITHUB_API_URL -e GITHUB_GRAPHQL_URL -e GITHUB_WORKSPACE -e GITHUB_ACTION -e GITHUB_EVENT_PATH -e GITHUB_PATH -e GITHUB_ENV -e RUNNER_OS -e RUNNER_TOOL_CACHE -e RUNNER_TEMP -e RUNNER_WORKSPACE -e ACTIONS_RUNTIME_URL -e ACTIONS_RUNTIME_TOKEN -e ACTIONS_CACHE_URL -e GITHUB_ACTIONS=true -e CI=true -v ""/var/run/docker.sock"":""/var/run/docker.sock"" -v ""/home/runner/work/_temp/_github_home"":""/github/home"" -v ""/home/runner/work/_temp/_github_workflow"":""/github/workflow"" -v ""/home/runner/work/_temp/_runner_file_commands"":""/github/file_commands"" -v ""/home/runner/work/Theano-PyMC/Theano-PyMC"":""/github/workspace"" 1e5c35:70f78f745ad14bcd878b0e92176cdb26  ""--github-token"" ""***"" ""--parallel"" ""false"" ""--parallel-finished"" ""true"" ""--debug"" ""false""
{'error': 'No build matching CI build number 2121162aaa4fc23bf7aedc13a4bbc8afaaac23ee-PR-80 found'}
ExitCode.FAILURE
```

@dfm, any ideas?",bug important CI,dfm,2020-10-06 00:24:40,2020-10-06 17:43:18,"brandonwillard labeled 2020-10-06 00:24:40,brandonwillard labeled 2020-10-06 00:24:40,brandonwillard labeled 2020-10-06 00:24:40,dfm mentioned 2020-10-06 00:24:40,dfm subscribed 2020-10-06 00:24:40,dfm assigned 2020-10-06 00:26:42,brandonwillard closed 2020-10-06 17:43:18",dfm brandonwillard,1
76,84,"Theano top-level tests module is not resolved when running coverage on a different project that has a ""tests"" module",michaelosthege,"I wanted to run the tests on our `calibr8` package, but `import Theano` blew up on this line:
https://github.com/pymc-devs/Theano-PyMC/blob/master/theano/__init__.py#L170

I can `import theano` from a jupyter notebook without problems so it must be related to some changes that `coverage` does to the path variables.

Nevertheless the problem did not appear with the original Theano.

Explanation of the traceback: `calibr8.utils` imports `theano`, but `theano` imports `tests` which is a directory in our project. The code in `tests/test.py` imports a class (from the core module) that is defined *after* `import theano`.

```
Traceback (most recent call last):
  File ""tests/tests.py"", line 9, in <module>
    import calibr8
  File ""c:\\users\\osthege\\repos\\calibr8\\calibr8\\calibr8\\__init__.py"", line 1, in <module>
    from . core import  *
  File ""c:\\users\\osthege\\repos\\calibr8\\calibr8\\calibr8\\core.py"", line 10, in <module>
    from . import utils
  File ""c:\\users\\osthege\\repos\\calibr8\\calibr8\\calibr8\\utils.py"", line 10, in <module>
    import theano
  File ""c:\\users\\osthege\\repos\\theano-pymc\\theano\\__init__.py"", line 170, in <module>
    import tests
  File ""C:\\Users\\osthege\\Repos\\calibr8\\calibr8\\tests\\tests.py"", line 30, in <module>
    class _TestModel(calibr8.ErrorModel):
AttributeError: module 'calibr8' has no attribute 'ErrorModel'
```",bug important,,2020-10-06 08:50:50,2020-10-07 06:29:20,"michaelosthege labeled 2020-10-06 08:50:50,brandonwillard labeled 2020-10-07 01:12:24,brandonwillard milestoned 2020-10-07 01:12:29,brandonwillard closed 2020-10-07 06:29:21",michaelosthege brandonwillard,2
79,87,Simplify/combine status checks,brandonwillard,"Our CI workflow sets up a bunch of independent jobs via the matrix settings, and each one of those creates a possible status check under this repo's branch protection settings.  It would be better if we had a single job that could serve as a status check for all the matrix jobs, especially as we add/remove jobs via the matrix settings (e.g. new Python versions).

So far, [this](https://github.community/t/status-check-for-a-matrix-jobs/127354/7?u=brandonwillard) was the best advice I could find; it says to create a new job that simply checks the matrix build status.",enhancement testing CI,,2020-10-06 22:43:21,2020-10-08 19:07:05,"brandonwillard labeled 2020-10-06 22:43:33,brandonwillard labeled 2020-10-06 22:43:33,brandonwillard labeled 2020-10-06 22:43:40,brandonwillard closed 2020-10-08 19:07:05",dfm brandonwillard,2
81,89,"jax.numpy.unique: ""Abstract tracer value encountered where concrete value is expected."" error",brandonwillard,"The most recent `jax`/`jaxlib` update (i.e. 0.2.1) has removed ""symbolic"" inputs support for `jax.numpy.unique`.  We need to&mdash;at the very least&mdash;fix the tests for the corresponding `Op`.",bug JAX important,,2020-10-07 18:29:54,2020-10-08 00:48:27,"brandonwillard labeled 2020-10-07 18:29:54,brandonwillard labeled 2020-10-07 18:29:54,brandonwillard labeled 2020-10-07 18:29:54,brandonwillard connected 2020-10-07 19:13:38,brandonwillard closed 2020-10-08 00:48:27",brandonwillard,3
83,91,Conda-forge build is broken on import test,twiecki,"```
import: 'theano.compile'
import: 'theano.compile.sandbox'
import: 'theano.compile.tests'
Traceback (most recent call last):
  File ""/home/conda/feedstock_root/build_artifacts/theano-pymc_1602124543407/test_tmp/run_test.py"", line 11, in <module>
    import theano.compile.tests
```

from https://dev.azure.com/conda-forge/feedstock-builds/_build/results?buildId=219395&view=logs&j=d0d954b5-f111-5dc4-4d76-03b6c9d0cf7e&t=841356e0-85bb-57d8-dbbc-852e683d1642&l=2075

This happens because the recipe tests all kinds of imports: https://github.com/pymc-devs/Theano-PyMC/blob/master/conda/meta.yaml#L32",bug good first issue important,,2020-10-08 07:16:29,2020-10-11 02:05:47,"twiecki labeled 2020-10-08 07:20:48,brandonwillard labeled 2020-10-08 16:14:53,brandonwillard labeled 2020-10-08 16:14:53,brandonwillard mentioned 2020-10-09 11:09:40,brandonwillard subscribed 2020-10-09 11:09:40,twiecki mentioned 2020-10-09 19:28:20,twiecki subscribed 2020-10-09 19:28:20,ericmjl mentioned 2020-10-09 19:28:20,ericmjl subscribed 2020-10-09 19:28:20,brandonwillard closed 2020-10-11 02:05:47",ericmjl dfm twiecki brandonwillard,9
84,92,Document/Email that announce Theano-PyMC,nouiz,"Hi,

I recall seeing an email/url that annonce Theano-PyMC fork, but I do not find it. Does it really exist? If so, it would be good to link to it from the README in this repo.",question,,2020-10-08 15:38:36,2020-10-08 16:35:57,"brandonwillard labeled 2020-10-08 16:20:14,nouiz closed 2020-10-08 16:35:57",brandonwillard twiecki nouiz,2
85,94,Refactor theano.config,brandonwillard,"I've recently noticed that the config system&mdash;operating mostly from the file [`theano.configdefaults`](https://github.com/pymc-devs/Theano-PyMC/blob/master/theano/configdefaults.py)&mdash;introduces a considerable tangle of import dependencies.  

For instance, the `theano.config` object is created when `theano/__init__.py` is loaded, and from there it attempts to do all sorts of config value validation, and this validation explicitly (and perhaps _unnecessarily_) requires that a large amount of actual class objects be loaded immediately (e.g. everything from `theano.compile.mode`, which itself requires other `theano.config` values to load, as well as all the linkers objects from `theano.gof.*`, etc.)  This triggers further imports for modules with overly strict `theano.config`-dependent top-level conditions (e.g. [here](https://github.com/pymc-devs/Theano-PyMC/blob/master/theano/gof/vm.py#L694) a class definition itself is dependent on the value of `theano.config.cxx`!)

If anything, these module-level dependencies on `theano.config` shouldn't be necessary and/or as pervasive as they currently are.

Furthermore, aren't there well established config-file libraries we can use instead of maintaining our own?

_Originally posted by @brandonwillard in https://github.com/pymc-devs/Theano-PyMC/issue_comments/705671530_",help wanted refactor,michaelosthege,2020-10-08 16:08:46,2020-12-15 22:34:39,"brandonwillard mentioned 2020-10-08 16:08:46,brandonwillard subscribed 2020-10-08 16:08:46,brandonwillard labeled 2020-10-08 16:08:56,brandonwillard labeled 2020-10-08 16:08:56,brandonwillard milestoned 2020-10-08 16:08:59,brandonwillard labeled 2020-10-08 16:14:03,brandonwillard unlabeled 2020-10-08 16:14:03,brandonwillard mentioned 2020-11-19 11:13:50,brandonwillard subscribed 2020-11-19 11:13:50,michaelosthege assigned 2020-11-19 12:09:42,michaelosthege referenced 2020-12-14 18:28:28,michaelosthege demilestoned 2020-12-15 19:31:11,michaelosthege milestoned 2020-12-15 19:31:11,brandonwillard referenced 2020-12-15 22:32:34,brandonwillard closed 2020-12-15 22:34:40",michaelosthege brandonwillard,2
87,97,Finish/fix the implementation of `local_subtensor_make_vector`,brandonwillard,"There are two conditions under which the optimization `local_subtensor_make_vector` will unnecessarily throw exceptions: [here](https://github.com/pymc-devs/Theano-PyMC/blob/master/theano/tensor/opt.py#L2268) and [here](https://github.com/pymc-devs/Theano-PyMC/blob/master/theano/tensor/opt.py#L2283).

The first condition that raises a `TypeError` can be reached when both the graph being indexed and the indices are the result of a `theano.tensor.opt.MakeVector`.  This isn't an unreasonable graph scenario, so it should be fixed (e.g. either cover the case or simply return without raising an exception).

I don't know when/how the second `TypeError` condition can be reached, but it's definitely not clear that a `TypeError` is needed in that scenario&mdash;especially since `pass`ing seems to be fine in other failure cases.",bug enhancement,brandonwillard,2020-10-11 04:31:09,2021-12-05 04:55:28,"brandonwillard labeled 2020-10-11 04:31:09,brandonwillard labeled 2020-10-11 04:31:09,brandonwillard milestoned 2020-10-11 04:31:09,michaelosthege labeled 2020-12-15 19:33:12,brandonwillard referenced 2021-11-25 01:24:55,brandonwillard referenced 2021-11-28 01:52:55,brandonwillard referenced 2021-12-03 02:07:56,brandonwillard renamed 2021-12-03 02:08:19,brandonwillard assigned 2021-12-03 02:08:26,brandonwillard unlabeled 2021-12-03 02:08:32,brandonwillard referenced 2021-12-03 17:08:37,brandonwillard closed 2021-12-05 04:55:28,brandonwillard referenced 2021-12-05 04:55:29",michaelosthege brandonwillard,1
88,98,Fix theano.tensor.basic.as_tensor_variable inconsistencies,brandonwillard,"The ubiquitous `theano.tensor.basic.as_tensor_variable` function has some very troubling inconsistencies, as demonstrated in the following for the same constant input:

```python
import theano
import theano.tensor as tt

from theano.printing import debugprint as tt_dprint


a = tt.as_tensor([1, 2])
```
```python
>>> tt_dprint(a)
TensorConstant{[1 2]} [id A]
```
```python
a = tt.as_tensor([tt.as_tensor(1), tt.as_tensor(2)])
```
```python
>>> tt_dprint(a)
MakeVector{dtype='int8'} [id A] ''   
 |TensorConstant{1} [id B]
 |TensorConstant{2} [id C]
```

In this latter example, the result should be fully ""reduced"" to a `TensorConstant` like the initial example.

Now, the most egregious case involves the use of `ScalarConstant`s:

```python
import theano.scalar as ts


a = tt.as_tensor([ts.constant(1), ts.constant(2)])
```
```python
>>> tt_dprint(a)
Join [id A] ''   
 |TensorConstant{0} [id B]
 |InplaceDimShuffle{x} [id C] ''   
 | |TensorFromScalar [id D] ''   
 |   |Constant{1} [id E]
 |InplaceDimShuffle{x} [id F] ''   
   |TensorFromScalar [id G] ''   
     |Constant{2} [id H]
```

This result should also be the same as the very first example, but&mdash;instead&mdash;it adds an operation that ""undoes"" the `ScalarConstant`s (i.e. the `TensorFromScalar` `Op`s) **and** unnecessarily brings a `Join` into the mix.  At the very least, `MakeVector` could've been used, as it was in the second example.

Here's just one simple example demonstrating why even the partially correct `MakeVector` approach is vastly superior to the confounding `Join`:

```python
>>> tt.get_vector_length(a)
ValueError: length not known: Join [id A] ''   
 |TensorConstant{0} [id B]
 |InplaceDimShuffle{x} [id C] ''   
 | |TensorFromScalar [id D] ''   
 |   |Constant{1} [id E]
 |InplaceDimShuffle{x} [id F] ''   
   |TensorFromScalar [id G] ''   
     |Constant{2} [id H]
```
```python
a = tt.opt.make_vector(*[ts.constant(1), ts.constant(2)])
```
```python
>>> tt.get_vector_length(a)
2
```

Now, one _could_ write even more conditions into `get_vector_length` to handle these particular `Join`s, but that would only further drag the codebase into unnecessary complexity and disorganization.  The `theano.tensor.subtensor` module is a good example of how these irregularities in Theano functions ultimately bring about circuitous and convoluted forms of ad-hoc type checking and conversion steps in nearly every non-trivial `Op`.",bug help wanted important,,2020-10-11 05:34:02,2020-10-12 02:42:07,"brandonwillard labeled 2020-10-11 05:34:02,brandonwillard labeled 2020-10-11 05:34:02,brandonwillard labeled 2020-10-11 05:34:02,brandonwillard milestoned 2020-10-11 05:34:02,brandonwillard closed 2020-10-12 02:42:07",brandonwillard,1
89,99,Remove constant caching,brandonwillard,"Following up on #37&mdash;and after testing some solutions to #98, I believe we should completely remove the constant caching functionality.

This functionality is driven exclusively by the logic [here](https://github.com/pymc-devs/Theano-PyMC/blob/master/theano/tensor/basic.py#L263), which clearly shows how constrained the functionality already is (e.g. it only caches 10,000 `TensorConstant`s within the range -10 to 10).  If this subset of objects takes up so much memory that this relatively small amount of caching makes a real difference, then it's probably the class/object implementations that needs to be fixed.

Also, this caching has to be considered alongside the very extreme requirements imposed by it.  The most germane of which is the excessive graph cloning needed in order to avoid errors during `FunctionGraph` use.  That alone could easily eclipse the effect of this limited constant caching, because a single cached constant requires a complete clone of a graph (including the constant itself).  If we imagine that this constant caching is supposed to help when a user creates an extremely large graph containing numerous small constants, then, as soon as that graph is compiled/optimized, isn't that caching undone because of the cloning?  

This problem is further exacerbated by the `FunctionGraph`'s ""ownership"" of its constituent graph objects (i.e. the `fgraph` property spontaneously added to terms).  Besides this being another confounding step in the optimization process, it can also impose more cloning requirements that undo the effects of constant caching.

Otherwise, as mentioned in #37, it's not entirely clear why `FunctionGraph` cannot use cached constants, so fixing the problem there might involve more unpleasant surprises.

Regarding solutions to #98, through that work it became painfully clear that the constant caching was only initialized through the use of `theano.tensor.basic.constant`, which is called by `theano.tensor.basic.as_tensor_variable`.  These two points of entry are common, but not universal, so often parts of the Theano codebase will inadvertently avoid constant caching issues because they create constants through other means.  

This situation only demonstrates how constant caching makes the current use of `FunctionGraph`&mdash;direct or otherwise&mdash;very inconsistent.  For example, when `FunctionGraph` is used without cloning the entire graph (e.g. many tests and utility functions), then the implicit requirement is that whatever produces these graphs cannot introduce cached constants.  This requirement necessarily propagates up to&mdash;and throughout&mdash;**all** the logic that produced said graphs, which adds a very complex set of entirely unwritten assumptions to numerous parts of the codebase.
",enhancement,,2020-10-11 17:29:29,2020-10-12 02:42:06,"brandonwillard labeled 2020-10-11 17:30:23,brandonwillard closed 2020-10-12 02:42:06",brandonwillard,0
90,100,Remove FunctionGraph constituent ownership,brandonwillard,"`FunctionGraph`s add a `fgraph` property to the graph object added to them (see [here](https://github.com/pymc-devs/Theano-PyMC/blob/master/theano/gof/fg.py#L198), [here](https://github.com/pymc-devs/Theano-PyMC/blob/master/theano/gof/fg.py#L206), [here](https://github.com/pymc-devs/Theano-PyMC/blob/master/theano/gof/fg.py#L225), [here](https://github.com/pymc-devs/Theano-PyMC/blob/master/theano/gof/fg.py#L240), [here](https://github.com/pymc-devs/Theano-PyMC/blob/master/theano/gof/fg.py#L243), [here](https://github.com/pymc-devs/Theano-PyMC/blob/master/theano/gof/fg.py#L243), [here](https://github.com/pymc-devs/Theano-PyMC/blob/master/theano/gof/fg.py#L382), [here](https://github.com/pymc-devs/Theano-PyMC/blob/master/theano/gof/fg.py#L418), [here](https://github.com/pymc-devs/Theano-PyMC/blob/master/theano/gof/fg.py#L445), [here](https://github.com/pymc-devs/Theano-PyMC/blob/master/theano/gof/fg.py#L482), [here](https://github.com/pymc-devs/Theano-PyMC/blob/master/theano/gof/fg.py#L742), and [here](https://github.com/pymc-devs/Theano-PyMC/blob/master/theano/gof/fg.py#L771)).  This property is another questionable feature that complicates the use of `FunctionGraphs` and the optimization process in general (e.g. by requiring more graph cloning and unnecessarily imposing mutability on an otherwise easily immutable object model).

This functionality appears to be used as a means of conveniently obtaining `FunctionGraph`-provided information about given a `Variable`/`Node` via the `FunctionGraph`'s attached ""features"" and to obtain the `FunctionGraph` being optimized within an optimizer (e.g. [`LocalOptGroup`](https://github.com/pymc-devs/Theano-PyMC/blob/master/theano/gof/opt.py#L925)).  These two use-cases seem to be the most common (or possibly the only ones), and their requirements are easily met without the use of this `fgraph` property.

We should remove this feature and refactor the code that currently uses it.  The latter will also help clarify the dependencies/interfaces of such code.",enhancement important graph rewriting refactor,brandonwillard,2020-10-11 17:57:30,2020-11-23 17:54:28,"brandonwillard labeled 2020-10-11 17:57:30,brandonwillard labeled 2020-10-11 17:57:30,brandonwillard labeled 2020-10-11 17:57:30,brandonwillard labeled 2020-10-11 17:57:30,brandonwillard milestoned 2020-10-11 17:57:30,brandonwillard labeled 2020-11-14 23:08:26,brandonwillard assigned 2020-11-14 23:08:31,brandonwillard unlabeled 2020-11-14 23:08:38,brandonwillard closed 2020-11-23 17:54:28",brandonwillard,0
93,103,Cut new release,twiecki,"https://github.com/pymc-devs/Theano-PyMC/pull/102

CC @dfm ",,,2020-10-14 06:40:16,2020-10-14 15:54:27,"dfm mentioned 2020-10-14 06:40:16,dfm subscribed 2020-10-14 06:40:16,brandonwillard closed 2020-10-14 15:54:27",dfm twiecki brandonwillard,1
95,105,Gradient of AdvancedBooleanIncSubtensor is broken,brandonwillard,"When taking the gradient of an `AdvancedBooleanIncSubtensor` `Op`, an exception is immediately thrown:
```python
import theano
import theano.tensor as tt


theano.config.cxx = """"
theano.config.on_opt_error = ""raise""

x = tt.vector()
idx = tt.vector(dtype=""bool"")
y = tt.set_subtensor(x[idx], 0)
```
```python
>>> theano.grad(y.sum(), x)
...
TypeError: AdvancedSubtensor does not support boolean masks for indexing. Use AdvancedBooleanSubtensor instead. 
```

Apparently, the problem is caused by a call to the wrong function: [`advanced_set_subtensor` instead of `advanced_boolean_inc_subtensor`](https://github.com/pymc-devs/Theano-PyMC/commit/a9a0d5aaca6bb902505b32652f7cd28b7da670ef#diff-ce4f5afcdb7dc89728c79c66f55eaed0ec19a0bc4f7684e51010a70f4b3e6b3aR2415).",bug important,brandonwillard,2020-10-14 23:03:42,2020-10-15 01:33:01,"brandonwillard labeled 2020-10-14 23:03:42,brandonwillard labeled 2020-10-14 23:03:42,brandonwillard milestoned 2020-10-14 23:03:42,brandonwillard assigned 2020-10-14 23:03:43,brandonwillard connected 2020-10-15 00:33:23,brandonwillard closed 2020-10-15 01:33:01,brandonwillard referenced 2020-10-15 01:33:06",brandonwillard,0
97,107,Calculate coverage for each test,brandonwillard,"As a means of automating the search for poor/wasteful tests, a tool that tells us the coverage provided by each test would be extremely helpful.  

With such a tool, we could rapidly remove a lot of very time consuming and redundant tests (i.e. #23), or even tell whether or not certain tests are doing what they advertise (i.e. the work in #16).  The latter is extremely important, because, due to the rampant misplaced use of graph optimizations throughout the tests, many tests _aren't_ testing what their names and/or descriptions say they are.

~So far, [this project](https://github.com/chrisbeaumont/smother) looks like it might do the job, so we should give it a try.~  Looks like we're talking about the [""who tests what"" feature](https://nedbatchelder.com/blog/201810/who_tests_what_is_here.html) available in `coverage.py`.

_Originally posted by @brandonwillard in https://github.com/pymc-devs/Theano-PyMC/issue_comments/708809638_",help wanted testing important refactor,,2020-10-15 01:07:11,2020-10-17 20:20:50,"brandonwillard mentioned 2020-10-15 01:07:12,brandonwillard subscribed 2020-10-15 01:07:12,brandonwillard labeled 2020-10-15 01:08:21,brandonwillard labeled 2020-10-15 01:08:21,brandonwillard labeled 2020-10-15 01:08:21,brandonwillard milestoned 2020-10-15 01:08:25,brandonwillard pinned 2020-10-15 01:09:12,brandonwillard labeled 2020-10-17 16:55:24,brandonwillard closed 2020-10-17 20:20:50,brandonwillard unpinned 2020-10-19 17:13:18",brandonwillard,2
99,109,Test values can be set to incompatible values,brandonwillard,"It seems like there are no checks for test values, which can lead to numerous cryptic errors occurring almost anywhere in the Theano codebase.

Here's an example in which a Theano vector is given a matrix test value:
```python
import numpy as np

import theano
import theano.tensor as tt


theano.config.compute_test_value = ""raise""

x = tt.vector(""x"")
x.tag.test_value = np.random.sample((2, 2))
```
```python
>>> x * 2

TypeError: unsupported operand type(s) for *: 'TensorVariable' and 'int'
```
```python
>>> 2 * x

TypeError: For compute_test_value, one input test value does not have the requested type.
```

This issue can also manifest as errors in `ShapeFeature` during compilation.

We should have a check (e.g. implemented through `__setattr__`) in `theano.gof.utils.scratchpad` that makes sure that the assigned value is compatible with the `Variable`'s type.",bug good first issue help wanted,,2020-10-16 17:07:43,2020-10-18 00:53:58,"brandonwillard labeled 2020-10-16 17:07:43,brandonwillard labeled 2020-10-16 17:07:43,brandonwillard labeled 2020-10-16 17:07:43,brandonwillard milestoned 2020-10-16 17:07:43,brandonwillard closed 2020-10-18 00:53:58",brandonwillard,0
105,115,Provide a reasonable matrix_power implementation,brandonwillard,"I just noticed that Theano's [`matrix_power`](https://github.com/pymc-devs/Theano-PyMC/blob/master/theano/tensor/nlinalg.py#L665) function is incredibly naive; it needs to be replaced with&mdash;at the very least&mdash;the standard binary powering algorithm.

To complement that, we should also have an optimization that merges iterated dot products (i.e. the kind produced by the naive `matrix_power`) and replaces them with the new `matrix_power`.  

For that matter, we could easily accomplish both goals by creating a `MatrixPower` `Op` that can replace these iterated dot products and simply call an existing and efficient matrix power implementation (e.g. assuming NumPy's `linalg.matrix_power` uses something like binary powering, that would work).  This would also allow matrix powers with symbolic exponents.",enhancement good first issue help wanted graph rewriting,,2020-10-18 02:28:18,2020-10-22 00:41:57,"brandonwillard labeled 2020-10-18 02:28:18,brandonwillard labeled 2020-10-18 02:28:18,brandonwillard labeled 2020-10-18 02:28:18,brandonwillard labeled 2020-10-18 02:28:18,brandonwillard labeled 2020-10-18 02:28:18,brandonwillard connected 2020-10-19 15:20:25,brandonwillard closed 2020-10-22 00:41:57",brandonwillard,0
108,118,CI should fail on new uncovered lines,brandonwillard,We need to set up our CI/Coveralls to fail on newly introduced uncovered lines&mdash;at the very least!,bug help wanted testing important CI,,2020-10-19 17:12:54,2020-10-24 00:22:39,"brandonwillard labeled 2020-10-19 17:12:54,brandonwillard labeled 2020-10-19 17:12:54,brandonwillard labeled 2020-10-19 17:12:54,brandonwillard labeled 2020-10-19 17:12:54,brandonwillard labeled 2020-10-19 17:12:54,brandonwillard pinned 2020-10-19 17:13:41,dfm mentioned 2020-10-19 17:40:06,dfm subscribed 2020-10-19 17:40:06,dfm mentioned 2020-10-19 18:33:22,dfm subscribed 2020-10-19 18:33:22,twiecki closed 2020-10-20 06:56:14,brandonwillard connected 2020-10-23 21:58:44,brandonwillard reopened 2020-10-23 21:59:02,brandonwillard closed 2020-10-24 00:22:39,eigenfoo unpinned 2020-11-08 01:10:54",eigenfoo OriolAbril brandonwillard dfm twiecki,6
110,121,Implement JAX conversion for Eye Op,krishvishal,"If you have any questions, please ask the [theano-user mailing list](https://groups.google.com/forum/#!forum/theano-users) or [stackoverflow](http://stackoverflow.com/) (using the ""theano"" tag) first.

## Description of your problem or feature request
I'm following the [notebook](https://gist.github.com/junpenglao/fe5e1b451c076cc7b4ca16acdd7d6472) to make use of JAX and Tensorflow probability for inference of pymc3 model. The following is the code for my model.

**Please provide a minimal, self-contained, and reproducible example.**
```python
y = np.random.rand(10,1)
A = np.log(np.random.rand(10,10))
m,n= np.shape(A)
mc = np.shape(y)[1]

with pm.Model() as model:
    q = pm.Uniform('scale', 4, 5)
    lsd = pm.Uniform('lsd', 1, 2, shape=n)
    u = pm.Bound(pm.Laplace, lower=1, upper=10)('u', mu=5.0, b=lsd, shape=n) 
    mu = pmmath.logsumexp(A - u / q, axis=0)
    noise_sigma = pm.Bound(pm.Normal, lower=0)('noise_sigma', mu=0, sigma=1)
    y_rv = pm.MvNormal('y', mu=tt.tile(mu, (mc,1)).T, chol=noise_sigma * tt.eye(mc), shape=(m,mc), observed=y)
```

The model runs without errors but when I jax compile it, I get

```
TypeError: 'list' object is not callable
```

**Please provide the full traceback of any errors.**
https://pastebin.com/yL85rCL1

**Please provide any additional information below.**


## Versions and main components

* Theano version: 1.0.9
* Theano config (`python -c ""import theano; print(theano.config)""`)
* Python version: 3.8.5
* Operating system: Ubuntu 18.04
* How did you install Theano: install from source

@junpenglao ",JAX,canyon289,2020-10-20 08:43:35,2020-11-15 15:41:12,"junpenglao mentioned 2020-10-20 08:43:36,junpenglao subscribed 2020-10-20 08:43:36,brandonwillard renamed 2020-11-07 21:27:11,brandonwillard labeled 2020-11-07 21:27:16,canyon289 assigned 2020-11-09 05:01:35,eigenfoo closed 2020-11-15 15:41:12",junpenglao eigenfoo brandonwillard canyon289 krishvishal,3
115,126,Build documentation in CI,brandonwillard,"We should start automatically building the documentation in CI, then we can keep our https://theano-pymc.readthedocs.io/ site up-to-date and start linking to it.",documentation help wanted CI,,2020-10-22 20:39:40,2021-05-19 22:55:17,"brandonwillard labeled 2020-10-22 20:39:40,brandonwillard labeled 2020-10-22 20:39:40,brandonwillard labeled 2020-10-22 20:39:40,brandonwillard milestoned 2020-10-22 20:39:40,michaelosthege demilestoned 2020-12-15 19:36:16,michaelosthege milestoned 2020-12-15 19:36:16,brandonwillard closed 2021-05-19 22:55:17",michaelosthege dfm brandonwillard,4
116,127,Narrowing error on macOS,dfm,"Compiling ops with fast_run on macOS results in an error because of ""narrowing"". This is a known issue (e.g. https://github.com/pymc-devs/pymc3/issues/3695) that is handled by downstream projects like PyMC3 (https://github.com/pymc-devs/pymc3/pull/3767). I propose that we fix this here by adding the `-Wno-c++11-narrowing` flag to `theano.config.gcc.cxxflags` by default on macOS.

**Please provide a minimal, self-contained, and reproducible example.**
```python
import theano
import theano.tensor as tt
a = tt.dmatrix()
b = tt.dvector()
c = tt.dmatrix()
theano.function([a, b, c], tt.dot(a, b + c))
```

**Please provide the full traceback of any errors.**
```python
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/Users/dforeman/src/pymc-devs/Theano-PyMC/theano/compile/function.py"", line 341, in function
    fn = pfunc(
  File ""/Users/dforeman/src/pymc-devs/Theano-PyMC/theano/compile/pfunc.py"", line 538, in pfunc
    return orig_function(
  File ""/Users/dforeman/src/pymc-devs/Theano-PyMC/theano/compile/function_module.py"", line 1997, in orig_function
    fn = m.create(defaults)
  File ""/Users/dforeman/src/pymc-devs/Theano-PyMC/theano/compile/function_module.py"", line 1850, in create
    _fn, _i, _o = self.linker.make_thunk(
  File ""/Users/dforeman/src/pymc-devs/Theano-PyMC/theano/gof/link.py"", line 738, in make_thunk
    return self.make_all(
  File ""/Users/dforeman/src/pymc-devs/Theano-PyMC/theano/gof/vm.py"", line 1145, in make_all
    node.op.make_thunk(node, storage_map, compute_map, [], impl=impl)
  File ""/Users/dforeman/src/pymc-devs/Theano-PyMC/theano/gof/op.py"", line 980, in make_thunk
    return self.make_c_thunk(node, storage_map, compute_map, no_recycling)
  File ""/Users/dforeman/src/pymc-devs/Theano-PyMC/theano/gof/op.py"", line 877, in make_c_thunk
    outputs = cl.make_thunk(
  File ""/Users/dforeman/src/pymc-devs/Theano-PyMC/theano/gof/cc.py"", line 1275, in make_thunk
    cthunk, module, in_storage, out_storage, error_storage = self.__compile__(
  File ""/Users/dforeman/src/pymc-devs/Theano-PyMC/theano/gof/cc.py"", line 1204, in __compile__
    thunk, module = self.cthunk_factory(
  File ""/Users/dforeman/src/pymc-devs/Theano-PyMC/theano/gof/cc.py"", line 1720, in cthunk_factory
    module = get_module_cache().module_from_key(
  File ""/Users/dforeman/src/pymc-devs/Theano-PyMC/theano/gof/cmodule.py"", line 1229, in module_from_key
    module = lnk.compile_cmodule(location)
  File ""/Users/dforeman/src/pymc-devs/Theano-PyMC/theano/gof/cc.py"", line 1610, in compile_cmodule
    module = c_compiler.compile_str(
  File ""/Users/dforeman/src/pymc-devs/Theano-PyMC/theano/gof/cmodule.py"", line 2567, in compile_str
    raise Exception(
Exception: ('The following error happened while compiling the node', Elemwise{add,no_inplace}(InplaceDimShuffle{x,0}.0, <TensorType(float64, matrix)>), '\\n', ""Compilation failed (return status=1): /Users/dforeman/.theano/compiledir_macOS-10.15.7-x86_64-i386-64bit-i386-3.8.5-64/tmpsatm5hgs/mod.cpp:524:27: error: non-constant-expression cannot be narrowed from type 'npy_intp' (aka 'long') to 'int' in initializer list [-Wc++11-narrowing].     int init_totals[2] = {V5_n0, V3_n1};.                           ^~~~~. /Users/dforeman/.theano/compiledir_macOS-10.15.7-x86_64-i386-64bit-i386-3.8.5-64/tmpsatm5hgs/mod.cpp:524:27: note: insert an explicit cast to silence this issue.     int init_totals[2] = {V5_n0, V3_n1};.                           ^~~~~.                           static_cast<int>( ). /Users/dforeman/.theano/compiledir_macOS-10.15.7-x86_64-i386-64bit-i386-3.8.5-64/tmpsatm5hgs/mod.cpp:524:34: error: non-constant-expression cannot be narrowed from type 'npy_intp' (aka 'long') to 'int' in initializer list [-Wc++11-narrowing].     int init_totals[2] = {V5_n0, V3_n1};.                                  ^~~~~. /Users/dforeman/.theano/compiledir_macOS-10.15.7-x86_64-i386-64bit-i386-3.8.5-64/tmpsatm5hgs/mod.cpp:524:34: note: insert an explicit cast to silence this issue.     int init_totals[2] = {V5_n0, V3_n1};.                                  ^~~~~.                                  static_cast<int>( ). /Users/dforeman/.theano/compiledir_macOS-10.15.7-x86_64-i386-64bit-i386-3.8.5-64/tmpsatm5hgs/mod.cpp:536:12: error: non-constant-expression cannot be narrowed from type 'ssize_t' (aka 'long') to 'int' in initializer list [-Wc++11-narrowing].         0, V3_stride1, .            ^~~~~~~~~~. /Users/dforeman/.theano/compiledir_macOS-10.15.7-x86_64-i386-64bit-i386-3.8.5-64/tmpsatm5hgs/mod.cpp:536:12: note: insert an explicit cast to silence this issue.         0, V3_stride1, .            ^~~~~~~~~~.            static_cast<int>( ). /Users/dforeman/.theano/compiledir_macOS-10.15.7-x86_64-i386-64bit-i386-3.8.5-64/tmpsatm5hgs/mod.cpp:537:1: error: non-constant-expression cannot be narrowed from type 'ssize_t' (aka 'long') to 'int' in initializer list [-Wc++11-narrowing]. V5_stride0, V5_stride1, . ^~~~~~~~~~. /Users/dforeman/.theano/compiledir_macOS-10.15.7-x86_64-i386-64bit-i386-3.8.5-64/tmpsatm5hgs/mod.cpp:537:1: note: insert an explicit cast to silence this issue. V5_stride0, V5_stride1, . ^~~~~~~~~~. static_cast<int>( ). /Users/dforeman/.theano/compiledir_macOS-10.15.7-x86_64-i386-64bit-i386-3.8.5-64/tmpsatm5hgs/mod.cpp:537:13: error: non-constant-expression cannot be narrowed from type 'ssize_t' (aka 'long') to 'int' in initializer list [-Wc++11-narrowing]. V5_stride0, V5_stride1, .             ^~~~~~~~~~. /Users/dforeman/.theano/compiledir_macOS-10.15.7-x86_64-i386-64bit-i386-3.8.5-64/tmpsatm5hgs/mod.cpp:537:13: note: insert an explicit cast to silence this issue. V5_stride0, V5_stride1, .             ^~~~~~~~~~.             static_cast<int>( ). /Users/dforeman/.theano/compiledir_macOS-10.15.7-x86_64-i386-64bit-i386-3.8.5-64/tmpsatm5hgs/mod.cpp:538:1: error: non-constant-expression cannot be narrowed from type 'ssize_t' (aka 'long') to 'int' in initializer list [-Wc++11-narrowing]. V1_stride0, V1_stride1. ^~~~~~~~~~. /Users/dforeman/.theano/compiledir_macOS-10.15.7-x86_64-i386-64bit-i386-3.8.5-64/tmpsatm5hgs/mod.cpp:538:1: note: insert an explicit cast to silence this issue. V1_stride0, V1_stride1. ^~~~~~~~~~. static_cast<int>( ). /Users/dforeman/.theano/compiledir_macOS-10.15.7-x86_64-i386-64bit-i386-3.8.5-64/tmpsatm5hgs/mod.cpp:538:13: error: non-constant-expression cannot be narrowed from type 'ssize_t' (aka 'long') to 'int' in initializer list [-Wc++11-narrowing]. V1_stride0, V1_stride1.             ^~~~~~~~~~. /Users/dforeman/.theano/compiledir_macOS-10.15.7-x86_64-i386-64bit-i386-3.8.5-64/tmpsatm5hgs/mod.cpp:538:13: note: insert an explicit cast to silence this issue. V1_stride0, V1_stride1.             ^~~~~~~~~~.             static_cast<int>( ). 7 errors generated.. "", '[Elemwise{add,no_inplace}(<TensorType(float64, row)>, <TensorType(float64, matrix)>)]')
```

**Please provide any additional information below.**


## Versions and main components

* Theano version: GitHub master
* Theano config (`python -c ""import theano; print(theano.config)""`)
* Python version: 3.8
* Operating system: macos
* How did you install Theano: pip (-e)
",bug enhancement C-backend MacOS,dfm,2020-10-23 12:05:07,2023-02-17 02:49:20,"brandonwillard labeled 2020-10-23 21:49:19,brandonwillard labeled 2020-10-23 21:49:19,brandonwillard labeled 2020-10-23 21:49:19,dfm assigned 2020-10-23 23:03:59,brandonwillard labeled 2020-10-30 16:24:00,dgerlanc referenced 2022-05-13 15:10:30,dgerlanc referenced 2022-05-24 18:01:28,dgerlanc referenced 2022-05-31 20:39:16,dgerlanc referenced 2022-05-31 20:50:19,dgerlanc referenced 2022-06-07 16:15:09,dgerlanc referenced 2022-06-09 18:12:12,dgerlanc referenced 2022-06-14 15:16:21,dgerlanc referenced 2022-10-09 23:24:15,dgerlanc referenced 2022-11-29 02:39:23,dgerlanc referenced 2022-11-30 01:29:51,dgerlanc referenced 2022-12-02 21:03:46,brandonwillard referenced 2023-02-15 22:50:34,dgerlanc referenced 2023-02-17 01:56:10,brandonwillard referenced 2023-02-17 02:04:12,dgerlanc closed 2023-02-17 02:49:21,dgerlanc referenced 2023-02-17 02:49:21",dgerlanc axiezai dfm brandonwillard,11
118,129,Update Cython usage in scan_perform.pyx,brandonwillard,"We're currently versioning the C code produced by Cython for the `Scan` `Op`, and I'm not sure if that's necessary (or the correct approach).  It also requires a manual compilation and patch step when the relevant code is changed, which seems a little less than desirable.  

Since it looks like we're compiling the Cython-generated C code with Theano's GCC compilation framework, I'm guessing that Cython's compilation framework somehow isn't/wasn't compatible.  Even so, this situation raises some important questions:

1. If we can&mdash;apparently&mdash;use Cython to generate C code that's compatible with Theano's C framework, why aren't we doing that more often?  Better yet, why don't we have a `Linker` that does this automatically, and why don't we use something like that to replace all these string-based `c_code` implementations?  

Cython already has a well developed means of both separating and combining Python code from its more implementation specific C details, so using that framework could dramatically improve the unnecessary tangle of high and low-level details brought about by the current Theano C transpilation framework.

2. Why can't we use Cython's compilation framework?

There's really no reason to think that Cython's compilation framework isn't already far ahead of Theano's in many ways (e.g. default parameters and host system detection/configuration for one).  My initial guess is that we would need to use it programatically, at least in a way that's similar to what Theano is currently doing with `gcc` and the like, but I don't see any reason why it wouldn't be just as amenable&mdash;if not considerably more (e.g. if it has a Python interface that helps us avoid writing our own onerous system calls).
",enhancement help wanted question important refactor C-backend Scan,,2020-10-24 22:18:40,2022-08-04 16:45:54,"brandonwillard labeled 2020-10-24 22:18:40,brandonwillard labeled 2020-10-24 22:18:40,brandonwillard labeled 2020-10-24 22:18:40,brandonwillard labeled 2020-10-24 22:18:40,brandonwillard labeled 2020-10-24 22:18:41,brandonwillard labeled 2020-10-24 22:18:41,brandonwillard labeled 2021-09-12 22:34:31,brandonwillard closed 2022-08-04 16:45:54",brandonwillard,1
120,133,Choose operator doesn't accept valid arguments,brandonwillard,"The `Op` `theano.tensor.basic.Choose` doesn't handle valid arguments properly.

For example, in NumPy, the following works as expected:
```python
import numpy as np


a_np = [0, 1, 1, 0]
choices_np = [0.1, 0.2]
```
```python
>>> np.choose(a_np, choices_np)
array([0.1, 0.2, 0.2, 0.1])
```
In Theano, the equivalent usage fails:
```python
import theano.tensor as tt


x_tt, y_tt = tt.scalar(), tt.scalar()

a_tt = tt.as_tensor([0, 1, 1, 0])
choices_tt = [x_tt, y_tt]
```
```python
>>> tt.choose(a_tt, choices_tt)
...
NotImplementedError: We currently didn't implemented that case. To make it work, explicitly add dimensions of size one for dimensions that will be broadcasted
```

It can be made to work when the `list` of scalars, `choices_tt`, is converted to a `TensorVariable` as follows:
```python
>>> z_tt = tt.choose(a_tt, tt.as_tensor(choices_tt))
>>> z_tt.eval({x_tt: 0.1, y_tt: 0.2})
array([0.1, 0.2, 0.2, 0.1])
```

Oddly, the problem isn't present for vectors:
```python
>>> x_tt, y_tt = tt.vector(), tt.vector()
>>> choices_tt = [x_tt, y_tt]
>>> tt.choose(a_tt, choices_tt).eval({x_tt: np.r_[0.1], y_tt: np.r_[0.2]})
array([0.1, 0.2, 0.2, 0.1])
```

The issue seems to be [here](https://github.com/pymc-devs/Theano-PyMC/blob/master/theano/tensor/basic.py#L7142) in `Choose.make_node`, where it decides to use `theano.typed_list.make_list` for some reason.  Instead, it could use `theano.tensor.basic.as_tensor_variable` (or `theano.tensor.opt.MakeVector` directly)&mdash;at least for scalar entries.",bug good first issue help wanted,,2020-10-27 21:07:02,2020-11-18 23:13:30,"brandonwillard labeled 2020-10-27 21:07:02,brandonwillard labeled 2020-10-27 21:10:07,brandonwillard labeled 2020-10-28 00:14:11,canyon289 assigned 2020-11-07 21:18:42,canyon289 unassigned 2020-11-07 21:20:14,brandonwillard closed 2020-11-18 23:13:30",canyon289 brandonwillard,0
121,135,Copy error when computing test values,brandonwillard,"`theano.gof.compute_test_value` raises an exception when `Op` arguments specified in the `destroy_map` do not have a `copy` method.

Here's a MWE:
```python
import numpy as np

import theano
import theano.tensor as tt

from theano.tensor.raw_random import RandomFunction


theano.config.compute_test_value = ""raise""

rng_R = theano.shared(np.random.RandomState())
op = RandomFunction(""normal"", tt.TensorType(dtype=""float64"", broadcastable=[False, False]), inplace=True)
```
```python
>>> op(rng_R, (2, 3), 4.0, 2.0)
...
AttributeError: 'numpy.random.mtrand.RandomState' object has no attribute 'copy'
```",bug important,,2020-10-29 21:16:09,2020-10-29 22:20:57,"brandonwillard labeled 2020-10-29 21:16:09,brandonwillard labeled 2020-10-29 21:16:09,brandonwillard closed 2020-10-29 22:20:57",brandonwillard,0
124,138,Implement JAX conversion for Second Op,LegrandNico,"When using the pymc3jax branch, to sample from my model with:

```python
with model:
  trace = pm.sampling_jax.sample_numpyro_nuts(2000, tune=2000, target_accept=.9)
```
I am getting the following error code:

```python
AttributeError                            Traceback (most recent call last)

<ipython-input-13-5b371bd0c6dd> in <module>()
      1 with model:
----> 2   trace2 = pm.sampling_jax.sample_numpyro_nuts(2000, tune=2000, target_accept=.9)

23 frames

/usr/local/lib/python3.6/dist-packages/pymc3/sampling_jax.py in sample_numpyro_nuts(draws, tune, chains, target_accept, random_seed, model, progress_bar)
    115 
    116     fgraph = theano.gof.FunctionGraph(model.free_RVs, [model.logpt])
--> 117     fns = theano.sandbox.jaxify.jax_funcify(fgraph)
    118     logp_fn_jax = fns[0]
    119 

/usr/lib/python3.6/functools.py in wrapper(*args, **kw)
    805                             '1 positional argument')
    806 
--> 807         return dispatch(args[0].__class__)(*args, **kw)
    808 
    809     funcname = getattr(func, '__name__', 'singledispatch function')

/usr/local/lib/python3.6/dist-packages/theano/sandbox/jaxify.py in jax_funcify_FunctionGraph(fgraph)
    595 
    596     out_nodes = [r.owner for r in fgraph.outputs if r.owner is not None]
--> 597     jax_funcs = [compose_jax_funcs(o, fgraph.inputs) for o in out_nodes]
    598 
    599     return jax_funcs

/usr/local/lib/python3.6/dist-packages/theano/sandbox/jaxify.py in <listcomp>(.0)
    595 
    596     out_nodes = [r.owner for r in fgraph.outputs if r.owner is not None]
--> 597     jax_funcs = [compose_jax_funcs(o, fgraph.inputs) for o in out_nodes]
    598 
    599     return jax_funcs

/usr/local/lib/python3.6/dist-packages/theano/sandbox/jaxify.py in compose_jax_funcs(out_node, fgraph_inputs, memo)
    141             input_f = jax_data_func
    142         else:
--> 143             input_f = compose_jax_funcs(i.owner, fgraph_inputs, memo)
    144 
    145         input_funcs.append(input_f)

/usr/local/lib/python3.6/dist-packages/theano/sandbox/jaxify.py in compose_jax_funcs(out_node, fgraph_inputs, memo)
    141             input_f = jax_data_func
    142         else:
--> 143             input_f = compose_jax_funcs(i.owner, fgraph_inputs, memo)
    144 
    145         input_funcs.append(input_f)

/usr/local/lib/python3.6/dist-packages/theano/sandbox/jaxify.py in compose_jax_funcs(out_node, fgraph_inputs, memo)
    141             input_f = jax_data_func
    142         else:
--> 143             input_f = compose_jax_funcs(i.owner, fgraph_inputs, memo)
    144 
    145         input_funcs.append(input_f)

/usr/local/lib/python3.6/dist-packages/theano/sandbox/jaxify.py in compose_jax_funcs(out_node, fgraph_inputs, memo)
    141             input_f = jax_data_func
    142         else:
--> 143             input_f = compose_jax_funcs(i.owner, fgraph_inputs, memo)
    144 
    145         input_funcs.append(input_f)

/usr/local/lib/python3.6/dist-packages/theano/sandbox/jaxify.py in compose_jax_funcs(out_node, fgraph_inputs, memo)
    141             input_f = jax_data_func
    142         else:
--> 143             input_f = compose_jax_funcs(i.owner, fgraph_inputs, memo)
    144 
    145         input_funcs.append(input_f)

/usr/local/lib/python3.6/dist-packages/theano/sandbox/jaxify.py in compose_jax_funcs(out_node, fgraph_inputs, memo)
    141             input_f = jax_data_func
    142         else:
--> 143             input_f = compose_jax_funcs(i.owner, fgraph_inputs, memo)
    144 
    145         input_funcs.append(input_f)

/usr/local/lib/python3.6/dist-packages/theano/sandbox/jaxify.py in compose_jax_funcs(out_node, fgraph_inputs, memo)
    141             input_f = jax_data_func
    142         else:
--> 143             input_f = compose_jax_funcs(i.owner, fgraph_inputs, memo)
    144 
    145         input_funcs.append(input_f)

/usr/local/lib/python3.6/dist-packages/theano/sandbox/jaxify.py in compose_jax_funcs(out_node, fgraph_inputs, memo)
    141             input_f = jax_data_func
    142         else:
--> 143             input_f = compose_jax_funcs(i.owner, fgraph_inputs, memo)
    144 
    145         input_funcs.append(input_f)

/usr/local/lib/python3.6/dist-packages/theano/sandbox/jaxify.py in compose_jax_funcs(out_node, fgraph_inputs, memo)
    141             input_f = jax_data_func
    142         else:
--> 143             input_f = compose_jax_funcs(i.owner, fgraph_inputs, memo)
    144 
    145         input_funcs.append(input_f)

/usr/local/lib/python3.6/dist-packages/theano/sandbox/jaxify.py in compose_jax_funcs(out_node, fgraph_inputs, memo)
    141             input_f = jax_data_func
    142         else:
--> 143             input_f = compose_jax_funcs(i.owner, fgraph_inputs, memo)
    144 
    145         input_funcs.append(input_f)

/usr/local/lib/python3.6/dist-packages/theano/sandbox/jaxify.py in compose_jax_funcs(out_node, fgraph_inputs, memo)
    141             input_f = jax_data_func
    142         else:
--> 143             input_f = compose_jax_funcs(i.owner, fgraph_inputs, memo)
    144 
    145         input_funcs.append(input_f)

/usr/local/lib/python3.6/dist-packages/theano/sandbox/jaxify.py in compose_jax_funcs(out_node, fgraph_inputs, memo)
    141             input_f = jax_data_func
    142         else:
--> 143             input_f = compose_jax_funcs(i.owner, fgraph_inputs, memo)
    144 
    145         input_funcs.append(input_f)

/usr/local/lib/python3.6/dist-packages/theano/sandbox/jaxify.py in compose_jax_funcs(out_node, fgraph_inputs, memo)
    141             input_f = jax_data_func
    142         else:
--> 143             input_f = compose_jax_funcs(i.owner, fgraph_inputs, memo)
    144 
    145         input_funcs.append(input_f)

/usr/local/lib/python3.6/dist-packages/theano/sandbox/jaxify.py in compose_jax_funcs(out_node, fgraph_inputs, memo)
    141             input_f = jax_data_func
    142         else:
--> 143             input_f = compose_jax_funcs(i.owner, fgraph_inputs, memo)
    144 
    145         input_funcs.append(input_f)

/usr/local/lib/python3.6/dist-packages/theano/sandbox/jaxify.py in compose_jax_funcs(out_node, fgraph_inputs, memo)
    141             input_f = jax_data_func
    142         else:
--> 143             input_f = compose_jax_funcs(i.owner, fgraph_inputs, memo)
    144 
    145         input_funcs.append(input_f)

/usr/local/lib/python3.6/dist-packages/theano/sandbox/jaxify.py in compose_jax_funcs(out_node, fgraph_inputs, memo)
    114         return memo[out_node]
    115 
--> 116     jax_return_func = jax_funcify(out_node.op)
    117 
    118     input_funcs = []

/usr/lib/python3.6/functools.py in wrapper(*args, **kw)
    805                             '1 positional argument')
    806 
--> 807         return dispatch(args[0].__class__)(*args, **kw)
    808 
    809     funcname = getattr(func, '__name__', 'singledispatch function')

/usr/local/lib/python3.6/dist-packages/theano/sandbox/jaxify.py in jax_funcify_Elemwise(op)
    372 def jax_funcify_Elemwise(op):
    373     scalar_op = op.scalar_op
--> 374     return jax_funcify(scalar_op)
    375 
    376 

/usr/lib/python3.6/functools.py in wrapper(*args, **kw)
    805                             '1 positional argument')
    806 
--> 807         return dispatch(args[0].__class__)(*args, **kw)
    808 
    809     funcname = getattr(func, '__name__', 'singledispatch function')

/usr/local/lib/python3.6/dist-packages/theano/sandbox/jaxify.py in jax_funcify_ScalarOp(op)
    181 @jax_funcify.register(ScalarOp)
    182 def jax_funcify_ScalarOp(op):
--> 183     func_name = op.nfunc_spec[0]
    184 
    185     if ""."" in func_name:

AttributeError: 'Second' object has no attribute 'nfunc_spec'
```

The non-jaxified sampler is working correctly with this model. 

You can run this notebook to reproduce the error: https://colab.research.google.com/drive/1gQgCkfQCt-7LRxJfyj2KZTxjvcDRHB2B?usp=sharing",JAX,LegrandNico,2020-10-31 15:16:16,2020-11-19 19:29:31,"junpenglao labeled 2020-10-31 15:38:03,brandonwillard labeled 2020-10-31 17:35:38,brandonwillard unlabeled 2020-11-07 21:06:17,brandonwillard renamed 2020-11-07 21:06:31,LegrandNico assigned 2020-11-14 17:48:55,LegrandNico mentioned 2020-11-15 23:52:39,LegrandNico subscribed 2020-11-15 23:52:39,LegrandNico mentioned 2020-11-18 08:57:25,LegrandNico subscribed 2020-11-18 08:57:25,brandonwillard closed 2020-11-19 19:29:31",LegrandNico junpenglao dfm brandonwillard,5
127,142,Unify sparse and dense tensor class design,brandonwillard,"The sparse tensors should use exactly the same interfaces/types (and much of the same code) as the dense tensors, since we're supposed to be modeling high-level tensors with these interfaces, and **not** their implementation details.  Consider this: in what world is a sparse tensor not a tensor?  (Answer: Theano.)

To start, the mixin `theano.sparse.basic._sparse_py_operators` needs to be replaced with a possibly generalized version of `theano.tensor.basic._tensor_py_operators`.

Next, `theano.sparse.basic.SparseVariable` should extend `theano.tensor.var.TensorVariable` (again, this class could be generalized a little, if necessary).  Same with `theano.sparse.type.SparseType` and `theano.tensor.type.TensorType`.

Overall, we should make the changes necessary to allow the use of fundamental `Op`s with sparse tensor arguments (e.g. `Op`s like `*Subtensor*`).  Speaking of `*Subtensor*`, all the `theano.sparse.basic.GetItem*` and `theano.sparse.basic.Transpose` `Op`s are painfully limited and redundant when `*Subtensor*` is compatible with sparse tensors.  None of those `Op`s actually do anything specific to sparse tensors, aside from some `csr` and `csc` argument conversions that could be done almost anywhere (if at all).

_Originally posted by @brandonwillard in https://github.com/pymc-devs/Theano-PyMC/issues/52#issuecomment-720233156_",help wanted important refactor sparse tensors,,2020-11-02 04:56:30,2022-03-16 01:26:11,"brandonwillard mentioned 2020-11-02 04:56:30,brandonwillard subscribed 2020-11-02 04:56:30,brandonwillard labeled 2020-11-02 04:56:53,brandonwillard labeled 2020-11-02 05:08:05,brandonwillard milestoned 2020-11-02 05:08:15,michaelosthege demilestoned 2020-12-15 19:41:11,michaelosthege milestoned 2020-12-15 19:41:11,brandonwillard labeled 2021-02-02 19:18:24,brandonwillard renamed 2021-05-19 22:54:13,brandonwillard demilestoned 2021-05-19 22:54:44,brandonwillard labeled 2022-02-27 02:00:52,brandonwillard closed 2022-03-16 01:26:11",michaelosthege brandonwillard,0
130,146,Implement JAX conversions for RandomFunction/RandomVariable Ops,ferrine,"Random functions do not work with jax linker

```python
opts = theano.gof.Query(include=[None], exclude=[""cxx_only"", ""BlasOpt""])
jax_mode = theano.compile.mode.Mode(theano.sandbox.jax_linker.JAXLinker(), opts)
r = theano.tensor.shared_randomstreams.RandomStreams()
rfj = theano.function([], r.normal(), mode=jax_mode)
```
raises
```
NotImplementedError: No JAX conversion for the given `Op`: RandomFunction{normal}
```",help wanted JAX important,kc611,2020-11-06 15:55:44,2021-03-12 09:40:45,"ferrine renamed 2020-11-06 15:56:14,twiecki labeled 2020-11-06 16:00:07,brandonwillard renamed 2020-11-07 21:08:28,brandonwillard labeled 2020-12-15 00:33:22,brandonwillard labeled 2020-12-15 00:33:28,ferrine mentioned 2020-12-15 19:42:40,ferrine subscribed 2020-12-15 19:42:40,michaelosthege milestoned 2020-12-15 19:43:08,kc611 mentioned 2021-02-03 21:31:20,kc611 subscribed 2021-02-03 21:31:20,kc611 assigned 2021-02-03 21:31:27,brandonwillard mentioned 2021-02-04 18:28:41,brandonwillard subscribed 2021-02-04 18:28:41,brandonwillard mentioned 2021-02-05 18:07:51,brandonwillard subscribed 2021-02-05 18:07:51,twiecki closed 2021-03-12 09:40:45",kc611 brandonwillard ferrine michaelosthege twiecki,13
131,148,Rename project to Aesara,eigenfoo,"There's a consensus among the PyMC developers to rename this project to Aesara, after the daughter of Theano and Pythagoras (at least, according to a minor tradition).

I propose the following import bindings, which seem aesthetically pleasing. If there's agreement on these bindings, we might as well set an example and use them in our own imports (e.g. our test suite).

```python
import aesara as ae
import aesara.tensor as aet
```

This will be a large and probably disruptive change:

- [ ] The library itself, but particularly `__init__.py` and `setup.py` must be updated
- [ ] Our test suite will need to use the new name
- [ ] Whatever documentation we have should reference Aesara and not Theano
- [x] The package will have to be re-uploaded to PyPI
- [ ] The GitHub repository itself will also need to be renamed",enhancement,brandonwillard,2020-11-07 22:47:32,2021-01-29 05:56:22,"eigenfoo labeled 2020-11-07 22:47:32,eigenfoo labeled 2020-11-07 22:47:32,brandonwillard labeled 2020-11-07 22:49:43,eigenfoo milestoned 2020-11-08 01:19:10,eigenfoo demilestoned 2020-11-08 01:19:14,twiecki mentioned 2020-11-08 17:35:09,twiecki subscribed 2020-11-08 17:35:09,brandonwillard connected 2020-11-09 23:57:34,eigenfoo unlabeled 2020-11-10 03:52:42,twiecki unlabeled 2020-11-15 06:46:59,brandonwillard disconnected 2020-11-15 19:18:23,michaelosthege milestoned 2020-12-15 19:43:14,brandonwillard assigned 2021-01-27 20:29:31,brandonwillard closed 2021-01-29 05:56:22",eigenfoo brandonwillard canyon289 michaelosthege twiecki,2
140,160,Method not defined `I0e.c_code`,Sayam753,"If you have any questions, please ask the [theano-user mailing list](https://groups.google.com/forum/#!forum/theano-users) or [stackoverflow](http://stackoverflow.com/) (using the ""theano"" tag) first.

I was on the way to merge `fix_mv_random` branch from PR [pymc-devs/pymc3#4207](https://github.com/pymc-devs/pymc3/pull/4207) with @michaelosthege's PR [pymc-devs/pymc3#4214](https://github.com/pymc-devs/pymc3/pull/4214) branch. Then, there arises a theano issue while running the test suite locally.

## Description of your problem or feature request

**Please provide a minimal, self-contained, and reproducible example.**
Here is a reproducible snippet
```bash
$ mkvirtualenv test_pymc3
(test_pymc3)$ git clone https://github.com/pymc-devs/pymc3.git
(test_pymc3)$ cd pymc3; pip install -e .
(test_pymc3)$ pip install --upgrade git+https://github.com/pymc-devs/Theano-PyMC.git@master pytest
(test_pymc3)$ gh pr checkout 4218
(test_pymc3)$ git rebase master
(test_pymc3)$ pytest -v pymc3/tests/test_distributions.py::TestMatchesScipy::test_rice
```

**Please provide the full traceback of any errors.**
<details>
<summary>Method not defined `I0e.c_code`</summary>

```bash

=================================================== test session starts ===================================================
platform darwin -- Python 3.8.5, pytest-6.1.1, py-1.9.0, pluggy-0.13.1 -- /Users/user/Env/test_pymc3/bin/python
cachedir: .pytest_cache
rootdir: /Users/user/pymc3, configfile: setup.cfg
collected 1 item                                                                                                          

pymc3/tests/test_distributions.py::TestMatchesScipy::test_rice FAILED                                               [100%]

======================================================== FAILURES =========================================================
_______________________________________________ TestMatchesScipy.test_rice ________________________________________________

self = <pymc3.tests.test_distributions.TestMatchesScipy object at 0x12016fbe0>

    def test_rice(self):
>       self.pymc3_matches_scipy(
            Rice,
            Rplus,
            {""nu"": Rplus, ""sigma"": Rplusbig},
            lambda value, nu, sigma: sp.rice.logpdf(value, b=nu / sigma, loc=0, scale=sigma),
        )

pymc3/tests/test_distributions.py:1630: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
pymc3/tests/test_distributions.py:527: in pymc3_matches_scipy
    self.check_logp(model, value, domain, paramdomains, logp, decimal=decimal)
pymc3/tests/test_distributions.py:532: in check_logp
    logp = model.fastlogp
pymc3/model.py:438: in fastlogp
    return self.model.fastfn(self.logpt)
pymc3/model.py:1277: in fastfn
    f = self.makefn(outs, mode, *args, **kwargs)
pymc3/model.py:1238: in makefn
    return theano.function(
/usr/local/lib/python3.8/site-packages/theano/compile/function/__init__.py:337: in function
    fn = pfunc(
/usr/local/lib/python3.8/site-packages/theano/compile/function/pfunc.py:538: in pfunc
    return orig_function(
/usr/local/lib/python3.8/site-packages/theano/compile/function/types.py:1997: in orig_function
    fn = m.create(defaults)
/usr/local/lib/python3.8/site-packages/theano/compile/function/types.py:1850: in create
    _fn, _i, _o = self.linker.make_thunk(
/usr/local/lib/python3.8/site-packages/theano/gof/link.py:738: in make_thunk
    return self.make_all(
/usr/local/lib/python3.8/site-packages/theano/gof/vm.py:1202: in make_all
    vm = self.make_vm(
/usr/local/lib/python3.8/site-packages/theano/gof/vm.py:978: in make_vm
    dependency_map_list = [
/usr/local/lib/python3.8/site-packages/theano/gof/vm.py:979: in <listcomp>
    [vars_idx[d] for d in dependency_map[vars_idx_inv[i]]]
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

.0 = <list_iterator object at 0x124ca6460>

>       [vars_idx[d] for d in dependency_map[vars_idx_inv[i]]]
        for i in range(len(vars_idx_inv))
    ]
E   KeyError: Elemwise{Composite{i0e((i0 * i1))}}.0

/usr/local/lib/python3.8/site-packages/theano/gof/vm.py:979: KeyError
-------------------------------------------------- Captured stderr call ---------------------------------------------------
WARNING (theano.tensor.opt): i0e does not implement the c_code function. As well as being potentially slow, this disables loop fusion of this op.
WARNING (theano.tensor.opt): i0e does not implement the c_code function. As well as being potentially slow, this disables loop fusion of this op.
WARNING (theano.tensor.opt): i0e does not implement the c_code function. As well as being potentially slow, this disables loop fusion of this op.
WARNING (theano.tensor.opt): i0e does not implement the c_code function. As well as being potentially slow, this disables loop fusion of this op.
WARNING (theano.tensor.opt): i0e does not implement the c_code function. As well as being potentially slow, this disables loop fusion of this op.
WARNING (theano.tensor.opt): i0e does not implement the c_code function. As well as being potentially slow, this disables loop fusion of this op.
WARNING (theano.tensor.opt): i0e does not implement the c_code function. As well as being potentially slow, this disables loop fusion of this op.
WARNING (theano.tensor.opt): i0e does not implement the c_code function. As well as being potentially slow, this disables loop fusion of this op.
WARNING (theano.tensor.opt): i0e does not implement the c_code function. As well as being potentially slow, this disables loop fusion of this op.
WARNING (theano.tensor.opt): i0e does not implement the c_code function. As well as being potentially slow, this disables loop fusion of this op.
WARNING (theano.tensor.opt): Composite{i0e((i0 * i1))} does not implement the c_code function. As well as being potentially slow, this disables loop fusion of this op.
ERROR (theano.gof.opt): Optimization failure due to: LocalOptGroup(local_inplace_gemm,local_inplace_gemv,local_inplace_ger)
ERROR (theano.gof.opt): node: Elemwise{Composite{i0e((i0 * i1))}}(Elemwise{true_div,no_inplace}.0, Elemwise{true_div,no_inplace}.0)
ERROR (theano.gof.opt): TRACEBACK:
ERROR (theano.gof.opt): Traceback (most recent call last):
  File ""/usr/local/lib/python3.8/site-packages/theano/gof/opt.py"", line 2129, in process_node
    replacements = lopt.transform(node)
  File ""/usr/local/lib/python3.8/site-packages/theano/gof/opt.py"", line 1428, in transform
    + self.track_map[node.op]
  File ""/usr/local/lib/python3.8/site-packages/theano/gof/utils.py"", line 204, in __hash__
    return hash((type(self), tuple(getattr(self, a) for a in props)))
  File ""/usr/local/lib/python3.8/site-packages/theano/scalar/basic.py"", line 4442, in __hash__
    self.init_c_code()  # self._c_code and self.nodenames
  File ""/usr/local/lib/python3.8/site-packages/theano/scalar/basic.py"", line 4164, in init_c_code
    s = node.op.c_code(
  File ""/usr/local/lib/python3.8/site-packages/theano/gof/op.py"", line 364, in c_code
    raise MethodNotDefined(""%s.c_code"" % self.__class__.__name__)
theano.gof.utils.MethodNotDefined: I0e.c_code

ERROR (theano.gof.opt): Optimization failure due to: LocalOptGroup(make_c_ger_destructive,make_c_gemv_destructive)
ERROR (theano.gof.opt): node: Elemwise{Composite{i0e((i0 * i1))}}(Elemwise{true_div,no_inplace}.0, Elemwise{true_div,no_inplace}.0)
ERROR (theano.gof.opt): TRACEBACK:
ERROR (theano.gof.opt): Traceback (most recent call last):
  File ""/usr/local/lib/python3.8/site-packages/theano/gof/opt.py"", line 2129, in process_node
    replacements = lopt.transform(node)
  File ""/usr/local/lib/python3.8/site-packages/theano/gof/opt.py"", line 1428, in transform
    + self.track_map[node.op]
  File ""/usr/local/lib/python3.8/site-packages/theano/gof/utils.py"", line 204, in __hash__
    return hash((type(self), tuple(getattr(self, a) for a in props)))
  File ""/usr/local/lib/python3.8/site-packages/theano/scalar/basic.py"", line 4442, in __hash__
    self.init_c_code()  # self._c_code and self.nodenames
  File ""/usr/local/lib/python3.8/site-packages/theano/scalar/basic.py"", line 4164, in init_c_code
    s = node.op.c_code(
  File ""/usr/local/lib/python3.8/site-packages/theano/gof/op.py"", line 364, in c_code
    raise MethodNotDefined(""%s.c_code"" % self.__class__.__name__)
theano.gof.utils.MethodNotDefined: I0e.c_code

ERROR (theano.gof.opt): SeqOptimizer apply <theano.tensor.opt.InplaceElemwiseOptimizer object at 0x11acde6a0>
ERROR (theano.gof.opt): Traceback:
ERROR (theano.gof.opt): Traceback (most recent call last):
  File ""/usr/local/lib/python3.8/site-packages/theano/gof/opt.py"", line 249, in apply
    sub_prof = optimizer.optimize(fgraph)
  File ""/usr/local/lib/python3.8/site-packages/theano/gof/opt.py"", line 89, in optimize
    ret = self.apply(fgraph, *args, **kwargs)
  File ""/usr/local/lib/python3.8/site-packages/theano/tensor/opt.py"", line 472, in apply
    fgraph.replace(
  File ""/usr/local/lib/python3.8/site-packages/theano/gof/fg.py"", line 528, in replace
    self.change_input(node, i, new_r, reason=reason)
  File ""/usr/local/lib/python3.8/site-packages/theano/gof/fg.py"", line 440, in change_input
    self.__import_r__(new_r, reason=reason)
  File ""/usr/local/lib/python3.8/site-packages/theano/gof/fg.py"", line 326, in __import_r__
    self.__import__(variable.owner, reason=reason)
  File ""/usr/local/lib/python3.8/site-packages/theano/gof/fg.py"", line 396, in __import__
    self.execute_callbacks(""on_import"", node, reason)
  File ""/usr/local/lib/python3.8/site-packages/theano/gof/fg.py"", line 608, in execute_callbacks
    fn(self, *args, **kwargs)
  File ""/usr/local/lib/python3.8/site-packages/theano/gof/opt.py"", line 606, in on_import
    self.process_node(fgraph, node)
  File ""/usr/local/lib/python3.8/site-packages/theano/gof/opt.py"", line 738, in process_node
    if inputs_match and node.op == candidate.op:
  File ""/usr/local/lib/python3.8/site-packages/theano/gof/utils.py"", line 211, in __eq__
    return type(self) == type(other) and tuple(
  File ""/usr/local/lib/python3.8/site-packages/theano/scalar/basic.py"", line 4437, in __eq__
    self.init_c_code()  # self._c_code and self.nodenames
  File ""/usr/local/lib/python3.8/site-packages/theano/scalar/basic.py"", line 4164, in init_c_code
    s = node.op.c_code(
  File ""/usr/local/lib/python3.8/site-packages/theano/gof/op.py"", line 364, in c_code
    raise MethodNotDefined(""%s.c_code"" % self.__class__.__name__)
theano.gof.utils.MethodNotDefined: I0e.c_code

---------------------------------------------------- Captured log call ----------------------------------------------------
WARNING  theano.tensor.opt:opt.py:7625 i0e does not implement the c_code function. As well as being potentially slow, this disables loop fusion of this op.
WARNING  theano.tensor.opt:opt.py:7625 i0e does not implement the c_code function. As well as being potentially slow, this disables loop fusion of this op.
WARNING  theano.tensor.opt:opt.py:7625 i0e does not implement the c_code function. As well as being potentially slow, this disables loop fusion of this op.
WARNING  theano.tensor.opt:opt.py:7625 i0e does not implement the c_code function. As well as being potentially slow, this disables loop fusion of this op.
WARNING  theano.tensor.opt:opt.py:7625 i0e does not implement the c_code function. As well as being potentially slow, this disables loop fusion of this op.
WARNING  theano.tensor.opt:opt.py:7625 i0e does not implement the c_code function. As well as being potentially slow, this disables loop fusion of this op.
WARNING  theano.tensor.opt:opt.py:7625 i0e does not implement the c_code function. As well as being potentially slow, this disables loop fusion of this op.
WARNING  theano.tensor.opt:opt.py:7625 i0e does not implement the c_code function. As well as being potentially slow, this disables loop fusion of this op.
WARNING  theano.tensor.opt:opt.py:7625 i0e does not implement the c_code function. As well as being potentially slow, this disables loop fusion of this op.
WARNING  theano.tensor.opt:opt.py:7694 i0e does not implement the c_code function. As well as being potentially slow, this disables loop fusion of this op.
WARNING  theano.tensor.opt:opt.py:7625 Composite{i0e((i0 * i1))} does not implement the c_code function. As well as being potentially slow, this disables loop fusion of this op.
ERROR    theano.gof.opt:opt.py:2007 Optimization failure due to: LocalOptGroup(local_inplace_gemm,local_inplace_gemv,local_inplace_ger)
ERROR    theano.gof.opt:opt.py:2008 node: Elemwise{Composite{i0e((i0 * i1))}}(Elemwise{true_div,no_inplace}.0, Elemwise{true_div,no_inplace}.0)
ERROR    theano.gof.opt:opt.py:2009 TRACEBACK:
ERROR    theano.gof.opt:opt.py:2010 Traceback (most recent call last):
  File ""/usr/local/lib/python3.8/site-packages/theano/gof/opt.py"", line 2129, in process_node
    replacements = lopt.transform(node)
  File ""/usr/local/lib/python3.8/site-packages/theano/gof/opt.py"", line 1428, in transform
    + self.track_map[node.op]
  File ""/usr/local/lib/python3.8/site-packages/theano/gof/utils.py"", line 204, in __hash__
    return hash((type(self), tuple(getattr(self, a) for a in props)))
  File ""/usr/local/lib/python3.8/site-packages/theano/scalar/basic.py"", line 4442, in __hash__
    self.init_c_code()  # self._c_code and self.nodenames
  File ""/usr/local/lib/python3.8/site-packages/theano/scalar/basic.py"", line 4164, in init_c_code
    s = node.op.c_code(
  File ""/usr/local/lib/python3.8/site-packages/theano/gof/op.py"", line 364, in c_code
    raise MethodNotDefined(""%s.c_code"" % self.__class__.__name__)
theano.gof.utils.MethodNotDefined: I0e.c_code

ERROR    theano.gof.opt:opt.py:2007 Optimization failure due to: LocalOptGroup(make_c_ger_destructive,make_c_gemv_destructive)
ERROR    theano.gof.opt:opt.py:2008 node: Elemwise{Composite{i0e((i0 * i1))}}(Elemwise{true_div,no_inplace}.0, Elemwise{true_div,no_inplace}.0)
ERROR    theano.gof.opt:opt.py:2009 TRACEBACK:
ERROR    theano.gof.opt:opt.py:2010 Traceback (most recent call last):
  File ""/usr/local/lib/python3.8/site-packages/theano/gof/opt.py"", line 2129, in process_node
    replacements = lopt.transform(node)
  File ""/usr/local/lib/python3.8/site-packages/theano/gof/opt.py"", line 1428, in transform
    + self.track_map[node.op]
  File ""/usr/local/lib/python3.8/site-packages/theano/gof/utils.py"", line 204, in __hash__
    return hash((type(self), tuple(getattr(self, a) for a in props)))
  File ""/usr/local/lib/python3.8/site-packages/theano/scalar/basic.py"", line 4442, in __hash__
    self.init_c_code()  # self._c_code and self.nodenames
  File ""/usr/local/lib/python3.8/site-packages/theano/scalar/basic.py"", line 4164, in init_c_code
    s = node.op.c_code(
  File ""/usr/local/lib/python3.8/site-packages/theano/gof/op.py"", line 364, in c_code
    raise MethodNotDefined(""%s.c_code"" % self.__class__.__name__)
theano.gof.utils.MethodNotDefined: I0e.c_code

ERROR    theano.gof.opt:opt.py:188 SeqOptimizer apply <theano.tensor.opt.InplaceElemwiseOptimizer object at 0x11acde6a0>
ERROR    theano.gof.opt:opt.py:189 Traceback:
ERROR    theano.gof.opt:opt.py:190 Traceback (most recent call last):
  File ""/usr/local/lib/python3.8/site-packages/theano/gof/opt.py"", line 249, in apply
    sub_prof = optimizer.optimize(fgraph)
  File ""/usr/local/lib/python3.8/site-packages/theano/gof/opt.py"", line 89, in optimize
    ret = self.apply(fgraph, *args, **kwargs)
  File ""/usr/local/lib/python3.8/site-packages/theano/tensor/opt.py"", line 472, in apply
    fgraph.replace(
  File ""/usr/local/lib/python3.8/site-packages/theano/gof/fg.py"", line 528, in replace
    self.change_input(node, i, new_r, reason=reason)
  File ""/usr/local/lib/python3.8/site-packages/theano/gof/fg.py"", line 440, in change_input
    self.__import_r__(new_r, reason=reason)
  File ""/usr/local/lib/python3.8/site-packages/theano/gof/fg.py"", line 326, in __import_r__
    self.__import__(variable.owner, reason=reason)
  File ""/usr/local/lib/python3.8/site-packages/theano/gof/fg.py"", line 396, in __import__
    self.execute_callbacks(""on_import"", node, reason)
  File ""/usr/local/lib/python3.8/site-packages/theano/gof/fg.py"", line 608, in execute_callbacks
    fn(self, *args, **kwargs)
  File ""/usr/local/lib/python3.8/site-packages/theano/gof/opt.py"", line 606, in on_import
    self.process_node(fgraph, node)
  File ""/usr/local/lib/python3.8/site-packages/theano/gof/opt.py"", line 738, in process_node
    if inputs_match and node.op == candidate.op:
  File ""/usr/local/lib/python3.8/site-packages/theano/gof/utils.py"", line 211, in __eq__
    return type(self) == type(other) and tuple(
  File ""/usr/local/lib/python3.8/site-packages/theano/scalar/basic.py"", line 4437, in __eq__
    self.init_c_code()  # self._c_code and self.nodenames
  File ""/usr/local/lib/python3.8/site-packages/theano/scalar/basic.py"", line 4164, in init_c_code
    s = node.op.c_code(
  File ""/usr/local/lib/python3.8/site-packages/theano/gof/op.py"", line 364, in c_code
    raise MethodNotDefined(""%s.c_code"" % self.__class__.__name__)
theano.gof.utils.MethodNotDefined: I0e.c_code
================================================= short test summary info =================================================
FAILED pymc3/tests/test_distributions.py::TestMatchesScipy::test_rice - KeyError: Elemwise{Composite{i0e((i0 * i1))}}.0
==================================================== 1 failed in 9.58s ====================================================
```

</details>


## Versions and main components

* Theano version: Theano-PyMC@master
* Theano config (`python -c ""import theano; print(theano.config)""`)
<details>
<summary>
</summary>

```bash

floatX (('float64', 'float32', 'float16')) 
    Doc:  Default floating-point precision for python casts.

Note: float16 support is experimental, use at your own risk.
    Value:  float64

warn_float64 (('ignore', 'warn', 'raise', 'pdb')) 
    Doc:  Do an action when a tensor variable with float64 dtype is created. They can't be run on the GPU with the current(old) gpu back-end and are slow with gamer GPUs.
    Value:  ignore

pickle_test_value (<function BoolParam.<locals>.booltype at 0x114a380d0>) 
    Doc:  Dump test values while pickling model. If True, test values will be dumped with model.
    Value:  True

cast_policy (('custom', 'numpy+floatX')) 
    Doc:  Rules for implicit type casting
    Value:  custom

int_division (('int', 'raise', 'floatX')) 
    Doc:  What to do when one computes x / y, where both x and y are of integer types
    Value:  int

deterministic (('default', 'more')) 
    Doc:  If `more`, sometimes we will select some implementation that are more deterministic, but slower. In particular, on the GPU, we will avoid using AtomicAdd. Sometimes we will still use non-deterministic implementaion, e.g. when we do not have a GPU implementation that is deterministic. Also see the dnn.conv.algo* flags to cover more cases.
    Value:  default

device (cpu, opencl*, cuda*) 
    Doc:  Default device for computations. If cuda* or opencl*, change thedefault to try to move computation to the GPU. Do not use upper caseletters, only lower case even if NVIDIA uses capital letters.
    Value:  cpu

init_gpu_device (, opencl*, cuda*) 
    Doc:  Initialize the gpu device to use, works only if device=cpu. Unlike 'device', setting this option will NOT move computations, nor shared variables, to the specified GPU. It can be used to run GPU-specific tests on a particular GPU.
    Value:  

force_device (<function BoolParam.<locals>.booltype at 0x114a38670>) 
    Doc:  Raise an error if we can't use the specified device
    Value:  False

conv.assert_shape (<function BoolParam.<locals>.booltype at 0x114a38820>) 
    Doc:  If True, AbstractConv* ops will verify that user-provided shapes match the runtime shapes (debugging option, may slow down compilation)
    Value:  False

print_global_stats (<function BoolParam.<locals>.booltype at 0x114a389d0>) 
    Doc:  Print some global statistics (time spent) at the end
    Value:  False

<theano.configdefaults.ContextsParam object at 0x114a32a60>
    Doc:  
    Context map for multi-gpu operation. Format is a
    semicolon-separated list of names and device names in the
    'name->dev_name' format. An example that would map name 'test' to
    device 'cuda0' and name 'test2' to device 'opencl0:0' follows:
    ""test->cuda0;test2->opencl0:0"".

    Invalid context names are 'cpu', 'cuda*' and 'opencl*'
    
    Value:  

print_active_device (<function BoolParam.<locals>.booltype at 0x114a38ca0>) 
    Doc:  Print active device at when the GPU device is initialized.
    Value:  True

<theano.configparser.ConfigParam object at 0x114a32b80>
    Doc:  This flag is deprecated and will be removed in next Theano release.
    Value:  False

gpuarray.preallocate (<class 'float'>) 
    Doc:  If negative it disables the allocation cache. If
             between 0 and 1 it enables the allocation cache and
             preallocates that fraction of the total GPU memory.  If 1
             or greater it will preallocate that amount of memory (in
             megabytes).
    Value:  0.0

gpuarray.sched (('default', 'multi', 'single')) 
    Doc:  The sched parameter passed for context creation to pygpu.
                With CUDA, using ""multi"" is equivalent to using the parameter
                cudaDeviceScheduleBlockingSync. This is useful to lower the
                CPU overhead when waiting for GPU. One user found that it
                speeds up his other processes that was doing data augmentation.
             
    Value:  default

gpuarray.single_stream (<function BoolParam.<locals>.booltype at 0x114a3a040>) 
    Doc:  
             If your computations are mostly lots of small elements,
             using single-stream will avoid the synchronization
             overhead and usually be faster.  For larger elements it
             does not make a difference yet.  In the future when true
             multi-stream is enabled in libgpuarray, this may change.
             If you want to make sure to have optimal performance,
             check both options.
             
    Value:  True

cuda.root (<class 'str'>) 
    Doc:  Location of the cuda installation
    Value:  

cuda.include_path (<class 'str'>) 
    Doc:  Location of the cuda includes
    Value:  

<theano.configparser.ConfigParam object at 0x114a3c040>
    Doc:  This flag is deprecated; use dnn.conv.algo_fwd.
    Value:  True

<theano.configparser.ConfigParam object at 0x114a3c190>
    Doc:  This flag is deprecated; use `dnn.conv.algo_bwd_filter` and `dnn.conv.algo_bwd_data` instead.
    Value:  True

<theano.configparser.ConfigParam object at 0x114a3c1f0>
    Doc:  This flag is deprecated; use dnn.conv.algo_bwd_data and dnn.conv.algo_bwd_filter.
    Value:  True

dnn.conv.algo_fwd (('small', 'none', 'large', 'fft', 'fft_tiling', 'winograd', 'winograd_non_fused', 'guess_once', 'guess_on_shape_change', 'time_once', 'time_on_shape_change')) 
    Doc:  Default implementation to use for cuDNN forward convolution.
    Value:  small

dnn.conv.algo_bwd_data (('none', 'deterministic', 'fft', 'fft_tiling', 'winograd', 'winograd_non_fused', 'guess_once', 'guess_on_shape_change', 'time_once', 'time_on_shape_change')) 
    Doc:  Default implementation to use for cuDNN backward convolution to get the gradients of the convolution with regard to the inputs.
    Value:  none

dnn.conv.algo_bwd_filter (('none', 'deterministic', 'fft', 'small', 'winograd_non_fused', 'fft_tiling', 'guess_once', 'guess_on_shape_change', 'time_once', 'time_on_shape_change')) 
    Doc:  Default implementation to use for cuDNN backward convolution to get the gradients of the convolution with regard to the filters.
    Value:  none

dnn.conv.precision (('as_input_f32', 'as_input', 'float16', 'float32', 'float64')) 
    Doc:  Default data precision to use for the computation in cuDNN convolutions (defaults to the same dtype as the inputs of the convolutions, or float32 if inputs are float16).
    Value:  as_input_f32

dnn.base_path (<class 'str'>) 
    Doc:  Install location of cuDNN.
    Value:  

dnn.include_path (<class 'str'>) 
    Doc:  Location of the cudnn header
    Value:  

dnn.library_path (<class 'str'>) 
    Doc:  Location of the cudnn link library.
    Value:  

dnn.bin_path (<class 'str'>) 
    Doc:  Location of the cuDNN load library (on non-windows platforms, this is the same as dnn.library_path)
    Value:  

dnn.enabled (('auto', 'True', 'False', 'no_check')) 
    Doc:  'auto', use cuDNN if available, but silently fall back to not using it if not present. If True and cuDNN can not be used, raise an error. If False, disable cudnn even if present. If no_check, assume present and the version between header and library match (so less compilation at context init)
    Value:  auto

magma.include_path (<class 'str'>) 
    Doc:  Location of the magma header
    Value:  

magma.library_path (<class 'str'>) 
    Doc:  Location of the magma library
    Value:  

magma.enabled (<function BoolParam.<locals>.booltype at 0x114a3ae50>) 
    Doc:   If True, use magma for matrix computation. If False, disable magma
    Value:  False

assert_no_cpu_op (('ignore', 'warn', 'raise', 'pdb')) 
    Doc:  Raise an error/warning if there is a CPU op in the computational graph.
    Value:  ignore

<theano.configparser.ConfigParam object at 0x114a3cac0>
    Doc:  Default compilation mode
    Value:  Mode

cxx (<class 'str'>) 
    Doc:  The C++ compiler to use. Currently only g++ is supported, but supporting additional compilers should not be too difficult. If it is empty, no C++ code is compiled.
    Value:  /usr/bin/clang++

linker (('cvm', 'c|py', 'py', 'c', 'c|py_nogc', 'vm', 'vm_nogc', 'cvm_nogc')) 
    Doc:  Default linker used if the theano flags mode is Mode
    Value:  cvm

allow_gc (<function BoolParam.<locals>.booltype at 0x114a3ed30>) 
    Doc:  Do we default to delete intermediate results during Theano function calls? Doing so lowers the memory requirement, but asks that we reallocate memory at the next function call. This is implemented for the default linker, but may not work for all linkers.
    Value:  True

optimizer (('o4', 'o3', 'o2', 'o1', 'unsafe', 'fast_run', 'fast_compile', 'merge', 'None')) 
    Doc:  Default optimizer. If not None, will use this optimizer with the Mode
    Value:  o4

optimizer_verbose (<function BoolParam.<locals>.booltype at 0x114a3ef70>) 
    Doc:  If True, we print all optimization being applied
    Value:  False

on_opt_error (('warn', 'raise', 'pdb', 'ignore')) 
    Doc:  What to do when an optimization crashes: warn and skip it, raise the exception, or fall into the pdb debugger.
    Value:  warn

nocleanup (<function BoolParam.<locals>.booltype at 0x114a4e1f0>) 
    Doc:  Suppress the deletion of code files that did not compile cleanly
    Value:  False

on_unused_input (('raise', 'warn', 'ignore')) 
    Doc:  What to do if a variable in the 'inputs' list of  theano.function() is not used in the graph.
    Value:  raise

tensor.cmp_sloppy (<class 'int'>) 
    Doc:  Relax tensor._allclose (0) not at all, (1) a bit, (2) more
    Value:  0

tensor.local_elemwise_fusion (<function BoolParam.<locals>.booltype at 0x114a4e550>) 
    Doc:  Enable or not in fast_run mode(fast_run optimization) the elemwise fusion optimization
    Value:  True

gpu.local_elemwise_fusion (<function BoolParam.<locals>.booltype at 0x114a4e700>) 
    Doc:  Enable or not in fast_run mode(fast_run optimization) the gpu elemwise fusion optimization
    Value:  True

lib.amdlibm (<function BoolParam.<locals>.booltype at 0x114a4e8b0>) 
    Doc:  Use amd's amdlibm numerical library
    Value:  False

gpuelemwise.sync (<function BoolParam.<locals>.booltype at 0x114a4ea60>) 
    Doc:  when true, wait that the gpu fct finished and check it error code.
    Value:  True

traceback.limit (<class 'int'>) 
    Doc:  The number of stack to trace. -1 mean all.
    Value:  8

traceback.compile_limit (<class 'int'>) 
    Doc:  The number of stack to trace to keep during compilation. -1 mean all. If greater then 0, will also make us save Theano internal stack trace.
    Value:  0

experimental.unpickle_gpu_on_cpu (<function BoolParam.<locals>.booltype at 0x114a4ed30>) 
    Doc:  Allow unpickling of pickled GpuArrays as numpy.ndarrays.This is useful, if you want to open a GpuArray without having cuda installed.If you have cuda installed, this will force unpickling tobe done on the cpu to numpy.ndarray.Please be aware that this may get you access to the data,however, trying to unpicke gpu functions will not succeed.This flag is experimental and may be removed any time, whengpu<>cpu transparency is solved.
    Value:  False

numpy.seterr_all (('ignore', 'warn', 'raise', 'call', 'print', 'log', 'None')) 
    Doc:  (""Sets numpy's behaviour for floating-point errors, "", ""see numpy.seterr. 'None' means not to change numpy's default, which can be different for different numpy releases. This flag sets the default behaviour for all kinds of floating-point errors, its effect can be overriden for specific errors by the following flags: seterr_divide, seterr_over, seterr_under and seterr_invalid."")
    Value:  ignore

numpy.seterr_divide (('None', 'ignore', 'warn', 'raise', 'call', 'print', 'log')) 
    Doc:  Sets numpy's behavior for division by zero, see numpy.seterr. 'None' means using the default, defined by numpy.seterr_all.
    Value:  None

numpy.seterr_over (('None', 'ignore', 'warn', 'raise', 'call', 'print', 'log')) 
    Doc:  Sets numpy's behavior for floating-point overflow, see numpy.seterr. 'None' means using the default, defined by numpy.seterr_all.
    Value:  None

numpy.seterr_under (('None', 'ignore', 'warn', 'raise', 'call', 'print', 'log')) 
    Doc:  Sets numpy's behavior for floating-point underflow, see numpy.seterr. 'None' means using the default, defined by numpy.seterr_all.
    Value:  None

numpy.seterr_invalid (('None', 'ignore', 'warn', 'raise', 'call', 'print', 'log')) 
    Doc:  Sets numpy's behavior for invalid floating-point operation, see numpy.seterr. 'None' means using the default, defined by numpy.seterr_all.
    Value:  None

warn.ignore_bug_before (('0.9', 'None', 'all', '0.3', '0.4', '0.4.1', '0.5', '0.6', '0.7', '0.8', '0.8.1', '0.8.2', '0.9', '0.10', '1.0', '1.0.1', '1.0.2', '1.0.3', '1.0.4', '1.0.5')) 
    Doc:  If 'None', we warn about all Theano bugs found by default. If 'all', we don't warn about Theano bugs found by default. If a version, we print only the warnings relative to Theano bugs found after that version. Warning for specific bugs can be configured with specific [warn] flags.
    Value:  0.9

warn.argmax_pushdown_bug (<function BoolParam.<locals>.booltype at 0x114a523a0>) 
    Doc:  Warn if in past version of Theano we generated a bug with the theano.tensor.nnet.nnet.local_argmax_pushdown optimization. Was fixed 27 may 2010
    Value:  False

warn.gpusum_01_011_0111_bug (<function BoolParam.<locals>.booltype at 0x114a52550>) 
    Doc:  Warn if we are in a case where old version of Theano had a silent bug with GpuSum pattern 01,011 and 0111 when the first dimensions was bigger then 4096. Was fixed 31 may 2010
    Value:  False

warn.sum_sum_bug (<function BoolParam.<locals>.booltype at 0x114a52700>) 
    Doc:  Warn if we are in a case where Theano version between version 9923a40c7b7a and the 2 august 2010 (fixed date), generated an error in that case. This happens when there are 2 consecutive sums in the graph, bad code was generated. Was fixed 2 August 2010
    Value:  False

warn.sum_div_dimshuffle_bug (<function BoolParam.<locals>.booltype at 0x114a528b0>) 
    Doc:  Warn if previous versions of Theano (between rev. 3bd9b789f5e8, 2010-06-16, and cfc6322e5ad4, 2010-08-03) would have given incorrect result. This bug was triggered by sum of division of dimshuffled tensors.
    Value:  False

warn.subtensor_merge_bug (<function BoolParam.<locals>.booltype at 0x114a52a60>) 
    Doc:  Warn if previous versions of Theano (before 0.5rc2) could have given incorrect results when indexing into a subtensor with negative stride (for instance, for instance, x[a:b:-1][c]).
    Value:  False

warn.gpu_set_subtensor1 (<function BoolParam.<locals>.booltype at 0x114a52c10>) 
    Doc:  Warn if previous versions of Theano (before 0.6) could have given incorrect results when moving to the gpu set_subtensor(x[int vector], new_value)
    Value:  False

warn.vm_gc_bug (<function BoolParam.<locals>.booltype at 0x114a52dc0>) 
    Doc:  There was a bug that existed in the default Theano configuration, only in the development version between July 5th 2012 and July 30th 2012. This was not in a released version. If your code was affected by this bug, a warning will be printed during the code execution if you use the `linker=vm,vm.lazy=True,warn.vm_gc_bug=True` Theano flags. This warning is disabled by default as the bug was not released.
    Value:  False

warn.signal_conv2d_interface (<function BoolParam.<locals>.booltype at 0x114a52f70>) 
    Doc:  Warn we use the new signal.conv2d() when its interface changed mid June 2014
    Value:  False

warn.reduce_join (<function BoolParam.<locals>.booltype at 0x114a54160>) 
    Doc:  Your current code is fine, but Theano versions prior to 0.7 (or this development version) might have given an incorrect result. To disable this warning, set the Theano flag warn.reduce_join to False. The problem was an optimization, that modified the pattern ""Reduce{scalar.op}(Join(axis=0, a, b), axis=0)"", did not check the reduction axis. So if the reduction axis was not 0, you got a wrong answer.
    Value:  False

warn.inc_set_subtensor1 (<function BoolParam.<locals>.booltype at 0x114a54310>) 
    Doc:  Warn if previous versions of Theano (before 0.7) could have given incorrect results for inc_subtensor and set_subtensor when using some patterns of advanced indexing (indexing with one vector or matrix of ints).
    Value:  False

warn.round (<function BoolParam.<locals>.booltype at 0x114a544c0>) 
    Doc:  Warn when using `tensor.round` with the default mode. Round changed its default from `half_away_from_zero` to `half_to_even` to have the same default as NumPy.
    Value:  False

warn.inc_subtensor1_opt (<function BoolParam.<locals>.booltype at 0x114a54670>) 
    Doc:  Warn if previous versions of Theano (before 0.10) could have given incorrect results when computing inc_subtensor(zeros[idx], x)[idx], when idx is an array of integers with duplicated values.
    Value:  True

compute_test_value (('off', 'ignore', 'warn', 'raise', 'pdb')) 
    Doc:  If 'True', Theano will run each op at graph build time, using Constants, SharedVariables and the tag 'test_value' as inputs to the function. This helps the user track down problems in the graph before it gets optimized.
    Value:  off

print_test_value (<function BoolParam.<locals>.booltype at 0x114a548b0>) 
    Doc:  If 'True', the __eval__ of a Theano variable will return its test_value when this is available. This has the practical conseguence that, e.g., in debugging `my_var` will print the same as `my_var.tag.test_value` when a test value is defined.
    Value:  False

compute_test_value_opt (('off', 'ignore', 'warn', 'raise', 'pdb')) 
    Doc:  For debugging Theano optimization only. Same as compute_test_value, but is used during Theano optimization
    Value:  off

unpickle_function (<function BoolParam.<locals>.booltype at 0x114a54af0>) 
    Doc:  Replace unpickled Theano functions with None. This is useful to unpickle old graphs that pickled them when it shouldn't
    Value:  True

reoptimize_unpickled_function (<function BoolParam.<locals>.booltype at 0x114a54ca0>) 
    Doc:  Re-optimize the graph when a theano function is unpickled from the disk.
    Value:  False

exception_verbosity (('low', 'high')) 
    Doc:  If 'low', the text of exceptions will generally refer to apply nodes with short names such as Elemwise{add_no_inplace}. If 'high', some exceptions will also refer to apply nodes with long descriptions  like:
    A. Elemwise{add_no_inplace}
            B. log_likelihood_v_given_h
            C. log_likelihood_h
    Value:  low

openmp (<function BoolParam.<locals>.booltype at 0x114a54ee0>) 
    Doc:  Allow (or not) parallel computation on the CPU with OpenMP. This is the default value used when creating an Op that supports OpenMP parallelization. It is preferable to define it via the Theano configuration file ~/.theanorc or with the environment variable THEANO_FLAGS. Parallelization is only done for some operations that implement it, and even for operations that implement parallelism, each operation is free to respect this flag or not. You can control the number of threads used with the environment variable OMP_NUM_THREADS. If it is set to 1, we disable openmp in Theano by default.
    Value:  False

openmp_elemwise_minsize (<class 'int'>) 
    Doc:  If OpenMP is enabled, this is the minimum size of vectors for which the openmp parallelization is enabled in element wise ops.
    Value:  200000

check_input (<function BoolParam.<locals>.booltype at 0x114a57160>) 
    Doc:  Specify if types should check their input in their C code. It can be used to speed up compilation, reduce overhead (particularly for scalars) and reduce the number of generated C files.
    Value:  True

cache_optimizations (<function BoolParam.<locals>.booltype at 0x114a57310>) 
    Doc:  WARNING: work in progress, does not work yet. Specify if the optimization cache should be used. This cache will any optimized graph and its optimization. Actually slow downs a lot the first optimization, and could possibly still contains some bugs. Use at your own risks.
    Value:  False

unittests.rseed (<class 'str'>) 
    Doc:  Seed to use for randomized unit tests. Special value 'random' means using a seed of None.
    Value:  666

NanGuardMode.nan_is_error (<function BoolParam.<locals>.booltype at 0x114a575e0>) 
    Doc:  Default value for nan_is_error
    Value:  True

NanGuardMode.inf_is_error (<function BoolParam.<locals>.booltype at 0x114a57790>) 
    Doc:  Default value for inf_is_error
    Value:  True

NanGuardMode.big_is_error (<function BoolParam.<locals>.booltype at 0x114a57940>) 
    Doc:  Default value for big_is_error
    Value:  True

NanGuardMode.action (('raise', 'warn', 'pdb')) 
    Doc:  What NanGuardMode does when it finds a problem
    Value:  raise

optimizer_excluding (<class 'str'>) 
    Doc:  When using the default mode, we will remove optimizer with these tags. Separate tags with ':'.
    Value:  

optimizer_including (<class 'str'>) 
    Doc:  When using the default mode, we will add optimizer with these tags. Separate tags with ':'.
    Value:  

optimizer_requiring (<class 'str'>) 
    Doc:  When using the default mode, we will require optimizer with these tags. Separate tags with ':'.
    Value:  

DebugMode.patience (<class 'int'>) 
    Doc:  Optimize graph this many times to detect inconsistency
    Value:  10

DebugMode.check_c (<function BoolParam.<locals>.booltype at 0x114a57ee0>) 
    Doc:  Run C implementations where possible
    Value:  True

DebugMode.check_py (<function BoolParam.<locals>.booltype at 0x114a5c0d0>) 
    Doc:  Run Python implementations where possible
    Value:  True

DebugMode.check_finite (<function BoolParam.<locals>.booltype at 0x114a5c280>) 
    Doc:  True -> complain about NaN/Inf results
    Value:  True

DebugMode.check_strides (<class 'int'>) 
    Doc:  Check that Python- and C-produced ndarrays have same strides. On difference: (0) - ignore, (1) warn, or (2) raise error
    Value:  0

DebugMode.warn_input_not_reused (<function BoolParam.<locals>.booltype at 0x114a5c550>) 
    Doc:  Generate a warning when destroy_map or view_map says that an op works inplace, but the op did not reuse the input for its output.
    Value:  True

DebugMode.check_preallocated_output (<class 'str'>) 
    Doc:  Test thunks with pre-allocated memory as output storage. This is a list of strings separated by "":"". Valid values are: ""initial"" (initial storage in storage map, happens with Scan),""previous"" (previously-returned memory), ""c_contiguous"", ""f_contiguous"", ""strided"" (positive and negative strides), ""wrong_size"" (larger and smaller dimensions), and ""ALL"" (all of the above).
    Value:  

DebugMode.check_preallocated_output_ndim (<class 'int'>) 
    Doc:  When testing with ""strided"" preallocated output memory, test all combinations of strides over that number of (inner-most) dimensions. You may want to reduce that number to reduce memory or time usage, but it is advised to keep a minimum of 2.
    Value:  4

profiling.time_thunks (<function BoolParam.<locals>.booltype at 0x114a5c940>) 
    Doc:  Time individual thunks when profiling
    Value:  True

profiling.n_apply (<class 'int'>) 
    Doc:  Number of Apply instances to print by default
    Value:  20

profiling.n_ops (<class 'int'>) 
    Doc:  Number of Ops to print by default
    Value:  20

profiling.output_line_width (<class 'int'>) 
    Doc:  Max line width for the profiling output
    Value:  512

profiling.min_memory_size (<class 'int'>) 
    Doc:  For the memory profile, do not print Apply nodes if the size
             of their outputs (in bytes) is lower than this threshold
    Value:  1024

profiling.min_peak_memory (<function BoolParam.<locals>.booltype at 0x114a5cf70>) 
    Doc:  The min peak memory usage of the order
    Value:  False

profiling.destination (<class 'str'>) 
    Doc:  
             File destination of the profiling output
             
    Value:  stderr

profiling.debugprint (<function BoolParam.<locals>.booltype at 0x114a5f1f0>) 
    Doc:  
             Do a debugprint of the profiled functions
             
    Value:  False

profiling.ignore_first_call (<function BoolParam.<locals>.booltype at 0x114a5f3a0>) 
    Doc:  
             Do we ignore the first call of a Theano function.
             
    Value:  False

optdb.position_cutoff (<class 'float'>) 
    Doc:  Where to stop eariler during optimization. It represent the position of the optimizer where to stop.
    Value:  inf

optdb.max_use_ratio (<class 'float'>) 
    Doc:  A ratio that prevent infinite loop in EquilibriumOptimizer.
    Value:  8.0

gcc.cxxflags (<class 'str'>) 
    Doc:  Extra compiler flags for gcc
    Value:  

cmodule.warn_no_version (<function BoolParam.<locals>.booltype at 0x114a5f700>) 
    Doc:  If True, will print a warning when compiling one or more Op with C code that can't be cached because there is no c_code_cache_version() function associated to at least one of those Ops.
    Value:  False

cmodule.remove_gxx_opt (<function BoolParam.<locals>.booltype at 0x114a5f8b0>) 
    Doc:  If True, will remove the -O* parameter passed to g++.This is useful to debug in gdb modules compiled by Theano.The parameter -g is passed by default to g++
    Value:  False

cmodule.compilation_warning (<function BoolParam.<locals>.booltype at 0x114a5fa60>) 
    Doc:  If True, will print compilation warnings.
    Value:  False

cmodule.preload_cache (<function BoolParam.<locals>.booltype at 0x114a5fc10>) 
    Doc:  If set to True, will preload the C module cache at import time
    Value:  False

cmodule.age_thresh_use (<class 'int'>) 
    Doc:  In seconds. The time after which Theano won't reuse a compile c module.
    Value:  2073600

cmodule.debug (<function BoolParam.<locals>.booltype at 0x114a5fe50>) 
    Doc:  If True, define a DEBUG macro (if not exists) for any compiled C code.
    Value:  False

blas.ldflags (<class 'str'>) 
    Doc:  lib[s] to include for [Fortran] level-3 blas implementation
    Value:  -lblas

blas.check_openmp (<function BoolParam.<locals>.booltype at 0x114a62280>) 
    Doc:  Check for openmp library conflict.
WARNING: Setting this to False leaves you open to wrong results in blas-related operations.
    Value:  True

metaopt.verbose (<class 'int'>) 
    Doc:  0 for silent, 1 for only warnings, 2 for full output withtimings and selected implementation
    Value:  0

metaopt.optimizer_excluding (<class 'str'>) 
    Doc:  exclude optimizers with these tags. Separate tags with ':'.
    Value:  

metaopt.optimizer_including (<class 'str'>) 
    Doc:  include optimizers with these tags. Separate tags with ':'.
    Value:  

profile (<function BoolParam.<locals>.booltype at 0x114a625e0>) 
    Doc:  If VM should collect profile information
    Value:  False

profile_optimizer (<function BoolParam.<locals>.booltype at 0x114a62790>) 
    Doc:  If VM should collect optimizer profile information
    Value:  False

profile_memory (<function BoolParam.<locals>.booltype at 0x114a62940>) 
    Doc:  If VM should collect memory profile information and print it
    Value:  False

<theano.configparser.ConfigParam object at 0x114a61cd0>
    Doc:  Useful only for the vm linkers. When lazy is None, auto detect if lazy evaluation is needed and use the appropriate version. If lazy is True/False, force the version used between Loop/LoopGC and Stack.
    Value:  None

warn.identify_1pexp_bug (<function BoolParam.<locals>.booltype at 0x114a62b80>) 
    Doc:  Warn if Theano versions prior to 7987b51 (2011-12-18) could have yielded a wrong result due to a bug in the is_1pexp function
    Value:  False

on_shape_error (('warn', 'raise')) 
    Doc:  warn: print a warning and use the default value. raise: raise an error
    Value:  warn

tensor.insert_inplace_optimizer_validate_nb (<class 'int'>) 
    Doc:  -1: auto, if graph have less then 500 nodes 1, else 10
    Value:  -1

experimental.local_alloc_elemwise (<function BoolParam.<locals>.booltype at 0x114a62ee0>) 
    Doc:  DEPRECATED: If True, enable the experimental optimization local_alloc_elemwise. Generates error if not True. Use optimizer_excluding=local_alloc_elemwise to dsiable.
    Value:  True

experimental.local_alloc_elemwise_assert (<function BoolParam.<locals>.booltype at 0x114a62f70>) 
    Doc:  When the local_alloc_elemwise is applied, add an assert to highlight shape errors.
    Value:  True

scan.allow_gc (<function BoolParam.<locals>.booltype at 0x114a6b1f0>) 
    Doc:  Allow/disallow gc inside of Scan (default: False)
    Value:  False

scan.allow_output_prealloc (<function BoolParam.<locals>.booltype at 0x114a6b3a0>) 
    Doc:  Allow/disallow memory preallocation for outputs inside of scan (default: True)
    Value:  True

scan.debug (<function BoolParam.<locals>.booltype at 0x114a6b550>) 
    Doc:  If True, enable extra verbose output related to scan
    Value:  False

compile.wait (<class 'int'>) 
    Doc:  Time to wait before retrying to acquire the compile lock.
    Value:  5

cycle_detection (('regular', 'fast')) 
    Doc:  If cycle_detection is set to regular, most inplaces are allowed,but it is slower. If cycle_detection is set to faster, less inplacesare allowed, but it makes the compilation faster.The interaction of which one give the lower peak memory usage iscomplicated and not predictable, so if you are close to the peakmemory usage, triyng both could give you a small gain.
    Value:  regular

check_stack_trace (('off', 'log', 'warn', 'raise')) 
    Doc:  A flag for checking the stack trace during the optimization process. default (off): does not check the stack trace of any optimization log: inserts a dummy stack trace that identifies the optimizationthat inserted the variable that had an empty stack trace.warn: prints a warning if a stack trace is missing and also a dummystack trace is inserted that indicates which optimization insertedthe variable that had an empty stack trace.raise: raises an exception if a stack trace is missing
    Value:  off

compile.timeout (<class 'int'>) 
    Doc:  In seconds, time that a process will wait before deciding to
override an existing lock. An override only happens when the existing
lock is held by the same owner *and* has not been 'refreshed' by this
owner for more than this period. Refreshes are done every half timeout
period for running processes.
    Value:  120

compiledir_format (<class 'str'>) 
    Doc:  Format string for platform-dependent compiled module subdirectory
(relative to base_compiledir). Available keys: device, gxx_version,
hostname, numpy_version, platform, processor, python_bitwidth,
python_int_bitwidth, python_version, short_platform, theano_version.
Defaults to 'compiledir_%(short_platform)s-%(processor)s-%(python_vers
ion)s-%(python_bitwidth)s'.
    Value:  compiledir_%(short_platform)s-%(processor)s-%(python_version)s-%(python_bitwidth)s

<theano.configparser.ConfigParam object at 0x114a69e80>
    Doc:  platform-independent root directory for compiled modules
    Value:  /Users/user/.theano

<theano.configparser.ConfigParam object at 0x114a73be0>
    Doc:  platform-dependent cache directory for compiled modules
    Value:  /Users/user/.theano/compiledir_macOS-10.15.7-x86_64-i386-64bit-i386-3.8.5-64

<theano.configparser.ConfigParam object at 0x114a803a0>
    Doc:  Directory to cache pre-compiled kernels for the gpuarray backend.
    Value:  /Users/user/.theano/compiledir_macOS-10.15.7-x86_64-i386-64bit-i386-3.8.5-64/gpuarray_kernels

ctc.root (<class 'str'>) 
    Doc:  Directory which contains the root of Baidu CTC library. It is assumed     that the compiled library is either inside the build, lib or lib64     subdirectory, and the header inside the include directory.
    Value:  

```

</details>

* Python version: 3.8.5
* Operating system: Macos
* How did you install Theano: (conda/pip) pip
",bug important graph rewriting,brandonwillard,2020-11-14 09:02:30,2020-11-15 03:22:36,"michaelosthege mentioned 2020-11-14 09:02:31,michaelosthege subscribed 2020-11-14 09:02:31,brandonwillard labeled 2020-11-15 00:39:49,brandonwillard labeled 2020-11-15 00:39:49,brandonwillard labeled 2020-11-15 00:39:49,brandonwillard assigned 2020-11-15 00:39:51,brandonwillard closed 2020-11-15 03:22:37",michaelosthege Sayam753 brandonwillard,1
143,163,Run PyMC3 test suite,twiecki,With all this heavy development work it will be very easy to inadvertently break PyMC3 which we will only discover in unrelated PyMC3 PRs. I thus propose to run the PyMC3 tests for Theano PRs as well.,question,,2020-11-15 06:50:27,2020-11-16 10:21:50,"brandonwillard labeled 2020-11-15 07:36:52,twiecki closed 2020-11-16 10:21:50",twiecki brandonwillard,3
145,165,Issue with JAX-ifying ops with multiple outputs,dfm,"There seems to be an issue with the jaxifying logic when treating nodes with multiple outputs. I haven't dug too much, but I found this for an op I'm working on for https://github.com/dfm/exoplanet. It was easy to reproduce using `MaxAndArgmax` which was the only op implemented in `sandbox.jaxify` that has multiple outputs (as far as I can tell). The following code will throw the relevant error:

```python
import theano
import numpy as np

x = theano.tensor.dvector()
mx, amx = theano.tensor.MaxAndArgmax([0])(x)
result = mx + amx

fgraph = theano.gof.FunctionGraph([x], [result])
opts = theano.gof.Query(include=[None], exclude=[""cxx_only"", ""BlasOpt""])
jax_mode = theano.compile.mode.Mode(theano.sandbox.jax_linker.JAXLinker(), opts)
py_mode = theano.compile.Mode(""py"", opts)

theano_jax_fn = theano.function(fgraph.inputs, fgraph.outputs, mode=jax_mode)
theano_jax_fn(np.random.randn(10))
```

This results in a long error message which ends with:

```python
~/miniconda3/envs/exoplanet/lib/python3.8/site-packages/theano/sandbox/jaxify.py in <listcomp>(.0)
    153         def jax_func(*inputs):
--> 154             func_args = [fn(*inputs) for fn in input_funcs]
    155             return return_func(*func_args)

FilteredStackTrace: TypeError: 'list' object is not callable
```

cc @junpenglao ",bug JAX,brandonwillard,2020-11-15 23:43:19,2020-11-16 01:57:10,"junpenglao mentioned 2020-11-15 23:43:19,junpenglao subscribed 2020-11-15 23:43:19,brandonwillard labeled 2020-11-15 23:45:11,brandonwillard mentioned 2020-11-16 00:00:16,brandonwillard subscribed 2020-11-16 00:00:16,brandonwillard closed 2020-11-16 01:57:10,brandonwillard labeled 2020-11-16 01:57:29,brandonwillard assigned 2020-11-16 01:57:31",junpenglao dfm brandonwillard,3
150,170,Ensure that there's only one evaluation of a multi-output JAXified `Op`,brandonwillard,"@brandonwillard: One question about this implementation because I'm not sure that I fully understand everything that's happening: Will this lead (on the JAX side) to the function being evaluated multiple times, once for each output? I'm not sure that I know enough about the JAX implementation details to know!

_Originally posted by @dfm in https://github.com/pymc-devs/Theano-PyMC/issues/166#issuecomment-728091044_",enhancement JAX,,2020-11-16 16:55:10,2020-11-17 00:59:12,"dfm mentioned 2020-11-16 16:55:10,dfm subscribed 2020-11-16 16:55:10,brandonwillard mentioned 2020-11-16 16:55:10,brandonwillard subscribed 2020-11-16 16:55:10,brandonwillard labeled 2020-11-16 17:03:37,brandonwillard labeled 2020-11-16 17:03:37,brandonwillard closed 2020-11-17 00:59:12,dfm mentioned 2020-11-17 01:01:33,dfm subscribed 2020-11-17 01:01:33",dfm brandonwillard,5
151,172,NumPy typed NaN conversion issue,brandonwillard,"This example raises an issue during type filtering:
```python
import numpy as np

import theano
import theano.scalar as ts


x = ts.Scalar(""float64"")
nan = np.array([np.nan], dtype=""float64"")[0]

```
```python
>>>  x.filter(nan)
...
TypeError: Value cannot accurately be converted to dtype (float64) and allow_downcast is not True
```",bug,brandonwillard,2020-11-16 19:03:27,2020-11-17 00:47:38,"brandonwillard labeled 2020-11-16 19:03:27,brandonwillard assigned 2020-11-16 22:16:25,brandonwillard closed 2020-11-17 00:47:38",brandonwillard,0
154,175,Prevent Codecov from prematurely reporting failures,brandonwillard,"Our current CI setup reports intermediate coverage states and results in temporary and misleading error reports.  If we can prevent it from reporting failures for intermediate results, that would be great, but, if not, let's change it to only show/send a report when it's completely finished with all test jobs.",enhancement CI,,2020-11-16 21:39:54,2020-11-26 03:38:17,"brandonwillard labeled 2020-11-16 21:39:54,brandonwillard labeled 2020-11-16 21:39:54,OriolAbril mentioned 2020-11-16 22:23:59,OriolAbril subscribed 2020-11-16 22:24:00,brandonwillard connected 2020-11-25 20:31:49,brandonwillard closed 2020-11-26 03:38:17",OriolAbril brandonwillard,1
156,177,Many failures when jit-ing JAX-ified ops,dfm,"If we turn on `jax.jit` in the JAX tests

<details>
<summary>See diff</summary>

```diff
diff --git a/tests/sandbox/test_jax.py b/tests/sandbox/test_jax.py
index 89c46ff9b..eb51e82ed 100644
--- a/tests/sandbox/test_jax.py
+++ b/tests/sandbox/test_jax.py
@@ -56,7 +56,7 @@ def compare_jax_and_py(
     py_mode = theano.compile.Mode(""py"", opts)

     theano_jax_fn = theano.function(fgraph.inputs, fgraph.outputs, mode=jax_mode)
-    jax_res = theano_jax_fn(*inputs)
+    jax_res = jax.jit(theano_jax_fn)(*inputs)

     if must_be_device_array:
         if isinstance(jax_res, list):
```
</details>

most of the unit tests fail with errors related to calling numpy functions on JAX trace objects. This will be a huge problem (JAX isn't all that useful without `jit`!) so we should definitely be testing for and supporting this.

Am I missing something here?",,,2020-11-17 13:49:07,2020-11-17 15:20:46,dfm closed 2020-11-17 15:20:46,junpenglao dfm,2
157,178,IfElse JAX conversion should use cond,dfm,"Related to #177:

The current implementation of the JAX translation for the IfElse op is going to be problematic [because of how JAX handles control flow when `jit`-ing](https://jax.readthedocs.io/en/latest/notebooks/Common_Gotchas_in_JAX.html#%F0%9F%94%AA-Control-Flow). We should probably be using [`jax.lax.cond`](https://jax.readthedocs.io/en/latest/_autosummary/jax.lax.cond.html) instead.",help wanted JAX,eigenfoo,2020-11-17 13:53:22,2020-11-26 03:44:09,"twiecki labeled 2020-11-17 14:57:05,twiecki labeled 2020-11-17 14:57:05,eigenfoo assigned 2020-11-18 00:48:11,dfm mentioned 2020-11-18 01:31:08,dfm subscribed 2020-11-18 01:31:08,eigenfoo mentioned 2020-11-19 18:59:00,eigenfoo subscribed 2020-11-19 18:59:00,dfm mentioned 2020-11-19 23:57:58,dfm subscribed 2020-11-19 23:57:58,dfm mentioned 2020-11-20 00:22:42,dfm subscribed 2020-11-20 00:22:42,brandonwillard closed 2020-11-26 03:44:09",twiecki eigenfoo dfm brandonwillard,6
160,183,Implement vector-proof Softmax function,AlexAndorra,"## Description
_Originally reported by @ricardoV94 [here](https://github.com/pymc-devs/pymc3/pull/4229)_

Currently, `tt.nnet.softmax` raises `UserWarning: DEPRECATION: If x is a vector, Softmax will not automatically pad x anymore in next releases. If you need it, please do it manually. The vector case is gonna be supported soon and the output will be a vector.` in two places:

- https://github.com/pymc-devs/Theano-PyMC/blob/a9275c3dcc998c8cca5719037e493809b23422ff/theano/tensor/nnet/nnet.py#L443
- https://github.com/pymc-devs/Theano-PyMC/blob/a9275c3dcc998c8cca5719037e493809b23422ff/theano/tensor/nnet/nnet.py#L641

We actually use a custom version of Softmax in the PyMC3 codebase, to go around this limitation: https://github.com/pymc-devs/pymc3/blob/a05684b9588208882db164be113954eb21604ea1/pymc3/distributions/transforms.py#L463

```python
    def backward(self, y_):
        y = y_.T
        y = tt.concatenate([y, -tt.sum(y, 0, keepdims=True)])
        # ""softmax"" with vector support and no deprication warning:
        e_y = tt.exp(y - tt.max(y, 0, keepdims=True)) <-- this is the important line
        x = e_y / tt.sum(e_y, 0, keepdims=True) <-- this is the important line
        return floatX(x.T)
```

## Action to take

Assuming they indeed do the same things, we should probably **replace the current Theano implementation of the Softmax by the custom one we use in PyMC3**, thus getting a more robust implementation and taking care of the warning 🎉 

## Versions and main components

* Theano-PyMC 1.0.11.",enhancement good first issue help wanted,,2020-11-19 10:10:19,2021-12-13 18:33:46,"ricardoV94 mentioned 2020-11-19 10:10:20,ricardoV94 subscribed 2020-11-19 10:10:20,AlexAndorra labeled 2020-11-19 10:10:50,AlexAndorra labeled 2020-11-19 10:10:50,AlexAndorra labeled 2020-11-19 10:10:50,ricardoV94 mentioned 2020-11-21 10:26:48,ricardoV94 subscribed 2020-11-21 10:26:48,katosh mentioned 2020-11-21 14:38:02,katosh subscribed 2020-11-21 14:38:02,brandonwillard mentioned 2021-01-20 19:23:12,brandonwillard subscribed 2021-01-20 19:23:12,brandonwillard closed 2021-12-13 18:33:46",brandonwillard ricardoV94 dfm katosh AlexAndorra,7
164,188,Move JAX linker from sandbox,brandonwillard,"At some point soon, we should move the JAX linker (i.e. `theano.sandbox.jaxify` and `theano.sandbox.jax_linker`) out of `theano.sandbox` and into wherever we keep the linkers.  

Currently, the linkers and their modules don't have their own sub-package, but they probably should, so we'll need to sort that out before moving the JAX code.",enhancement help wanted refactor,,2020-11-20 02:54:49,2020-12-14 12:42:44,"brandonwillard labeled 2020-11-20 02:54:49,brandonwillard labeled 2020-11-20 02:54:49,brandonwillard labeled 2020-11-20 02:54:49,twiecki milestoned 2020-11-24 08:26:38,michaelosthege referenced 2020-12-12 16:06:09,michaelosthege referenced 2020-12-12 19:41:11,twiecki closed 2020-12-14 12:42:44,twiecki referenced 2020-12-14 12:42:45",michaelosthege twiecki brandonwillard,0
165,189,Cannot access Theano documentation,Sayam753,I just went to http://www.deeplearning.net/software/theano for theano documentation and it gives 404 error. Is this because of changes in Theano-PyMC?,,,2020-11-20 11:32:37,2020-11-20 12:46:56,"dfm mentioned 2020-11-20 11:52:31,dfm subscribed 2020-11-20 11:52:32,twiecki mentioned 2020-11-20 11:52:32,twiecki subscribed 2020-11-20 11:52:32,dfm mentioned 2020-11-20 12:33:29,dfm subscribed 2020-11-20 12:33:29,Sayam753 closed 2020-11-20 12:46:56,dfm mentioned 2020-11-20 12:46:56,dfm subscribed 2020-11-20 12:46:56",twiecki dfm Sayam753,6
166,190,Make merge_two_slices use the FunctionGraph framework,brandonwillard,"The optimization helper function `merge_two_slices` uses the function `pre_greedy_local_optimizer` multiple times, which results in multiple in-place updates to old and newly generated variables.  

It actually tries to avoid making changes to old variables that have a `FunctionGraph`, but this logic relies entirely on mutations to the variables themselves (i.e. the addition of a `Variable.fgraph` property), which is already very undesirable.
In general, the approach taken by `merge_two_slices` is unnecessary, needlessly complicated, works completely outside of the established graph processing framework (e.g. doesn't use `FunctionGraph`s properly&mdash;or at all), and makes the optimization code extremely brittle. ",enhancement graph rewriting,,2020-11-21 07:19:43,2020-11-23 17:54:30,"brandonwillard labeled 2020-11-21 07:19:43,brandonwillard labeled 2020-11-21 07:19:43,brandonwillard milestoned 2020-11-21 07:19:43,brandonwillard referenced 2020-11-21 08:12:14,brandonwillard referenced 2020-11-23 03:14:12,brandonwillard closed 2020-11-23 17:54:30",brandonwillard,0
168,192,Remove remaining side-effects and in-place updates during graph optimization,brandonwillard,"After #158 is merged, most of the unnecessary side-effect-generating and in-place updating logic that occurs during optimization will be gone; however, the `FunctionGraph` constructor's `outputs` argument is still a list that is **not** copied, so any changes made to it will alter the state of the `FunctionGraph`s that use it.  We need to fix that.
",enhancement important graph rewriting,brandonwillard,2020-11-23 01:45:21,2021-05-19 23:00:18,"brandonwillard labeled 2020-11-23 01:45:21,brandonwillard labeled 2020-11-23 01:45:21,brandonwillard labeled 2020-11-23 01:45:21,brandonwillard milestoned 2020-11-23 01:45:21,brandonwillard assigned 2020-11-23 01:45:21,michaelosthege demilestoned 2020-12-15 19:51:45,michaelosthege milestoned 2020-12-15 19:51:45,brandonwillard closed 2021-05-19 23:00:18",michaelosthege brandonwillard,1
173,198,`jax_funcify` fails on `AdvancedIncSubtensor1`,martiningram,"Hi Theano-PyMC devs,

Thanks for the fantastic work on the JAX backend, it's really exciting!

I realise it's very early days and I feel bad for even opening an issue given that you may all be well aware of this already, but I was trying to use the new JAX backend on the LKJ example:
https://github.com/pymc-devs/pymc3/blob/master/docs/source/notebooks/LKJ.ipynb
and I ran into an error that I thought you might like to know about.

All I have added to that notebook is:
```python
import pymc3.sampling_jax
with model:
    hierarchical_trace_jax = pm.sampling_jax.sample_numpyro_nuts(
        500, tune=500, target_accept=.9)
```

I've also attached the notebook in case that's easier: [LKJ.zip](https://github.com/pymc-devs/Theano-PyMC/files/5588060/LKJ.zip)

This is the error that appears:

```python
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
<ipython-input-15-8dae948c5188> in <module>
      3 with model:
      4     hierarchical_trace_jax = pm.sampling_jax.sample_numpyro_nuts(
----> 5         500, tune=500, target_accept=.9)

~/projects/experiments/pymc3_advi/pymc3/pymc3/sampling_jax.py in sample_numpyro_nuts(draws, tune, chains, target_accept, random_seed, model, progress_bar)
    132 
    133     fgraph = theano.gof.FunctionGraph(model.free_RVs, [model.logpt])
--> 134     fns = theano.sandbox.jaxify.jax_funcify(fgraph)
    135     logp_fn_jax = fns[0]
    136 

~/miniconda3/envs/pymc3/lib/python3.7/functools.py in wrapper(*args, **kw)
    838                             '1 positional argument')
    839 
--> 840         return dispatch(args[0].__class__)(*args, **kw)
    841 
    842     funcname = getattr(func, '__name__', 'singledispatch function')

~/miniconda3/envs/pymc3/lib/python3.7/site-packages/theano/sandbox/jaxify.py in jax_funcify_FunctionGraph(fgraph)
    663 
    664     out_nodes = [r.owner for r in fgraph.outputs if r.owner is not None]
--> 665     jax_funcs = [compose_jax_funcs(o, fgraph.inputs) for o in out_nodes]
    666 
    667     return jax_funcs

~/miniconda3/envs/pymc3/lib/python3.7/site-packages/theano/sandbox/jaxify.py in <listcomp>(.0)
    663 
    664     out_nodes = [r.owner for r in fgraph.outputs if r.owner is not None]
--> 665     jax_funcs = [compose_jax_funcs(o, fgraph.inputs) for o in out_nodes]
    666 
    667     return jax_funcs

~/miniconda3/envs/pymc3/lib/python3.7/site-packages/theano/sandbox/jaxify.py in compose_jax_funcs(out_node, fgraph_inputs, memo)
    152             # This input is the output of another node, so we need to
    153             # generate a JAX-able function for its subgraph
--> 154             input_f = compose_jax_funcs(i.owner, fgraph_inputs, memo)
    155 
    156             if i.owner.nout > 1:

~/miniconda3/envs/pymc3/lib/python3.7/site-packages/theano/sandbox/jaxify.py in compose_jax_funcs(out_node, fgraph_inputs, memo)
    152             # This input is the output of another node, so we need to
    153             # generate a JAX-able function for its subgraph
--> 154             input_f = compose_jax_funcs(i.owner, fgraph_inputs, memo)
    155 
    156             if i.owner.nout > 1:

~/miniconda3/envs/pymc3/lib/python3.7/site-packages/theano/sandbox/jaxify.py in compose_jax_funcs(out_node, fgraph_inputs, memo)
    152             # This input is the output of another node, so we need to
    153             # generate a JAX-able function for its subgraph
--> 154             input_f = compose_jax_funcs(i.owner, fgraph_inputs, memo)
    155 
    156             if i.owner.nout > 1:

~/miniconda3/envs/pymc3/lib/python3.7/site-packages/theano/sandbox/jaxify.py in compose_jax_funcs(out_node, fgraph_inputs, memo)
    152             # This input is the output of another node, so we need to
    153             # generate a JAX-able function for its subgraph
--> 154             input_f = compose_jax_funcs(i.owner, fgraph_inputs, memo)
    155 
    156             if i.owner.nout > 1:

~/miniconda3/envs/pymc3/lib/python3.7/site-packages/theano/sandbox/jaxify.py in compose_jax_funcs(out_node, fgraph_inputs, memo)
    152             # This input is the output of another node, so we need to
    153             # generate a JAX-able function for its subgraph
--> 154             input_f = compose_jax_funcs(i.owner, fgraph_inputs, memo)
    155 
    156             if i.owner.nout > 1:

~/miniconda3/envs/pymc3/lib/python3.7/site-packages/theano/sandbox/jaxify.py in compose_jax_funcs(out_node, fgraph_inputs, memo)
    152             # This input is the output of another node, so we need to
    153             # generate a JAX-able function for its subgraph
--> 154             input_f = compose_jax_funcs(i.owner, fgraph_inputs, memo)
    155 
    156             if i.owner.nout > 1:

~/miniconda3/envs/pymc3/lib/python3.7/site-packages/theano/sandbox/jaxify.py in compose_jax_funcs(out_node, fgraph_inputs, memo)
    152             # This input is the output of another node, so we need to
    153             # generate a JAX-able function for its subgraph
--> 154             input_f = compose_jax_funcs(i.owner, fgraph_inputs, memo)
    155 
    156             if i.owner.nout > 1:

~/miniconda3/envs/pymc3/lib/python3.7/site-packages/theano/sandbox/jaxify.py in compose_jax_funcs(out_node, fgraph_inputs, memo)
    152             # This input is the output of another node, so we need to
    153             # generate a JAX-able function for its subgraph
--> 154             input_f = compose_jax_funcs(i.owner, fgraph_inputs, memo)
    155 
    156             if i.owner.nout > 1:

~/miniconda3/envs/pymc3/lib/python3.7/site-packages/theano/sandbox/jaxify.py in compose_jax_funcs(out_node, fgraph_inputs, memo)
    152             # This input is the output of another node, so we need to
    153             # generate a JAX-able function for its subgraph
--> 154             input_f = compose_jax_funcs(i.owner, fgraph_inputs, memo)
    155 
    156             if i.owner.nout > 1:

~/miniconda3/envs/pymc3/lib/python3.7/site-packages/theano/sandbox/jaxify.py in compose_jax_funcs(out_node, fgraph_inputs, memo)
    152             # This input is the output of another node, so we need to
    153             # generate a JAX-able function for its subgraph
--> 154             input_f = compose_jax_funcs(i.owner, fgraph_inputs, memo)
    155 
    156             if i.owner.nout > 1:

~/miniconda3/envs/pymc3/lib/python3.7/site-packages/theano/sandbox/jaxify.py in compose_jax_funcs(out_node, fgraph_inputs, memo)
    152             # This input is the output of another node, so we need to
    153             # generate a JAX-able function for its subgraph
--> 154             input_f = compose_jax_funcs(i.owner, fgraph_inputs, memo)
    155 
    156             if i.owner.nout > 1:

~/miniconda3/envs/pymc3/lib/python3.7/site-packages/theano/sandbox/jaxify.py in compose_jax_funcs(out_node, fgraph_inputs, memo)
    152             # This input is the output of another node, so we need to
    153             # generate a JAX-able function for its subgraph
--> 154             input_f = compose_jax_funcs(i.owner, fgraph_inputs, memo)
    155 
    156             if i.owner.nout > 1:

~/miniconda3/envs/pymc3/lib/python3.7/site-packages/theano/sandbox/jaxify.py in compose_jax_funcs(out_node, fgraph_inputs, memo)
    152             # This input is the output of another node, so we need to
    153             # generate a JAX-able function for its subgraph
--> 154             input_f = compose_jax_funcs(i.owner, fgraph_inputs, memo)
    155 
    156             if i.owner.nout > 1:

~/miniconda3/envs/pymc3/lib/python3.7/site-packages/theano/sandbox/jaxify.py in compose_jax_funcs(out_node, fgraph_inputs, memo)
    118         return memo[out_node]
    119 
--> 120     jax_return_func = jax_funcify(out_node.op)
    121 
    122     # We create a list of JAX-able functions that produce the values of each

~/miniconda3/envs/pymc3/lib/python3.7/functools.py in wrapper(*args, **kw)
    838                             '1 positional argument')
    839 
--> 840         return dispatch(args[0].__class__)(*args, **kw)
    841 
    842     funcname = getattr(func, '__name__', 'singledispatch function')

~/miniconda3/envs/pymc3/lib/python3.7/site-packages/theano/sandbox/jaxify.py in jax_funcify_IncSubtensor(op)
    624 def jax_funcify_IncSubtensor(op):
    625 
--> 626     idx_list = op.idx_list
    627 
    628     if getattr(op, ""set_instead_of_inc"", False):

AttributeError: 'AdvancedIncSubtensor1' object has no attribute 'idx_list'
```
Once again, I hope this is helpful! This is using pymc3's `pymc3jax` branch, and the master branch of `Theano-PyMC`. Watermark is below.

**Please provide any additional information below.**

## Versions and main components

Python implementation: CPython
Python version       : 3.7.9
IPython version      : 7.19.0

arviz     : 0.10.0
seaborn   : 0.11.0
pymc3     : 3.9.3
matplotlib: 3.3.3
numpy     : 1.19.2

Watermark: 2.1.0",bug good first issue help wanted JAX,,2020-11-24 07:25:58,2020-11-24 20:49:19,"twiecki labeled 2020-11-24 08:03:47,twiecki labeled 2020-11-24 08:03:47,twiecki labeled 2020-11-24 08:03:47,twiecki labeled 2020-11-24 08:05:29,martiningram mentioned 2020-11-24 08:23:51,martiningram subscribed 2020-11-24 08:23:51,twiecki milestoned 2020-11-24 08:26:18,brandonwillard closed 2020-11-24 20:49:19",martiningram twiecki brandonwillard,2
176,202,Remove Python 2 style int division,michaelosthege,"The config setting `int_division` allows for Python 2 style integer division. Python 2 was EOL and we should stop supporting this.

The `int_division` config setting is accessed in 6 places of `basic.py`.
",good first issue refactor,,2020-11-29 12:03:54,2020-12-14 10:54:21,"michaelosthege labeled 2020-11-29 12:04:15,michaelosthege milestoned 2020-11-29 12:04:17,michaelosthege labeled 2020-11-29 12:04:26,michaelosthege closed 2020-12-14 10:54:22",michaelosthege,1
177,203,Use a more robust file/directory lock,brandonwillard,"The lock currently used for preventing race conditions during compilation&mdash;i.e. [`theano.gof.compilelock`](https://github.com/pymc-devs/Theano-PyMC/blob/55ef0e6b7380e062bdd09fcd7f820643f8521fb0/theano/gof/compilelock.py#L1)&mdash;appears to rely on a shared memory context (e.g. [here](https://github.com/pymc-devs/Theano-PyMC/blob/55ef0e6b7380e062bdd09fcd7f820643f8521fb0/theano/gof/compilelock.py#L65) and [here](https://github.com/pymc-devs/Theano-PyMC/blob/55ef0e6b7380e062bdd09fcd7f820643f8521fb0/theano/gof/compilelock.py#L71) via values set on [a function object](https://github.com/pymc-devs/Theano-PyMC/blob/55ef0e6b7380e062bdd09fcd7f820643f8521fb0/theano/gof/compilelock.py#L112)) and, thus, only works for the [""fork"" method](https://docs.python.org/3/library/multiprocessing.html#contexts-and-start-methods).

From local tests, it definitely seems like the spawn method doesn't work, since it produces a lot of cryptic errors and doesn't look like it's locking anything during compilation.

There are some external libraries with multi-platform support (e.g. [`filelock`](https://github.com/benediktschmitt/py-filelock)) that we should consider using instead.",bug enhancement good first issue help wanted important refactor C-backend,,2020-11-30 17:54:34,2021-01-02 20:43:04,"brandonwillard labeled 2020-11-30 18:02:47,brandonwillard labeled 2020-11-30 18:02:47,brandonwillard labeled 2020-11-30 18:02:47,brandonwillard labeled 2020-11-30 18:02:47,brandonwillard labeled 2020-11-30 18:02:47,brandonwillard labeled 2020-11-30 18:02:47,brandonwillard labeled 2020-11-30 18:02:47,brandonwillard milestoned 2020-11-30 18:02:51,michaelosthege demilestoned 2020-12-15 19:53:27,michaelosthege milestoned 2020-12-15 19:53:27,michaelosthege referenced 2020-12-28 20:01:06,brandonwillard referenced 2020-12-28 23:48:37,michaelosthege referenced 2020-12-29 11:21:02,michaelosthege referenced 2020-12-29 11:28:34,michaelosthege referenced 2020-12-29 11:38:15,michaelosthege referenced 2020-12-31 12:04:03,brandonwillard referenced 2020-12-31 20:42:29,brandonwillard referenced 2020-12-31 21:10:50,brandonwillard referenced 2020-12-31 23:34:09,michaelosthege referenced 2021-01-01 11:45:01,brandonwillard referenced 2021-01-02 18:36:27,brandonwillard closed 2021-01-02 20:43:05",michaelosthege brandonwillard,0
179,205,BLAS ldflags initialization warning string error,brandonwillard,@canyon289 found an issue in the process of determining the BLAS ldflags.  Apparently [an old uncovered change](https://github.com/pymc-devs/Theano-PyMC/commit/4ec4edaa35c96af0943fddeefcfe03fc75b6be90) stores a single `logging` warning string to print when the `mkl` libraries aren't present (i.e. [here](https://github.com/pymc-devs/Theano-PyMC/blob/d660ea780d3d92725609cdb4906351c63fb7ec20/theano/configdefaults.py#L1772)).  Recent f-string updates conflict with this odd scheme and cause the error.,bug,brandonwillard,2020-12-02 00:20:24,2020-12-02 04:03:41,"brandonwillard labeled 2020-12-02 00:20:24,canyon289 mentioned 2020-12-02 00:20:24,canyon289 subscribed 2020-12-02 00:20:24,brandonwillard assigned 2020-12-02 00:20:24,brandonwillard renamed 2020-12-02 00:20:45,brandonwillard closed 2020-12-02 04:03:41",canyon289 brandonwillard,0
183,210,Pointer to Theano docs in README.md is out-of-date,rpgoldman,"Points to docs hosted on `deeplearning.net`, but those seem to be gone now.",documentation,,2020-12-05 17:27:43,2020-12-05 19:32:32,"brandonwillard connected 2020-12-05 19:30:42,brandonwillard closed 2020-12-05 19:32:32,brandonwillard labeled 2020-12-05 21:18:55",rpgoldman brandonwillard,0
188,215,Add Theano JAX Op for batched_dot,skoval,"I have a multi-level model with a multivariate outcome and group effects for multiple covariates. I constructed the observation-level effects (`alpha` with shape (n, p, d)) and used `theano.tensor.batched_dot` with the model matrix `X`, shape (n, p), to get the observation-level means `mu` of shape (n, d). This returns the following error when running the jax NUTS sampler:

```python
NotImplementedError: No JAX conversion for the given `Op`: BatchedDot
```

Would it be possible to add a JAX Op for batched_dot? If not, could someone suggest an alternative jax-compatible operation?

## Versions and main components

* PyMC3 Version: v3.9.3
* Theano Version: 1.0.11
* Python Version: 3.7.8
* Operating system: GNU/Linux 10
* How did you install PyMC3: (conda/pip) pip install pymc3jax
",good first issue JAX,,2020-12-07 06:57:34,2020-12-12 18:24:34,"twiecki transferred 2020-12-07 08:18:21,twiecki labeled 2020-12-07 08:18:32,twiecki labeled 2020-12-07 08:18:32,skoval mentioned 2020-12-07 08:24:47,skoval subscribed 2020-12-07 08:24:47,twiecki closed 2020-12-12 18:24:34",skoval junpenglao twiecki,3
189,216,JAX error when using pymc3.Dirichlet: 'list' object is not callable,ricardoV94,"I was testing out the new jax backend in one of the models I am working with recently, and it fails with the error message in the title. I tried several permutations of explicit shape specification, but the error happens all the time. Any ideas?


## Description of your problem

**Please provide a minimal, self-contained, and reproducible example.**
```python
import numpy as np
import pymc3 as pm
import pymc3.sampling_jax

with pm.Model() as m:
    rho_pop = pm.Dirichlet('rho_pop', np.ones(3))

with m:
    trace = pm.sampling_jax.sample_numpyro_nuts(2000, tune=2000, target_accept=0.9)
```

**This is the end of the trace (truncated because it is huge)**
```python
~/Documents/Projects/pymc3-jax/lib/python3.8/site-packages/theano/sandbox/jaxify.py in <listcomp>(.0)
    153 
    154         def jax_func(*inputs):
--> 155             func_args = [fn(*inputs) for fn in input_funcs]
    156             return return_func(*func_args)
    157 

~/Documents/Projects/pymc3-jax/lib/python3.8/site-packages/theano/sandbox/jaxify.py in jax_func(*inputs)
    153 
    154         def jax_func(*inputs):
--> 155             func_args = [fn(*inputs) for fn in input_funcs]
    156             return return_func(*func_args)
    157 

~/Documents/Projects/pymc3-jax/lib/python3.8/site-packages/theano/sandbox/jaxify.py in <listcomp>(.0)
    153 
    154         def jax_func(*inputs):
--> 155             func_args = [fn(*inputs) for fn in input_funcs]
    156             return return_func(*func_args)
    157 

TypeError: 'list' object is not callable


```

**Please provide any additional information below.**

## Versions and main components

* PyMC3 Version: 3.10.0
* Theano Version: '1.0.11' (pymc)
* Python Version: 3.8.5
* Operating system: Ubuntu 20.04
* How did you install PyMC3: pip
",bug good first issue help wanted JAX,,2020-12-07 17:17:02,2020-12-15 20:01:52,"ricardoV94 renamed 2020-12-07 17:23:34,twiecki transferred 2020-12-07 19:48:13,twiecki labeled 2020-12-07 19:48:29,twiecki labeled 2020-12-07 19:48:29,twiecki labeled 2020-12-07 19:48:29,twiecki labeled 2020-12-07 19:48:34,twiecki renamed 2020-12-07 19:49:06,ricardoV94 mentioned 2020-12-08 00:58:17,ricardoV94 subscribed 2020-12-08 00:58:17,ricardoV94 mentioned 2020-12-08 06:41:14,ricardoV94 subscribed 2020-12-08 06:41:14,dfm mentioned 2020-12-08 20:44:47,dfm subscribed 2020-12-08 20:44:47,michaelosthege closed 2020-12-15 20:01:52,ricardoV94 mentioned 2020-12-15 20:01:52,ricardoV94 subscribed 2020-12-15 20:01:52",brandonwillard ricardoV94 dfm michaelosthege twiecki,6
193,220,Implement a Type for the new NumPy RNG framework,brandonwillard,"We need to implement a `Type` and shared `Variable` class for [NumPy's new RNGs](https://numpy.org/doc/stable/reference/random/new-or-different.html#new-or-different) (i.e. [`Generator`](https://numpy.org/doc/stable/reference/random/generator.html#numpy.random.Generator)).  There are apparently [some performance improvements](https://numpy.org/doc/stable/reference/random/performance.html) in these new classes, so this task is not merely for compatibility.

Otherwise, these implementations are probably only slight deviations from the [existing `RandomStateType`](https://github.com/pymc-devs/aesara/blob/36161c3c0da25adef72b6abe00f5ffd37dfe7ade/aesara/tensor/random/type.py#L9) and [` RandomStateSharedVariable`](https://github.com/pymc-devs/aesara/blob/36161c3c0da25adef72b6abe00f5ffd37dfe7ade/aesara/tensor/random/var.py#L9), so there might not be a lot of work involved.",enhancement help wanted important random variables,kc611,2020-12-14 00:48:36,2021-06-25 16:47:04,"brandonwillard labeled 2020-12-14 00:49:00,brandonwillard labeled 2020-12-14 00:49:00,michaelosthege milestoned 2020-12-15 20:02:05,brandonwillard labeled 2021-02-02 19:06:58,brandonwillard renamed 2021-02-02 19:07:53,brandonwillard labeled 2021-05-19 16:23:02,brandonwillard connected 2021-05-19 16:24:10,kc611 assigned 2021-05-19 22:52:44,brandonwillard closed 2021-06-25 16:47:04",kc611 michaelosthege brandonwillard,0
195,223,Move the remaining Linkers to theano.link,brandonwillard,"Now that we have a `theano.link` sub-package, we should move the other `Linker`s&mdash;and perhaps `theano.gof.link`&mdash;there, as well.",help wanted important refactor,,2020-12-15 00:37:58,2020-12-22 02:33:59,"brandonwillard labeled 2020-12-15 00:37:58,brandonwillard labeled 2020-12-15 00:37:58,brandonwillard labeled 2020-12-15 00:37:58,brandonwillard milestoned 2020-12-15 00:37:58,michaelosthege demilestoned 2020-12-15 20:02:49,michaelosthege milestoned 2020-12-15 20:02:49,michaelosthege referenced 2020-12-19 14:48:33,brandonwillard referenced 2020-12-20 21:46:54,brandonwillard referenced 2020-12-21 01:49:23,brandonwillard referenced 2020-12-21 03:01:32,brandonwillard closed 2020-12-22 02:33:59",michaelosthege brandonwillard,0
196,224,Return NaN in C implementations of SciPy Ops,ricardoV94,"This is related to https://github.com/pymc-devs/pymc3/issues/4340

I am trying to use the `GammaIncc` Op, which makes use of the C code function `GammaQ`, which begins with an assertion for the two parameters. This is problematic when using the tensor version of the scalar op (via @_scal_elemwise), as I don't see how to gracefully avoid the assertion error.

To make this concrete, gammaincc is being used in the logcdf method of the pymc3 InverseGamma distribution. It is wrapped inside a `bound` which seems to lazy evaluate when working with scalars, but not with tensors. This allows it to gracefully return -np.inf when evaluated with invalid scalar arguments, but not when using a tensor/array.

The `scipy.special.gammaincc(x, k)` counterpart does not have this issue.

Does anyone know a workaround to avoid this issue when working with tensors?

```python
pm.InverseGamma.dist(alpha=-1, beta=5).logcdf(1).eval()  # returns -np.inf
pm.InverseGamma.dist(alpha=-1, beta=5).logcdf(np.array([1])).eval()  # fails assertion and exits process
```

```python
python3: /home/ricardo/.theano/compiledir_Linux-5.4--generic-x86_64-with-glibc2.29-x86_64-3.8.5-64/tmpf82kd0bh/mod.cpp:240: double GammaQ(double, double): Assertion `(n > 0) && (x >= 0)' failed.
Aborted (core dumped)

```

https://github.com/pymc-devs/pymc3/blob/043129243436b875a9ab18180549491c4678b7c6/pymc3/distributions/continuous.py#L2654-L2659

```python
return bound(
    tt.log(tt.gammaincc(alpha, beta / value)),
    value >= 0,
    alpha > 0,
    beta > 0,
)
```

https://github.com/pymc-devs/Theano-PyMC/blob/2601e7acf8442422cab19db5bd029a5581f0bdec/theano/tensor/basic.py#L2529-L2531

https://github.com/pymc-devs/Theano-PyMC/blob/819122e66bcd089b2a3f472542800feead0bf755/theano/scalar/basic_scipy.py#L598-L639

https://github.com/pymc-devs/Theano-PyMC/blob/819122e66bcd089b2a3f472542800feead0bf755/theano/scalar/c_code/gamma.c#L227-L233
",bug help wanted C-backend backend compatibility,,2020-12-15 09:40:04,2021-02-08 21:32:30,"michaelosthege labeled 2020-12-15 19:54:55,michaelosthege labeled 2020-12-15 19:54:55,michaelosthege labeled 2020-12-15 19:54:55,brandonwillard renamed 2020-12-21 02:39:48,brandonwillard labeled 2020-12-21 02:41:06,brandonwillard closed 2021-02-08 21:32:30",ricardoV94 michaelosthege dfm brandonwillard,1
197,226,MacOS Theano Error ---- ['MRG_RandomStreams' from 'theano.sandbox.rng_mrg'],CloudChaoszero,"## Description of your problem or feature request

Hey team, hope you are doing well & are safe! :) 
### TL;DR
 I receive an error from Theano, typically after creating new conda environment, and running examples

The latest error:
```python

ImportError: cannot import name 'MRG_RandomStreams' from 'theano.sandbox.rng_mrg' (/anaconda3/envs/temp/lib/python3.8/site-packages/theano/sandbox/rng_mrg.py)
```
### 1.New Conda environment

```
conda create -n temp python=3.8 -y
conda activate temp
conda install -c conda-forge pymc3
```

I first installed Pymc3 through conda forge
`conda install -c conda-forge pymc3`

After, I was faced with an error quite similar to #127, when I ran this [api quickstart] piece of code in a Notebook:

### 2. First Import and Runthrough

#### Code
```python
import arviz as az
import matplotlib.pyplot as plt
import numpy as np
import pymc3 as pm
import theano.tensor as tt
import warnings
warnings.simplefilter(action=""ignore"", category=FutureWarning)


%config InlineBackend.figure_format = 'retina'
az.style.use('arviz-darkgrid')
print('Running on PyMC3 v{}'.format(pm.__version__))
print('Running on ArviZ v{}'.format(az.__version__))

with pm.Model() as model:
    mu = pm.Normal(""mu"", mu=0, sigma=1)
    obs = pm.Normal(""obs"", mu=mu, sigma=1, observed=np.random.randn(100))
```

#### Error

```python
    thunk, module = self.cthunk_factory(
  File ""/Users/dforeman/src/pymc-devs/Theano-PyMC/theano/gof/cc.py"", line 1720, in cthunk_factory
    module = get_module_cache().module_from_key(
  File ""/Users/dforeman/src/pymc-devs/Theano-PyMC/theano/gof/cmodule.py"", line 1229, in module_from_key
    module = lnk.compile_cmodule(location)
  File ""/Users/dforeman/src/pymc-devs/Theano-PyMC/theano/gof/cc.py"", line 1610, in compile_cmodule
    module = c_compiler.compile_str(
  File ""/Users/dforeman/src/pymc-devs/Theano-PyMC/theano/gof/cmodule.py"", line 2567, in compile_str
    raise Exception(
Exception: ('The following error happened while compiling the node', Elemwise{add,no_inplace}(InplaceDimShuffle{x,0}.0, <TensorType(float64, matrix)>), '\\n', ""Compilation failed (return status=1): /Users/dforeman/.theano/compiledir_macOS-10.15.7-x86_64-i386-64bit-i386-3.8.5-64/tmpsatm5hgs/mod.cpp:524:27: error: non-constant-expression cannot be narrowed from type 'npy_intp' (aka 'long') to 'int' in initializer list [-Wc++11-narrowing].     int init_totals[2] = {V5_n0, V3_n1};.                           ^~~~~. /Users/dforeman/.theano/compiledir_macOS-10.15.7-x86_64-i386-64bit-i386-3.8.5-64/tmpsatm5hgs/mod.cpp:524:27: note: insert an explicit cast to silence this issue.     int init_totals[2] = {V5_n0, V3_n1};.                           ^~~~~.                           static_cast<int>( ). /Users/dforeman/.theano/compiledir_macOS-10.15.7-x86_64-i386-64bit-i386-3.8.5-64/tmpsatm5hgs/mod.cpp:524:34: error: non-constant-expression cannot be narrowed from type 'npy_intp' (aka 'long') to 'int' in initializer list [-Wc++11-narrowing].     int init_totals[2] = {V5_n0, V3_n1};.                                  ^~~~~. /Users/dforeman/.theano/compiledir_macOS-10.15.7-x86_64-i386-64bit-i386-3.8.5-64/tmpsatm5hgs/mod.cpp:524:34: note: insert an explicit cast to silence this issue.     int init_totals[2] = {V5_n0, V3_n1};.                                  ^~~~~.                                  static_cast<int>( ). /Users/dforeman/.theano/compiledir_macOS-10.15.7-x86_64-i386-64bit-i386-3.8.5-64/tmpsatm5hgs/mod.cpp:536:12: error: non-constant-expression cannot be narrowed from type 'ssize_t' (aka 'long') to 'int' in initializer list [-Wc++11-narrowing].         0, V3_stride1, .            ^~~~~~~~~~. /Users/dforeman/.theano/compiledir_macOS-10.15.7-x86_64-i386-64bit-i386-3.8.5-64/tmpsatm5hgs/mod.cpp:536:12: note: insert an explicit cast to silence this issue.         0, V3_stride1, .            ^~~~~~~~~~.            static_cast<int>( ). /Users/dforeman/.theano/compiledir_macOS-10.15.7-x86_64-i386-64bit-i386-3.8.5-64/tmpsatm5hgs/mod.cpp:537:1: error: non-constant-expression cannot be narrowed from type 'ssize_t' (aka 'long') to 'int' in initializer list [-Wc++11-narrowing]. V5_stride0, V5_stride1, . ^~~~~~~~~~. /Users/dforeman/.theano/compiledir_macOS-10.15.7-x86_64-i386-64bit-i386-3.8.5-64/tmpsatm5hgs/mod.cpp:537:1: note: insert an explicit cast to silence this issue. V5_stride0, V5_stride1, . ^~~~~~~~~~. static_cast<int>( ). /Users/dforeman/.theano/compiledir_macOS-10.15.7-x86_64-i386-64bit-i386-3.8.5-64/tmpsatm5hgs/mod.cpp:537:13: error: non-constant-expression cannot be narrowed from type 'ssize_t' (aka 'long') to 'int' in initializer list [-Wc++11-narrowing]. V5_stride0, V5_stride1, .             ^~~~~~~~~~. /Users/dforeman/.theano/compiledir_macOS-10.15.7-x86_64-i386-64bit-i386-3.8.5-64/tmpsatm5hgs/mod.cpp:537:13: note: insert an explicit cast to silence this issue. V5_stride0, V5_stride1, .             ^~~~~~~~~~.             static_cast<int>( ). /Users/dforeman/.theano/compiledir_macOS-10.15.7-x86_64-i386-64bit-i386-3.8.5-64/tmpsatm5hgs/mod.cpp:538:1: error: non-constant-expression cannot be narrowed from type 'ssize_t' (aka 'long') to 'int' in initializer list [-Wc++11-narrowing]. V1_stride0, V1_stride1. ^~~~~~~~~~. /Users/dforeman/.theano/compiledir_macOS-10.15.7-x86_64-i386-64bit-i386-3.8.5-64/tmpsatm5hgs/mod.cpp:538:1: note: insert an explicit cast to silence this issue. V1_stride0, V1_stride1. ^~~~~~~~~~. static_cast<int>( ). /Users/dforeman/.theano/compiledir_macOS-10.15.7-x86_64-i386-64bit-i386-3.8.5-64/tmpsatm5hgs/mod.cpp:538:13: error: non-constant-expression cannot be narrowed from type 'ssize_t' (aka 'long') to 'int' in initializer list [-Wc++11-narrowing]. V1_stride0, V1_stride1.             ^~~~~~~~~~. /Users/dforeman/.theano/compiledir_macOS-10.15.7-x86_64-i386-64bit-i386-3.8.5-64/tmpsatm5hgs/mod.cpp:538:13: note: insert an explicit cast to silence this issue. V1_stride0, V1_stride1.             ^~~~~~~~~~.             static_cast<int>( ). 7 errors generated.. "", '[Elemwise{add,no_inplace}(<TensorType(float64, row)>, <TensorType(float64, matrix)>)]')
```

### 3. Theano Update

After seeing the latest pushes, I went ahead and only updated Theano PyMC
`pip install git+https://github.com/pymc-devs/Theano-PyMC`

Running the same code, I got this error from `import pymc3 as pm`

#### Code

#### Error
```python
/anaconda3/envs/temp/lib/python3.8/site-packages/pymc3/theanof.py in <module>
     21 from theano.gof import Op
     22 from theano.gof.graph import inputs
---> 23 from theano.sandbox.rng_mrg import MRG_RandomStreams
     24 
     25 from .blocking import ArrayOrdering

ImportError: cannot import name 'MRG_RandomStreams' from 'theano.sandbox.rng_mrg' (/anaconda3/envs/temp/lib/python3.8/site-packages/theano/sandbox/rng_mrg.py)
```
### 4. (See conda list down in comments)

## Versions and main components

* Theano version: theano-pymc  1.0.13+7.g9f7cf7d09
* Theano config (`python -c ""import theano; print(theano.config)""`): Unknown
* Python version: 3.8: 3.8.5
* Operating system: [Clang 10.0.0 ] :: Anaconda, Inc. on darwin
* How did you install Theano: (conda/pip):
  * First it was from Pymc3, but then I did `pip install git+https://github.com/pymc-devs/Theano-PyMC`",,,2020-12-16 07:46:01,2020-12-16 21:28:45,"michaelosthege mentioned 2020-12-16 18:21:18,michaelosthege subscribed 2020-12-16 18:21:18,brandonwillard closed 2020-12-16 21:28:45",michaelosthege CloudChaoszero brandonwillard,5
200,229,Add type hints to signatures of basic linker types,michaelosthege,"After #228 is merged - or even on the same branch we urgently need type <s>hints</s> <s>annotations</s> hints on some core classes:
+ `Linker`, `LocalLinker`, `PerformLinker`
+ `VM`
+ `FunctionGraph`

And also on the related helper functions such as `streamline` or `map_storage`.

The description under this links is a good entry point for starters: https://theano-pymc.readthedocs.io/en/latest/extending/pipeline.html",enhancement good first issue important,,2020-12-16 23:26:28,2021-02-13 12:24:58,"michaelosthege labeled 2020-12-16 23:26:28,michaelosthege labeled 2020-12-16 23:26:28,michaelosthege labeled 2020-12-16 23:26:28,michaelosthege milestoned 2020-12-16 23:26:28,michaelosthege demilestoned 2020-12-27 11:35:31,michaelosthege milestoned 2020-12-27 11:35:31,michaelosthege renamed 2021-01-16 17:13:15,michaelosthege closed 2021-02-13 12:24:58",michaelosthege LegrandNico brandonwillard,4
201,230,Add tutorial on how to write new JAX implementations for new OPs,twiecki,"To make it easier for new contributors to add JAX implementations, we should have a small NB that gives an example.",documentation JAX,canyon289,2020-12-17 20:58:12,2021-06-27 08:23:44,"twiecki labeled 2020-12-17 20:58:12,canyon289 assigned 2020-12-28 05:13:17,brandonwillard connected 2021-03-11 17:23:43,brandonwillard labeled 2021-03-11 17:25:51,twiecki closed 2021-06-27 08:23:44",ricardoV94 canyon289 twiecki brandonwillard,2
202,231,Warning floods,rpgoldman,"## Description of your problem or feature request

Running one of my models, I see almost 50 lines of 

```
WARNING (theano.tensor.opt): The Op erfcx does not provide a C implementation. As well as being potentially slow, this also disables loop fusion.
```

As far as I can tell, this is generated by one of these two code blocks: 

https://github.com/pymc-devs/Theano-PyMC/blob/9f7cf7d09fb2139ba15a06e385dae50527614f83/theano/tensor/opt.py#L7640 

and/or 

https://github.com/pymc-devs/Theano-PyMC/blob/9f7cf7d09fb2139ba15a06e385dae50527614f83/theano/tensor/opt.py#L7572

Suggestion: maintain a set of op warnings that have already been issued, and check this set before emitting the warning, something like:

```
if i.owner.op.scalar_op not in NO_CFUNC_WARNS:
  NO_CFUNC_WARNS.add(i.owner.op.scalar_op)
  ..... emit warning .....
```

The warning is emitted by the logger, not by the warning function, so I don't see any obvious way to throttle it to avoid repeated warnings.

Interestingly, I see this message when calling `arviz.from_pymc3()` to translate a PyMC3 trace from `MultiTrace` to Arviz `InferenceData`. That seems quite surprising, but we might have to recreate some PyMC3 data structures to make that translation possible.

## Versions and main components

* Theano version: Theano-PyMC==1.0.11
* Theano config: see below
* Python version: 3.8
* Operating system: MacOS Catalina
* How did you install Theano: pip


## Theano config:
```
floatX (('float64', 'float32', 'float16')) 
    Doc:  Default floating-point precision for python casts.

Note: float16 support is experimental, use at your own risk.
    Value:  float64

warn_float64 (('ignore', 'warn', 'raise', 'pdb')) 
    Doc:  Do an action when a tensor variable with float64 dtype is created. They can't be run on the GPU with the current(old) gpu back-end and are slow with gamer GPUs.
    Value:  ignore

pickle_test_value (<function BoolParam.<locals>.booltype at 0x13c537940>) 
    Doc:  Dump test values while pickling model. If True, test values will be dumped with model.
    Value:  True

cast_policy (('custom', 'numpy+floatX')) 
    Doc:  Rules for implicit type casting
    Value:  custom

int_division (('int', 'raise', 'floatX')) 
    Doc:  What to do when one computes x / y, where both x and y are of integer types
    Value:  int

deterministic (('default', 'more')) 
    Doc:  If `more`, sometimes we will select some implementation that are more deterministic, but slower. In particular, on the GPU, we will avoid using AtomicAdd. Sometimes we will still use non-deterministic implementaion, e.g. when we do not have a GPU implementation that is deterministic. Also see the dnn.conv.algo* flags to cover more cases.
    Value:  default

device (cpu, opencl*, cuda*) 
    Doc:  Default device for computations. If cuda* or opencl*, change thedefault to try to move computation to the GPU. Do not use upper caseletters, only lower case even if NVIDIA uses capital letters.
    Value:  cpu

init_gpu_device (, opencl*, cuda*) 
    Doc:  Initialize the gpu device to use, works only if device=cpu. Unlike 'device', setting this option will NOT move computations, nor shared variables, to the specified GPU. It can be used to run GPU-specific tests on a particular GPU.
    Value:  

force_device (<function BoolParam.<locals>.booltype at 0x13c537ee0>) 
    Doc:  Raise an error if we can't use the specified device
    Value:  False

conv.assert_shape (<function BoolParam.<locals>.booltype at 0x13c5390d0>) 
    Doc:  If True, AbstractConv* ops will verify that user-provided shapes match the runtime shapes (debugging option, may slow down compilation)
    Value:  False

print_global_stats (<function BoolParam.<locals>.booltype at 0x13c539280>) 
    Doc:  Print some global statistics (time spent) at the end
    Value:  False

<theano.configdefaults.ContextsParam object at 0x13c52ff40>
    Doc:  
    Context map for multi-gpu operation. Format is a
    semicolon-separated list of names and device names in the
    'name->dev_name' format. An example that would map name 'test' to
    device 'cuda0' and name 'test2' to device 'opencl0:0' follows:
    ""test->cuda0;test2->opencl0:0"".

    Invalid context names are 'cpu', 'cuda*' and 'opencl*'
    
    Value:  

print_active_device (<function BoolParam.<locals>.booltype at 0x13c539550>) 
    Doc:  Print active device at when the GPU device is initialized.
    Value:  True

<theano.configparser.ConfigParam object at 0x13c53c0a0>
    Doc:  This flag is deprecated and will be removed in next Theano release.
    Value:  False

gpuarray.preallocate (<class 'float'>) 
    Doc:  If negative it disables the allocation cache. If
             between 0 and 1 it enables the allocation cache and
             preallocates that fraction of the total GPU memory.  If 1
             or greater it will preallocate that amount of memory (in
             megabytes).
    Value:  0.0

gpuarray.sched (('default', 'multi', 'single')) 
    Doc:  The sched parameter passed for context creation to pygpu.
                With CUDA, using ""multi"" is equivalent to using the parameter
                cudaDeviceScheduleBlockingSync. This is useful to lower the
                CPU overhead when waiting for GPU. One user found that it
                speeds up his other processes that was doing data augmentation.
             
    Value:  default

gpuarray.single_stream (<function BoolParam.<locals>.booltype at 0x13c5398b0>) 
    Doc:  
             If your computations are mostly lots of small elements,
             using single-stream will avoid the synchronization
             overhead and usually be faster.  For larger elements it
             does not make a difference yet.  In the future when true
             multi-stream is enabled in libgpuarray, this may change.
             If you want to make sure to have optimal performance,
             check both options.
             
    Value:  True

cuda.root (<class 'str'>) 
    Doc:  Location of the cuda installation
    Value:  

cuda.include_path (<class 'str'>) 
    Doc:  Location of the cuda includes
    Value:  

<theano.configparser.ConfigParam object at 0x13c53c520>
    Doc:  This flag is deprecated; use dnn.conv.algo_fwd.
    Value:  True

<theano.configparser.ConfigParam object at 0x13c53c670>
    Doc:  This flag is deprecated; use `dnn.conv.algo_bwd_filter` and `dnn.conv.algo_bwd_data` instead.
    Value:  True

<theano.configparser.ConfigParam object at 0x13c53c6d0>
    Doc:  This flag is deprecated; use dnn.conv.algo_bwd_data and dnn.conv.algo_bwd_filter.
    Value:  True

dnn.conv.algo_fwd (('small', 'none', 'large', 'fft', 'fft_tiling', 'winograd', 'winograd_non_fused', 'guess_once', 'guess_on_shape_change', 'time_once', 'time_on_shape_change')) 
    Doc:  Default implementation to use for cuDNN forward convolution.
    Value:  small

dnn.conv.algo_bwd_data (('none', 'deterministic', 'fft', 'fft_tiling', 'winograd', 'winograd_non_fused', 'guess_once', 'guess_on_shape_change', 'time_once', 'time_on_shape_change')) 
    Doc:  Default implementation to use for cuDNN backward convolution to get the gradients of the convolution with regard to the inputs.
    Value:  none

dnn.conv.algo_bwd_filter (('none', 'deterministic', 'fft', 'small', 'winograd_non_fused', 'fft_tiling', 'guess_once', 'guess_on_shape_change', 'time_once', 'time_on_shape_change')) 
    Doc:  Default implementation to use for cuDNN backward convolution to get the gradients of the convolution with regard to the filters.
    Value:  none

dnn.conv.precision (('as_input_f32', 'as_input', 'float16', 'float32', 'float64')) 
    Doc:  Default data precision to use for the computation in cuDNN convolutions (defaults to the same dtype as the inputs of the convolutions, or float32 if inputs are float16).
    Value:  as_input_f32

dnn.base_path (<class 'str'>) 
    Doc:  Install location of cuDNN.
    Value:  

dnn.include_path (<class 'str'>) 
    Doc:  Location of the cudnn header
    Value:  

dnn.library_path (<class 'str'>) 
    Doc:  Location of the cudnn link library.
    Value:  

dnn.bin_path (<class 'str'>) 
    Doc:  Location of the cuDNN load library (on non-windows platforms, this is the same as dnn.library_path)
    Value:  

dnn.enabled (('auto', 'True', 'False', 'no_check')) 
    Doc:  'auto', use cuDNN if available, but silently fall back to not using it if not present. If True and cuDNN can not be used, raise an error. If False, disable cudnn even if present. If no_check, assume present and the version between header and library match (so less compilation at context init)
    Value:  auto

magma.include_path (<class 'str'>) 
    Doc:  Location of the magma header
    Value:  

magma.library_path (<class 'str'>) 
    Doc:  Location of the magma library
    Value:  

magma.enabled (<function BoolParam.<locals>.booltype at 0x13c541700>) 
    Doc:   If True, use magma for matrix computation. If False, disable magma
    Value:  False

assert_no_cpu_op (('ignore', 'warn', 'raise', 'pdb')) 
    Doc:  Raise an error/warning if there is a CPU op in the computational graph.
    Value:  ignore

<theano.configparser.ConfigParam object at 0x13c53cfa0>
    Doc:  Default compilation mode
    Value:  Mode

cxx (<class 'str'>) 
    Doc:  The C++ compiler to use. Currently only g++ is supported, but supporting additional compilers should not be too difficult. If it is empty, no C++ code is compiled.
    Value:  /usr/bin/clang++

linker (('cvm', 'c|py', 'py', 'c', 'c|py_nogc', 'vm', 'vm_nogc', 'cvm_nogc')) 
    Doc:  Default linker used if the theano flags mode is Mode
    Value:  cvm

allow_gc (<function BoolParam.<locals>.booltype at 0x13c54c820>) 
    Doc:  Do we default to delete intermediate results during Theano function calls? Doing so lowers the memory requirement, but asks that we reallocate memory at the next function call. This is implemented for the default linker, but may not work for all linkers.
    Value:  True

optimizer (('o4', 'o3', 'o2', 'o1', 'unsafe', 'fast_run', 'fast_compile', 'merge', 'None')) 
    Doc:  Default optimizer. If not None, will use this optimizer with the Mode
    Value:  o4

optimizer_verbose (<function BoolParam.<locals>.booltype at 0x13c54ca60>) 
    Doc:  If True, we print all optimization being applied
    Value:  False

on_opt_error (('warn', 'raise', 'pdb', 'ignore')) 
    Doc:  What to do when an optimization crashes: warn and skip it, raise the exception, or fall into the pdb debugger.
    Value:  warn

nocleanup (<function BoolParam.<locals>.booltype at 0x13c54cca0>) 
    Doc:  Suppress the deletion of code files that did not compile cleanly
    Value:  False

on_unused_input (('raise', 'warn', 'ignore')) 
    Doc:  What to do if a variable in the 'inputs' list of  theano.function() is not used in the graph.
    Value:  raise

tensor.cmp_sloppy (<class 'int'>) 
    Doc:  Relax tensor._allclose (0) not at all, (1) a bit, (2) more
    Value:  0

tensor.local_elemwise_fusion (<function BoolParam.<locals>.booltype at 0x13c550040>) 
    Doc:  Enable or not in fast_run mode(fast_run optimization) the elemwise fusion optimization
    Value:  True

gpu.local_elemwise_fusion (<function BoolParam.<locals>.booltype at 0x13c5501f0>) 
    Doc:  Enable or not in fast_run mode(fast_run optimization) the gpu elemwise fusion optimization
    Value:  True

lib.amdlibm (<function BoolParam.<locals>.booltype at 0x13c5503a0>) 
    Doc:  Use amd's amdlibm numerical library
    Value:  False

gpuelemwise.sync (<function BoolParam.<locals>.booltype at 0x13c550550>) 
    Doc:  when true, wait that the gpu fct finished and check it error code.
    Value:  True

traceback.limit (<class 'int'>) 
    Doc:  The number of stack to trace. -1 mean all.
    Value:  8

traceback.compile_limit (<class 'int'>) 
    Doc:  The number of stack to trace to keep during compilation. -1 mean all. If greater then 0, will also make us save Theano internal stack trace.
    Value:  0

experimental.unpickle_gpu_on_cpu (<function BoolParam.<locals>.booltype at 0x13c550820>) 
    Doc:  Allow unpickling of pickled GpuArrays as numpy.ndarrays.This is useful, if you want to open a GpuArray without having cuda installed.If you have cuda installed, this will force unpickling tobe done on the cpu to numpy.ndarray.Please be aware that this may get you access to the data,however, trying to unpicke gpu functions will not succeed.This flag is experimental and may be removed any time, whengpu<>cpu transparency is solved.
    Value:  False

numpy.seterr_all (('ignore', 'warn', 'raise', 'call', 'print', 'log', 'None')) 
    Doc:  (""Sets numpy's behaviour for floating-point errors, "", ""see numpy.seterr. 'None' means not to change numpy's default, which can be different for different numpy releases. This flag sets the default behaviour for all kinds of floating-point errors, its effect can be overriden for specific errors by the following flags: seterr_divide, seterr_over, seterr_under and seterr_invalid."")
    Value:  ignore

numpy.seterr_divide (('None', 'ignore', 'warn', 'raise', 'call', 'print', 'log')) 
    Doc:  Sets numpy's behavior for division by zero, see numpy.seterr. 'None' means using the default, defined by numpy.seterr_all.
    Value:  None

numpy.seterr_over (('None', 'ignore', 'warn', 'raise', 'call', 'print', 'log')) 
    Doc:  Sets numpy's behavior for floating-point overflow, see numpy.seterr. 'None' means using the default, defined by numpy.seterr_all.
    Value:  None

numpy.seterr_under (('None', 'ignore', 'warn', 'raise', 'call', 'print', 'log')) 
    Doc:  Sets numpy's behavior for floating-point underflow, see numpy.seterr. 'None' means using the default, defined by numpy.seterr_all.
    Value:  None

numpy.seterr_invalid (('None', 'ignore', 'warn', 'raise', 'call', 'print', 'log')) 
    Doc:  Sets numpy's behavior for invalid floating-point operation, see numpy.seterr. 'None' means using the default, defined by numpy.seterr_all.
    Value:  None

warn.ignore_bug_before (('0.9', 'None', 'all', '0.3', '0.4', '0.4.1', '0.5', '0.6', '0.7', '0.8', '0.8.1', '0.8.2', '0.9', '0.10', '1.0', '1.0.1', '1.0.2', '1.0.3', '1.0.4', '1.0.5')) 
    Doc:  If 'None', we warn about all Theano bugs found by default. If 'all', we don't warn about Theano bugs found by default. If a version, we print only the warnings relative to Theano bugs found after that version. Warning for specific bugs can be configured with specific [warn] flags.
    Value:  0.9

warn.argmax_pushdown_bug (<function BoolParam.<locals>.booltype at 0x13c550e50>) 
    Doc:  Warn if in past version of Theano we generated a bug with the theano.tensor.nnet.nnet.local_argmax_pushdown optimization. Was fixed 27 may 2010
    Value:  False

warn.gpusum_01_011_0111_bug (<function BoolParam.<locals>.booltype at 0x13c553040>) 
    Doc:  Warn if we are in a case where old version of Theano had a silent bug with GpuSum pattern 01,011 and 0111 when the first dimensions was bigger then 4096. Was fixed 31 may 2010
    Value:  False

warn.sum_sum_bug (<function BoolParam.<locals>.booltype at 0x13c5531f0>) 
    Doc:  Warn if we are in a case where Theano version between version 9923a40c7b7a and the 2 august 2010 (fixed date), generated an error in that case. This happens when there are 2 consecutive sums in the graph, bad code was generated. Was fixed 2 August 2010
    Value:  False

warn.sum_div_dimshuffle_bug (<function BoolParam.<locals>.booltype at 0x13c5533a0>) 
    Doc:  Warn if previous versions of Theano (between rev. 3bd9b789f5e8, 2010-06-16, and cfc6322e5ad4, 2010-08-03) would have given incorrect result. This bug was triggered by sum of division of dimshuffled tensors.
    Value:  False

warn.subtensor_merge_bug (<function BoolParam.<locals>.booltype at 0x13c553550>) 
    Doc:  Warn if previous versions of Theano (before 0.5rc2) could have given incorrect results when indexing into a subtensor with negative stride (for instance, for instance, x[a:b:-1][c]).
    Value:  False

warn.gpu_set_subtensor1 (<function BoolParam.<locals>.booltype at 0x13c553700>) 
    Doc:  Warn if previous versions of Theano (before 0.6) could have given incorrect results when moving to the gpu set_subtensor(x[int vector], new_value)
    Value:  False

warn.vm_gc_bug (<function BoolParam.<locals>.booltype at 0x13c5538b0>) 
    Doc:  There was a bug that existed in the default Theano configuration, only in the development version between July 5th 2012 and July 30th 2012. This was not in a released version. If your code was affected by this bug, a warning will be printed during the code execution if you use the `linker=vm,vm.lazy=True,warn.vm_gc_bug=True` Theano flags. This warning is disabled by default as the bug was not released.
    Value:  False

warn.signal_conv2d_interface (<function BoolParam.<locals>.booltype at 0x13c553a60>) 
    Doc:  Warn we use the new signal.conv2d() when its interface changed mid June 2014
    Value:  False

warn.reduce_join (<function BoolParam.<locals>.booltype at 0x13c553c10>) 
    Doc:  Your current code is fine, but Theano versions prior to 0.7 (or this development version) might have given an incorrect result. To disable this warning, set the Theano flag warn.reduce_join to False. The problem was an optimization, that modified the pattern ""Reduce{scalar.op}(Join(axis=0, a, b), axis=0)"", did not check the reduction axis. So if the reduction axis was not 0, you got a wrong answer.
    Value:  False

warn.inc_set_subtensor1 (<function BoolParam.<locals>.booltype at 0x13c553dc0>) 
    Doc:  Warn if previous versions of Theano (before 0.7) could have given incorrect results for inc_subtensor and set_subtensor when using some patterns of advanced indexing (indexing with one vector or matrix of ints).
    Value:  False

warn.round (<function BoolParam.<locals>.booltype at 0x13c553f70>) 
    Doc:  Warn when using `tensor.round` with the default mode. Round changed its default from `half_away_from_zero` to `half_to_even` to have the same default as NumPy.
    Value:  False

warn.inc_subtensor1_opt (<function BoolParam.<locals>.booltype at 0x13c555160>) 
    Doc:  Warn if previous versions of Theano (before 0.10) could have given incorrect results when computing inc_subtensor(zeros[idx], x)[idx], when idx is an array of integers with duplicated values.
    Value:  True

compute_test_value (('off', 'ignore', 'warn', 'raise', 'pdb')) 
    Doc:  If 'True', Theano will run each op at graph build time, using Constants, SharedVariables and the tag 'test_value' as inputs to the function. This helps the user track down problems in the graph before it gets optimized.
    Value:  raise

print_test_value (<function BoolParam.<locals>.booltype at 0x13c5553a0>) 
    Doc:  If 'True', the __eval__ of a Theano variable will return its test_value when this is available. This has the practical conseguence that, e.g., in debugging `my_var` will print the same as `my_var.tag.test_value` when a test value is defined.
    Value:  False

compute_test_value_opt (('off', 'ignore', 'warn', 'raise', 'pdb')) 
    Doc:  For debugging Theano optimization only. Same as compute_test_value, but is used during Theano optimization
    Value:  off

unpickle_function (<function BoolParam.<locals>.booltype at 0x13c5555e0>) 
    Doc:  Replace unpickled Theano functions with None. This is useful to unpickle old graphs that pickled them when it shouldn't
    Value:  True

reoptimize_unpickled_function (<function BoolParam.<locals>.booltype at 0x13c555790>) 
    Doc:  Re-optimize the graph when a theano function is unpickled from the disk.
    Value:  False

exception_verbosity (('low', 'high')) 
    Doc:  If 'low', the text of exceptions will generally refer to apply nodes with short names such as Elemwise{add_no_inplace}. If 'high', some exceptions will also refer to apply nodes with long descriptions  like:
    A. Elemwise{add_no_inplace}
            B. log_likelihood_v_given_h
            C. log_likelihood_h
    Value:  low

openmp (<function BoolParam.<locals>.booltype at 0x13c5559d0>) 
    Doc:  Allow (or not) parallel computation on the CPU with OpenMP. This is the default value used when creating an Op that supports OpenMP parallelization. It is preferable to define it via the Theano configuration file ~/.theanorc or with the environment variable THEANO_FLAGS. Parallelization is only done for some operations that implement it, and even for operations that implement parallelism, each operation is free to respect this flag or not. You can control the number of threads used with the environment variable OMP_NUM_THREADS. If it is set to 1, we disable openmp in Theano by default.
    Value:  False

openmp_elemwise_minsize (<class 'int'>) 
    Doc:  If OpenMP is enabled, this is the minimum size of vectors for which the openmp parallelization is enabled in element wise ops.
    Value:  200000

check_input (<function BoolParam.<locals>.booltype at 0x13c555c10>) 
    Doc:  Specify if types should check their input in their C code. It can be used to speed up compilation, reduce overhead (particularly for scalars) and reduce the number of generated C files.
    Value:  True

cache_optimizations (<function BoolParam.<locals>.booltype at 0x13c555dc0>) 
    Doc:  WARNING: work in progress, does not work yet. Specify if the optimization cache should be used. This cache will any optimized graph and its optimization. Actually slow downs a lot the first optimization, and could possibly still contains some bugs. Use at your own risks.
    Value:  False

unittests.rseed (<class 'str'>) 
    Doc:  Seed to use for randomized unit tests. Special value 'random' means using a seed of None.
    Value:  666

NanGuardMode.nan_is_error (<function BoolParam.<locals>.booltype at 0x13c5580d0>) 
    Doc:  Default value for nan_is_error
    Value:  True

NanGuardMode.inf_is_error (<function BoolParam.<locals>.booltype at 0x13c558280>) 
    Doc:  Default value for inf_is_error
    Value:  True

NanGuardMode.big_is_error (<function BoolParam.<locals>.booltype at 0x13c558430>) 
    Doc:  Default value for big_is_error
    Value:  True

NanGuardMode.action (('raise', 'warn', 'pdb')) 
    Doc:  What NanGuardMode does when it finds a problem
    Value:  raise

optimizer_excluding (<class 'str'>) 
    Doc:  When using the default mode, we will remove optimizer with these tags. Separate tags with ':'.
    Value:  

optimizer_including (<class 'str'>) 
    Doc:  When using the default mode, we will add optimizer with these tags. Separate tags with ':'.
    Value:  

optimizer_requiring (<class 'str'>) 
    Doc:  When using the default mode, we will require optimizer with these tags. Separate tags with ':'.
    Value:  

DebugMode.patience (<class 'int'>) 
    Doc:  Optimize graph this many times to detect inconsistency
    Value:  10

DebugMode.check_c (<function BoolParam.<locals>.booltype at 0x13c5589d0>) 
    Doc:  Run C implementations where possible
    Value:  True

DebugMode.check_py (<function BoolParam.<locals>.booltype at 0x13c558b80>) 
    Doc:  Run Python implementations where possible
    Value:  True

DebugMode.check_finite (<function BoolParam.<locals>.booltype at 0x13c558d30>) 
    Doc:  True -> complain about NaN/Inf results
    Value:  True

DebugMode.check_strides (<class 'int'>) 
    Doc:  Check that Python- and C-produced ndarrays have same strides. On difference: (0) - ignore, (1) warn, or (2) raise error
    Value:  0

DebugMode.warn_input_not_reused (<function BoolParam.<locals>.booltype at 0x13c55b040>) 
    Doc:  Generate a warning when destroy_map or view_map says that an op works inplace, but the op did not reuse the input for its output.
    Value:  True

DebugMode.check_preallocated_output (<class 'str'>) 
    Doc:  Test thunks with pre-allocated memory as output storage. This is a list of strings separated by "":"". Valid values are: ""initial"" (initial storage in storage map, happens with Scan),""previous"" (previously-returned memory), ""c_contiguous"", ""f_contiguous"", ""strided"" (positive and negative strides), ""wrong_size"" (larger and smaller dimensions), and ""ALL"" (all of the above).
    Value:  

DebugMode.check_preallocated_output_ndim (<class 'int'>) 
    Doc:  When testing with ""strided"" preallocated output memory, test all combinations of strides over that number of (inner-most) dimensions. You may want to reduce that number to reduce memory or time usage, but it is advised to keep a minimum of 2.
    Value:  4

profiling.time_thunks (<function BoolParam.<locals>.booltype at 0x13c55b430>) 
    Doc:  Time individual thunks when profiling
    Value:  True

profiling.n_apply (<class 'int'>) 
    Doc:  Number of Apply instances to print by default
    Value:  20

profiling.n_ops (<class 'int'>) 
    Doc:  Number of Ops to print by default
    Value:  20

profiling.output_line_width (<class 'int'>) 
    Doc:  Max line width for the profiling output
    Value:  512

profiling.min_memory_size (<class 'int'>) 
    Doc:  For the memory profile, do not print Apply nodes if the size
             of their outputs (in bytes) is lower than this threshold
    Value:  1024

profiling.min_peak_memory (<function BoolParam.<locals>.booltype at 0x13c55ba60>) 
    Doc:  The min peak memory usage of the order
    Value:  False

profiling.destination (<class 'str'>) 
    Doc:  
             File destination of the profiling output
             
    Value:  stderr

profiling.debugprint (<function BoolParam.<locals>.booltype at 0x13c55bca0>) 
    Doc:  
             Do a debugprint of the profiled functions
             
    Value:  False

profiling.ignore_first_call (<function BoolParam.<locals>.booltype at 0x13c55be50>) 
    Doc:  
             Do we ignore the first call of a Theano function.
             
    Value:  False

optdb.position_cutoff (<class 'float'>) 
    Doc:  Where to stop eariler during optimization. It represent the position of the optimizer where to stop.
    Value:  inf

optdb.max_use_ratio (<class 'float'>) 
    Doc:  A ratio that prevent infinite loop in EquilibriumOptimizer.
    Value:  8.0

gcc.cxxflags (<class 'str'>) 
    Doc:  Extra compiler flags for gcc
    Value:   -Wno-c++11-narrowing

cmodule.warn_no_version (<function BoolParam.<locals>.booltype at 0x13c55e1f0>) 
    Doc:  If True, will print a warning when compiling one or more Op with C code that can't be cached because there is no c_code_cache_version() function associated to at least one of those Ops.
    Value:  False

cmodule.remove_gxx_opt (<function BoolParam.<locals>.booltype at 0x13c55e3a0>) 
    Doc:  If True, will remove the -O* parameter passed to g++.This is useful to debug in gdb modules compiled by Theano.The parameter -g is passed by default to g++
    Value:  False

cmodule.compilation_warning (<function BoolParam.<locals>.booltype at 0x13c55e550>) 
    Doc:  If True, will print compilation warnings.
    Value:  False

cmodule.preload_cache (<function BoolParam.<locals>.booltype at 0x13c55e700>) 
    Doc:  If set to True, will preload the C module cache at import time
    Value:  False

cmodule.age_thresh_use (<class 'int'>) 
    Doc:  In seconds. The time after which Theano won't reuse a compile c module.
    Value:  2073600

cmodule.debug (<function BoolParam.<locals>.booltype at 0x13c55e940>) 
    Doc:  If True, define a DEBUG macro (if not exists) for any compiled C code.
    Value:  False

blas.ldflags (<class 'str'>) 
    Doc:  lib[s] to include for [Fortran] level-3 blas implementation
    Value:  -lblas

blas.check_openmp (<function BoolParam.<locals>.booltype at 0x13c55ed30>) 
    Doc:  Check for openmp library conflict.
WARNING: Setting this to False leaves you open to wrong results in blas-related operations.
    Value:  True

metaopt.verbose (<class 'int'>) 
    Doc:  0 for silent, 1 for only warnings, 2 for full output withtimings and selected implementation
    Value:  0

metaopt.optimizer_excluding (<class 'str'>) 
    Doc:  exclude optimizers with these tags. Separate tags with ':'.
    Value:  

metaopt.optimizer_including (<class 'str'>) 
    Doc:  include optimizers with these tags. Separate tags with ':'.
    Value:  

profile (<function BoolParam.<locals>.booltype at 0x13c5620d0>) 
    Doc:  If VM should collect profile information
    Value:  False

profile_optimizer (<function BoolParam.<locals>.booltype at 0x13c562280>) 
    Doc:  If VM should collect optimizer profile information
    Value:  False

profile_memory (<function BoolParam.<locals>.booltype at 0x13c562430>) 
    Doc:  If VM should collect memory profile information and print it
    Value:  False

<theano.configparser.ConfigParam object at 0x13c563100>
    Doc:  Useful only for the vm linkers. When lazy is None, auto detect if lazy evaluation is needed and use the appropriate version. If lazy is True/False, force the version used between Loop/LoopGC and Stack.
    Value:  None

warn.identify_1pexp_bug (<function BoolParam.<locals>.booltype at 0x13c562670>) 
    Doc:  Warn if Theano versions prior to 7987b51 (2011-12-18) could have yielded a wrong result due to a bug in the is_1pexp function
    Value:  False

on_shape_error (('warn', 'raise')) 
    Doc:  warn: print a warning and use the default value. raise: raise an error
    Value:  warn

tensor.insert_inplace_optimizer_validate_nb (<class 'int'>) 
    Doc:  -1: auto, if graph have less then 500 nodes 1, else 10
    Value:  -1

experimental.local_alloc_elemwise (<function BoolParam.<locals>.booltype at 0x13c5629d0>) 
    Doc:  DEPRECATED: If True, enable the experimental optimization local_alloc_elemwise. Generates error if not True. Use optimizer_excluding=local_alloc_elemwise to dsiable.
    Value:  True

experimental.local_alloc_elemwise_assert (<function BoolParam.<locals>.booltype at 0x13c562a60>) 
    Doc:  When the local_alloc_elemwise is applied, add an assert to highlight shape errors.
    Value:  True

scan.allow_gc (<function BoolParam.<locals>.booltype at 0x13c562ca0>) 
    Doc:  Allow/disallow gc inside of Scan (default: False)
    Value:  False

scan.allow_output_prealloc (<function BoolParam.<locals>.booltype at 0x13c562e50>) 
    Doc:  Allow/disallow memory preallocation for outputs inside of scan (default: True)
    Value:  True

scan.debug (<function BoolParam.<locals>.booltype at 0x13c564040>) 
    Doc:  If True, enable extra verbose output related to scan
    Value:  False

compile.wait (<class 'int'>) 
    Doc:  Time to wait before retrying to acquire the compile lock.
    Value:  5

cycle_detection (('regular', 'fast')) 
    Doc:  If cycle_detection is set to regular, most inplaces are allowed,but it is slower. If cycle_detection is set to faster, less inplacesare allowed, but it makes the compilation faster.The interaction of which one give the lower peak memory usage iscomplicated and not predictable, so if you are close to the peakmemory usage, triyng both could give you a small gain.
    Value:  regular

check_stack_trace (('off', 'log', 'warn', 'raise')) 
    Doc:  A flag for checking the stack trace during the optimization process. default (off): does not check the stack trace of any optimization log: inserts a dummy stack trace that identifies the optimizationthat inserted the variable that had an empty stack trace.warn: prints a warning if a stack trace is missing and also a dummystack trace is inserted that indicates which optimization insertedthe variable that had an empty stack trace.raise: raises an exception if a stack trace is missing
    Value:  off

compile.timeout (<class 'int'>) 
    Doc:  In seconds, time that a process will wait before deciding to
override an existing lock. An override only happens when the existing
lock is held by the same owner *and* has not been 'refreshed' by this
owner for more than this period. Refreshes are done every half timeout
period for running processes.
    Value:  120

compiledir_format (<class 'str'>) 
    Doc:  Format string for platform-dependent compiled module subdirectory
(relative to base_compiledir). Available keys: device, gxx_version,
hostname, numpy_version, platform, processor, python_bitwidth,
python_int_bitwidth, python_version, short_platform, theano_version.
Defaults to 'compiledir_%(short_platform)s-%(processor)s-%(python_vers
ion)s-%(python_bitwidth)s'.
    Value:  compiledir_%(short_platform)s-%(processor)s-%(python_version)s-%(python_bitwidth)s

<theano.configparser.ConfigParam object at 0x13c563c10>
    Doc:  platform-independent root directory for compiled modules
    Value:  /Users/rpg/.theano

<theano.configparser.ConfigParam object at 0x13c563ee0>
    Doc:  platform-dependent cache directory for compiled modules
    Value:  /Users/rpg/.theano/compiledir_macOS-10.15.7-x86_64-i386-64bit-i386-3.8.0-64

<theano.configparser.ConfigParam object at 0x13c563d60>
    Doc:  Directory to cache pre-compiled kernels for the gpuarray backend.
    Value:  /Users/rpg/.theano/compiledir_macOS-10.15.7-x86_64-i386-64bit-i386-3.8.0-64/gpuarray_kernels

ctc.root (<class 'str'>) 
    Doc:  Directory which contains the root of Baidu CTC library. It is assumed     that the compiled library is either inside the build, lib or lib64     subdirectory, and the header inside the include directory.
    Value:  
```

",,,2020-12-17 21:42:04,2021-01-22 09:26:56,"rpgoldman mentioned 2021-01-07 19:48:52,rpgoldman subscribed 2021-01-07 19:48:52,michaelosthege mentioned 2021-01-07 21:16:47,michaelosthege subscribed 2021-01-07 21:16:47,michaelosthege mentioned 2021-01-08 00:45:39,michaelosthege subscribed 2021-01-08 00:45:40,michaelosthege closed 2021-01-10 19:57:03,rpgoldman mentioned 2021-01-10 19:57:03,rpgoldman subscribed 2021-01-10 19:57:03,ricardoV94 reopened 2021-01-21 09:20:53,michaelosthege mentioned 2021-01-21 17:02:43,michaelosthege subscribed 2021-01-21 17:02:43,michaelosthege mentioned 2021-01-21 17:04:35,michaelosthege subscribed 2021-01-21 17:04:35,rpgoldman mentioned 2021-01-21 17:04:48,rpgoldman subscribed 2021-01-21 17:04:48,michaelosthege mentioned 2021-01-21 23:28:30,michaelosthege subscribed 2021-01-21 23:28:31,michaelosthege closed 2021-01-22 09:26:56",ricardoV94 rpgoldman michaelosthege,16
204,234,Import config from theano.configdefaults throughout the codebase,michaelosthege,"We should not rely on the top-level ""theano"" module for referencing things from within the codebase.

There should be no need to `import theano` anywhere in the codebase (see #235).",good first issue refactor,michaelosthege,2020-12-20 15:03:08,2020-12-30 14:01:36,"michaelosthege labeled 2020-12-20 15:03:08,michaelosthege labeled 2020-12-20 15:03:08,michaelosthege milestoned 2020-12-20 16:38:05,michaelosthege demilestoned 2020-12-26 08:30:22,michaelosthege milestoned 2020-12-26 08:30:23,michaelosthege assigned 2020-12-27 11:05:25,michaelosthege referenced 2020-12-27 15:22:33,michaelosthege referenced 2020-12-29 13:34:25,twiecki closed 2020-12-30 14:01:36",michaelosthege twiecki,0
206,238,"Rename ""optimization"" to ""rewrite""",brandonwillard,"What we're naively calling ""optimizations"" throughout Theano-PyMC are actually ""rewrites"".  The term ""optimization"" has some very specific&mdash;and strong&mdash;implications that are **not** met in far too many cases, so we absolutely must correct this sooner than later.",important refactor,,2020-12-24 20:35:21,2023-02-10 18:07:12,"brandonwillard labeled 2020-12-24 20:35:21,brandonwillard labeled 2020-12-24 20:35:21,brandonwillard milestoned 2020-12-24 20:35:21,michaelosthege demilestoned 2020-12-26 08:29:48,michaelosthege milestoned 2020-12-26 08:29:48,michaelosthege demilestoned 2020-12-27 11:35:08,michaelosthege milestoned 2020-12-27 11:35:08,brandonwillard closed 2023-02-10 18:07:12",canyon289 michaelosthege brandonwillard,3
208,240,Some local functions cause pickling issues in PyMC3 ,michaelosthege,See https://github.com/pymc-devs/pymc3/pull/4382,bug important,michaelosthege,2020-12-26 08:26:07,2020-12-27 10:20:24,"michaelosthege labeled 2020-12-26 08:26:07,michaelosthege labeled 2020-12-26 08:26:07,michaelosthege milestoned 2020-12-26 08:26:07,michaelosthege assigned 2020-12-26 14:24:02,michaelosthege referenced 2020-12-26 14:26:56,twiecki closed 2020-12-27 10:20:25,twiecki referenced 2020-12-27 10:20:26",michaelosthege twiecki,0
211,243,Failing PyMC3 tests: with ValueError: Trying to rebroadcast non-existent dimension,twiecki,"From https://github.com/pymc-devs/pymc3/pull/4382:
```
self = <theano.compile.ops.Rebroadcast object at 0x7faa084385b0>, x = normal_rv.out

    def make_node(self, x):
        if self.axis.keys() and (x.ndim <= max(self.axis.keys())):
>           raise ValueError(""Trying to rebroadcast non-existent dimension"")
E           ValueError: Trying to rebroadcast non-existent dimension

Theano-PyMC/theano/compile/ops.py:719: ValueError
[...]
FAILED pymc3/tests/test_sampling.py::TestSample::test_sample_init - ValueError: Trying to rebroadcast non-existent dimension
FAILED pymc3/tests/test_sampling.py::test_exec_nuts_init[advi] - ValueError: Trying to rebroadcast non-existent dimension
FAILED pymc3/tests/test_variational_inference.py::test_replacements_in_sample_node_aevb[NormalizingFlowGroup: {'flow': 'scale'}] - ValueError: Trying to rebroadcast non-existent dimension
[...]
```",important,michaelosthege,2020-12-27 10:25:27,2020-12-27 20:32:50,"twiecki labeled 2020-12-27 10:25:27,twiecki labeled 2020-12-27 10:25:27,michaelosthege milestoned 2020-12-27 11:33:02,michaelosthege assigned 2020-12-27 11:42:44,brandonwillard unlabeled 2020-12-27 20:15:18,brandonwillard closed 2020-12-27 20:32:50",michaelosthege twiecki brandonwillard,6
217,249,Beta and Kumaraswamy distributions break the Jax Sampling (numpyro_nuts/sample_tfp_nuts),luke14free,"I am fully aware that this is 100% experimental but wanted to report that using any beta distribution in your model will break jax sampling.

**Please provide a minimal, self-contained, and reproducible example.**
```python
import pymc3 as pm
import pymc3.sampling_jax

with pm.Model() as hierarchical_model:
    b = pm.Beta('beta', 0.2, 0.2) # same with Kumaraswamy
    trace_jax = pm.sampling_jax.sample_numpyro_nuts(2000, tune=2000, target_accept=0.9) # same with sample_tfp_nuts
```

**Please provide the full traceback.**
```python
...
~/.local/share/virtualenvs/luca-J6Q-PZUv/lib/python3.7/site-packages/theano/sandbox/jaxify.py in jax_funcify_ScalarOp(op)
    182 @jax_funcify.register(ScalarOp)
    183 def jax_funcify_ScalarOp(op):
--> 184     func_name = op.nfunc_spec[0]
    185 
    186     if ""."" in func_name:
AttributeError: 'Second' object has no attribute 'nfunc_spec'
```

Requires the master of [Theano-PyMC](https://github.com/pymc-devs/Theano-PyMC), as well as JAX, TFP-nightly and numpyro.

## Versions and main components

* PyMC3 Version: v3.10.0
* Theano Version: 1.0.11
* Python Version: 3.7.5
* Operating system: osx 11.1
* How did you install PyMC3: pip",bug good first issue help wanted JAX,,2020-12-29 18:00:48,2020-12-30 15:20:33,"luke14free renamed 2020-12-29 18:07:35,twiecki transferred 2020-12-30 14:06:54,luke14free mentioned 2020-12-30 14:08:07,luke14free subscribed 2020-12-30 14:08:07,twiecki labeled 2020-12-30 14:08:22,twiecki labeled 2020-12-30 14:08:22,twiecki labeled 2020-12-30 14:08:22,twiecki labeled 2020-12-30 14:08:22,luke14free mentioned 2020-12-30 14:30:59,luke14free subscribed 2020-12-30 14:30:59,luke14free closed 2020-12-30 15:20:34",luke14free dfm twiecki,10
225,257,Rename gof submodule,twiecki,"To this day I have no idea what `gof` means, but what I understand is that it contains graph utilities.

Ideas for new names: 
* `graph`
* `graph_utils`",,,2021-01-01 11:18:24,2021-01-09 18:57:42,"twiecki milestoned 2021-01-01 11:19:35,michaelosthege demilestoned 2021-01-08 16:36:57,michaelosthege milestoned 2021-01-08 16:36:57,brandonwillard connected 2021-01-09 06:48:00,brandonwillard closed 2021-01-09 18:57:42",michaelosthege twiecki brandonwillard,0
226,258,Rewrite graph traversal functions as generators,brandonwillard,"It seems like the graph traversal functions in `theano.gof.graph` (e.g. `stack_search` and all the ones based on it) could be turned into generator functions.  

Generator versions of such functions would allow us to more flexibly short-circuit searches, instead of always needlessly walking the entire graph.  For example, with generators, an expression like `node in ancestors([graph])` would stop walking the graph immediately after finding `node`.",enhancement important graph rewriting,brandonwillard,2021-01-02 00:01:58,2021-01-03 20:49:51,"brandonwillard labeled 2021-01-02 00:01:58,brandonwillard labeled 2021-01-02 00:01:58,brandonwillard labeled 2021-01-02 00:01:58,brandonwillard labeled 2021-01-02 00:01:58,brandonwillard unlabeled 2021-01-02 00:27:13,brandonwillard assigned 2021-01-02 00:29:15,brandonwillard connected 2021-01-03 05:31:39,brandonwillard closed 2021-01-03 20:49:51",brandonwillard,0
228,260,Issue with softplus approximation,ricardoV94,"The current implemenation of softplus (aka log1pexp) uses two bounds to decide whether to return `0, log1p(exp(x)), or x`

https://github.com/pymc-devs/Theano-PyMC/blob/511c778f8a595444e009bcad738d552413b16f2c/theano/tensor/nnet/sigm.py#L356-L366

So far so good (almost). However, different bounds are used for the `c_code` depending on the precision being used https://github.com/pymc-devs/Theano-PyMC/blob/511c778f8a595444e009bcad738d552413b16f2c/theano/tensor/nnet/sigm.py#L376-L400

Unless I am missing something there might be an issue with both of the upper bounds used in the `c_code` (`14.0f and 16.0`). 

According to [Machler, 2012](https://cran.r-project.org/web/packages/Rmpfr/vignettes/log1mexp-note.pdf) who describes a refined method for `log1pexp` there is unnecessary error in using the approximation `log1pexp = x` for values of x < 33.3 (green line in Figure 3).

![image](https://user-images.githubusercontent.com/28983449/103476628-d7cfb600-4db7-11eb-8772-5b70592c6a27.png)


I don't know why the upper bounds are so low. It seems like `log1p(exp(x))` shouldn't overflow so soon. Using the code mentioned in the comment I get overflow only at +89 for float32 and at +710 for float64. This makes me very confident that the reason for choosing these was computational speed, but according to Machler (2012) a safer cutoff would have been 33.3, or an equally good intermediate approximation `x + exp(-x)` could be used until 33.3.

***

For reference here is the entire Machler (2012) suggestion, which tries to achieve a good compromise between speed and numerical precision for the entire real range:

```python
    if x <= -37.0: 
        return exp(x)
    elif x <= 18.0:
        return log1p(exp(x))
    elif x <= 33.3:
        return x + exp(-x)
    else:
        return x   
```
Plus the current appropriate lower bounds in `c_code` to return 0 instead of underflowing when x is very negative.

The changes are:
1. Using the approximation `exp(x)` for values below -37, instead of the current approach of returning `0` for values below 30 in `Theano` or the more costly `log1p(exp(x))` for any values that don't underflow in `c`.
2. Using the approximation `x + exp(-x)` in the range [18, 33.3] instead of the more costly (according to Machler and, it seems, the Theano guys as well) `log1p(exp(x))`
3. Using the approximation `x` only when it is no worse than `log1p(exp(x))` or `x + exp(-x)`, which occurs at 33.3 (again according to Machler).
",,,2021-01-03 10:54:17,2021-01-06 15:54:58,"twiecki mentioned 2021-01-03 13:21:57,twiecki subscribed 2021-01-03 13:21:58,twiecki closed 2021-01-06 15:54:58",ricardoV94 twiecki,2
236,270,tt.switch behaves surprisingly with homogenous non boolean input,ricardoV94,"```python
print(tt.switch(np.ones(10), 0, 1).eval())
print(tt.switch(np.ones(10).astype(bool), 0, 1).eval())

[0]
[0 0 0 0 0 0 0 0 0 0]
```
However it works fine with non-boolean inputs, as long as they are not all the same.
```python
a = np.ones(10)
a[0] = 0
print(tt.switch(a, 0, 1).eval())

[1 0 0 0 0 0 0 0 0 0]
```
This was behind a subtle bug in the Bernoulli distribution of PyMC (https://github.com/pymc-devs/pymc3/issues/4389), which seems to have been there since pymc3's inception...",bug important,brandonwillard,2021-01-15 16:25:07,2021-01-16 03:39:12,"ricardoV94 renamed 2021-01-15 18:13:47,brandonwillard labeled 2021-01-15 19:06:42,brandonwillard labeled 2021-01-15 19:06:42,brandonwillard referenced 2021-01-16 01:17:45,brandonwillard assigned 2021-01-16 01:17:46,brandonwillard referenced 2021-01-16 01:19:09,brandonwillard referenced 2021-01-16 01:21:21,brandonwillard referenced 2021-01-16 01:22:58,brandonwillard referenced 2021-01-16 02:02:01,brandonwillard closed 2021-01-16 03:39:12,brandonwillard referenced 2021-01-16 03:39:14",ricardoV94 brandonwillard,6
239,276,Simplify CAReduce,brandonwillard,"The implementation of `CAReduce` and its subclasses (e.g. `Sum`, `Mean`, `Prod`, `Any`, `All`, etc.) is unnecessarily complex, and its python `Op.perform` actually `for`-loops through the reduction dimensions instead of using the corresponding NumPy functions (e.g. `np.sum`, `np.mean`, `np.prod`, etc.)

Worse yet, it uses hard-coded logic for specific `self.scalar_op`s (see [`CAReduce.set_ufunc`](https://github.com/pymc-devs/Theano-PyMC/blob/b379b0f2eb309c8e6e1a081974fd93129a83d354/theano/tensor/elemwise.py#L1345) and [this line](https://github.com/pymc-devs/Theano-PyMC/blob/b379b0f2eb309c8e6e1a081974fd93129a83d354/theano/tensor/elemwise.py#L1573) in the C code generation process).  This alone is a reason to completely refactor the class.

The only noticeable difference between `CAReduce`'s current use of `for`-loops + `ufunc.reduce` and the corresponding NumPy functions is the former's use of a configurable ""accumulation"" dtype, `acc_dtype`, and a general output dtype.  The NumPy functions have an optional `dtype` parameter that seems to correspond to `acc_dtype`, so that might take care of it.",help wanted important refactor,,2021-01-21 05:17:57,2021-02-18 18:49:52,"brandonwillard labeled 2021-01-21 05:17:57,brandonwillard labeled 2021-01-21 05:17:57,brandonwillard labeled 2021-01-21 05:17:57,brandonwillard milestoned 2021-01-21 05:17:57,michaelosthege closed 2021-02-18 18:49:52",michaelosthege brandonwillard,0
243,282,Remove static `Op` list in `aesara.tensor.basic.Alloc.do_constant_folding`,brandonwillard,These `Op`s should instead register themselves to a no-`Alloc`-constant-folding collection/class (e.g. using [`ABC.register`](https://docs.python.org/3/library/abc.html#abc.ABCMeta.register)).  This would prevent the need to reference/import classes from other modules within a base module like `aesara.tensor.basic`.  It would also make the functionality generally extensible in the first place.,duplicate enhancement good first issue help wanted refactor,,2021-01-27 04:34:21,2022-01-20 16:39:08,"brandonwillard labeled 2021-01-27 04:34:21,brandonwillard labeled 2021-01-27 04:34:21,brandonwillard labeled 2021-01-27 04:34:22,brandonwillard labeled 2022-01-05 22:06:33,brandonwillard milestoned 2022-01-05 22:06:39,brandonwillard renamed 2022-01-05 22:06:49,kc611 closed 2022-01-20 16:39:08,brandonwillard labeled 2022-01-20 17:34:26",kc611 brandonwillard,1
244,283,Remove documentation contents that are no longer relevant to the project,michaelosthege,"The `/doc` folder contains material such as conference presentations (`cifarSC2011`, `nextml2015`, ...) and `proposals`.
They often include code snippets that became incompatible or have to be refactored.
Some proposals (`mongodb_cache`, ...) are no longer relevant to this project.

We should remove all of this content as it's still archived in https://github.com/Theano/Theano/tree/master/doc

",documentation good first issue,,2021-01-27 11:09:47,2021-01-29 05:56:22,"michaelosthege labeled 2021-01-27 11:09:47,michaelosthege labeled 2021-01-27 11:09:47,Saurav-Sutaria mentioned 2021-01-27 14:21:57,Saurav-Sutaria subscribed 2021-01-27 14:21:57,Saurav-Sutaria mentioned 2021-01-28 10:31:55,Saurav-Sutaria subscribed 2021-01-28 10:31:55,michaelosthege mentioned 2021-01-28 10:43:50,michaelosthege subscribed 2021-01-28 10:43:51,brandonwillard connected 2021-01-29 01:12:13,brandonwillard closed 2021-01-29 05:56:22",michaelosthege brandonwillard Saurav-Sutaria,5
250,291,ERROR (theano.graph.opt): Optimization failure due to: local_grad_log_erfc_neg,ricardoV94,"From the PyMC discourse: https://discourse.pymc.io/t/theano-warnings-when-fitting-a-simple-psychometric-function-cumulative-normal-distribution/6650

**Please provide a minimal, self-contained, and reproducible example.**
```python
import numpy as np 
import pymc3 as pm 
import arviz as az 
from theano import tensor as tt                                         

def cumulative_normal(x, alpha, beta): 
  # Cumulative distribution function for the standard normal distribution 
  # return 0.5 + 0.5 * tt.erf((x-alpha)/(beta*tt.sqrt(2)))  # Works fine
  return 0.5 * (1 + tt.erf((x-alpha)/(beta*tt.sqrt(2)))) 

xij = np.array([-40.5, -40., 40.5]) 
nij = np.array([1.0, 0.0, 1.0])                                                                                      
rij = np.array([0, 0, 1.])  

with pm.Model(): 
  alpha = pm.Uniform('alpha', lower=-40.5, upper=40.5) 
  beta = pm.Uniform(""beta"", lower=10e-5, upper=40) 
  thetaij = pm.Deterministic('thetaij', cumulative_normal(xij, alpha, beta)) 
  # thetaij = pm.Deterministic('thetaij', pm.distributions.dist_math.std_cdf((xij-alpha)/beta))  # Also works fine
  rij_ = pm.Binomial('rij', p=thetaij, n=nij, observed=rij) 
  trace = pm.sample()
```

**Please provide the full traceback of any errors.**
```python
Auto-assigning NUTS sampler...
Initializing NUTS using jitter+adapt_diag...
ERROR (theano.graph.opt): Optimization failure due to: local_grad_log_erfc_neg
ERROR (theano.graph.opt): node: Elemwise{true_div,no_inplace}(Elemwise{mul,no_inplace}.0, Elemwise{erfc,no_inplace}.0)
ERROR (theano.graph.opt): TRACEBACK:
ERROR (theano.graph.opt): Traceback (most recent call last):
  File ""/home/ricardo/.local/lib/python3.8/site-packages/theano/graph/opt.py"", line 2017, in process_node
    replacements = lopt.transform(fgraph, node)
  File ""/home/ricardo/.local/lib/python3.8/site-packages/theano/graph/opt.py"", line 1209, in transform
    return self.fn(*args, **kwargs)
  File ""/home/ricardo/.local/lib/python3.8/site-packages/theano/tensor/opt.py"", line 7307, in local_grad_log_erfc_neg
    if not exp_in.owner.inputs[0].owner:
AttributeError: 'NoneType' object has no attribute 'owner'
```
According to the user it still samples correctly.

Also, the equivalent `pm.distributions.dist_math.std_cdf` or `return 0.5 + 0.5 * tt.erf((x-alpha)/(beta*tt.sqrt(2)))` do not raise this warning.

## Versions and main components

* Aesara version: Theno 1.1.0
* PyMC version: 3.11.0
* Operating system: Linux
",,,2021-02-01 10:31:47,2021-02-06 01:44:26,brandonwillard closed 2021-02-06 01:44:26,ricardoV94 brandonwillard,1
264,309,TypeError in pydotprint,Spaak,"As part of the larger effort of moving over PyMC3's graph printing (and string representations) to v4 (see https://github.com/pymc-devs/pymc3/issues/4494), I encountered the following issue:

```
Traceback (most recent call last):
  File ""/home/eelke/python/tmp.py"", line 35, in <module>
    pydotprint(x, '~/tmp.pdf')
  File ""/home/eelke/repos/Theano-PyMC/aesara/printing.py"", line 983, in pydotprint
    nw_node = Node(aid, label=astr, shape=apply_shape)
TypeError: Node() takes no arguments
```

when executing the following:

```
import pymc3 as pm
from aesara.printing import pydotprint, debugprint

model = pm.Model()
with model:
    x = pm.Normal('x', mu=0, sigma=1)

pydotprint(x, '~/tmp.pdf')
```

In case it's relevant:

```
>>> debugprint(x)
normal_rv.1 [id A] 'x'   
 |RandomStateSharedVariable(<RandomState(MT19937) at 0x7FC3AF2FEA40>) [id B]
 |TensorConstant{[]} [id C]
 |TensorConstant{11} [id D]
 |TensorConstant{0} [id E]
 |TensorConstant{1.0} [id F]
```

Aesara is at current `master` (5795760d3d9f160fca3d20b266a224e87d86a6b4), PyMC3 is at current `v4` (main repository, pymc-devs/pymc3@932672a6995647052224f9ef0782b8bfb78bd8e7).
",bug,brandonwillard,2021-03-01 08:09:13,2021-03-01 23:29:49,"brandonwillard assigned 2021-03-01 22:18:50,brandonwillard labeled 2021-03-01 22:18:54,brandonwillard closed 2021-03-01 23:29:49,michaelosthege referenced 2021-03-31 08:39:16,michaelosthege referenced 2021-03-31 11:11:09",michaelosthege brandonwillard Spaak,2
267,312,Replace C-code generation and compilation backend,brandonwillard,"The text-based C-code generation and compilation backend in Aesara is difficult to use, debug, maintain, and extend.  We need to fix that ASAP.

[Cython](https://cython.org/) is a well established Python-to-C transpiler that provides a much cleaner, automatic means of generating the same kind of Python C API code that's written by hand in Aesara.  Here are some possible benefits to replacing our current C implementations with Cython-generated C code:
- we could make more of our logic transparent to pure-Python readers
- automatically benefit from updates and new features provided by Cython over time (e.g. Python C API version and capability updates)
- use Cython's build and code caching features, which could have much better support for different platforms and environments
- attempt automatic conversion of Python-only `Op` implementations, resulting in more C-only code (i.e. fewer calls to-and-from Python/C during graph evaluation)
- C-level interactions with NumPy are much easier in Cython, so we might&mdash;for example&mdash;be able to generate C code for all the `Subtensor*`/indexing operations with a little bit of Cython (instead of our very limited C implementations for only certain types of indexing)

This general idea has been brought up in numerous different locations, so I'm creating this issue as a means of collecting all the relevant details, ideas, discussions, requirements, etc., into one place.

Related issues:
- #306 
- #129 
- #10 
",enhancement help wanted question important refactor C-backend,,2021-03-02 01:00:41,2021-04-16 19:06:41,"brandonwillard labeled 2021-03-02 01:00:41,brandonwillard labeled 2021-03-02 01:00:41,brandonwillard labeled 2021-03-02 01:00:41,brandonwillard labeled 2021-03-02 01:00:41,brandonwillard labeled 2021-03-02 01:00:41,brandonwillard labeled 2021-03-02 01:00:41,brandonwillard pinned 2021-03-02 01:00:49,brandonwillard renamed 2021-03-02 18:25:49,aseyboldt mentioned 2021-03-09 16:18:56,aseyboldt subscribed 2021-03-09 16:18:56,brandonwillard closed 2021-04-16 19:06:41,aesara-devs locked 2021-04-16 19:06:42,brandonwillard unpinned 2021-04-16 19:08:48",aseyboldt aesara-devs twiecki brandonwillard,5
268,313,JAX conversion of aet.diag(...) does not work while aet.nlinalg.diag(...) works,sokol11,"I get the `AttributeError: 'Second' object has no attribute 'nfunc_spec'` error when trying to sample my Gaussian Random Walk model with `pm.sampling_jax.sample_numpyro_nuts`. I know that this is an experimental library, so it is probably expected behavior at this point. But if you have some insight into what is going on, please share. Thank you.",bug JAX,,2021-03-03 03:44:03,2021-03-05 00:31:09,"sokol11 mentioned 2021-03-03 09:15:53,sokol11 subscribed 2021-03-03 09:15:53,sokol11 mentioned 2021-03-03 20:54:57,sokol11 subscribed 2021-03-03 20:54:57,twiecki renamed 2021-03-03 20:57:11,brandonwillard labeled 2021-03-04 01:15:24,brandonwillard labeled 2021-03-04 02:29:15,twiecki renamed 2021-03-04 07:37:13,twiecki renamed 2021-03-04 07:37:23,twiecki unlabeled 2021-03-04 07:37:32,twiecki labeled 2021-03-04 07:37:32,brandonwillard closed 2021-03-05 00:31:09",sokol11 twiecki brandonwillard,12
269,314,Add JAX Op for wrapping user-defined JAX functions,twiecki,"@dfm has built this cool wrapper that allows JAX functions to be turned into Aesara Ops easily:
https://gist.github.com/dfm/a2db466f46ab931947882b08b2f21558

This would be a great feature to include as PyMC3 users could add arbitrary deterministics coded in JAX to their PyMC3 model.

Proposing to add this to the library.",good first issue help wanted wontfix JAX Op implementation,,2021-03-03 09:18:31,2022-09-15 08:44:06,"twiecki labeled 2021-03-03 09:18:31,twiecki labeled 2021-03-03 09:18:31,twiecki labeled 2021-03-03 09:18:31,twiecki labeled 2021-03-03 09:18:31,dfm mentioned 2021-03-03 09:18:32,dfm subscribed 2021-03-03 09:18:32,peterroelants subscribed 2021-03-03 10:12:11,peterroelants subscribed 2021-03-03 10:12:14,dfm mentioned 2021-03-04 01:47:24,dfm subscribed 2021-03-04 01:47:25,brandonwillard mentioned 2021-03-04 08:29:21,brandonwillard subscribed 2021-03-04 08:29:21,dfm mentioned 2022-03-30 18:46:37,dfm subscribed 2022-03-30 18:46:37,brandonwillard labeled 2022-07-11 18:03:13,brandonwillard unlabeled 2022-07-11 18:03:31,twiecki labeled 2022-09-15 08:44:00,twiecki closed 2022-09-15 08:44:06,brandonwillard closed 2022-09-15 13:31:02",rlouf zaxtax brandonwillard ricardoV94 dfm twiecki peterroelants,11
272,318,Porting sympy code to theano-pymc,jsmolic,"Hi, 
I'm trying to port sympy theano code [[1]](https://github.com/sympy/sympy/blob/master/sympy/printing/theanocode.py), [[2]](https://github.com/sympy/sympy/blob/master/sympy/printing/tests/test_theanocode.py) to work with theano-pymc, as it's currently supporting only older theano. Initially I ran into the following test failures


```
FAILED test_theanocode.py::test_Derivative - AttributeError: module 'theano' has no attribute 'gof'
FAILED test_theanocode.py::test_theano_function_simple - AttributeError: module 'theano' has no attribute 'Variable'
FAILED test_theanocode.py::test_theano_function_multi - AttributeError: module 'theano' has no attribute 'Variable'
FAILED test_theanocode.py::test_theano_function_numpy - AttributeError: module 'theano' has no attribute 'Variable'
FAILED test_theanocode.py::test_theano_function_matrix - AttributeError: module 'theano' has no attribute 'Variable'
FAILED test_theanocode.py::test_theano_function_kwargs - AttributeError: module 'theano' has no attribute 'Variable'
FAILED test_theanocode.py::test_theano_function_scalar - AttributeError: module 'theano' has no attribute 'Variable'
FAILED test_theanocode.py::test_MatrixSlice - ImportError: cannot import name 'Constant' from 'theano' (/usr/lib/python3.9/site-packages/theano/__init__.py)
FAILED test_theanocode.py::test_cache_complex - AttributeError: module 'theano' has no attribute 'gof'
FAILED test_theanocode.py::test_constantfunctions - AttributeError: module 'theano' has no attribute 'Variable'
```

I've menaged to patch most of the failures with:
```diff
--- a/sympy/printing/tests/test_theanocode.py
+++ b/sympy/printing/tests/test_theanocode.py
@@ -67,9 +67,9 @@ def fgraph_of(*exprs):
     theano.gof.FunctionGraph
     """"""
     outs = list(map(theano_code_, exprs))
-    ins = theano.gof.graph.inputs(outs)
-    ins, outs = theano.gof.graph.clone(ins, outs)
-    return theano.gof.FunctionGraph(ins, outs)
+    ins = theano.graph.basic.graph_inputs(outs)
+    ins, outs = theano.graph.basic.clone(ins, outs)
+    return theano.graph.fg.FunctionGraph(ins, outs)


 def theano_simplify(fgraph):
@@ -350,7 +350,7 @@ def test_theano_function_scalar():
             f = theano_function_(inputs, outputs, dims=in_dims, scalar=scalar)

             # Check the theano_function attribute is set whether wrapped or not
-            assert isinstance(f.theano_function, theano.compile.function_module.Function)
+            assert isinstance(f.theano_function, theano.compile.function.types.Function)

             # Feed in inputs of the appropriate size and get outputs
             in_values = [
@@ -401,7 +401,7 @@ def theq_slice(s1, s2):
     assert theq_slice(theano_code_(slice(1, x, 3), dtypes=dtypes), slice(1, xt, 3))

 def test_MatrixSlice():
-    from theano import Constant
+    from theano.graph.basic import Constant

     cache = {}

@@ -556,9 +556,9 @@ def test_cache_complex():
     # Iterate through variables in the Theano computational graph that the
     # printed expression depends on
     seen = set()
-    for v in theano.gof.graph.ancestors([expr_t]):
+    for v in theano.graph.basic.ancestors([expr_t]):
         # Owner-less, non-constant variables should be our symbols
-        if v.owner is None and not isinstance(v, theano.gof.graph.Constant):
+        if v.owner is None and not isinstance(v, theano.graph.basic.Constant):
             # Check it corresponds to a symbol and appears only once
             assert v.name in symbol_names
             assert v.name not in seen
--- a/sympy/printing/theanocode.py
+++ b/sympy/printing/theanocode.py
@@ -494,7 +494,7 @@ def theano_function(inputs, outputs, scalar=False, *,
     toutputs = list(map(code, outputs))

     #fix constant expressions as variables
-    toutputs = [output if isinstance(output, theano.Variable) else tt.as_tensor_variable(output) for output in toutputs]
+    toutputs = [output if isinstance(output, theano.graph.basic.Variable) else tt.as_tensor_variable(output) for output in toutputs]

     if len(toutputs) == 1:
         toutputs = toutputs[0]
```
But `test_theanocode.py::test_Derivative` still fails to pass with
```
FAILED test_theanocode.py::test_Derivative - theano.graph.fg.MissingInputError: Input 0 of the graph (indices start from 0), used to compute Elemwise{second,no_inplace}(x, TensorConstant{1.0}), was not provided and not given a value.
```
I'm wondering if there is something missing in my patch. Do you have any suggestions?
Thanks",,,2021-03-04 11:40:13,2021-03-29 22:16:36,"jsmolic mentioned 2021-03-04 22:55:25,jsmolic subscribed 2021-03-04 22:55:25,brandonwillard mentioned 2021-03-05 16:08:00,brandonwillard subscribed 2021-03-05 16:08:00,jsmolic closed 2021-03-29 22:16:36",brandonwillard jsmolic,4
274,320,Aesara import with GPU fails,SyedMuhammadAli,"## Aesara doesn't seem work with old GPU (Nvidia 630M - CUDA compatibility 2.1). The package ""pygpu"" could not be loaded, but the installation via pip completed without errors. Error trace below:

### When using ""device = gpu"" in .aesarac.txt
```python
>>> import aesara
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\\Users\\syedali\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\aesara\\__init__.py"", line 62, in <module>
    from aesara.configdefaults import config
  File ""C:\\Users\\syedali\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\aesara\\configdefaults.py"", line 1837, in <module>
    add_basic_configvars()
  File ""C:\\Users\\syedali\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\aesara\\configdefaults.py"", line 347, in add_basic_configvars
    in_c_key=False,
  File ""C:\\Users\\syedali\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\aesara\\configparser.py"", line 177, in add
    configparam.__get__(self, type(self), delete_key=True)
  File ""C:\\Users\\syedali\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\aesara\\configparser.py"", line 361, in __get__
    self.__set__(cls, val_str)
  File ""C:\\Users\\syedali\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\aesara\\configparser.py"", line 369, in __set__
    applied = self.apply(val)
  File ""C:\\Users\\syedali\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\aesara\\configparser.py"", line 325, in apply
    return self._apply(value)
  File ""C:\\Users\\syedali\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\aesara\\configparser.py"", line 464, in _apply
    ""You are tring to use the old GPU back-end. ""
ValueError: You are tring to use the old GPU back-end. It was removed from Aesara. Use device=cuda* now. See https://github.com/pymc-devs/aesara/wiki/Converting-to-the-new-gpu-back-end%28gpuarray%29 for more information.
```

### When using ""device = cuda*"" in .aesarac.txt
```python
>>> import aesara
ERROR (aesara.gpuarray): pygpu was configured but could not be imported or is too old (version 0.7 or higher required)
NoneType: None
```




## Versions and main components

* Aesara version: 2.02
* Aesara config (`python -c ""import aesara; print(aesara.config)""`)
* Python version: 3.6.8
* Operating system: Windows 10
* How did you install Aesara: (pip install aesara)
",,,2021-03-06 00:52:59,2021-03-06 21:22:48,brandonwillard closed 2021-03-06 21:22:48,SyedMuhammadAli brandonwillard,1
276,322,No JAX conversion for the given `Op`: DotModulo,twiecki,"Trying to run ADVI in JAX mode.

Reproducing example:
```python
import aesara
aesara.config.mode = ""JAX""

import pymc as pm

with pm.Model():
    x = pm.Normal(""x"")
    pm.fit()
```

Error:
```
File ~/miniforge3/envs/pymc4/lib/python3.10/site-packages/pymc/variational/inference.py:744, in fit(n, method, model, random_seed, start, inf_kwargs, **kwargs)
    742 else:
    743     raise TypeError(f""method should be one of {set(_select.keys())} or Inference instance"")
--> 744 return inference.fit(n, **kwargs)

File ~/miniforge3/envs/pymc4/lib/python3.10/site-packages/pymc/variational/inference.py:138, in Inference.fit(self, n, score, callbacks, progressbar, **kwargs)
    136     callbacks = []
    137 score = self._maybe_score(score)
--> 138 step_func = self.objective.step_function(score=score, **kwargs)
    139 if progressbar:
    140     progress = progress_bar(range(n), display=progressbar)

File ~/miniforge3/envs/pymc4/lib/python3.10/site-packages/aesara/configparser.py:47, in _ChangeFlagsDecorator.__call__.<locals>.res(*args, **kwargs)
     44 @wraps(f)
     45 def res(*args, **kwargs):
     46     with self:
---> 47         return f(*args, **kwargs)

File ~/miniforge3/envs/pymc4/lib/python3.10/site-packages/pymc/variational/opvi.py:355, in ObjectiveFunction.step_function(self, obj_n_mc, tf_n_mc, obj_optimizer, test_optimizer, more_obj_params, more_tf_params, more_updates, more_replacements, total_grad_norm_constraint, score, fn_kwargs)
    353 if score and not self.op.returns_loss:
    354     raise NotImplementedError(""%s does not have loss"" % self.op)
--> 355 updates = self.updates(
    356     obj_n_mc=obj_n_mc,
    357     tf_n_mc=tf_n_mc,
    358     obj_optimizer=obj_optimizer,
    359     test_optimizer=test_optimizer,
    360     more_obj_params=more_obj_params,
    361     more_tf_params=more_tf_params,
    362     more_updates=more_updates,
    363     more_replacements=more_replacements,
    364     total_grad_norm_constraint=total_grad_norm_constraint,
    365 )
    366 if score:
    367     step_fn = compile_pymc([], updates.loss, updates=updates, **fn_kwargs)

File ~/miniforge3/envs/pymc4/lib/python3.10/site-packages/pymc/variational/opvi.py:244, in ObjectiveFunction.updates(self, obj_n_mc, tf_n_mc, obj_optimizer, test_optimizer, more_obj_params, more_tf_params, more_updates, more_replacements, total_grad_norm_constraint)
    242     if more_tf_params:
    243         _warn_not_used(""more_tf_params"", self.op)
--> 244 self.add_obj_updates(
    245     resulting_updates,
    246     obj_n_mc=obj_n_mc,
    247     obj_optimizer=obj_optimizer,
    248     more_obj_params=more_obj_params,
    249     more_replacements=more_replacements,
    250     total_grad_norm_constraint=total_grad_norm_constraint,
    251 )
    252 resulting_updates.update(more_updates)
    253 return resulting_updates

File ~/miniforge3/envs/pymc4/lib/python3.10/site-packages/pymc/variational/opvi.py:289, in ObjectiveFunction.add_obj_updates(self, updates, obj_n_mc, obj_optimizer, more_obj_params, more_replacements, total_grad_norm_constraint)
    287 if more_replacements is None:
    288     more_replacements = dict()
--> 289 obj_target = self(
    290     obj_n_mc, more_obj_params=more_obj_params, more_replacements=more_replacements
    291 )
    292 grads = pm.updates.get_or_compute_grads(obj_target, self.obj_params + more_obj_params)
    293 if total_grad_norm_constraint is not None:

File ~/miniforge3/envs/pymc4/lib/python3.10/site-packages/aesara/configparser.py:47, in _ChangeFlagsDecorator.__call__.<locals>.res(*args, **kwargs)
     44 @wraps(f)
     45 def res(*args, **kwargs):
     46     with self:
---> 47         return f(*args, **kwargs)

File ~/miniforge3/envs/pymc4/lib/python3.10/site-packages/pymc/variational/opvi.py:407, in ObjectiveFunction.__call__(self, nmc, **kwargs)
    405     m = 1.0
    406 a = self.op.apply(self.tf)
--> 407 a = self.approx.set_size_and_deterministic(a, nmc, 0, kwargs.get(""more_replacements""))
    408 return m * self.op.T(a)

File ~/miniforge3/envs/pymc4/lib/python3.10/site-packages/aesara/configparser.py:47, in _ChangeFlagsDecorator.__call__.<locals>.res(*args, **kwargs)
     44 @wraps(f)
     45 def res(*args, **kwargs):
     46     with self:
---> 47         return f(*args, **kwargs)

File ~/miniforge3/envs/pymc4/lib/python3.10/site-packages/pymc/variational/opvi.py:1359, in Approximation.set_size_and_deterministic(self, node, s, d, more_replacements)
   1357 _node = node
   1358 optimizations = self.get_optimization_replacements(s, d)
-> 1359 flat2rand = self.make_size_and_deterministic_replacements(s, d, more_replacements)
   1360 node = aesara.clone_replace(node, optimizations)
   1361 node = aesara.clone_replace(node, flat2rand)

File ~/miniforge3/envs/pymc4/lib/python3.10/site-packages/pymc/variational/opvi.py:1333, in Approximation.make_size_and_deterministic_replacements(self, s, d, more_replacements)
   1331 flat2rand = collections.OrderedDict()
   1332 for g in self.groups:
-> 1333     flat2rand.update(g.make_size_and_deterministic_replacements(s, d, more_replacements))
   1334 flat2rand.update(more_replacements)
   1335 return flat2rand

File ~/miniforge3/envs/pymc4/lib/python3.10/site-packages/pymc/variational/opvi.py:1067, in Group.make_size_and_deterministic_replacements(self, s, d, more_replacements)
   1050 def make_size_and_deterministic_replacements(self, s, d, more_replacements=None):
   1051     """"""*Dev* - creates correct replacements for initial depending on
   1052     sample size and deterministic flag
   1053
   (...)
   1065     dict with replacements for initial
   1066     """"""
-> 1067     initial = self._new_initial(s, d, more_replacements)
   1068     initial = at.specify_shape(initial, self.symbolic_initial.type.shape)
   1069     if more_replacements:

File ~/miniforge3/envs/pymc4/lib/python3.10/site-packages/pymc/variational/opvi.py:978, in Group._new_initial(self, size, deterministic, more_replacements)
    976         return at.ones(shape, dtype) * dist_map
    977     else:
--> 978         return getattr(self._rng, dist_name)(size=shape)
    979 else:
    980     sample = getattr(self._rng, dist_name)(size=shape)

File ~/miniforge3/envs/pymc4/lib/python3.10/site-packages/aesara/sandbox/rng_mrg.py:1184, in MRG_RandomStream.normal(self, size, avg, std, ndim, dtype, nstreams, truncate, **kwargs)
   1182     n_odd_samples = prod(size, dtype=""int64"")
   1183 n_even_samples = n_odd_samples + n_odd_samples % 2
-> 1184 uniform = self.uniform(
   1185     (n_even_samples,),
   1186     low=0.0,
   1187     high=1.0,
   1188     ndim=1,
   1189     dtype=dtype,
   1190     nstreams=nstreams,
   1191     **kwargs,
   1192 )
   1194 # box-muller transform
   1195 u1 = uniform[: n_even_samples // 2]

File ~/miniforge3/envs/pymc4/lib/python3.10/site-packages/aesara/sandbox/rng_mrg.py:914, in MRG_RandomStream.uniform(self, size, low, high, ndim, dtype, nstreams, **kwargs)
    912 if nstreams is None:
    913     nstreams = self.n_streams(size)
--> 914 rstates = self.get_substream_rstates(nstreams, dtype)
    916 d = {}
    917 if ""target"" in kwargs:

File ~/miniforge3/envs/pymc4/lib/python3.10/site-packages/aesara/configparser.py:47, in _ChangeFlagsDecorator.__call__.<locals>.res(*args, **kwargs)
     44 @wraps(f)
     45 def res(*args, **kwargs):
     46     with self:
---> 47         return f(*args, **kwargs)

File ~/miniforge3/envs/pymc4/lib/python3.10/site-packages/aesara/sandbox/rng_mrg.py:818, in MRG_RandomStream.get_substream_rstates(self, n_streams, dtype, inc_rstate)
    816 # If multMatVect.dot_modulo isn't compiled, compile it.
    817 if multMatVect.dot_modulo is None:
--> 818     multMatVect(rval[0], A1p72, M1, A2p72, M2)
    820 # This way of calling the Aesara fct is done to bypass Aesara overhead.
    821 f = multMatVect.dot_modulo

File ~/miniforge3/envs/pymc4/lib/python3.10/site-packages/aesara/sandbox/rng_mrg.py:66, in multMatVect(v, A, m1, B, m2)
     64     m2_sym = iscalar(""m2"")
     65     o = DotModulo()(A_sym, s_sym, m_sym, A2_sym, s2_sym, m2_sym)
---> 66     multMatVect.dot_modulo = function(
     67         [A_sym, s_sym, m_sym, A2_sym, s2_sym, m2_sym], o, profile=False
     68     )
     70 # This way of calling the Aesara fct is done to bypass Aesara overhead.
     71 f = multMatVect.dot_modulo

File ~/miniforge3/envs/pymc4/lib/python3.10/site-packages/aesara/compile/function/__init__.py:317, in function(inputs, outputs, mode, updates, givens, no_default_updates, accept_inplace, name, rebuild_strict, allow_input_downcast, profile, on_unused_input)
    311     fn = orig_function(
    312         inputs, outputs, mode=mode, accept_inplace=accept_inplace, name=name
    313     )
    314 else:
    315     # note: pfunc will also call orig_function -- orig_function is
    316     #      a choke point that all compilation must pass through
--> 317     fn = pfunc(
    318         params=inputs,
    319         outputs=outputs,
    320         mode=mode,
    321         updates=updates,
    322         givens=givens,
    323         no_default_updates=no_default_updates,
    324         accept_inplace=accept_inplace,
    325         name=name,
    326         rebuild_strict=rebuild_strict,
    327         allow_input_downcast=allow_input_downcast,
    328         on_unused_input=on_unused_input,
    329         profile=profile,
    330         output_keys=output_keys,
    331     )
    332 return fn

File ~/miniforge3/envs/pymc4/lib/python3.10/site-packages/aesara/compile/function/pfunc.py:374, in pfunc(params, outputs, mode, updates, givens, no_default_updates, accept_inplace, name, rebuild_strict, allow_input_downcast, profile, on_unused_input, output_keys, fgraph)
    360     profile = ProfileStats(message=profile)
    362 inputs, cloned_outputs = construct_pfunc_ins_and_outs(
    363     params,
    364     outputs,
   (...)
    371     fgraph=fgraph,
    372 )
--> 374 return orig_function(
    375     inputs,
    376     cloned_outputs,
    377     mode,
    378     accept_inplace=accept_inplace,
    379     name=name,
    380     profile=profile,
    381     on_unused_input=on_unused_input,
    382     output_keys=output_keys,
    383     fgraph=fgraph,
    384 )

File ~/miniforge3/envs/pymc4/lib/python3.10/site-packages/aesara/compile/function/types.py:1759, in orig_function(inputs, outputs, mode, accept_inplace, name, profile, on_unused_input, output_keys, fgraph)
   1747     m = Maker(
   1748         inputs,
   1749         outputs,
   (...)
   1756         fgraph=fgraph,
   1757     )
   1758     with config.change_flags(compute_test_value=""off""):
-> 1759         fn = m.create(defaults)
   1760 finally:
   1761     t2 = time.time()

File ~/miniforge3/envs/pymc4/lib/python3.10/site-packages/aesara/compile/function/types.py:1652, in FunctionMaker.create(self, input_storage, trustme, storage_map)
   1649 start_import_time = aesara.link.c.cmodule.import_time
   1651 with config.change_flags(traceback__limit=config.traceback__compile_limit):
-> 1652     _fn, _i, _o = self.linker.make_thunk(
   1653         input_storage=input_storage_lists, storage_map=storage_map
   1654     )
   1656 end_linker = time.time()
   1658 linker_time = end_linker - start_linker

File ~/miniforge3/envs/pymc4/lib/python3.10/site-packages/aesara/link/basic.py:254, in LocalLinker.make_thunk(self, input_storage, output_storage, storage_map, **kwargs)
    247 def make_thunk(
    248     self,
    249     input_storage: Optional[""InputStorageType""] = None,
   (...)
    252     **kwargs,
    253 ) -> Tuple[""BasicThunkType"", ""InputStorageType"", ""OutputStorageType""]:
--> 254     return self.make_all(
    255         input_storage=input_storage,
    256         output_storage=output_storage,
    257         storage_map=storage_map,
    258     )[:3]

File ~/miniforge3/envs/pymc4/lib/python3.10/site-packages/aesara/link/basic.py:697, in JITLinker.make_all(self, input_storage, output_storage, storage_map)
    694 for k in storage_map:
    695     compute_map[k] = [k.owner is None]
--> 697 thunks, nodes, jit_fn = self.create_jitable_thunk(
    698     compute_map, nodes, input_storage, output_storage, storage_map
    699 )
    701 computed, last_user = gc_helper(nodes)
    703 if self.allow_gc:

File ~/miniforge3/envs/pymc4/lib/python3.10/site-packages/aesara/link/basic.py:646, in JITLinker.create_jitable_thunk(self, compute_map, order, input_storage, output_storage, storage_map)
    619 r""""""Create a thunk for each output of the `Linker`\\s `FunctionGraph`.
    620
    621 This is differs from the other thunk-making function in that it only
   (...)
    642
    643 """"""
    644 output_nodes = [o.owner for o in self.fgraph.outputs]
--> 646 converted_fgraph = self.fgraph_convert(
    647     self.fgraph,
    648     order=order,
    649     input_storage=input_storage,
    650     output_storage=output_storage,
    651     storage_map=storage_map,
    652 )
    654 thunk_inputs = self.create_thunk_inputs(storage_map)
    656 thunks = []

File ~/miniforge3/envs/pymc4/lib/python3.10/site-packages/aesara/link/jax/linker.py:13, in JAXLinker.fgraph_convert(self, fgraph, **kwargs)
     10 def fgraph_convert(self, fgraph, **kwargs):
     11     from aesara.link.jax.dispatch import jax_funcify
---> 13     return jax_funcify(fgraph, **kwargs)

File ~/miniforge3/envs/pymc4/lib/python3.10/functools.py:889, in singledispatch.<locals>.wrapper(*args, **kw)
    885 if not args:
    886     raise TypeError(f'{funcname} requires at least '
    887                     '1 positional argument')
--> 889 return dispatch(args[0].__class__)(*args, **kw)

File ~/miniforge3/envs/pymc4/lib/python3.10/site-packages/aesara/link/jax/dispatch.py:671, in jax_funcify_FunctionGraph(fgraph, node, fgraph_name, **kwargs)
    664 @jax_funcify.register(FunctionGraph)
    665 def jax_funcify_FunctionGraph(
    666     fgraph,
   (...)
    669     **kwargs,
    670 ):
--> 671     return fgraph_to_python(
    672         fgraph,
    673         jax_funcify,
    674         type_conversion_fn=jax_typify,
    675         fgraph_name=fgraph_name,
    676         **kwargs,
    677     )

File ~/miniforge3/envs/pymc4/lib/python3.10/site-packages/aesara/link/utils.py:741, in fgraph_to_python(fgraph, op_conversion_fn, type_conversion_fn, order, input_storage, output_storage, storage_map, fgraph_name, global_env, local_env, get_name_for_object, squeeze_output, **kwargs)
    739 body_assigns = []
    740 for node in order:
--> 741     compiled_func = op_conversion_fn(
    742         node.op, node=node, storage_map=storage_map, **kwargs
    743     )
    745     # Create a local alias with a unique name
    746     local_compiled_func_name = unique_name(compiled_func)

File ~/miniforge3/envs/pymc4/lib/python3.10/functools.py:889, in singledispatch.<locals>.wrapper(*args, **kw)
    885 if not args:
    886     raise TypeError(f'{funcname} requires at least '
    887                     '1 positional argument')
--> 889 return dispatch(args[0].__class__)(*args, **kw)

File ~/miniforge3/envs/pymc4/lib/python3.10/site-packages/aesara/link/jax/dispatch.py:144, in jax_funcify(op, node, storage_map, **kwargs)
    141 @singledispatch
    142 def jax_funcify(op, node=None, storage_map=None, **kwargs):
    143     """"""Create a JAX compatible function from an Aesara `Op`.""""""
--> 144     raise NotImplementedError(f""No JAX conversion for the given `Op`: {op}"")

NotImplementedError: No JAX conversion for the given `Op`: DotModulo
```",wontfix JAX,,2021-03-07 22:36:02,2022-09-15 16:03:40,"twiecki labeled 2021-03-07 22:36:02,kc611 mentioned 2021-03-10 00:35:08,kc611 subscribed 2021-03-10 00:35:08,twiecki mentioned 2022-09-14 21:03:54,twiecki subscribed 2022-09-14 21:03:54,twiecki labeled 2022-09-15 08:43:22,rlouf closed 2022-09-15 16:03:40",rlouf kc611 twiecki brandonwillard,8
277,323,Flood of filelock warnings,twiecki,"Running a normal PyMC3 model and getting a ton of:

```
INFO:filelock:Lock 140695035376688 released on /Users/twiecki/.aesara/compiledir_macOS-10.15.6-x86_64-i386-64bit-i386-3.8.5-64/.lock
```

Already ran `aesara-cache clear` which didn't help.",,,2021-03-07 22:37:01,2022-01-05 22:09:07,brandonwillard closed 2022-01-05 22:09:07,twiecki brandonwillard,2
278,324,Replace handwritten MRG sampler,brandonwillard,"The handwritten MRG sampler in `aesara.sandbox.rng_mrg` seems unnecessary.  It also doesn't use the `aesara.tensor.random` API, which causes it to require special attention in order to stay compatible, well tested, etc.

If we want to improve sampling performance, we should at least provide `RandomVariable`s with C implementations via the [NumPy C API](https://numpy.org/devdocs/reference/random/c-api.html) before resorting to a custom RNG implementation.  Also, MRG isn't exactly the best choice, and NumPy provides more/better choices.",enhancement help wanted random variables,,2021-03-08 01:57:59,2022-09-22 09:59:16,"brandonwillard labeled 2021-03-08 01:57:59,brandonwillard labeled 2021-03-08 01:57:59,brandonwillard labeled 2021-05-19 16:22:22,rlouf closed 2022-09-22 09:59:17",rlouf kc611 brandonwillard,6
279,325,Stop using deprecated numpy aliases of builtin types,michaelosthege,"## Description of your problem or feature request

NumPy `v1.20` deprecated types like `np.bool`, `np.int` that are just aliases of builtins.
See https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations

In the `Cast` `Op` we're running into this deprecation warning: https://github.com/pymc-devs/aesara/blob/65c410b08f971313df24312a52dfce31d20ece72/aesara/scalar/basic.py#L2416

```python
\\scalar\\basic.py:2412: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
  Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
    self.ctor = getattr(np, o_type.dtype)
```

We'll need to find a way to prioritize using the builtins over the aliases from NumPy.

## Versions and main components
* Aesara version: `theano-pymc 1.1.2` and latest `master`
* Python version: irrelevant
* Operating system: irrelevant

",good first issue,,2021-03-08 09:38:57,2022-01-05 22:11:55,"michaelosthege labeled 2021-03-08 09:38:57,brandonwillard closed 2022-01-05 22:11:55",michaelosthege brandonwillard,1
280,327,Attempt to run Python Ops with numba,twiecki,"After an interesting discussion with @brandonwillard he mentioned how our Python implementations of `Op`s that happen in the `.perform()` method could be attempted to be run with `numba.jit` as a way to auto-compile for added speed. If this compilation fails, we can always fall back to the regular Python implementation. This would probably be done in the `make_thunk` code.

Even better would be if we then provided c-level access to the numba-compiled function so that it interplays nicely with our other `COp`s. @aseyboldt mentioned that getting a c-pointer to a numba-compiled function should be possible.  A quick google turned this up: https://numba.pydata.org/numba-doc/dev/user/cfunc.html and this: http://numba.pydata.org/numba-doc/0.8/interface_c.html#using-numba-functions-in-external-code

Related to https://github.com/pymc-devs/aesara/issues/312 which argues for using Cython instead of Numba for a similar idea.",help wanted graph rewriting C-backend,,2021-03-09 09:15:29,2021-05-19 23:16:35,"twiecki labeled 2021-03-09 09:15:29,twiecki labeled 2021-03-09 09:15:29,twiecki labeled 2021-03-09 09:15:29,brandonwillard mentioned 2021-03-09 09:15:29,brandonwillard subscribed 2021-03-09 09:15:29,aseyboldt mentioned 2021-03-09 09:15:29,aseyboldt subscribed 2021-03-09 09:15:29,aseyboldt mentioned 2021-03-22 15:01:52,aseyboldt subscribed 2021-03-22 15:01:52,brandonwillard connected 2021-04-08 23:18:52,brandonwillard connected 2021-05-19 23:16:13,brandonwillard closed 2021-05-19 23:16:35",aseyboldt twiecki brandonwillard,6
281,337,Import UnicodeDecodeError on windows,weidongzhou1994,"when I want to import pyms3, it encounters a problem：UnicodeDecodeError: 'utf-8' codec can't decode byte 0xbb in position 0: invalid start byte

![image](https://user-images.githubusercontent.com/40510496/110569169-1ce4df80-818f-11eb-89c1-2382812bb21a.png)
",,,2021-03-10 02:55:59,2021-03-14 17:20:24,"aesara-devs comment_deleted 2021-03-11 09:59:09,weidongzhou1994 closed 2021-03-14 14:20:43,weidongzhou1994 mentioned 2021-03-14 14:30:44,weidongzhou1994 subscribed 2021-03-14 14:30:44,twiecki transferred 2021-03-14 15:11:10,twiecki renamed 2021-03-14 15:17:00,twiecki reopened 2021-03-14 15:17:05,twiecki closed 2021-03-14 17:20:24",aesara-devs weidongzhou1994 michaelosthege twiecki,12
282,328,Change import aesara.tensor as aet to at,twiecki,"We decided on `import aesara.tensor as aet` but I think it should be `import aesara.tensor as at`. The reason is that the extra character isn't fun to look at for longer math expressions:
`aet.exp(aet.pow(aet.dot()))`
`at.exp(at.pow(at.dot()))`

With that it would also match `np`, `sp`, and the old `tt`. ",good first issue help wanted,,2021-03-10 10:59:30,2022-01-10 12:38:28,"brandonwillard labeled 2021-03-11 16:36:46,brandonwillard labeled 2021-03-11 16:36:46,twiecki renamed 2021-03-11 16:59:12,twiecki unlabeled 2021-03-11 16:59:24,twiecki labeled 2021-03-11 16:59:24,twiecki labeled 2021-03-11 16:59:24,twiecki unlabeled 2021-03-11 16:59:34,brandonwillard mentioned 2021-12-16 13:05:27,brandonwillard subscribed 2021-12-16 13:05:27,brandonwillard mentioned 2021-12-16 16:48:44,brandonwillard subscribed 2021-12-16 16:48:44,twiecki closed 2022-01-10 12:38:29",ricardoV94 twiecki brandonwillard,4
283,330,Conda can't find Aesara,JamesPHoughton,"
As per: https://aesara.readthedocs.io/en/latest/install_ubuntu.html#with-conda I'm trying to install Aesara:
```
conda install aesara
```
And I receive the message:
```
Collecting package metadata (current_repodata.json): done
Solving environment: failed with initial frozen solve. Retrying with flexible solve.
Collecting package metadata (repodata.json): done
Solving environment: failed with initial frozen solve. Retrying with flexible solve.

PackagesNotFoundError: The following packages are not available from current channels:

  - aesara

Current channels:

  - https://repo.anaconda.com/pkgs/main/osx-64
  - https://repo.anaconda.com/pkgs/main/noarch
  - https://repo.anaconda.com/pkgs/r/osx-64
  - https://repo.anaconda.com/pkgs/r/noarch

To search for alternate channels that may provide the conda package you're
looking for, navigate to

    https://anaconda.org

and use the search bar at the top of the page.
```

Possibly an issue with the name change?",,,2021-03-10 22:00:08,2021-03-11 16:11:58,JamesPHoughton closed 2021-03-11 16:11:58,OriolAbril JamesPHoughton,2
284,331,JAX can't find GPU,JamesPHoughton,"## Description of your problem or feature request
Trying to run with a GPU on google colab, abandoned the approach described in [the docs](https://aesara.readthedocs.io/en/latest/tutorial/using_gpu.html) after seeing [this issue](
https://github.com/pymc-devs/aesara/issues/320) and switched to JAX.

However, running the [test script](https://aesara.readthedocs.io/en/latest/tutorial/using_gpu.html#testing-aesara-with-gpu) as test.py:

```
!AESARA_FLAGS='floatX=float32,mode=""JAX""' python test.py
```
gives the error:
```
[Elemwise{exp,no_inplace}(<TensorType(float32, vector)>)]
WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
Looping 1000 times took 0.431833 seconds
Result is [1.2317803 1.6187934 1.5227807 ... 2.2077181 2.2996776 1.6232328]
Used the cpu
```
(Unfortunately setting the TF flag doesn't seem to do anything, so I can't give more than that...)


Do I need to tell JAX more about  how to find the GPU? How?

**Reproducible example.**
You can run this colab notebook including all environment setups here:
https://colab.research.google.com/drive/18JIY-TEio7OiIMck9Wn14NkgTkY7oxif?usp=sharing



## Versions and main components
Everything is latest version as of today, miniconda, aesara, dependencies
- aesara: conda-forge/linux-64::aesara-2.0.2-py38h709712a_2

Aesara config (`python -c ""import aesara; print(aesara.config)""`)
see the notebook: https://aesara.readthedocs.io/en/latest/tutorial/using_gpu.html#testing-aesara-with-gpu
",,,2021-03-11 16:44:27,2021-03-11 17:06:31,brandonwillard closed 2021-03-11 17:06:31,JamesPHoughton brandonwillard,3
285,332,Add deprecation warnings to gpuarray-related docs,brandonwillard,"We need to update the documentation to include a warning that says we're not actively maintaining the `gpuarray`-related code.

For that matter, we might need to do the same for the entire `gpuarray` sub-package.",documentation good first issue help wanted important,,2021-03-11 17:10:58,2022-08-04 17:05:31,"brandonwillard labeled 2021-03-11 17:10:58,brandonwillard labeled 2021-03-11 17:10:58,brandonwillard labeled 2021-03-11 17:10:58,brandonwillard labeled 2021-03-11 17:10:58,brandonwillard closed 2022-08-04 17:05:32",brandonwillard,1
287,335,C Elemwise implementation doesn't broadcast variables,brandonwillard,"I'm seeing a very weird error in `Elemwise`:

First, here's a basic broadcasting operation in NumPy:

```python
import numpy as np

x = np.array([[-1.32720483],
              [ 0.23442016]])

m = np.array([0., 0.])

z = x - m
```

```python
>>> z
array([[-1.32720483, -1.32720483],
       [ 0.23442016,  0.23442016]])
```

In Aesara, here's the equivalent operation using `TensorConstant`s:

```python
import aesara
import aesara.tensor as at


x_at = at.as_tensor(x)
m_at = at.as_tensor(m)

z_at = x_at - m_at

```

```python
>>> aesara.dprint(z_at)
Elemwise{sub,no_inplace} [id A] ''
 |TensorConstant{[[-1.32720..23442016]]} [id B]
 |InplaceDimShuffle{x,0} [id C] ''
   |TensorConstant{(2,) of 0.0} [id D]
```

The resulting graph is a simple `Elemwise` for the subtraction `Op`&#x2013;as expected. There's also an `InplaceDimShuffle` that adds a broadcastable dimension to the second argument, so that both inputs have the same number of dimensions. This `InplaceDimShuffle` is equivalent to `np.expand_dims(m, 0)`, which&#x2013;when subtracted from `x`&#x2013;yields the same value as `z`.

So far, everything is good, because

```python
>>> np.array_equal(aesara.function([], z_at)(), z)
True
```

Now, when we replace the `TensorConstant`s with generic `TensorVariable`s, we get a strange error:

```python
x_v = at.matrix(""x"")
m_v = at.vector(""m"")

z_v = x_v - m_v
```

```python
>>> aesara.function([x_v, m_v], z_v)(x, m)
<ipython-input-23-66cc28afa70f> in <module>
----> 1 aesara.function([x_v, m_v], z_v)(x, m)

~/projects/code/python/Aesara/aesara/compile/function/types.py in __call__(self, *args, **kwargs)
    989                     node=self.fn.nodes[self.fn.position_of_error],
    990                     thunk=thunk,
--> 991                     storage_map=getattr(self.fn, ""storage_map"", None),
    992                 )
    993             else:

~/projects/code/python/Aesara/aesara/link/utils.py in raise_with_op(fgraph, node, thunk, exc_info, storage_map)
    506         # Some exception need extra parameter in inputs. So forget the
    507         # extra long error message in that case.
--> 508     raise exc_value.with_traceback(exc_trace)
    509
    510

~/projects/code/python/Aesara/aesara/compile/function/types.py in __call__(self, *args, **kwargs)
    973             outputs = (
    974                 self.fn()
--> 975                 if output_subset is None
    976                 else self.fn(output_subset=output_subset)
    977             )

ValueError: Input dimension mis-match. (input[0].shape[1] = 1, input[1].shape[1] = 2)
Apply node that caused the error: Elemwise{sub,no_inplace}(x, InplaceDimShuffle{x,0}.0)
Toposort index: 1
Inputs types: [TensorType(float64, matrix), TensorType(float64, row)]
Inputs shapes: [(2, 1), (1, 2)]
Inputs strides: [(8, 8), (16, 8)]
Inputs values: [array([[-1.32720483],
       [ 0.23442016]]), array([[0., 0.]])]
Outputs clients: [['output']]
```

We can emulate this issue using the Python implementation of `Elemwise`, as well:

```python
>>> aesara.function([x_v, m_v], z_v, mode=""FAST_COMPILE"")(x, m)
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
~/projects/code/python/Aesara/aesara/link/vm.py in __call__(self)
    312                 ):
--> 313                     thunk()
    314                     for old_s in old_storage:

~/projects/code/python/Aesara/aesara/graph/op.py in rval(p, i, o, n)
    472             def rval(p=p, i=node_input_storage, o=node_output_storage, n=node):
--> 473                 r = p(n, [x[0] for x in i], o)
    474                 for o in node.outputs:

~/projects/code/python/Aesara/aesara/tensor/elemwise.py in perform(self, node, inputs, output_storage)
    760                 base_exc_str = f""Dimension mismatch; shapes are {', '.join(msg)}""
--> 761                 raise ValueError(base_exc_str)
    762

ValueError: Dimension mismatch; shapes are (2, 1), (*, 2)
```

From this output, we can see that this erroneous error is apparently the result of some bad input validation code in `Elemwise`.

The same is true for the C implementation, although that's a little less apparent from the output. In this case, the C code for this validation step is [here](https://github.com/pymc-devs/aesara/blob/master/aesara/tensor/elemwise_cgen.py#L98).",bug important,,2021-03-13 19:13:58,2022-05-06 20:56:04,"brandonwillard labeled 2021-03-13 19:13:58,brandonwillard labeled 2021-03-13 19:13:58,brandonwillard pinned 2021-03-13 19:22:42,brandonwillard renamed 2021-03-14 01:31:27,brandonwillard unpinned 2021-05-23 02:42:30,ricardoV94 closed 2022-05-06 20:56:04",ricardoV94 twiecki brandonwillard,6
289,338,Lots of new UserWarnings complaining variable is not in FunctionGraph,rodluger,"## Description of your problem or feature request

As of #298 I am getting *hundreds* of `UserWarnings` on code that used to run seamlessly. The warnings are issued [here](https://github.com/pymc-devs/aesara/blob/f26a508621beae0e8581c51d909c1e72a40b2c43/aesara/graph/fg.py#L492-L503). Previously, `theano` and `theano-pymc` simply failed silently at this step. Perhaps there's some config setting I can change to suppress these, but I couldn't find anything in the docs.

**Please provide a minimal, self-contained, and reproducible example.**

Below is just one of many ways to trigger the `UserWarning`:

```python
import aesara.tensor as tt

y = [0.0, 1.0]
print(tt.max(y).eval())
```

**Please provide the full traceback of any errors.**

The output is correct, but a warning is raised complaining about how a variable cannot be replaced as it is not in the `FunctionGraph`.

```python
/.../lib/python3.9/site-packages/aesara/graph/fg.py:500: UserWarning: Variable argmax cannot be replaced; it isn't in the FunctionGraph
  warnings.warn(
1.0
```

**Please provide any additional information below.**



## Versions and main components

* Aesara version: 2.0.2

<details><summary>Aesara config:</summary>

```
floatX ({'float64', 'float32', 'float16'}) 
    Doc:  Default floating-point precision for python casts.

Note: float16 support is experimental, use at your own risk.
    Value:  float64

warn_float64 ({'raise', 'pdb', 'ignore', 'warn'}) 
    Doc:  Do an action when a tensor variable with float64 dtype is created. They can't be run on the GPU with the current(old) gpu back-end and are slow with gamer GPUs.
    Value:  ignore

pickle_test_value (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f8b80f99c40>>) 
    Doc:  Dump test values while pickling model. If True, test values will be dumped with model.
    Value:  True

cast_policy ({'numpy+floatX', 'custom'}) 
    Doc:  Rules for implicit type casting
    Value:  custom

deterministic ({'more', 'default'}) 
    Doc:  If `more`, sometimes we will select some implementation that are more deterministic, but slower. In particular, on the GPU, we will avoid using AtomicAdd. Sometimes we will still use non-deterministic implementaion, e.g. when we do not have a GPU implementation that is deterministic. Also see the dnn.conv.algo* flags to cover more cases.
    Value:  default

device (cpu, opencl*, cuda*) 
    Doc:  Default device for computations. If cuda* or opencl*, change thedefault to try to move computation to the GPU. Do not use upper caseletters, only lower case even if NVIDIA uses capital letters. 'gpu' means let the driver select the gpu (needed for gpu in exclusive mode). 'gpuX' mean use the gpu number X.
    Value:  cpu

init_gpu_device (, opencl*, cuda*) 
    Doc:  Initialize the gpu device to use, works only if device=cpu. Unlike 'device', setting this option will NOT move computations, nor shared variables, to the specified GPU. It can be used to run GPU-specific tests on a particular GPU.
    Value:  

force_device (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f8b80fa3640>>) 
    Doc:  Raise an error if we can't use the specified device
    Value:  False

conv__assert_shape (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f8b80fa3670>>) 
    Doc:  If True, AbstractConv* ops will verify that user-provided shapes match the runtime shapes (debugging option, may slow down compilation)
    Value:  False

print_global_stats (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f8b80fa36a0>>) 
    Doc:  Print some global statistics (time spent) at the end
    Value:  False

<aesara.configparser.ContextsParam object at 0x7f8b80fa33a0>
    Doc:  
        Context map for multi-gpu operation. Format is a
        semicolon-separated list of names and device names in the
        'name->dev_name' format. An example that would map name 'test' to
        device 'cuda0' and name 'test2' to device 'opencl0:0' follows:
        ""test->cuda0;test2->opencl0:0"".

        Invalid context names are 'cpu', 'cuda*' and 'opencl*'
        
    Value:  

print_active_device (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f8b80fa3160>>) 
    Doc:  Print active device at when the GPU device is initialized.
    Value:  True

gpuarray__preallocate (<class 'float'>) 
    Doc:  If negative it disables the allocation cache. If
                 between 0 and 1 it enables the allocation cache and
                 preallocates that fraction of the total GPU memory.  If 1
                 or greater it will preallocate that amount of memory (in
                 megabytes).
    Value:  0.0

gpuarray__sched ({'multi', 'default', 'single'}) 
    Doc:  The sched parameter passed for context creation to pygpu.
                    With CUDA, using ""multi"" is equivalent to using the parameter
                    cudaDeviceScheduleBlockingSync. This is useful to lower the
                    CPU overhead when waiting for GPU. One user found that it
                    speeds up his other processes that was doing data augmentation.
                 
    Value:  default

gpuarray__single_stream (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f8b80fa34f0>>) 
    Doc:  
                 If your computations are mostly lots of small elements,
                 using single-stream will avoid the synchronization
                 overhead and usually be faster.  For larger elements it
                 does not make a difference yet.  In the future when true
                 multi-stream is enabled in libgpuarray, this may change.
                 If you want to make sure to have optimal performance,
                 check both options.
                 
    Value:  True

cuda__root (<class 'str'>) 
    Doc:  Location of the cuda installation
    Value:  

cuda__include_path (<class 'str'>) 
    Doc:  Location of the cuda includes
    Value:  

assert_no_cpu_op ({'raise', 'pdb', 'ignore', 'warn'}) 
    Doc:  Raise an error/warning if there is a CPU op in the computational graph.
    Value:  ignore

unpickle_function (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f8b80fa3550>>) 
    Doc:  Replace unpickled Aesara functions with None. This is useful to unpickle old graphs that pickled them when it shouldn't
    Value:  True

reoptimize_unpickled_function (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f8b80fa3730>>) 
    Doc:  Re-optimize the graph when an Aesara function is unpickled from the disk.
    Value:  False

dnn__conv__algo_fwd ({'fft', 'small', 'time_on_shape_change', 'none', 'fft_tiling', 'large', 'guess_once', 'time_once', 'winograd', 'guess_on_shape_change', 'winograd_non_fused'}) 
    Doc:  Default implementation to use for cuDNN forward convolution.
    Value:  small

dnn__conv__algo_bwd_data ({'fft', 'deterministic', 'time_on_shape_change', 'none', 'fft_tiling', 'guess_once', 'time_once', 'winograd', 'guess_on_shape_change', 'winograd_non_fused'}) 
    Doc:  Default implementation to use for cuDNN backward convolution to get the gradients of the convolution with regard to the inputs.
    Value:  none

dnn__conv__algo_bwd_filter ({'fft', 'deterministic', 'small', 'time_on_shape_change', 'none', 'fft_tiling', 'guess_once', 'time_once', 'guess_on_shape_change', 'winograd_non_fused'}) 
    Doc:  Default implementation to use for cuDNN backward convolution to get the gradients of the convolution with regard to the filters.
    Value:  none

dnn__conv__precision ({'float16', 'as_input_f32', 'float32', 'float64', 'as_input'}) 
    Doc:  Default data precision to use for the computation in cuDNN convolutions (defaults to the same dtype as the inputs of the convolutions, or float32 if inputs are float16).
    Value:  as_input_f32

dnn__base_path (<class 'str'>) 
    Doc:  Install location of cuDNN.
    Value:  

dnn__include_path (<class 'str'>) 
    Doc:  Location of the cudnn header
    Value:  

dnn__library_path (<class 'str'>) 
    Doc:  Location of the cudnn link library.
    Value:  

dnn__bin_path (<class 'str'>) 
    Doc:  Location of the cuDNN load library (on non-windows platforms, this is the same as dnn__library_path)
    Value:  

dnn__enabled ({'no_check', 'False', 'auto', 'True'}) 
    Doc:  'auto', use cuDNN if available, but silently fall back to not using it if not present. If True and cuDNN can not be used, raise an error. If False, disable cudnn even if present. If no_check, assume present and the version between header and library match (so less compilation at context init)
    Value:  auto

magma__include_path (<class 'str'>) 
    Doc:  Location of the magma header
    Value:  

magma__library_path (<class 'str'>) 
    Doc:  Location of the magma library
    Value:  

magma__enabled (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f8b80fa3ac0>>) 
    Doc:   If True, use magma for matrix computation. If False, disable magma
    Value:  False

<aesara.configparser.ConfigParam object at 0x7f8b80fa3af0>
    Doc:  Default compilation mode
    Value:  Mode

cxx (<class 'str'>) 
    Doc:  The C++ compiler to use. Currently only g++ is supported, but supporting additional compilers should not be too difficult. If it is empty, no C++ code is compiled.
    Value:  /usr/bin/clang++

linker ({'vm', 'cvm', 'c|py_nogc', 'cvm_nogc', 'c', 'vm_nogc', 'c|py', 'py'}) 
    Doc:  Default linker used if the aesara flags mode is Mode
    Value:  cvm

allow_gc (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f8b80fa3b20>>) 
    Doc:  Do we default to delete intermediate results during Aesara function calls? Doing so lowers the memory requirement, but asks that we reallocate memory at the next function call. This is implemented for the default linker, but may not work for all linkers.
    Value:  True

optimizer ({'o3', 'merge', 'None', 'fast_compile', 'o2', 'fast_run', 'o1', 'unsafe', 'o4'}) 
    Doc:  Default optimizer. If not None, will use this optimizer with the Mode
    Value:  o4

optimizer_verbose (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f8b80fa3bb0>>) 
    Doc:  If True, we print all optimization being applied
    Value:  False

on_opt_error ({'raise', 'pdb', 'ignore', 'warn'}) 
    Doc:  What to do when an optimization crashes: warn and skip it, raise the exception, or fall into the pdb debugger.
    Value:  warn

nocleanup (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f8b80fa3d00>>) 
    Doc:  Suppress the deletion of code files that did not compile cleanly
    Value:  False

on_unused_input ({'raise', 'ignore', 'warn'}) 
    Doc:  What to do if a variable in the 'inputs' list of  aesara.function() is not used in the graph.
    Value:  raise

gcc__cxxflags (<class 'str'>) 
    Doc:  Extra compiler flags for gcc
    Value:  

cmodule__warn_no_version (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f8b80fa3c10>>) 
    Doc:  If True, will print a warning when compiling one or more Op with C code that can't be cached because there is no c_code_cache_version() function associated to at least one of those Ops.
    Value:  False

cmodule__remove_gxx_opt (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f8b80fa3d60>>) 
    Doc:  If True, will remove the -O* parameter passed to g++.This is useful to debug in gdb modules compiled by Aesara.The parameter -g is passed by default to g++
    Value:  False

cmodule__compilation_warning (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f8b80fa3df0>>) 
    Doc:  If True, will print compilation warnings.
    Value:  False

cmodule__preload_cache (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f8b80fa3e20>>) 
    Doc:  If set to True, will preload the C module cache at import time
    Value:  False

cmodule__age_thresh_use (<class 'int'>) 
    Doc:  In seconds. The time after which Aesara won't reuse a compile c module.
    Value:  2073600

cmodule__debug (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f8b80fa3e80>>) 
    Doc:  If True, define a DEBUG macro (if not exists) for any compiled C code.
    Value:  False

compile__wait (<class 'int'>) 
    Doc:  Time to wait before retrying to acquire the compile lock.
    Value:  5

compile__timeout (<class 'int'>) 
    Doc:  In seconds, time that a process will wait before deciding to
    override an existing lock. An override only happens when the existing
    lock is held by the same owner *and* has not been 'refreshed' by this
    owner for more than this period. Refreshes are done every half timeout
    period for running processes.
    Value:  120

ctc__root (<class 'str'>) 
    Doc:  Directory which contains the root of Baidu CTC library. It is assumed         that the compiled library is either inside the build, lib or lib64         subdirectory, and the header inside the include directory.
    Value:  

tensor__cmp_sloppy (<class 'int'>) 
    Doc:  Relax aesara.tensor.math._allclose (0) not at all, (1) a bit, (2) more
    Value:  0

tensor__local_elemwise_fusion (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f8b80fbb040>>) 
    Doc:  Enable or not in fast_run mode(fast_run optimization) the elemwise fusion optimization
    Value:  True

lib__amblibm (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f8b80fbb100>>) 
    Doc:  Use amd's amdlibm numerical library
    Value:  False

tensor__insert_inplace_optimizer_validate_nb (<class 'int'>) 
    Doc:  -1: auto, if graph have less then 500 nodes 1, else 10
    Value:  -1

traceback__limit (<class 'int'>) 
    Doc:  The number of stack to trace. -1 mean all.
    Value:  8

traceback__compile_limit (<class 'int'>) 
    Doc:  The number of stack to trace to keep during compilation. -1 mean all. If greater then 0, will also make us save Aesara internal stack trace.
    Value:  0

experimental__unpickle_gpu_on_cpu (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f8b80fbb280>>) 
    Doc:  Allow unpickling of pickled GpuArrays as numpy.ndarrays.This is useful, if you want to open a GpuArray without having cuda installed.If you have cuda installed, this will force unpickling tobe done on the cpu to numpy.ndarray.Please be aware that this may get you access to the data,however, trying to unpicke gpu functions will not succeed.This flag is experimental and may be removed any time, whengpu<>cpu transparency is solved.
    Value:  False

experimental__local_alloc_elemwise (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f8b80f653a0>>) 
    Doc:  DEPRECATED: If True, enable the experimental optimization local_alloc_elemwise. Generates error if not True. Use optimizer_excluding=local_alloc_elemwise to dsiable.
    Value:  True

experimental__local_alloc_elemwise_assert (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f8b7861eb20>>) 
    Doc:  When the local_alloc_elemwise is applied, add an assert to highlight shape errors.
    Value:  True

warn__ignore_bug_before ({'0.8.1', '0.5', '0.10', '0.4.1', '0.8', 'None', '1.0', 'all', '0.3', '1.0.3', '1.0.4', '0.8.2', '0.7', '0.9', '1.0.5', '0.4', '1.0.1', '0.6', '1.0.2'}) 
    Doc:  If 'None', we warn about all Aesara bugs found by default. If 'all', we don't warn about Aesara bugs found by default. If a version, we print only the warnings relative to Aesara bugs found after that version. Warning for specific bugs can be configured with specific [warn] flags.
    Value:  0.9

exception_verbosity ({'low', 'high'}) 
    Doc:  If 'low', the text of exceptions will generally refer to apply nodes with short names such as Elemwise{add_no_inplace}. If 'high', some exceptions will also refer to apply nodes with long descriptions  like:
        A. Elemwise{add_no_inplace}
                B. log_likelihood_v_given_h
                C. log_likelihood_h
    Value:  low

print_test_value (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f8b80fbb340>>) 
    Doc:  If 'True', the __eval__ of an Aesara variable will return its test_value when this is available. This has the practical conseguence that, e.g., in debugging `my_var` will print the same as `my_var.tag.test_value` when a test value is defined.
    Value:  False

compute_test_value ({'off', 'warn', 'raise', 'ignore', 'pdb'}) 
    Doc:  If 'True', Aesara will run each op at graph build time, using Constants, SharedVariables and the tag 'test_value' as inputs to the function. This helps the user track down problems in the graph before it gets optimized.
    Value:  off

compute_test_value_opt ({'off', 'warn', 'raise', 'ignore', 'pdb'}) 
    Doc:  For debugging Aesara optimization only. Same as compute_test_value, but is used during Aesara optimization
    Value:  off

check_input (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f8b80fbb3d0>>) 
    Doc:  Specify if types should check their input in their C code. It can be used to speed up compilation, reduce overhead (particularly for scalars) and reduce the number of generated C files.
    Value:  True

NanGuardMode__nan_is_error (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f8b80fbb400>>) 
    Doc:  Default value for nan_is_error
    Value:  True

NanGuardMode__inf_is_error (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f8b80fbb430>>) 
    Doc:  Default value for inf_is_error
    Value:  True

NanGuardMode__big_is_error (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f8b80fbb4c0>>) 
    Doc:  Default value for big_is_error
    Value:  True

NanGuardMode__action ({'raise', 'pdb', 'warn'}) 
    Doc:  What NanGuardMode does when it finds a problem
    Value:  raise

DebugMode__patience (<class 'int'>) 
    Doc:  Optimize graph this many times to detect inconsistency
    Value:  10

DebugMode__check_c (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f8b80fbb550>>) 
    Doc:  Run C implementations where possible
    Value:  True

DebugMode__check_py (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f8b80fbb5e0>>) 
    Doc:  Run Python implementations where possible
    Value:  True

DebugMode__check_finite (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f8b80fbb610>>) 
    Doc:  True -> complain about NaN/Inf results
    Value:  True

DebugMode__check_strides (<class 'int'>) 
    Doc:  Check that Python- and C-produced ndarrays have same strides. On difference: (0) - ignore, (1) warn, or (2) raise error
    Value:  0

DebugMode__warn_input_not_reused (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f8b80fbb670>>) 
    Doc:  Generate a warning when destroy_map or view_map says that an op works inplace, but the op did not reuse the input for its output.
    Value:  True

DebugMode__check_preallocated_output (<class 'str'>) 
    Doc:  Test thunks with pre-allocated memory as output storage. This is a list of strings separated by "":"". Valid values are: ""initial"" (initial storage in storage map, happens with Scan),""previous"" (previously-returned memory), ""c_contiguous"", ""f_contiguous"", ""strided"" (positive and negative strides), ""wrong_size"" (larger and smaller dimensions), and ""ALL"" (all of the above).
    Value:  

DebugMode__check_preallocated_output_ndim (<class 'int'>) 
    Doc:  When testing with ""strided"" preallocated output memory, test all combinations of strides over that number of (inner-most) dimensions. You may want to reduce that number to reduce memory or time usage, but it is advised to keep a minimum of 2.
    Value:  4

profiling__time_thunks (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f8b80fbb700>>) 
    Doc:  Time individual thunks when profiling
    Value:  True

profiling__n_apply (<class 'int'>) 
    Doc:  Number of Apply instances to print by default
    Value:  20

profiling__n_ops (<class 'int'>) 
    Doc:  Number of Ops to print by default
    Value:  20

profiling__output_line_width (<class 'int'>) 
    Doc:  Max line width for the profiling output
    Value:  512

profiling__min_memory_size (<class 'int'>) 
    Doc:  For the memory profile, do not print Apply nodes if the size
                 of their outputs (in bytes) is lower than this threshold
    Value:  1024

profiling__min_peak_memory (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f8b80fbb850>>) 
    Doc:  The min peak memory usage of the order
    Value:  False

profiling__destination (<class 'str'>) 
    Doc:  File destination of the profiling output
    Value:  stderr

profiling__debugprint (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f8b80fbb8b0>>) 
    Doc:  Do a debugprint of the profiled functions
    Value:  False

profiling__ignore_first_call (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f8b80fbb8e0>>) 
    Doc:  Do we ignore the first call of an Aesara function.
    Value:  False

on_shape_error ({'raise', 'warn'}) 
    Doc:  warn: print a warning and use the default value. raise: raise an error
    Value:  warn

openmp (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f8b80fbb940>>) 
    Doc:  Allow (or not) parallel computation on the CPU with OpenMP. This is the default value used when creating an Op that supports OpenMP parallelization. It is preferable to define it via the Aesara configuration file ~/.aesararc or with the environment variable AESARA_FLAGS. Parallelization is only done for some operations that implement it, and even for operations that implement parallelism, each operation is free to respect this flag or not. You can control the number of threads used with the environment variable OMP_NUM_THREADS. If it is set to 1, we disable openmp in Aesara by default.
    Value:  False

openmp_elemwise_minsize (<class 'int'>) 
    Doc:  If OpenMP is enabled, this is the minimum size of vectors for which the openmp parallelization is enabled in element wise ops.
    Value:  200000

optimizer_excluding (<class 'str'>) 
    Doc:  When using the default mode, we will remove optimizer with these tags. Separate tags with ':'.
    Value:  

optimizer_including (<class 'str'>) 
    Doc:  When using the default mode, we will add optimizer with these tags. Separate tags with ':'.
    Value:  

optimizer_requiring (<class 'str'>) 
    Doc:  When using the default mode, we will require optimizer with these tags. Separate tags with ':'.
    Value:  

optdb__position_cutoff (<class 'float'>) 
    Doc:  Where to stop eariler during optimization. It represent the position of the optimizer where to stop.
    Value:  inf

optdb__max_use_ratio (<class 'float'>) 
    Doc:  A ratio that prevent infinite loop in EquilibriumOptimizer.
    Value:  8.0

cycle_detection ({'regular', 'fast'}) 
    Doc:  If cycle_detection is set to regular, most inplaces are allowed,but it is slower. If cycle_detection is set to faster, less inplacesare allowed, but it makes the compilation faster.The interaction of which one give the lower peak memory usage iscomplicated and not predictable, so if you are close to the peakmemory usage, triyng both could give you a small gain.
    Value:  regular

check_stack_trace ({'raise', 'log', 'warn', 'off'}) 
    Doc:  A flag for checking the stack trace during the optimization process. default (off): does not check the stack trace of any optimization log: inserts a dummy stack trace that identifies the optimizationthat inserted the variable that had an empty stack trace.warn: prints a warning if a stack trace is missing and also a dummystack trace is inserted that indicates which optimization insertedthe variable that had an empty stack trace.raise: raises an exception if a stack trace is missing
    Value:  off

metaopt__verbose (<class 'int'>) 
    Doc:  0 for silent, 1 for only warnings, 2 for full output withtimings and selected implementation
    Value:  0

metaopt__optimizer_excluding (<class 'str'>) 
    Doc:  exclude optimizers with these tags. Separate tags with ':'.
    Value:  

metaopt__optimizer_including (<class 'str'>) 
    Doc:  include optimizers with these tags. Separate tags with ':'.
    Value:  

profile (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f8b80fbbc40>>) 
    Doc:  If VM should collect profile information
    Value:  False

profile_optimizer (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f8b80fbbc70>>) 
    Doc:  If VM should collect optimizer profile information
    Value:  False

profile_memory (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f8b80fbbca0>>) 
    Doc:  If VM should collect memory profile information and print it
    Value:  False

<aesara.configparser.ConfigParam object at 0x7f8b80fbbcd0>
    Doc:  Useful only for the vm linkers. When lazy is None, auto detect if lazy evaluation is needed and use the appropriate version. If lazy is True/False, force the version used between Loop/LoopGC and Stack.
    Value:  None

cache_optimizations (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f8b80fbbd00>>) 
    Doc:  WARNING: work in progress, does not work yet. Specify if the optimization cache should be used. This cache will any optimized graph and its optimization. Actually slow downs a lot the first optimization, and could possibly still contains some bugs. Use at your own risks.
    Value:  False

unittests__rseed (<class 'str'>) 
    Doc:  Seed to use for randomized unit tests. Special value 'random' means using a seed of None.
    Value:  666

warn__identify_1pexp_bug (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f8b80fbbdc0>>) 
    Doc:  Warn if Aesara versions prior to 7987b51 (2011-12-18) could have yielded a wrong result due to a bug in the is_1pexp function
    Value:  False

gpu__local_elemwise_fusion (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f8b80fbbe50>>) 
    Doc:  Enable or not in fast_run mode(fast_run optimization) the gpu elemwise fusion optimization
    Value:  True

gpuelemwise__sync (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f8b80fbbe80>>) 
    Doc:  when true, wait that the gpu fct finished and check it error code.
    Value:  True

warn__argmax_pushdown_bug (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f8b80fbbf10>>) 
    Doc:  Warn if in past version of Aesara we generated a bug with the aesara.tensor.nnet.basic.local_argmax_pushdown optimization. Was fixed 27 may 2010
    Value:  False

warn__gpusum_01_011_0111_bug (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f8b80fbbfa0>>) 
    Doc:  Warn if we are in a case where old version of Aesara had a silent bug with GpuSum pattern 01,011 and 0111 when the first dimensions was bigger then 4096. Was fixed 31 may 2010
    Value:  False

warn__sum_sum_bug (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f8b80fbbfd0>>) 
    Doc:  Warn if we are in a case where Aesara version between version 9923a40c7b7a and the 2 august 2010 (fixed date), generated an error in that case. This happens when there are 2 consecutive sums in the graph, bad code was generated. Was fixed 2 August 2010
    Value:  False

warn__sum_div_dimshuffle_bug (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f8b80fbbf40>>) 
    Doc:  Warn if previous versions of Aesara (between rev. 3bd9b789f5e8, 2010-06-16, and cfc6322e5ad4, 2010-08-03) would have given incorrect result. This bug was triggered by sum of division of dimshuffled tensors.
    Value:  False

warn__subtensor_merge_bug (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f8b80fbbf70>>) 
    Doc:  Warn if previous versions of Aesara (before 0.5rc2) could have given incorrect results when indexing into a subtensor with negative stride (for instance, for instance, x[a:b:-1][c]).
    Value:  False

warn__gpu_set_subtensor1 (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f8b80fc20a0>>) 
    Doc:  Warn if previous versions of Aesara (before 0.6) could have given incorrect results when moving to the gpu set_subtensor(x[int vector], new_value)
    Value:  False

warn__vm_gc_bug (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f8b80fc20d0>>) 
    Doc:  There was a bug that existed in the default Aesara configuration, only in the development version between July 5th 2012 and July 30th 2012. This was not in a released version. If your code was affected by this bug, a warning will be printed during the code execution if you use the `linker=vm,vm__lazy=True,warn__vm_gc_bug=True` Aesara flags. This warning is disabled by default as the bug was not released.
    Value:  False

warn__signal_conv2d_interface (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f8b80fc2100>>) 
    Doc:  Warn we use the new signal.conv2d() when its interface changed mid June 2014
    Value:  False

warn__reduce_join (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f8b80fc2130>>) 
    Doc:  Your current code is fine, but Aesara versions prior to 0.7 (or this development version) might have given an incorrect result. To disable this warning, set the Aesara flag warn__reduce_join to False. The problem was an optimization, that modified the pattern ""Reduce{scalar.op}(Join(axis=0, a, b), axis=0)"", did not check the reduction axis. So if the reduction axis was not 0, you got a wrong answer.
    Value:  False

warn__inc_set_subtensor1 (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f8b80fc2160>>) 
    Doc:  Warn if previous versions of Aesara (before 0.7) could have given incorrect results for inc_subtensor and set_subtensor when using some patterns of advanced indexing (indexing with one vector or matrix of ints).
    Value:  False

warn__round (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f8b80fc2190>>) 
    Doc:  Warn when using `tensor.round` with the default mode. Round changed its default from `half_away_from_zero` to `half_to_even` to have the same default as NumPy.
    Value:  False

warn__inc_subtensor1_opt (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f8b80fc21c0>>) 
    Doc:  Warn if previous versions of Aesara (before 0.10) could have given incorrect results when computing inc_subtensor(zeros[idx], x)[idx], when idx is an array of integers with duplicated values.
    Value:  True

compiledir_format (<class 'str'>) 
    Doc:  Format string for platform-dependent compiled module subdirectory
(relative to base_compiledir). Available keys: aesara_version, device,
gxx_version, hostname, numpy_version, platform, processor,
python_bitwidth, python_int_bitwidth, python_version, short_platform.
Defaults to compiledir_%(short_platform)s-%(processor)s-%(python_versi
on)s-%(python_bitwidth)s.
    Value:  compiledir_%(short_platform)s-%(processor)s-%(python_version)s-%(python_bitwidth)s

<aesara.configparser.ConfigParam object at 0x7f8b80fc2280>
    Doc:  platform-independent root directory for compiled modules
    Value:  /Users/rluger/.aesara

<aesara.configparser.ConfigParam object at 0x7f8b80fc2220>
    Doc:  platform-dependent cache directory for compiled modules
    Value:  /Users/rluger/.aesara/compiledir_macOS-10.15.7-x86_64-i386-64bit-i386-3.9.2-64

<aesara.configparser.ConfigParam object at 0x7f8b80fc2250>
    Doc:  Directory to cache pre-compiled kernels for the gpuarray backend.
    Value:  /Users/rluger/.aesara/compiledir_macOS-10.15.7-x86_64-i386-64bit-i386-3.9.2-64/gpuarray_kernels

blas__ldflags (<class 'str'>) 
    Doc:  lib[s] to include for [Fortran] level-3 blas implementation
    Value:  -lblas

blas__check_openmp (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f8b78901220>>) 
    Doc:  Check for openmp library conflict.
WARNING: Setting this to False leaves you open to wrong results in blas-related operations.
    Value:  True

scan__allow_gc (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f8b80fc2070>>) 
    Doc:  Allow/disallow gc inside of Scan (default: False)
    Value:  False

scan__allow_output_prealloc (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f8ba84053a0>>) 
    Doc:  Allow/disallow memory preallocation for outputs inside of scan (default: True)
    Value:  True

scan__debug (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f8ba8405430>>) 
    Doc:  If True, enable extra verbose output related to scan
    Value:  False
```

</details>

* Python version: 3.9.2
* Operating system: Mac OSX Catalina
* How did you install Aesara: pip
",graph rewriting,brandonwillard,2021-03-15 18:42:14,2021-05-20 00:07:15,"brandonwillard labeled 2021-05-07 15:22:04,brandonwillard assigned 2021-05-19 23:13:27,brandonwillard closed 2021-05-20 00:07:15",rodluger brandonwillard,3
290,339,Add SoftmaxGrad Op implementation for JAX,twiecki,@aseyboldt already has an implementation for this.,JAX,OriolAbril,2021-03-16 17:19:58,2021-03-20 17:37:38,"aseyboldt mentioned 2021-03-16 17:19:59,aseyboldt subscribed 2021-03-16 17:19:59,twiecki labeled 2021-03-16 17:20:05,aseyboldt assigned 2021-03-16 17:20:28,OriolAbril assigned 2021-03-17 15:00:02,aseyboldt unassigned 2021-03-17 15:00:09,brandonwillard closed 2021-03-20 17:37:38",aseyboldt OriolAbril twiecki brandonwillard,0
291,340,JAX shape issue,twiecki,"@aseyboldt described an issue where in a tensor-dot the shape of the first two dimensions gets multiplied, but we need more information here.",JAX MWE needed,aseyboldt OriolAbril,2021-03-16 17:22:47,2021-03-20 17:37:38,"twiecki labeled 2021-03-16 17:22:47,aseyboldt mentioned 2021-03-16 17:22:47,aseyboldt subscribed 2021-03-16 17:22:47,aseyboldt assigned 2021-03-16 17:22:47,brandonwillard labeled 2021-03-16 18:14:19,OriolAbril assigned 2021-03-17 15:00:20,brandonwillard closed 2021-03-20 17:37:38",aseyboldt OriolAbril twiecki brandonwillard,0
295,346,Use AST-based approach to Python graph conversion,brandonwillard,"@aseyboldt has prototyped an AST-based approach to `Apply` node composition in [this example](https://github.com/pymc-devs/aesara/issues/327#issuecomment-804131888).  We should be able to replace the current function composition approach used by `jax_funcify` with this and simplify the graph (JIT) compilation process&mdash;for both JAX, Numba, and perhaps Cython as well.",enhancement JAX important backend compatibility,brandonwillard aseyboldt,2021-03-22 18:04:53,2021-05-19 23:15:18,"brandonwillard labeled 2021-03-22 18:04:53,brandonwillard labeled 2021-03-22 18:04:53,brandonwillard labeled 2021-03-22 18:04:53,brandonwillard labeled 2021-03-22 18:04:53,aseyboldt mentioned 2021-03-22 18:04:53,aseyboldt subscribed 2021-03-22 18:04:53,brandonwillard assigned 2021-03-22 21:56:23,aseyboldt assigned 2021-03-22 21:56:23,brandonwillard pinned 2021-04-06 20:45:31,brandonwillard connected 2021-04-08 23:18:52,brandonwillard unpinned 2021-04-15 05:01:50,brandonwillard connected 2021-05-19 23:15:12,brandonwillard closed 2021-05-19 23:15:18",aseyboldt brandonwillard,0
303,355,Fresh install Theano error,sammosummo,"## Totally fresh install, doesn't install Aesara, Theano-PyMC3 fails baby steps

After installation:

```python
from theano import *
import theano.tensor as T

# baby steps
x = T.dscalar('x')
y = T.dscalar('y')
z = x + y
f = function([x, y], z)
print(f(2, 3))
```

Produces this:

```python
WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.

You can find the C code in this temporary file: /var/folders/9w/4y2ckpfn4cz4y94fjg8jy9bm0000gn/T/theano_compilation_error_90qgq8as
library System is not found.
Traceback (most recent call last):
  File ""/Users/samuelrobertmathias/PycharmProjects/FrequencyRT/in_theano.py"", line 8, in <module>
    f = function([x, y], z)
  File ""/Users/samuelrobertmathias/miniconda3/envs/psychoacoustics/lib/python3.9/site-packages/theano/compile/function/__init__.py"", line 337, in function
    fn = pfunc(
  File ""/Users/samuelrobertmathias/miniconda3/envs/psychoacoustics/lib/python3.9/site-packages/theano/compile/function/pfunc.py"", line 524, in pfunc
    return orig_function(
  File ""/Users/samuelrobertmathias/miniconda3/envs/psychoacoustics/lib/python3.9/site-packages/theano/compile/function/types.py"", line 1981, in orig_function
    fn = m.create(defaults)
  File ""/Users/samuelrobertmathias/miniconda3/envs/psychoacoustics/lib/python3.9/site-packages/theano/compile/function/types.py"", line 1836, in create
    _fn, _i, _o = self.linker.make_thunk(
  File ""/Users/samuelrobertmathias/miniconda3/envs/psychoacoustics/lib/python3.9/site-packages/theano/link/basic.py"", line 266, in make_thunk
    return self.make_all(
  File ""/Users/samuelrobertmathias/miniconda3/envs/psychoacoustics/lib/python3.9/site-packages/theano/link/vm.py"", line 1131, in make_all
    node.op.make_thunk(node, storage_map, compute_map, [], impl=impl)
  File ""/Users/samuelrobertmathias/miniconda3/envs/psychoacoustics/lib/python3.9/site-packages/theano/graph/op.py"", line 634, in make_thunk
    return self.make_c_thunk(node, storage_map, compute_map, no_recycling)
  File ""/Users/samuelrobertmathias/miniconda3/envs/psychoacoustics/lib/python3.9/site-packages/theano/graph/op.py"", line 600, in make_c_thunk
    outputs = cl.make_thunk(
  File ""/Users/samuelrobertmathias/miniconda3/envs/psychoacoustics/lib/python3.9/site-packages/theano/link/c/basic.py"", line 1203, in make_thunk
    cthunk, module, in_storage, out_storage, error_storage = self.__compile__(
  File ""/Users/samuelrobertmathias/miniconda3/envs/psychoacoustics/lib/python3.9/site-packages/theano/link/c/basic.py"", line 1138, in __compile__
    thunk, module = self.cthunk_factory(
  File ""/Users/samuelrobertmathias/miniconda3/envs/psychoacoustics/lib/python3.9/site-packages/theano/link/c/basic.py"", line 1634, in cthunk_factory
    module = get_module_cache().module_from_key(key=key, lnk=self)
  File ""/Users/samuelrobertmathias/miniconda3/envs/psychoacoustics/lib/python3.9/site-packages/theano/link/c/cmodule.py"", line 1191, in module_from_key
    module = lnk.compile_cmodule(location)
  File ""/Users/samuelrobertmathias/miniconda3/envs/psychoacoustics/lib/python3.9/site-packages/theano/link/c/basic.py"", line 1543, in compile_cmodule
    module = c_compiler.compile_str(
  File ""/Users/samuelrobertmathias/miniconda3/envs/psychoacoustics/lib/python3.9/site-packages/theano/link/c/cmodule.py"", line 2546, in compile_str
    raise Exception(
Exception: ('The following error happened while compiling the node', Elemwise{add,no_inplace}(x, y), '\\n', 'Compilation failed (return status=1): ld: library not found for -lSystem. clang-11: error: linker command failed with exit code 1 (use -v to see invocation). ', 'FunctionGraph(Elemwise{add,no_inplace}(x, y))')
```

I'm not sure whether `conda-forge` needs to be updated to reflect the name change from Theano-pymc3 to Aesara or what, but currently pymc3 is unusable when install into a fresh env via Conda-forge on macOS.

## Versions and main components

* PyMC3 Version: 3.11.2 
* Aesara Version: theano-pymc-1.1.2  (**not Aesara?**)
* Python Version: 3.9.2
* Operating system: macOS 11.2.3
* How did you install PyMC3: `conda install -c conda-forge mkl pymc3`
",invalid,,2021-03-30 12:52:29,2021-03-31 20:11:44,"twiecki transferred 2021-03-30 20:41:53,brandonwillard closed 2021-03-31 04:06:14,brandonwillard labeled 2021-03-31 04:06:51,twiecki reopened 2021-03-31 11:36:06,brandonwillard mentioned 2021-03-31 11:36:07,brandonwillard subscribed 2021-03-31 11:36:07,sammosummo mentioned 2021-03-31 11:36:07,sammosummo subscribed 2021-03-31 11:36:07,twiecki mentioned 2021-03-31 18:17:24,twiecki subscribed 2021-03-31 18:17:24,brandonwillard mentioned 2021-03-31 18:17:24,brandonwillard subscribed 2021-03-31 18:17:24,sammosummo closed 2021-03-31 20:11:44",sammosummo twiecki brandonwillard,9
308,360,Numerical stabilization for log1mexp,ricardoV94,"We have a manual implementation on the PyMC side that stabilizes the operation `log(1 - exp(x))` by switching between `log1p` and `expm1` depending on where x lies:
 https://github.com/ricardoV94/pymc3/blob/d7f82a85dce9e0874f73dd6d48b096d5f8792313/pymc3/math.py#L221

This could be a simple and nice automatic rewrite to add to aesara that users could benefit from. It appears frequently when working with log cdfs for instance.

What would it require? Implementing a specific OP would be straightforward. But do we even need it or can rewrites just change one expression for another?

PS: I don't think aesara already does this one, but I will double check.",good first issue,ricardoV94,2021-04-03 04:29:33,2021-07-01 20:46:06,"ricardoV94 mentioned 2021-04-03 18:45:38,ricardoV94 subscribed 2021-04-03 18:45:38,ricardoV94 labeled 2021-04-06 07:56:13,ricardoV94 assigned 2021-06-23 10:14:06,ricardoV94 referenced 2021-06-23 15:09:20,ricardoV94 referenced 2021-06-23 16:48:30,ricardoV94 referenced 2021-06-23 16:49:30,ricardoV94 referenced 2021-06-23 16:56:52,ricardoV94 referenced 2021-06-25 06:46:37,ricardoV94 referenced 2021-06-25 17:51:46,ricardoV94 referenced 2021-06-25 18:00:02,ricardoV94 referenced 2021-06-27 16:44:04,ricardoV94 referenced 2021-06-30 11:15:50,ricardoV94 referenced 2021-07-01 08:45:18,ricardoV94 referenced 2021-07-01 08:50:26,brandonwillard closed 2021-07-01 20:46:06,brandonwillard referenced 2021-07-01 20:46:07",ricardoV94 twiecki brandonwillard,6
309,361,Add an option to skip type filtering during function evaluation,brandonwillard,"When evaluating a function compiled with `aesara.function`, the arguments are filtered and type checked during every call (see [here](https://github.com/pymc-devs/aesara/blob/06c179267ed26b365090760a1c7d4239b7153d85/aesara/compile/function/types.py#L861)).  In some cases, this overhead is unnecessary (e.g. tight loops with well controlled inputs), so we should provide an option to skip that step.

N.B.: We'll need to do a side-by-side performance comparison of this, as well.
",enhancement good first issue help wanted,,2021-04-03 20:32:01,2022-08-04 16:55:10,"brandonwillard labeled 2021-04-03 20:32:01,brandonwillard labeled 2021-04-03 20:32:01,brandonwillard labeled 2021-04-03 20:32:01,brandonwillard closed 2022-08-04 16:55:10",ricardoV94 brandonwillard,4
316,369,Add `dtype` option to `as_tensor_variable`,michaelosthege,"As commented by @brandonwillard in https://github.com/pymc-devs/aesara/pull/367#pullrequestreview-633036110 - please edit to add more information/context if applicable.

`as_tensor_variable` is kinda like the Aesara equivalent for `np.array`.

A `dtype` option would make a lot of code more concise.
It does raise the question whether there should be settings to configure casting behavior:
+ of non-`Variables`
+ of `Variables` to a different dtype",enhancement good first issue,,2021-04-11 19:00:53,2021-04-12 04:16:03,"michaelosthege labeled 2021-04-11 19:00:53,michaelosthege labeled 2021-04-11 19:00:53,brandonwillard mentioned 2021-04-11 19:00:53,brandonwillard subscribed 2021-04-11 19:00:53,brandonwillard closed 2021-04-12 04:16:03",michaelosthege brandonwillard,1
320,373,`Op` conversions for `NumbaLinker`,kc611,"This issue is to track and discuss the `Op`s we want to implement in Numba via #372.

The currently supported Ops in Numba are:

- `ScalarOp`
  - Needs testing for all of the derivative `Op`s
  - SciPy `Op`s are not supported until `numba-scipy` is fixed.  See https://github.com/numba/numba-scipy/pull/54.
- `Elemwise`
  - ~Needs proper vectorization. (https://github.com/brandonwillard/aesara/pull/1#discussion_r612915559)~ `numba.vectorize` is being used, but there's still a question about whether or not we should be vectorizing the NumPy `Op`s, or their basic scalar counterparts (e.g. `+`, `-`, `/`, etc.).
- `Composite`
- `*Subtensor*`
  - ~Currently, `AdvancedSubtensor` only works for indices without `slice`s, because Numba needs to jump into Python (i.e. object mode) in order to perform the advanced NumPy indexing, and, when it does this, the `MakeSlice` `Op` is used to construct the non-symbolic `slice` object; however, `MakeSlice` needs to box its native `slice` object in order for its result to be used in object mode.~
  - `*IncSubtensor*` support will be available once https://github.com/pymc-devs/aesara/pull/380 is merged.
- `MakeSlice`
  - ~See the `AdvancedSubtensor` issue, and https://github.com/numba/numba/issues/6931.~
- `DeepCopyOp`",help wanted important Numba,,2021-04-14 05:34:43,2021-05-10 20:29:11,"brandonwillard labeled 2021-04-14 05:41:06,brandonwillard labeled 2021-04-14 05:41:06,brandonwillard labeled 2021-04-14 05:41:06,aseyboldt mentioned 2021-04-14 10:10:21,aseyboldt subscribed 2021-04-14 10:10:21,aseyboldt mentioned 2021-04-14 12:48:45,aseyboldt subscribed 2021-04-14 12:48:45,brandonwillard renamed 2021-04-14 17:31:14,aseyboldt unsubscribed 2021-04-14 18:05:16,aseyboldt mentioned 2021-04-14 18:10:58,aseyboldt subscribed 2021-04-14 18:10:58,brandonwillard pinned 2021-04-15 05:02:09,brandonwillard unpinned 2021-05-10 20:27:37,brandonwillard closed 2021-05-10 20:29:11",aseyboldt kc611 twiecki brandonwillard,4
322,378,Move `aesara.tensor.nnet.sigm` to `aesara.[scalar|tensor].*`,brandonwillard,"The `Op`s and optimizations in `aesara.tensor.nnet.sigm` belong in `aesara.tensor.*`, `aesara.scalar.*`, etc.  

More specifically, the `UnaryScalarOp`s (e.g. [`Sigmoid`](https://github.com/pymc-devs/aesara/blob/master/aesara/tensor/nnet/sigm.py#L29)) belong in `aesara.scalar.basic_scipy`, the corresponding `Elemwise`s belong in `aesara.tensor.math` (e.g. [`sigmoid`](https://github.com/pymc-devs/aesara/blob/master/aesara/tensor/nnet/sigm.py#L180)), the inplace `Elemwise`s belong in `aesara.tensor.inplace` (e.g. [`sigmoid_inplace`](https://github.com/pymc-devs/aesara/blob/master/aesara/tensor/nnet/sigm.py#L182)), and the optimizations belong in `aesara.tensor.math_opt` (e.g. [`local_ultra_fast_sigmoid`](https://github.com/pymc-devs/aesara/blob/master/aesara/tensor/nnet/sigm.py#L268)). 

We also need to be able to access these `Op`s via their SciPy names (e.g. `expit = sigmoid`).",good first issue help wanted refactor,ricardoV94,2021-04-16 22:57:55,2021-05-09 11:07:59,"brandonwillard labeled 2021-04-16 22:57:55,brandonwillard labeled 2021-04-16 22:57:55,brandonwillard labeled 2021-04-16 22:57:55,ricardoV94 assigned 2021-04-23 17:52:47,twiecki closed 2021-05-09 11:07:59",ricardoV94 twiecki brandonwillard,2
323,379,Move scalar type instances and classes to a new `aesara.scalars.type` module,brandonwillard,"The [`Scalar`](https://github.com/pymc-devs/aesara/blob/master/aesara/scalar/basic.py#L318) `Type` class should match `aesara.tensor.type` and be in its own module, along with all the instances of `Scalar` (e.g. [`bool`](https://github.com/pymc-devs/aesara/blob/master/aesara/scalar/basic.py#L721), [`int8`](https://github.com/pymc-devs/aesara/blob/master/aesara/scalar/basic.py#L722), etc.)

We might also want to rename `aesara.scalar.basic_scipy` to `aesara.scalar.math`, like the `aesara.tensor.math` module.

As always, we'll need to update the corresponding test files to match the new modules.

",good first issue help wanted refactor,,2021-04-16 23:04:45,2021-06-01 17:32:26,"brandonwillard labeled 2021-04-16 23:04:45,brandonwillard labeled 2021-04-16 23:04:45,brandonwillard labeled 2021-04-16 23:04:45,brandonwillard connected 2021-05-29 19:32:00,ricardoV94 closed 2021-06-01 17:32:26",ricardoV94 brandonwillard,0
325,381,Enable in-place for `AdvancedIncSubtensor`,brandonwillard,"For some reason specifying `inplace=True` raises a `NotImplementedError` in the constructor of `AdvancedIncSubtensor`.

It's not clear to me why this should be disabled, so we should enable it, because&mdash;without it&mdash;`AdvancedIncSubtensor` copies the entire array every time `Op.perform` is called.",enhancement important,,2021-04-18 00:30:55,2021-04-18 03:44:44,"brandonwillard labeled 2021-04-18 00:30:55,brandonwillard labeled 2021-04-18 00:30:55,brandonwillard closed 2021-04-18 03:44:44",brandonwillard,0
331,387,"'DisconnectedType' object has no attribute 'dtype', in Theano gradient.py ",AmericaBG,"Hi!
I'm having an error with Theano and when I've written my issue in this project https://github.com/Theano/Theano/issues/6774#issue-867541871
I've been redirected here.

My problem is the next:
![image](https://user-images.githubusercontent.com/49905756/116207650-68078180-a740-11eb-9cd5-7168c14a0d62.png)
Could you help me to solve it please?

Thank you very much in advance :)
",MWE needed,,2021-04-27 08:08:08,2021-10-25 17:33:53,"AmericaBG mentioned 2021-05-03 07:33:24,AmericaBG subscribed 2021-05-03 07:33:24,brandonwillard labeled 2021-05-23 02:45:10,brandonwillard closed 2021-10-25 17:33:53",AmericaBG twiecki brandonwillard,2
333,390,Infer `RandomVariable` broadcastable pattern when `size` has a `Cast`,brandonwillard,"In the following example, the broadcast pattern isn't inferred because of a `Cast` in `size`:
```python
import aesara.tensor as at
import aesara.tensor.random.basic as ar


size_at = at.cast([3.0, 1.0], ""int64"")
# or
# size_at = at.cast([at.scalar(), 1.0], ""int64"")

norm_at = ar.normal(0, 1, size=size_at)
```
```python
>>> norm_at.type
TensorType(float64, matrix)
```

One solution could involve lifting the `Cast` `Op`, or&mdash;for this exact case&mdash;canonicalizing the graph for `size` would remove the `Cast` altogether.",bug enhancement good first issue help wanted important,michaelosthege,2021-04-29 23:16:06,2021-05-23 23:25:33,"brandonwillard labeled 2021-04-29 23:16:06,brandonwillard labeled 2021-04-29 23:16:06,brandonwillard mentioned 2021-05-03 07:37:30,brandonwillard subscribed 2021-05-03 07:37:30,twiecki labeled 2021-05-03 07:37:41,twiecki labeled 2021-05-03 07:37:41,brandonwillard labeled 2021-05-09 22:16:36,brandonwillard mentioned 2021-05-09 22:16:52,brandonwillard subscribed 2021-05-09 22:16:52,michaelosthege assigned 2021-05-11 14:09:56,brandonwillard mentioned 2021-05-14 22:18:57,brandonwillard subscribed 2021-05-14 22:18:57,brandonwillard referenced 2021-05-23 21:17:43,brandonwillard connected 2021-05-23 21:30:15,brandonwillard closed 2021-05-23 23:25:33,brandonwillard referenced 2021-05-23 23:25:34",michaelosthege twiecki brandonwillard,3
338,395,`local_useless_subtensor` optimization fails when broadcasting differs between old/new graph,michaelosthege,"## Problem description
I don't have a minimum example yet, but I think it's quite clear from the error output:

```
  Value Type: <class 'NoneType'>
  Old Value:  None
  New Value:  None
  Reason:  local_useless_subtensor. The type of the replacement must be the same.
  Old Graph:
  AdvancedSubtensor1 [id A] <TensorType(float64, (True,))> ''   
   |Subtensor{int64} [id B] <TensorType(float64, vector)> ''   
...

  New Graph:
  Subtensor{int64} [id B] <TensorType(float64, vector)> ''  
```
Note that `AdvancedSubtensor1` return type is `TensorType(float64, (True,))`, but its input is `TensorType(float64, vector)`.

## Versions and main components
* ❌  `theano-pymc 1.1.2`
* ❓  `aesara 2.x`",bug graph rewriting MWE needed,,2021-05-06 11:19:29,2021-10-25 17:33:26,"michaelosthege labeled 2021-05-06 11:19:29,michaelosthege labeled 2021-05-06 11:19:29,brandonwillard labeled 2021-05-06 17:06:15,brandonwillard closed 2021-10-25 17:33:26",michaelosthege brandonwillard,2
342,399,Remove deprecated `Flatten` and `Tile` `Op`s,brandonwillard,The [`Flatten`](https://github.com/pymc-devs/aesara/blob/f2ecc2e52cb704cacc1b4642fb0bf9c849bd975f/aesara/tensor/basic.py#L2859) and [`Tile`](https://github.com/pymc-devs/aesara/blob/f2ecc2e52cb704cacc1b4642fb0bf9c849bd975f/aesara/tensor/basic.py#L3090) `Op`s are deprecated and need to be removed.,good first issue help wanted,,2021-05-10 00:00:01,2022-05-12 20:55:17,"brandonwillard labeled 2021-05-10 00:00:01,brandonwillard labeled 2021-05-10 00:00:01,brandonwillard renamed 2021-05-10 00:00:52,brandonwillard renamed 2021-05-10 00:01:03,ricardoV94 closed 2022-05-12 20:55:17",hectormz ricardoV94 brandonwillard,3
343,400,Create a `Scan` `Op` conversion for Numba,brandonwillard,"The `Scan` `Op` needs a Numba conversion.

In the course of implementing this, we may also want to update `Scan`, since that's already long overdue.",important Numba Scan,,2021-05-10 20:30:51,2021-10-09 17:42:48,"brandonwillard labeled 2021-05-10 20:30:51,brandonwillard labeled 2021-05-10 20:30:51,brandonwillard mentioned 2021-05-11 12:57:15,brandonwillard subscribed 2021-05-11 12:57:15,brandonwillard mentioned 2021-05-11 19:03:31,brandonwillard subscribed 2021-05-11 19:03:31,brandonwillard milestoned 2021-05-19 23:16:58,brandonwillard labeled 2021-09-12 22:34:31,brandonwillard connected 2021-10-06 17:32:12,brandonwillard closed 2021-10-09 17:42:48",twiecki brandonwillard,3
344,401,Create an `IfElse` `Op` conversion for `Numba`,brandonwillard,We need a Numba conversion for the `IfElse` `Op`.,help wanted Numba,,2021-05-10 20:32:12,2021-10-19 00:59:12,"brandonwillard labeled 2021-05-10 20:32:12,brandonwillard labeled 2021-05-10 20:32:12,brandonwillard milestoned 2021-05-19 23:16:58,brandonwillard closed 2021-10-19 00:59:12",brandonwillard,0
345,402,Numba conversions for `RandomVariable`,brandonwillard,We need a Numba implementation of the `RandomVariable` `Op`s and the `RandomState` `Type`.,important Numba,kc611,2021-05-10 20:33:45,2021-06-25 22:52:39,"brandonwillard labeled 2021-05-10 20:33:45,brandonwillard labeled 2021-05-10 20:33:45,kc611 assigned 2021-05-10 21:20:56,kc611 connected 2021-05-11 12:21:09,brandonwillard milestoned 2021-05-19 23:16:58,brandonwillard closed 2021-06-25 22:52:39",kc611 brandonwillard,0
348,406,Ability to serialize aesara graphs,twiecki,,enhancement MWE needed,,2021-05-12 13:12:55,2021-10-25 17:35:27,"twiecki labeled 2021-05-12 13:12:55,brandonwillard labeled 2021-05-12 15:45:39,brandonwillard closed 2021-10-25 17:35:27",twiecki brandonwillard,2
349,407,PyMC3 Fails on Windows Python 3.8 (Aesara dll imports),hectormz,"## Description of your problem

Running `pymc3` on Windows fails for me when using python 3.8. Running the same code with same package versions on my machine with python 3.7 works. From what I can tell, this is a `Aesara` issue and not specifically a `pymc3` problem, but figured other users might find this issue in the future. I believe it is related to finding my `gcc` dlls, as it works if I add the `gcc` directory below.

It seems that the resolution of DLLs has changed in Python 3.8 for Windows:

> New in version 3.8: Previous versions of CPython would resolve DLLs using the default behavior for the current process. This led to inconsistencies, such as only sometimes searching PATH or the current working directory, and OS functions such as AddDllDirectory having no effect.

> DLL dependencies for extension modules and DLLs loaded with ctypes on Windows are now resolved more securely. Only the system paths, the directory containing the DLL or PYD file, and directories added with add_dll_directory() are searched for load-time dependencies. Specifically, PATH and the current working directory are no longer used, and modifications to these will no longer have any effect on normal DLL resolution. If your application relies on these mechanisms, you should check for add_dll_directory() and if it exists, use it to add your DLLs directory while loading your library. Note that Windows 7 users will need to ensure that Windows Update KB2533623 has been installed (this is also verified by the installer). (Contributed by Steve Dower in bpo-36085.)

* https://bugs.python.org/issue36085
* https://docs.python.org/3/whatsnew/3.8.html#bpo-36085-whatsnew
* https://docs.python.org/3/library/os.html#os.add_dll_directory



This is not a problem with `Anaconda`, which could be a solution for some folks.


**Please provide a minimal, self-contained, and reproducible example.**
```python
import pymc3 as pm
import numpy as np


# set the seed
np.random.seed(1)

n = 100 # The number of data points
X = np.linspace(0, 10, n)[:, None] # The inputs to the GP, they must be arranged as a column vector

# Define the true covariance function and its parameters
ℓ_true = 1.0
η_true = 3.0
cov_func = η_true**2 * pm.gp.cov.Matern52(1, ℓ_true)

# A mean function that is zero everywhere
mean_func = pm.gp.mean.Zero()

# The latent function values are one sample from a multivariate normal
# Note that we have to call `eval()` because PyMC3 built on top of Theano
f_true = np.random.multivariate_normal(mean_func(X).eval(),
                                       cov_func(X).eval() + 1e-8*np.eye(n), 1).flatten()

# The observed data is the latent function plus a small amount of T distributed noise
# The standard deviation of the noise is `sigma`, and the degrees of freedom is `nu`
e_true = 2.0
η_true = 3.0
y = f_true + e_true * np.random.standard_t(η_true, size=n)


with pm.Model() as model:
    ℓ = pm.Gamma(""ℓ"", alpha=2, beta=1)
    η = pm.HalfCauchy(""η"", beta=1)

    cov = η ** 2 * pm.gp.cov.Matern52(1, ℓ)
    gp = pm.gp.Latent(cov_func=cov)

    f = gp.prior(""f"", X=X)

    σ = pm.HalfCauchy(""σ"", beta=5)
    ν = pm.Gamma(""ν"", alpha=2, beta=0.1)
    y_ = pm.StudentT(""y"", mu=f, lam=1.0 / σ, nu=ν, observed=y)

    trace = pm.sample(1000, chains=2, cores=1, return_inferencedata=True)
```

**Please provide the full traceback.**
```python
WARNING (aesara.tensor.blas): Using NumPy C-API based implementation for BLAS functions.
Traceback (most recent call last):
  File ""latent_var.py"", line 22, in <module>
    f_true = np.random.multivariate_normal(mean_func(X).eval(),
  File ""C:\\Users\\hectormz\\.virtualenvs\\pymc3-venv-38\\lib\\site-packages\\aesara\\gof\\graph.py"", line 522, in eval
    self._fn_cache[inputs] = aesara.function(inputs, self)
  File ""C:\\Users\\hectormz\\.virtualenvs\\pymc3-venv-38\\lib\\site-packages\\aesara\\compile\\function.py"", line 306, in function
    fn = pfunc(params=inputs,
  File ""C:\\Users\\hectormz\\.virtualenvs\\pymc3-venv-38\\lib\\site-packages\\aesara\\compile\\pfunc.py"", line 483, in pfunc
    return orig_function(inputs, cloned_outputs, mode,
  File ""C:\\Users\\hectormz\\.virtualenvs\\pymc3-venv-38\\lib\\site-packages\\aesara\\compile\\function_module.py"", line 1841, in orig_function
    fn = m.create(defaults)
  File ""C:\\Users\\hectormz\\.virtualenvs\\pymc3-venv-38\\lib\\site-packages\\aesara\\compile\\function_module.py"", line 1714, in create
    _fn, _i, _o = self.linker.make_thunk(
  File ""C:\\Users\\hectormz\\.virtualenvs\\pymc3-venv-38\\lib\\site-packages\\aesara\\gof\\link.py"", line 697, in make_thunk
    return self.make_all(input_storage=input_storage,
  File ""C:\\Users\\hectormz\\.virtualenvs\\pymc3-venv-38\\lib\\site-packages\\aesara\\gof\\vm.py"", line 1087, in make_all
    thunks.append(node.op.make_thunk(node,
  File ""C:\\Users\\hectormz\\.virtualenvs\\pymc3-venv-38\\lib\\site-packages\\aesara\\gof\\op.py"", line 954, in make_thunk
    return self.make_c_thunk(node, storage_map, compute_map,
  File ""C:\\Users\\hectormz\\.virtualenvs\\pymc3-venv-38\\lib\\site-packages\\aesara\\gof\\op.py"", line 857, in make_c_thunk
    outputs = cl.make_thunk(input_storage=node_input_storage,
  File ""C:\\Users\\hectormz\\.virtualenvs\\pymc3-venv-38\\lib\\site-packages\\aesara\\gof\\cc.py"", line 1215, in make_thunk
    cthunk, module, in_storage, out_storage, error_storage = self.__compile__(
  File ""C:\\Users\\hectormz\\.virtualenvs\\pymc3-venv-38\\lib\\site-packages\\aesara\\gof\\cc.py"", line 1153, in __compile__
    thunk, module = self.cthunk_factory(error_storage,
  File ""C:\\Users\\hectormz\\.virtualenvs\\pymc3-venv-38\\lib\\site-packages\\aesara\\gof\\cc.py"", line 1623, in cthunk_factory
    module = get_module_cache().module_from_key(
  File ""C:\\Users\\hectormz\\.virtualenvs\\pymc3-venv-38\\lib\\site-packages\\aesara\\gof\\cmodule.py"", line 1189, in module_from_key
    module = lnk.compile_cmodule(location)
  File ""C:\\Users\\hectormz\\.virtualenvs\\pymc3-venv-38\\lib\\site-packages\\aesara\\gof\\cc.py"", line 1520, in compile_cmodule
    module = c_compiler.compile_str(
  File ""C:\\Users\\hectormz\\.virtualenvs\\pymc3-venv-38\\lib\\site-packages\\aesara\\gof\\cmodule.py"", line 2405, in compile_str
    return dlimport(lib_filename)
  File ""C:\\Users\\hectormz\\.virtualenvs\\pymc3-venv-38\\lib\\site-packages\\aesara\\gof\\cmodule.py"", line 317, in dlimport
    rval = __import__(module_name, {}, {}, [module_name])
ImportError: DLL load failed while importing m8f27cf6f6495a72b3883db0ff3f5d581f67a3adfd26460692f0372611b85620b: The specified module could not be found.
```

**Please provide any additional information below.**

I can fix this by adding the following chunk to https://github.com/pymc-devs/aesara/blob/2d3cb77530f8b80f60767c187ac6913740d0d996/aesara/link/c/cmodule.py#L289

```python
    if (sys.platform == ""win32"") & (hasattr(os, 'add_dll_directory')):
        gcc_path = shutil.which('gcc')
        if gcc_path is not None:
            os.add_dll_directory(os.path.dirname(gcc_path))
```
I'm sure this fix could go somewhere upstream, since `dlimport()` is called repeatedly

## Versions and main components

* Aesara version: 2.0.7
* Aesara config (`python -c ""import aesara; print(aesara.config)""`)
```
[gcc]
cxxflags = -D_hypot=hypot
```
* Python version: 3.8.9
* Operating system: Windows 10
* How did you install Aesara: pip
",bug C-backend Windows,,2021-05-12 14:47:50,2021-05-13 07:51:16,"brandonwillard labeled 2021-05-12 15:44:34,brandonwillard labeled 2021-05-12 15:44:34,brandonwillard labeled 2021-05-12 15:44:34,hectormz mentioned 2021-05-12 16:23:51,hectormz subscribed 2021-05-12 16:23:51,twiecki closed 2021-05-13 07:51:17",brandonwillard twiecki hectormz,2
351,409,Ability to set Numba's `parallel` and `fastmath` options ,twiecki,"`numba.njit(parallel=True, fastmath=True)` can give great speed-ups. Would be nice to be able to run aesara with these flags.",enhancement help wanted Numba,,2021-05-13 08:47:37,2022-08-04 16:23:56,"twiecki labeled 2021-05-13 08:47:37,brandonwillard milestoned 2021-05-19 23:16:58,brandonwillard renamed 2021-11-08 23:40:19,brandonwillard demilestoned 2021-11-08 23:40:46,brandonwillard labeled 2021-11-08 23:40:52,brandonwillard labeled 2021-11-08 23:40:52,brandonwillard connected 2021-11-08 23:41:23,brandonwillard closed 2022-08-04 16:23:56",twiecki brandonwillard,3
353,411,Embarrassing parallelization and filelock,FFroehlich,"## Description of your problem or feature request

**Please provide the full traceback of any errors.**
```python
filelock.Timeout: The file lock '/home/ff72/.aesara/compiledir_Linux-3.10-el7.x86_64-x86_64-with-centos-7.7.1908-Core-x86_64-3.7.4-64/.lock' could not be acquired.
```

**Please provide any additional information below.**

I am using aesara as part of an embarrassingly parallelizable task that is implemented using snakemake where each job runs completely independently, but each uses aesara to compile (the same) functions and I still see frequent failure of jobs due to lock timeouts. I thought about changing the compile directory, but it sounds like this cannot be done at runtime. Is there any other way to prevent conflicts between independent jobs?

## Versions and main components

* Aesara version: 2.0.7
* Aesara config (`python -c ""import aesara; print(aesara.config)""`)
```
WARNING (aesara.tensor.blas): Using NumPy C-API based implementation for BLAS functions.
floatX ({'float64', 'float32', 'float16'}) 
    Doc:  Default floating-point precision for python casts.

Note: float16 support is experimental, use at your own risk.
    Value:  float64

warn_float64 ({'warn', 'ignore', 'raise', 'pdb'}) 
    Doc:  Do an action when a tensor variable with float64 dtype is created. They can't be run on the GPU with the current(old) gpu back-end and are slow with gamer GPUs.
    Value:  ignore

pickle_test_value (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7fd8c74fb8d0>>) 
    Doc:  Dump test values while pickling model. If True, test values will be dumped with model.
    Value:  True

cast_policy ({'custom', 'numpy+floatX'}) 
    Doc:  Rules for implicit type casting
    Value:  custom

deterministic ({'default', 'more'}) 
    Doc:  If `more`, sometimes we will select some implementation that are more deterministic, but slower. In particular, on the GPU, we will avoid using AtomicAdd. Sometimes we will still use non-deterministic implementaion, e.g. when we do not have a GPU implementation that is deterministic. Also see the dnn.conv.algo* flags to cover more cases.
    Value:  default

device (cpu, opencl*, cuda*) 
    Doc:  Default device for computations. If cuda* or opencl*, change thedefault to try to move computation to the GPU. Do not use upper caseletters, only lower case even if NVIDIA uses capital letters. 'gpu' means let the driver select the gpu (needed for gpu in exclusive mode). 'gpuX' mean use the gpu number X.
    Value:  cpu

init_gpu_device (, opencl*, cuda*) 
    Doc:  Initialize the gpu device to use, works only if device=cpu. Unlike 'device', setting this option will NOT move computations, nor shared variables, to the specified GPU. It can be used to run GPU-specific tests on a particular GPU.
    Value:  

force_device (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7fd8c74fbdd0>>) 
    Doc:  Raise an error if we can't use the specified device
    Value:  False

conv__assert_shape (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7fd8c74fb450>>) 
    Doc:  If True, AbstractConv* ops will verify that user-provided shapes match the runtime shapes (debugging option, may slow down compilation)
    Value:  False

print_global_stats (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7fd8c7479890>>) 
    Doc:  Print some global statistics (time spent) at the end
    Value:  False

<aesara.configparser.ContextsParam object at 0x7fd8c7479750>
    Doc:  
        Context map for multi-gpu operation. Format is a
        semicolon-separated list of names and device names in the
        'name->dev_name' format. An example that would map name 'test' to
        device 'cuda0' and name 'test2' to device 'opencl0:0' follows:
        ""test->cuda0;test2->opencl0:0"".

        Invalid context names are 'cpu', 'cuda*' and 'opencl*'
        
    Value:  

print_active_device (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7fd8c7479c10>>) 
    Doc:  Print active device at when the GPU device is initialized.
    Value:  True

gpuarray__preallocate (<class 'float'>) 
    Doc:  If negative it disables the allocation cache. If
                 between 0 and 1 it enables the allocation cache and
                 preallocates that fraction of the total GPU memory.  If 1
                 or greater it will preallocate that amount of memory (in
                 megabytes).
    Value:  0.0

gpuarray__sched ({'default', 'single', 'multi'}) 
    Doc:  The sched parameter passed for context creation to pygpu.
                    With CUDA, using ""multi"" is equivalent to using the parameter
                    cudaDeviceScheduleBlockingSync. This is useful to lower the
                    CPU overhead when waiting for GPU. One user found that it
                    speeds up his other processes that was doing data augmentation.
                 
    Value:  default

gpuarray__single_stream (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7fd8c7479990>>) 
    Doc:  
                 If your computations are mostly lots of small elements,
                 using single-stream will avoid the synchronization
                 overhead and usually be faster.  For larger elements it
                 does not make a difference yet.  In the future when true
                 multi-stream is enabled in libgpuarray, this may change.
                 If you want to make sure to have optimal performance,
                 check both options.
                 
    Value:  True

cuda__root (<class 'str'>) 
    Doc:  Location of the cuda installation
    Value:  

cuda__include_path (<class 'str'>) 
    Doc:  Location of the cuda includes
    Value:  

assert_no_cpu_op ({'warn', 'ignore', 'raise', 'pdb'}) 
    Doc:  Raise an error/warning if there is a CPU op in the computational graph.
    Value:  ignore

unpickle_function (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7fd8c7479ad0>>) 
    Doc:  Replace unpickled Aesara functions with None. This is useful to unpickle old graphs that pickled them when it shouldn't
    Value:  True

reoptimize_unpickled_function (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7fd8c7479a90>>) 
    Doc:  Re-optimize the graph when an Aesara function is unpickled from the disk.
    Value:  False

dnn__conv__algo_fwd ({'fft_tiling', 'guess_once', 'none', 'time_once', 'winograd', 'time_on_shape_change', 'large', 'guess_on_shape_change', 'small', 'winograd_non_fused', 'fft'}) 
    Doc:  Default implementation to use for cuDNN forward convolution.
    Value:  small

dnn__conv__algo_bwd_data ({'fft_tiling', 'guess_once', 'none', 'time_once', 'winograd', 'time_on_shape_change', 'deterministic', 'guess_on_shape_change', 'winograd_non_fused', 'fft'}) 
    Doc:  Default implementation to use for cuDNN backward convolution to get the gradients of the convolution with regard to the inputs.
    Value:  none

dnn__conv__algo_bwd_filter ({'fft_tiling', 'guess_once', 'none', 'time_once', 'time_on_shape_change', 'deterministic', 'guess_on_shape_change', 'small', 'winograd_non_fused', 'fft'}) 
    Doc:  Default implementation to use for cuDNN backward convolution to get the gradients of the convolution with regard to the filters.
    Value:  none

dnn__conv__precision ({'float16', 'as_input_f32', 'as_input', 'float64', 'float32'}) 
    Doc:  Default data precision to use for the computation in cuDNN convolutions (defaults to the same dtype as the inputs of the convolutions, or float32 if inputs are float16).
    Value:  as_input_f32

dnn__base_path (<class 'str'>) 
    Doc:  Install location of cuDNN.
    Value:  

dnn__include_path (<class 'str'>) 
    Doc:  Location of the cudnn header
    Value:  

dnn__library_path (<class 'str'>) 
    Doc:  Location of the cudnn link library.
    Value:  

dnn__bin_path (<class 'str'>) 
    Doc:  Location of the cuDNN load library (on non-windows platforms, this is the same as dnn__library_path)
    Value:  

dnn__enabled ({'False', 'True', 'no_check', 'auto'}) 
    Doc:  'auto', use cuDNN if available, but silently fall back to not using it if not present. If True and cuDNN can not be used, raise an error. If False, disable cudnn even if present. If no_check, assume present and the version between header and library match (so less compilation at context init)
    Value:  auto

magma__include_path (<class 'str'>) 
    Doc:  Location of the magma header
    Value:  

magma__library_path (<class 'str'>) 
    Doc:  Location of the magma library
    Value:  

magma__enabled (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7fd8c74794d0>>) 
    Doc:   If True, use magma for matrix computation. If False, disable magma
    Value:  False

<aesara.configparser.ConfigParam object at 0x7fd8c7479b90>
    Doc:  Default compilation mode
    Value:  Mode

cxx (<class 'str'>) 
    Doc:  The C++ compiler to use. Currently only g++ is supported, but supporting additional compilers should not be too difficult. If it is empty, no C++ code is compiled.
    Value:  /n/app/gcc/6.2.0/bin/g++

linker ({'c|py', 'vm', 'c|py_nogc', 'c', 'py', 'cvm_nogc', 'cvm', 'vm_nogc'}) 
    Doc:  Default linker used if the aesara flags mode is Mode
    Value:  cvm

allow_gc (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7fd8c7479610>>) 
    Doc:  Do we default to delete intermediate results during Aesara function calls? Doing so lowers the memory requirement, but asks that we reallocate memory at the next function call. This is implemented for the default linker, but may not work for all linkers.
    Value:  True

optimizer ({'o2', 'o3', 'o4', 'fast_compile', 'unsafe', 'fast_run', 'None', 'merge', 'o1'}) 
    Doc:  Default optimizer. If not None, will use this optimizer with the Mode
    Value:  o4

optimizer_verbose (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7fd8c7479350>>) 
    Doc:  If True, we print all optimization being applied
    Value:  False

on_opt_error ({'warn', 'ignore', 'raise', 'pdb'}) 
    Doc:  What to do when an optimization crashes: warn and skip it, raise the exception, or fall into the pdb debugger.
    Value:  warn

nocleanup (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7fd8c7479290>>) 
    Doc:  Suppress the deletion of code files that did not compile cleanly
    Value:  False

on_unused_input ({'warn', 'raise', 'ignore'}) 
    Doc:  What to do if a variable in the 'inputs' list of  aesara.function() is not used in the graph.
    Value:  raise

gcc__cxxflags (<class 'str'>) 
    Doc:  Extra compiler flags for gcc
    Value:  

cmodule__warn_no_version (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7fd8c7479190>>) 
    Doc:  If True, will print a warning when compiling one or more Op with C code that can't be cached because there is no c_code_cache_version() function associated to at least one of those Ops.
    Value:  False

cmodule__remove_gxx_opt (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7fd8c7483090>>) 
    Doc:  If True, will remove the -O* parameter passed to g++.This is useful to debug in gdb modules compiled by Aesara.The parameter -g is passed by default to g++
    Value:  False

cmodule__compilation_warning (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7fd8c7483110>>) 
    Doc:  If True, will print compilation warnings.
    Value:  False

cmodule__preload_cache (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7fd8c7483150>>) 
    Doc:  If set to True, will preload the C module cache at import time
    Value:  False

cmodule__age_thresh_use (<class 'int'>) 
    Doc:  In seconds. The time after which Aesara won't reuse a compile c module.
    Value:  2073600

cmodule__debug (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7fd8c7483250>>) 
    Doc:  If True, define a DEBUG macro (if not exists) for any compiled C code.
    Value:  False

compile__wait (<class 'int'>) 
    Doc:  Time to wait before retrying to acquire the compile lock.
    Value:  5

compile__timeout (<class 'int'>) 
    Doc:  In seconds, time that a process will wait before deciding to
    override an existing lock. An override only happens when the existing
    lock is held by the same owner *and* has not been 'refreshed' by this
    owner for more than this period. Refreshes are done every half timeout
    period for running processes.
    Value:  120

ctc__root (<class 'str'>) 
    Doc:  Directory which contains the root of Baidu CTC library. It is assumed         that the compiled library is either inside the build, lib or lib64         subdirectory, and the header inside the include directory.
    Value:  

tensor__cmp_sloppy (<class 'int'>) 
    Doc:  Relax aesara.tensor.math._allclose (0) not at all, (1) a bit, (2) more
    Value:  0

tensor__local_elemwise_fusion (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7fd8c7483550>>) 
    Doc:  Enable or not in fast_run mode(fast_run optimization) the elemwise fusion optimization
    Value:  True

lib__amblibm (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7fd8c7483610>>) 
    Doc:  Use amd's amdlibm numerical library
    Value:  False

tensor__insert_inplace_optimizer_validate_nb (<class 'int'>) 
    Doc:  -1: auto, if graph have less then 500 nodes 1, else 10
    Value:  -1

traceback__limit (<class 'int'>) 
    Doc:  The number of stack to trace. -1 mean all.
    Value:  8

traceback__compile_limit (<class 'int'>) 
    Doc:  The number of stack to trace to keep during compilation. -1 mean all. If greater then 0, will also make us save Aesara internal stack trace.
    Value:  0

experimental__unpickle_gpu_on_cpu (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7fd8c7483850>>) 
    Doc:  Allow unpickling of pickled GpuArrays as numpy.ndarrays.This is useful, if you want to open a GpuArray without having cuda installed.If you have cuda installed, this will force unpickling tobe done on the cpu to numpy.ndarray.Please be aware that this may get you access to the data,however, trying to unpicke gpu functions will not succeed.This flag is experimental and may be removed any time, whengpu<>cpu transparency is solved.
    Value:  False

experimental__local_alloc_elemwise (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7fd8c74838d0>>) 
    Doc:  DEPRECATED: If True, enable the experimental optimization local_alloc_elemwise. Generates error if not True. Use optimizer_excluding=local_alloc_elemwise to dsiable.
    Value:  True

experimental__local_alloc_elemwise_assert (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7fd8c7483950>>) 
    Doc:  When the local_alloc_elemwise is applied, add an assert to highlight shape errors.
    Value:  True

warn__ignore_bug_before ({'0.3', '1.0.1', '1.0.5', '0.9', '0.5', '1.0.4', '0.10', '0.7', '0.6', 'None', '0.4', '0.4.1', '0.8', '0.8.2', '0.8.1', 'all', '1.0.3', '1.0.2', '1.0'}) 
    Doc:  If 'None', we warn about all Aesara bugs found by default. If 'all', we don't warn about Aesara bugs found by default. If a version, we print only the warnings relative to Aesara bugs found after that version. Warning for specific bugs can be configured with specific [warn] flags.
    Value:  0.9

exception_verbosity ({'low', 'high'}) 
    Doc:  If 'low', the text of exceptions will generally refer to apply nodes with short names such as Elemwise{add_no_inplace}. If 'high', some exceptions will also refer to apply nodes with long descriptions  like:
        A. Elemwise{add_no_inplace}
                B. log_likelihood_v_given_h
                C. log_likelihood_h
    Value:  low

print_test_value (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7fd8c7483ad0>>) 
    Doc:  If 'True', the __eval__ of an Aesara variable will return its test_value when this is available. This has the practical conseguence that, e.g., in debugging `my_var` will print the same as `my_var.tag.test_value` when a test value is defined.
    Value:  False

compute_test_value ({'raise', 'warn', 'off', 'ignore', 'pdb'}) 
    Doc:  If 'True', Aesara will run each op at graph build time, using Constants, SharedVariables and the tag 'test_value' as inputs to the function. This helps the user track down problems in the graph before it gets optimized.
    Value:  off

compute_test_value_opt ({'raise', 'warn', 'off', 'ignore', 'pdb'}) 
    Doc:  For debugging Aesara optimization only. Same as compute_test_value, but is used during Aesara optimization
    Value:  off

check_input (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7fd8c7483b50>>) 
    Doc:  Specify if types should check their input in their C code. It can be used to speed up compilation, reduce overhead (particularly for scalars) and reduce the number of generated C files.
    Value:  True

NanGuardMode__nan_is_error (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7fd8c7483b90>>) 
    Doc:  Default value for nan_is_error
    Value:  True

NanGuardMode__inf_is_error (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7fd8c7483c10>>) 
    Doc:  Default value for inf_is_error
    Value:  True

NanGuardMode__big_is_error (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7fd8c7483c90>>) 
    Doc:  Default value for big_is_error
    Value:  True

NanGuardMode__action ({'warn', 'raise', 'pdb'}) 
    Doc:  What NanGuardMode does when it finds a problem
    Value:  raise

DebugMode__patience (<class 'int'>) 
    Doc:  Optimize graph this many times to detect inconsistency
    Value:  10

DebugMode__check_c (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7fd8c7483e10>>) 
    Doc:  Run C implementations where possible
    Value:  True

DebugMode__check_py (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7fd8c7483ed0>>) 
    Doc:  Run Python implementations where possible
    Value:  True

DebugMode__check_finite (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7fd8c7483f50>>) 
    Doc:  True -> complain about NaN/Inf results
    Value:  True

DebugMode__check_strides (<class 'int'>) 
    Doc:  Check that Python- and C-produced ndarrays have same strides. On difference: (0) - ignore, (1) warn, or (2) raise error
    Value:  0

DebugMode__warn_input_not_reused (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7fd8c7487090>>) 
    Doc:  Generate a warning when destroy_map or view_map says that an op works inplace, but the op did not reuse the input for its output.
    Value:  True

DebugMode__check_preallocated_output (<class 'str'>) 
    Doc:  Test thunks with pre-allocated memory as output storage. This is a list of strings separated by "":"". Valid values are: ""initial"" (initial storage in storage map, happens with Scan),""previous"" (previously-returned memory), ""c_contiguous"", ""f_contiguous"", ""strided"" (positive and negative strides), ""wrong_size"" (larger and smaller dimensions), and ""ALL"" (all of the above).
    Value:  

DebugMode__check_preallocated_output_ndim (<class 'int'>) 
    Doc:  When testing with ""strided"" preallocated output memory, test all combinations of strides over that number of (inner-most) dimensions. You may want to reduce that number to reduce memory or time usage, but it is advised to keep a minimum of 2.
    Value:  4

profiling__time_thunks (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7fd8c7487210>>) 
    Doc:  Time individual thunks when profiling
    Value:  True

profiling__n_apply (<class 'int'>) 
    Doc:  Number of Apply instances to print by default
    Value:  20

profiling__n_ops (<class 'int'>) 
    Doc:  Number of Ops to print by default
    Value:  20

profiling__output_line_width (<class 'int'>) 
    Doc:  Max line width for the profiling output
    Value:  512

profiling__min_memory_size (<class 'int'>) 
    Doc:  For the memory profile, do not print Apply nodes if the size
                 of their outputs (in bytes) is lower than this threshold
    Value:  1024

profiling__min_peak_memory (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7fd8c7487490>>) 
    Doc:  The min peak memory usage of the order
    Value:  False

profiling__destination (<class 'str'>) 
    Doc:  File destination of the profiling output
    Value:  stderr

profiling__debugprint (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7fd8cd915650>>) 
    Doc:  Do a debugprint of the profiled functions
    Value:  False

profiling__ignore_first_call (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7fd8c7487590>>) 
    Doc:  Do we ignore the first call of an Aesara function.
    Value:  False

on_shape_error ({'warn', 'raise'}) 
    Doc:  warn: print a warning and use the default value. raise: raise an error
    Value:  warn

openmp (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7fd8c7487650>>) 
    Doc:  Allow (or not) parallel computation on the CPU with OpenMP. This is the default value used when creating an Op that supports OpenMP parallelization. It is preferable to define it via the Aesara configuration file ~/.aesararc or with the environment variable AESARA_FLAGS. Parallelization is only done for some operations that implement it, and even for operations that implement parallelism, each operation is free to respect this flag or not. You can control the number of threads used with the environment variable OMP_NUM_THREADS. If it is set to 1, we disable openmp in Aesara by default.
    Value:  False

openmp_elemwise_minsize (<class 'int'>) 
    Doc:  If OpenMP is enabled, this is the minimum size of vectors for which the openmp parallelization is enabled in element wise ops.
    Value:  200000

optimizer_excluding (<class 'str'>) 
    Doc:  When using the default mode, we will remove optimizer with these tags. Separate tags with ':'.
    Value:  

optimizer_including (<class 'str'>) 
    Doc:  When using the default mode, we will add optimizer with these tags. Separate tags with ':'.
    Value:  

optimizer_requiring (<class 'str'>) 
    Doc:  When using the default mode, we will require optimizer with these tags. Separate tags with ':'.
    Value:  

optdb__position_cutoff (<class 'float'>) 
    Doc:  Where to stop eariler during optimization. It represent the position of the optimizer where to stop.
    Value:  inf

optdb__max_use_ratio (<class 'float'>) 
    Doc:  A ratio that prevent infinite loop in EquilibriumOptimizer.
    Value:  8.0

cycle_detection ({'regular', 'fast'}) 
    Doc:  If cycle_detection is set to regular, most inplaces are allowed,but it is slower. If cycle_detection is set to faster, less inplacesare allowed, but it makes the compilation faster.The interaction of which one give the lower peak memory usage iscomplicated and not predictable, so if you are close to the peakmemory usage, triyng both could give you a small gain.
    Value:  regular

check_stack_trace ({'warn', 'off', 'log', 'raise'}) 
    Doc:  A flag for checking the stack trace during the optimization process. default (off): does not check the stack trace of any optimization log: inserts a dummy stack trace that identifies the optimizationthat inserted the variable that had an empty stack trace.warn: prints a warning if a stack trace is missing and also a dummystack trace is inserted that indicates which optimization insertedthe variable that had an empty stack trace.raise: raises an exception if a stack trace is missing
    Value:  off

metaopt__verbose (<class 'int'>) 
    Doc:  0 for silent, 1 for only warnings, 2 for full output withtimings and selected implementation
    Value:  0

metaopt__optimizer_excluding (<class 'str'>) 
    Doc:  exclude optimizers with these tags. Separate tags with ':'.
    Value:  

metaopt__optimizer_including (<class 'str'>) 
    Doc:  include optimizers with these tags. Separate tags with ':'.
    Value:  

profile (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7fd8c7487a50>>) 
    Doc:  If VM should collect profile information
    Value:  False

profile_optimizer (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7fd8d0945d10>>) 
    Doc:  If VM should collect optimizer profile information
    Value:  False

profile_memory (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7fd8c7487a90>>) 
    Doc:  If VM should collect memory profile information and print it
    Value:  False

<aesara.configparser.ConfigParam object at 0x7fd8c7487ad0>
    Doc:  Useful only for the vm linkers. When lazy is None, auto detect if lazy evaluation is needed and use the appropriate version. If lazy is True/False, force the version used between Loop/LoopGC and Stack.
    Value:  None

cache_optimizations (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7fd8c7487b50>>) 
    Doc:  WARNING: work in progress, does not work yet. Specify if the optimization cache should be used. This cache will any optimized graph and its optimization. Actually slow downs a lot the first optimization, and could possibly still contains some bugs. Use at your own risks.
    Value:  False

unittests__rseed (<class 'str'>) 
    Doc:  Seed to use for randomized unit tests. Special value 'random' means using a seed of None.
    Value:  666

warn__identify_1pexp_bug (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7fd8c7487c90>>) 
    Doc:  Warn if Aesara versions prior to 7987b51 (2011-12-18) could have yielded a wrong result due to a bug in the is_1pexp function
    Value:  False

gpu__local_elemwise_fusion (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7fd8c7487d50>>) 
    Doc:  Enable or not in fast_run mode(fast_run optimization) the gpu elemwise fusion optimization
    Value:  True

gpuelemwise__sync (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7fd8c7487e10>>) 
    Doc:  when true, wait that the gpu fct finished and check it error code.
    Value:  True

warn__argmax_pushdown_bug (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7fd8c7487e90>>) 
    Doc:  Warn if in past version of Aesara we generated a bug with the aesara.tensor.nnet.basic.local_argmax_pushdown optimization. Was fixed 27 may 2010
    Value:  False

warn__gpusum_01_011_0111_bug (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7fd8c7487f50>>) 
    Doc:  Warn if we are in a case where old version of Aesara had a silent bug with GpuSum pattern 01,011 and 0111 when the first dimensions was bigger then 4096. Was fixed 31 may 2010
    Value:  False

warn__sum_sum_bug (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7fd8c7487fd0>>) 
    Doc:  Warn if we are in a case where Aesara version between version 9923a40c7b7a and the 2 august 2010 (fixed date), generated an error in that case. This happens when there are 2 consecutive sums in the graph, bad code was generated. Was fixed 2 August 2010
    Value:  False

warn__sum_div_dimshuffle_bug (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7fd8c748c090>>) 
    Doc:  Warn if previous versions of Aesara (between rev. 3bd9b789f5e8, 2010-06-16, and cfc6322e5ad4, 2010-08-03) would have given incorrect result. This bug was triggered by sum of division of dimshuffled tensors.
    Value:  False

warn__subtensor_merge_bug (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7fd8c748c110>>) 
    Doc:  Warn if previous versions of Aesara (before 0.5rc2) could have given incorrect results when indexing into a subtensor with negative stride (for instance, for instance, x[a:b:-1][c]).
    Value:  False

warn__gpu_set_subtensor1 (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7fd8c748c190>>) 
    Doc:  Warn if previous versions of Aesara (before 0.6) could have given incorrect results when moving to the gpu set_subtensor(x[int vector], new_value)
    Value:  False

warn__vm_gc_bug (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7fd8c748c210>>) 
    Doc:  There was a bug that existed in the default Aesara configuration, only in the development version between July 5th 2012 and July 30th 2012. This was not in a released version. If your code was affected by this bug, a warning will be printed during the code execution if you use the `linker=vm,vm__lazy=True,warn__vm_gc_bug=True` Aesara flags. This warning is disabled by default as the bug was not released.
    Value:  False

warn__signal_conv2d_interface (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7fd8c748c290>>) 
    Doc:  Warn we use the new signal.conv2d() when its interface changed mid June 2014
    Value:  False

warn__reduce_join (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7fd8c748c2d0>>) 
    Doc:  Your current code is fine, but Aesara versions prior to 0.7 (or this development version) might have given an incorrect result. To disable this warning, set the Aesara flag warn__reduce_join to False. The problem was an optimization, that modified the pattern ""Reduce{scalar.op}(Join(axis=0, a, b), axis=0)"", did not check the reduction axis. So if the reduction axis was not 0, you got a wrong answer.
    Value:  False

warn__inc_set_subtensor1 (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7fd8c748c350>>) 
    Doc:  Warn if previous versions of Aesara (before 0.7) could have given incorrect results for inc_subtensor and set_subtensor when using some patterns of advanced indexing (indexing with one vector or matrix of ints).
    Value:  False

warn__round (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7fd8c748c3d0>>) 
    Doc:  Warn when using `tensor.round` with the default mode. Round changed its default from `half_away_from_zero` to `half_to_even` to have the same default as NumPy.
    Value:  False

warn__inc_subtensor1_opt (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7fd8c748c450>>) 
    Doc:  Warn if previous versions of Aesara (before 0.10) could have given incorrect results when computing inc_subtensor(zeros[idx], x)[idx], when idx is an array of integers with duplicated values.
    Value:  True

compiledir_format (<class 'str'>) 
    Doc:  Format string for platform-dependent compiled module subdirectory
(relative to base_compiledir). Available keys: aesara_version, device,
gxx_version, hostname, numpy_version, platform, processor,
python_bitwidth, python_int_bitwidth, python_version, short_platform.
Defaults to compiledir_%(short_platform)s-%(processor)s-%(python_versi
on)s-%(python_bitwidth)s.
    Value:  compiledir_%(short_platform)s-%(processor)s-%(python_version)s-%(python_bitwidth)s

<aesara.configparser.ConfigParam object at 0x7fd8cd9155d0>
    Doc:  platform-independent root directory for compiled modules
    Value:  /home/ff72/.aesara

<aesara.configparser.ConfigParam object at 0x7fd8c74798d0>
    Doc:  platform-dependent cache directory for compiled modules
    Value:  /home/ff72/.aesara/compiledir_Linux-3.10-el7.x86_64-x86_64-with-centos-7.7.1908-Core-x86_64-3.7.4-64

<aesara.configparser.ConfigParam object at 0x7fd8d094cc50>
    Doc:  Directory to cache pre-compiled kernels for the gpuarray backend.
    Value:  /home/ff72/.aesara/compiledir_Linux-3.10-el7.x86_64-x86_64-with-centos-7.7.1908-Core-x86_64-3.7.4-64/gpuarray_kernels

blas__ldflags (<class 'str'>) 
    Doc:  lib[s] to include for [Fortran] level-3 blas implementation
    Value:  

blas__check_openmp (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7fd8c6f64650>>) 
    Doc:  Check for openmp library conflict.
WARNING: Setting this to False leaves you open to wrong results in blas-related operations.
    Value:  True

scan__allow_gc (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7fd8c7487390>>) 
    Doc:  Allow/disallow gc inside of Scan (default: False)
    Value:  False

scan__allow_output_prealloc (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7fd8b81bea90>>) 
    Doc:  Allow/disallow memory preallocation for outputs inside of scan (default: True)
    Value:  True

scan__debug (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7fd8b81becd0>>) 
    Doc:  If True, enable extra verbose output related to scan
    Value:  False
 ```
* Python version: 3.7.4
* Operating system: CentOS Linux 7 (Core)
* How did you install Aesara: pip
",,,2021-05-14 16:25:20,2021-05-18 21:49:46,FFroehlich closed 2021-05-18 21:49:47,FFroehlich dfm brandonwillard,4
356,416,Import of pymc3 breaks because sigmoid opt in the wrong location,danituckerpersonal,"## Moving sigmoid opt to math breaks the import of pymc3, as it still thinks sigmoid opt is in nnet.

**Please provide a minimal, self-contained, and reproducible example.**
```python
import pymc3
```

**Please provide the full traceback of any errors.**
```python
     41 from pymc3.blocking import ArrayOrdering, DictToArrayBijection
     42 from pymc3.exceptions import ImputationWarning
---> 43 from pymc3.math import flatten_list
     44 from pymc3.util import WithMemoization, get_transformed_name, get_var_name, hash_key
     45 from pymc3.vartypes import continuous_types, discrete_types, isgenerator, typefilter

/opt/anaconda3/envs/env1/lib/python3.7/site-packages/pymc3/math.py in <module>
     79 
     80 from aesara.tensor.nlinalg import det, matrix_dot, matrix_inverse, trace
---> 81 from aesara.tensor.nnet import sigmoid
     82 from scipy.linalg import block_diag as scipy_block_diag
     83 

ImportError: cannot import name 'sigmoid' from 'aesara.tensor.nnet' (/opt/anaconda3/envs/threat_manager_env/lib/python3.7/site-packages/aesara/tensor/nnet/__init__.py)
```

**Please provide any additional information below.**


## Versions and main components

* Aesara version: 2.0.8
* Aesara config (`python -c ""import aesara; print(aesara.config)""`)
* Python version: 3.7.10
* Operating system: macOS Catalina 10.15.7
* How did you install Aesara: pip
",,,2021-05-17 17:23:05,2021-06-25 17:14:17,brandonwillard closed 2021-06-25 17:14:17,danituckerpersonal ricardoV94 brandonwillard,2
362,425,Use Numba caching,brandonwillard,"We need to enable and test Numba's caching option within our Numba backend.  The existing caching options will also need to be adapted to their Numba equivalents, when possible (e.g. `numba.config.CACHE_DIR` and our `aesara.config.compiledir`).",enhancement help wanted important Numba performance concern,kc611,2021-05-19 23:34:53,2022-01-18 16:38:54,"brandonwillard labeled 2021-05-19 23:34:53,brandonwillard labeled 2021-05-19 23:34:53,brandonwillard labeled 2021-05-19 23:34:53,brandonwillard labeled 2021-05-19 23:34:53,brandonwillard milestoned 2021-05-19 23:34:53,brandonwillard renamed 2021-05-19 23:35:12,brandonwillard connected 2021-11-08 23:38:05,kc611 assigned 2021-11-08 23:38:19,brandonwillard closed 2022-01-18 16:38:54,brandonwillard labeled 2022-07-31 19:56:59",kc611 brandonwillard,0
363,426,Documentation for the Numba backend,brandonwillard,"We need to add a description of our Numba backend to the documentation and provide a section detailing how a Numba implementation is created for an `Op`.

The latter should be almost a copy-paste of [`JaxOps.rst`](https://github.com/pymc-devs/aesara/blob/14c7373e4dd1d49ef2e73cfa08063a1a79e85380/doc/JaxOps.rst); however, we need to add it to `aesara.doc.extending` and make sure it shows up in the rendered site index.",documentation good first issue help wanted Numba,,2021-05-19 23:41:06,2022-01-14 23:55:11,"brandonwillard labeled 2021-05-19 23:41:07,brandonwillard labeled 2021-05-19 23:41:07,brandonwillard labeled 2021-05-19 23:41:07,brandonwillard labeled 2021-05-19 23:41:07,brandonwillard milestoned 2021-05-19 23:41:07,brandonwillard connected 2022-01-14 23:54:56,brandonwillard closed 2022-01-14 23:55:11",brandonwillard,0
364,427,Start deprecating the `aesara.gpuarray` sub-package,brandonwillard,"The `aesara.gpuarray` is no longer maintained and due for removal.  

At the very least, we need to add a deprecation warning stating as much.  This addition might also require the removal of any automatic `aesara.gpuarray` imports, since we don't want the warning to appear unless someone actually tries to use that sub-package.",good first issue help wanted important GPU,,2021-05-19 23:48:40,2022-04-07 00:35:45,"brandonwillard labeled 2021-05-19 23:48:40,brandonwillard labeled 2021-05-19 23:48:40,brandonwillard labeled 2021-05-19 23:48:40,brandonwillard labeled 2021-05-19 23:48:40,brandonwillard milestoned 2021-05-19 23:48:40,brandonwillard connected 2021-10-14 15:11:39,brandonwillard closed 2022-04-07 00:35:45",ferrine brandonwillard,2
365,428,Expose `aesara.tensor.random.basic` via `aesara.random`,brandonwillard,"To make it easier to use the `RandomVariable`s in `aesara.tensor.random.basic`, we should expose the module via `aesara.random` so that it more closely follows `np.random`.",enhancement good first issue help wanted,,2021-05-20 00:06:31,2021-05-23 23:25:33,"brandonwillard labeled 2021-05-20 00:06:31,brandonwillard labeled 2021-05-20 00:06:32,brandonwillard labeled 2021-05-20 00:06:32,brandonwillard milestoned 2021-05-20 00:06:32,brandonwillard referenced 2021-05-23 20:29:21,brandonwillard connected 2021-05-23 21:30:04,brandonwillard closed 2021-05-23 23:25:33,brandonwillard referenced 2021-05-23 23:25:34",ricardoV94 brandonwillard,2
367,430,Release a new version,twiecki,Would be great for https://github.com/pymc-devs/pymc3/pull/4695.,,,2021-05-21 10:21:42,2021-05-23 23:38:23,brandonwillard closed 2021-05-23 23:38:23,twiecki brandonwillard,2
368,431,Create a fixed-length/shape `TensorType` and `TensorVariable`,brandonwillard,"We need to clean up the handling of fixed-shape tensors, and creating a fixed-size `Type` (`TensorType`, really) is perhaps the best way to do that.

Such a `TensorType` would allow us to replace/remove and generalize most&mdash;if not all&mdash;of the distinct `Shape` handling logic and the haphazard use of `aesara.tensor.get_vector_length`, among other things.

Likewise, it provides a good solution to #42/#93.

For anyone not intimately familiar with these parts of Aesara's internals, users deal with `Variable` objects (e.g. `aesara.tensor.vector()` returns a `TensorVariable` instance, which is a subclass of `Variable`), and each `Variable` has a `Variable.type` value that extends `Type`.  `Type` **instances** are exactly what you'd guess they are: objects that emulate Python `type`s and provide static domain-specific type information.  For example, `TensorTypes` hold the data type (i.e. `TensorType.dtype`), and the number of dimensions/which dimensions are broadcastable (i.e. `TensorType.broadcastable`).

See #134 for more information about Aesara's types and how we might be able to simplify them by using actual Python `type`s instead.

## Background

If we were to implement a fixed-shape `TensorType`, and corresponding `TensorVariable`, the first question would be:
**""How do we change the results of every `Op.make_node` so that they produce outputs that are fixed-shape `TensorVariable`s when their inputs are fixed-shape?""**.  This is the real challenge underlying the idea.

Without addressing this question, we could have fixed-shape `TensorType`s/`Variable`s, but they would only be useful to the first `Op`s they encounter.  That's not necessarily a bad thing, because there are definitely a few `Op`s that could make good use of this (e.g. `size` and `shape`-based `Op` parameters).  Even so, we can definitely do better.

The `Shape` `Op`, `ShapeFeature`, `ShapeOptimizer`, and their associated `Op.infer_shape` methods are Aesara's current means of doing all this, and we might only need to cleverly repurpose the latter to get what we want.  

## Proposed Changes

More specifically, `Op.infer_shape` is the means through which an `Op` propagates the ""fixed-shape-edness"" of its inputs through to its outputs.  We could use these existing&mdash;albeit voluntary&mdash;implementations to determine whether or not an `Op`'s output should be a fixed-shape `TensorVariable` or not, and, if so, propagate the sizes and construct one.

In general, Aesara is designed in a way that makes Python-based typing very difficult to use constructively, because each individual `Op` is left to determine the exact types of the outputs it generates.  In nearly every instance, an `Op` will simply construct `TensorVariable`s from scratch and use those as its outputs, making it all but impossible to utilize `TensorVariable` subclasses&mdash;like our proposed fixed-shape `TensorVariable`&mdash;without changing all existing `Op.make_node` implementations.

Instead, we could actually make `Apply` something functional and utilize it for these purposes (and perhaps more).  At present, `Apply` is essentially just a container that performs no significant actions.  For instance, when an `Op.make_node` returns the `Apply` nodes it constructs, almost nothing is done by `Apply` other than trivial input validation.  We could make `Apply` a point of dispatch for iterative, type-based computations; that way, each `Op` isn't responsible for reasoning about things that are arguably outside of its scope.

In other words, when an `Op` is called with some inputs, `Op.make_node` constructs its outputs and creates an `Apply` node with said inputs and outputs, then the `Apply` node constructor (or `Apply.__new__`) calls the repurposed `Op.infer_shape`, which computes the concrete shapes of the outputs given the inputs, and creates updated fixed-shape outputs.

## Issues

### Computation

Such a change would move the somewhat elective shape inference process, which currently happens during optimization, into the model construction process.  Since shape inference isn't a particularly computationally intensive thing, this isn't a real concern.  Plus, we can always make shape propagation configurable, so there's no real need for a complete trade-off. 

Alternatively, we might be able to make shape propagation a lazily computed process.  This could be facilitated by changes to `TensorVariable` itself.  For instance, instead of `TensorVariable.shape` returning the output of a `Shape` `Op`, it could return a fixed-length `Sequence` that, when accessed would lazily compute the shape values.  

This could be a huge simplification relative to the current approach, because shape entries would be distinct objects.  Under the current approach, graphs can contain numerous distinct `Shape` `Op`s, and `*Subtensor*` `Op`s on those `Shape`s, that all refer to the same shapes.  Such a change would reduce the amount of merge-like work needed during the shape optimization process.  It might also remove the need for some/all of the convoluted `*Subtensor*` considerations in `ShapeFeature`.

Regardless, these considerations are somewhat tangential to the present issue.

### Unusual `Op`s

One issue with this approach is that an `Op` may for some reason want to hold on to the outputs it generates, and, if such an `Op` assumes that said outputs will be used down the line, it would be mistaken under these changes.  This is almost exclusively an `Op`-specific implementation issue; one that can&mdash;however&mdash;be easily remedied by first creating the `Apply` node and then using its refined outputs.

### In-place `Apply` updates

`FunctionGraph.replace` still operates by changing values in `Apply.inputs`.  This would circumvent the proposed changes.

Under certain assumptions, this might not be a problem, though.  For instance, if fixed-shape `TensorVariable`s have already been produced, and their shapes have already been computed, then moving them around shouldn't matter, because, by their very type alone, no shape computations are needed (i.e. they carry their static shape information with them at all times).

Otherwise, if there's a shape conflict that arises from a rewrite, it's really the fault of the rewrite operation.

The real concern arises when a fixed-shape `TensorVariable` replaces a non-fixed-shape `TensorVariable` input and vice versa, because we might need to update something downstream.  The type information itself would imply that the such a substitution isn't valid, at least not without assuming that the non-fixed-shape variables actually have the same fixed-shape (e.g. the fixed-shape types are refinement types and we're talking about something like substitutability).

There might not be many/any scenarios in which this is a real concern, especially when/if all of the fixed-shape information has already been determined within the graph containing the in-place updated `Apply` node.  Regardless, this is perhaps the main concern right now.",enhancement important refactor graph objects,,2021-05-21 16:21:50,2022-01-13 03:43:17,"brandonwillard labeled 2021-05-21 16:21:50,brandonwillard labeled 2021-05-21 16:21:50,brandonwillard labeled 2021-05-21 16:21:50,brandonwillard milestoned 2021-05-21 16:21:50,brandonwillard renamed 2021-05-21 21:07:25,brandonwillard demilestoned 2021-11-25 00:47:23,brandonwillard connected 2021-12-29 22:20:39,brandonwillard labeled 2022-01-07 19:15:28,brandonwillard closed 2022-01-13 03:43:17,twiecki mentioned 2022-02-05 11:12:16,twiecki subscribed 2022-02-05 11:12:16",ricardoV94 twiecki brandonwillard,3
374,438,Rename aesara.tensor.inv to aesara.tensor.reciprocal,rlouf,"Aesara implements `aesara.tensor.inv`, whose numpy equivalent is `numpy.reciprocal`. In order to get closer to numpy's API I suggest to rename `inv` to `reciprocal` and keep `inv` as an alias for retro-compatibility.",help wanted,brandonwillard,2021-05-24 11:39:39,2021-05-24 23:48:38,"brandonwillard labeled 2021-05-24 18:41:36,brandonwillard assigned 2021-05-24 19:00:11,brandonwillard closed 2021-05-24 23:48:38",rlouf brandonwillard,0
376,440,Add deprecation warnings for renamed classes and class instances,brandonwillard,"The `deprecated` wrapper works fine for deprecating _functions_ but not for renamed classes and class instances&mdash;e.g. we don't want to break things by changing the expected type of an object.

We need a means of adding deprecation warnings to renamed classes and class instances.

_Originally posted by @brandonwillard in https://github.com/pymc-devs/aesara/pull/439#discussion_r638252048_",enhancement good first issue help wanted,,2021-05-24 21:46:17,2022-08-19 17:52:56,"brandonwillard mentioned 2021-05-24 21:46:17,brandonwillard subscribed 2021-05-24 21:46:17,brandonwillard renamed 2021-05-24 21:46:28,brandonwillard labeled 2021-05-24 21:46:54,brandonwillard labeled 2021-05-24 21:46:54,brandonwillard labeled 2021-05-24 21:46:54,brandonwillard closed 2022-08-19 17:52:56",brandonwillard,2
378,444,Cherry pick #408 into theano-pymc branch ,ricardoV94,"Needed for next (hopefully last) pymc3 V3 release: https://github.com/pymc-devs/pymc3/issues/4658

PR: #408",,,2021-05-27 06:50:25,2021-07-30 05:57:44,brandonwillard closed 2021-07-30 05:57:44,ricardoV94 canyon289 brandonwillard,4
385,451,Fix flaky `test_categorical_samples`,brandonwillard,"It looks like the test `test_categorical_samples ` might be flaky (e.g. see [here](https://github.com/pymc-devs/aesara/runs/2703017078?check_suite_focus=true#step:6:1539)).
",bug,,2021-05-30 05:08:25,2021-10-25 17:32:17,"brandonwillard labeled 2021-05-30 05:08:25,brandonwillard closed 2021-10-25 17:32:17",brandonwillard,0
387,454,Fix some usability issues surrounding shared RNG objects,brandonwillard,"Work on https://github.com/pymc-devs/pymc3/pull/4729 has highlighted a potential confusion involving `RandomStateSharedVariable`s and in-place optimizations like `random_make_inplace`.

The problem is that in-place optimizations won't be performed on `RandomStateSharedVariable`s that do not have updates specified, since they're protected by the `FunctionGraph` `Supervisor` feature set up by `aesara.compile.function.types.std_fgraph`.

In order to get around this, one can add/use the `default_update` property on the `RandomStateSharedVariable` (e.g. see [here](https://github.com/pymc-devs/pymc3/blob/53f6f43fb3b23fcc50b7b924eaa0fe89613811aa/pymc3/distributions/distribution.py#L162) and `RandomStream.gen`); however, this isn't set automatically, so it can lead to confusion.

Let's consider adding the `default_update` property automatically.  This could probably be done in `RandomVariable.make_node`, but we first need to consider whether or not this will have other repercussions/restrictions/etc.",enhancement help wanted question random variables Scan,,2021-05-31 21:05:00,2023-03-17 22:14:53,"brandonwillard labeled 2021-05-31 21:05:00,brandonwillard labeled 2021-05-31 21:05:00,brandonwillard labeled 2021-05-31 21:05:00,brandonwillard labeled 2021-05-31 21:05:00,brandonwillard renamed 2022-01-05 22:31:45,brandonwillard labeled 2022-01-21 18:45:30,brandonwillard closed 2023-03-17 22:14:53,brandonwillard closed 2023-03-17 22:15:07",ricardoV94 brandonwillard,3
390,457,tensor function optimization failure due to: constant_folding,fanshi118,"## Ran into the following issue when trying to apply `aesara.tensor` functions to an array locally

**Please provide a minimal, self-contained, and reproducible example.**
```python
import numpy as np

import aesara
import aesara.tensor as at


aesara.config.on_opt_error = ""raise""

at.max(at.as_tensor_variable(np.array([[0., 1.], [1., 2.]]))).eval()
```

**Please provide the full traceback of any errors.**
<details>
  <summary>Click to expand traceback</summary>
WARNING (aesara.tensor.blas): Using NumPy C-API based implementation for BLAS functions.
ERROR (aesara.graph.opt): Optimization failure due to: constant_folding
ERROR (aesara.graph.opt): node: MaxAndArgmax{axis=(0, 1)}(TensorConstant{[[0. 1.]
 [1. 2.]]})
ERROR (aesara.graph.opt): TRACEBACK:
ERROR (aesara.graph.opt): Traceback (most recent call last):
  File ""/Users/sf/miniconda3/envs/aesara/lib/python3.8/site-packages/aesara/graph/opt.py"", line 2015, in process_node
    replacements = lopt.transform(fgraph, node)
  File ""/Users/sf/miniconda3/envs/aesara/lib/python3.8/site-packages/aesara/graph/opt.py"", line 1206, in transform
    return self.fn(*args, **kwargs)
  File ""/Users/sf/miniconda3/envs/aesara/lib/python3.8/site-packages/aesara/tensor/basic_opt.py"", line 4355, in constant_folding
    thunk = node.op.make_thunk(node, storage_map, compute_map, no_recycling=[])
  File ""/Users/sf/miniconda3/envs/aesara/lib/python3.8/site-packages/aesara/graph/op.py"", line 656, in make_thunk
    return self.make_c_thunk(node, storage_map, compute_map, no_recycling)
  File ""/Users/sf/miniconda3/envs/aesara/lib/python3.8/site-packages/aesara/graph/op.py"", line 622, in make_c_thunk
    outputs = cl.make_thunk(
  File ""/Users/sf/miniconda3/envs/aesara/lib/python3.8/site-packages/aesara/link/c/basic.py"", line 1204, in make_thunk
    cthunk, module, in_storage, out_storage, error_storage = self.__compile__(
  File ""/Users/sf/miniconda3/envs/aesara/lib/python3.8/site-packages/aesara/link/c/basic.py"", line 1139, in __compile__
    thunk, module = self.cthunk_factory(
  File ""/Users/sf/miniconda3/envs/aesara/lib/python3.8/site-packages/aesara/link/c/basic.py"", line 1635, in cthunk_factory
    module = get_module_cache().module_from_key(key=key, lnk=self)
  File ""/Users/sf/miniconda3/envs/aesara/lib/python3.8/site-packages/aesara/link/c/cmodule.py"", line 1198, in module_from_key
    module = lnk.compile_cmodule(location)
  File ""/Users/sf/miniconda3/envs/aesara/lib/python3.8/site-packages/aesara/link/c/basic.py"", line 1544, in compile_cmodule
    module = c_compiler.compile_str(
  File ""/Users/sf/miniconda3/envs/aesara/lib/python3.8/site-packages/aesara/link/c/cmodule.py"", line 2553, in compile_str
    raise CompileError(
aesara.link.c.exceptions.CompileError: Compilation failed (return status=1):
/Users/sf/miniconda3/envs/aesara/bin/clang++ -dynamiclib -g -O3 -fno-math-errno -Wno-unused-label -Wno-unused-variable -Wno-write-strings -DNPY_NO_DEPRECATED_API=NPY_1_7_API_VERSION -m64 -fPIC -undefined dynamic_lookup -I/Users/sf/miniconda3/envs/aesara/lib/python3.8/site-packages/numpy/core/include -I/Users/sf/miniconda3/envs/aesara/include/python3.8 -I/Users/sf/miniconda3/envs/aesara/lib/python3.8/site-packages/aesara/link/c/c_code -L/Users/sf/miniconda3/envs/aesara/lib -fvisibility=hidden -o /Users/sf/.aesara/compiledir_macOS-11.0.1-x86_64-i386-64bit-i386-3.8.10-64/tmpwunj4hgp/m4b2ef76bb0f9461399355330a611097ca43c41e28c2d7fc7506e5efb00d6b0e5.so /Users/sf/.aesara/compiledir_macOS-11.0.1-x86_64-i386-64bit-i386-3.8.10-64/tmpwunj4hgp/mod.cpp
In file included from /Users/sf/.aesara/compiledir_macOS-11.0.1-x86_64-i386-64bit-i386-3.8.10-64/tmpwunj4hgp/mod.cpp:1:
In file included from /Users/sf/miniconda3/envs/aesara/include/python3.8/Python.h:25:
/Users/sf/miniconda3/envs/aesara/bin/../include/c++/v1/stdio.h:107:15: fatal error: 'stdio.h' file not found
#include_next <stdio.h>
              ^~~~~~~~~
1 error generated.

ERROR (aesara.graph.opt): SeqOptimizer apply <aesara.graph.opt.EquilibriumOptimizer object at 0x18f84ef70>
ERROR (aesara.graph.opt): Traceback:
ERROR (aesara.graph.opt): Traceback (most recent call last):
  File ""/Users/sf/miniconda3/envs/aesara/lib/python3.8/site-packages/aesara/graph/opt.py"", line 246, in apply
    sub_prof = optimizer.optimize(fgraph)
  File ""/Users/sf/miniconda3/envs/aesara/lib/python3.8/site-packages/aesara/graph/opt.py"", line 84, in optimize
    ret = self.apply(fgraph, *args, **kwargs)
  File ""/Users/sf/miniconda3/envs/aesara/lib/python3.8/site-packages/aesara/graph/opt.py"", line 2563, in apply
    sub_prof = gopt.apply(fgraph)
  File ""/Users/sf/miniconda3/envs/aesara/lib/python3.8/site-packages/aesara/graph/opt.py"", line 2126, in apply
    nb += self.process_node(fgraph, node)
  File ""/Users/sf/miniconda3/envs/aesara/lib/python3.8/site-packages/aesara/graph/opt.py"", line 2018, in process_node
    self.failure_callback(
  File ""/Users/sf/miniconda3/envs/aesara/lib/python3.8/site-packages/aesara/graph/opt.py"", line 1916, in warn_inplace
    return NavigatorOptimizer.warn(exc, nav, repl_pairs, local_opt, node)
  File ""/Users/sf/miniconda3/envs/aesara/lib/python3.8/site-packages/aesara/graph/opt.py"", line 1902, in warn
    raise exc
  File ""/Users/sf/miniconda3/envs/aesara/lib/python3.8/site-packages/aesara/graph/opt.py"", line 2015, in process_node
    replacements = lopt.transform(fgraph, node)
  File ""/Users/sf/miniconda3/envs/aesara/lib/python3.8/site-packages/aesara/graph/opt.py"", line 1206, in transform
    return self.fn(*args, **kwargs)
  File ""/Users/sf/miniconda3/envs/aesara/lib/python3.8/site-packages/aesara/tensor/basic_opt.py"", line 4355, in constant_folding
    thunk = node.op.make_thunk(node, storage_map, compute_map, no_recycling=[])
  File ""/Users/sf/miniconda3/envs/aesara/lib/python3.8/site-packages/aesara/graph/op.py"", line 656, in make_thunk
    return self.make_c_thunk(node, storage_map, compute_map, no_recycling)
  File ""/Users/sf/miniconda3/envs/aesara/lib/python3.8/site-packages/aesara/graph/op.py"", line 622, in make_c_thunk
    outputs = cl.make_thunk(
  File ""/Users/sf/miniconda3/envs/aesara/lib/python3.8/site-packages/aesara/link/c/basic.py"", line 1204, in make_thunk
    cthunk, module, in_storage, out_storage, error_storage = self.__compile__(
  File ""/Users/sf/miniconda3/envs/aesara/lib/python3.8/site-packages/aesara/link/c/basic.py"", line 1139, in __compile__
    thunk, module = self.cthunk_factory(
  File ""/Users/sf/miniconda3/envs/aesara/lib/python3.8/site-packages/aesara/link/c/basic.py"", line 1635, in cthunk_factory
    module = get_module_cache().module_from_key(key=key, lnk=self)
  File ""/Users/sf/miniconda3/envs/aesara/lib/python3.8/site-packages/aesara/link/c/cmodule.py"", line 1198, in module_from_key
    module = lnk.compile_cmodule(location)
  File ""/Users/sf/miniconda3/envs/aesara/lib/python3.8/site-packages/aesara/link/c/basic.py"", line 1544, in compile_cmodule
    module = c_compiler.compile_str(
  File ""/Users/sf/miniconda3/envs/aesara/lib/python3.8/site-packages/aesara/link/c/cmodule.py"", line 2553, in compile_str
    raise CompileError(
aesara.link.c.exceptions.CompileError: Compilation failed (return status=1):
/Users/sf/miniconda3/envs/aesara/bin/clang++ -dynamiclib -g -O3 -fno-math-errno -Wno-unused-label -Wno-unused-variable -Wno-write-strings -DNPY_NO_DEPRECATED_API=NPY_1_7_API_VERSION -m64 -fPIC -undefined dynamic_lookup -I/Users/sf/miniconda3/envs/aesara/lib/python3.8/site-packages/numpy/core/include -I/Users/sf/miniconda3/envs/aesara/include/python3.8 -I/Users/sf/miniconda3/envs/aesara/lib/python3.8/site-packages/aesara/link/c/c_code -L/Users/sf/miniconda3/envs/aesara/lib -fvisibility=hidden -o /Users/sf/.aesara/compiledir_macOS-11.0.1-x86_64-i386-64bit-i386-3.8.10-64/tmpwunj4hgp/m4b2ef76bb0f9461399355330a611097ca43c41e28c2d7fc7506e5efb00d6b0e5.so /Users/sf/.aesara/compiledir_macOS-11.0.1-x86_64-i386-64bit-i386-3.8.10-64/tmpwunj4hgp/mod.cpp
In file included from /Users/sf/.aesara/compiledir_macOS-11.0.1-x86_64-i386-64bit-i386-3.8.10-64/tmpwunj4hgp/mod.cpp:1:
In file included from /Users/sf/miniconda3/envs/aesara/include/python3.8/Python.h:25:
/Users/sf/miniconda3/envs/aesara/bin/../include/c++/v1/stdio.h:107:15: fatal error: 'stdio.h' file not found
#include_next <stdio.h>
              ^~~~~~~~~
1 error generated.

You can find the C code in this temporary file: /var/folders/gl/s96rmcrx5nnc6qy4crb_h2xc0000gp/T/aesara_compilation_error_fz4nfdm9
---------------------------------------------------------------------------
CompileError                              Traceback (most recent call last)
<ipython-input-1-e6dbd04a9942> in <module>
      7 aesara.config.on_opt_error = ""raise""
      8 
----> 9 at.max(at.as_tensor_variable(np.array([[0., 1.], [1., 2.]]))).eval()

~/miniconda3/envs/aesara/lib/python3.8/site-packages/aesara/graph/basic.py in eval(self, inputs_to_values)
    550         inputs = tuple(sorted(inputs_to_values.keys(), key=id))
    551         if inputs not in self._fn_cache:
--> 552             self._fn_cache[inputs] = function(inputs, self)
    553         args = [inputs_to_values[param] for param in inputs]
    554 

~/miniconda3/envs/aesara/lib/python3.8/site-packages/aesara/compile/function/__init__.py in function(inputs, outputs, mode, updates, givens, no_default_updates, accept_inplace, name, rebuild_strict, allow_input_downcast, profile, on_unused_input)
    335         # note: pfunc will also call orig_function -- orig_function is
    336         #      a choke point that all compilation must pass through
--> 337         fn = pfunc(
    338             params=inputs,
    339             outputs=outputs,

~/miniconda3/envs/aesara/lib/python3.8/site-packages/aesara/compile/function/pfunc.py in pfunc(params, outputs, mode, updates, givens, no_default_updates, accept_inplace, name, rebuild_strict, allow_input_downcast, profile, on_unused_input, output_keys)
    522         inputs.append(si)
    523 
--> 524     return orig_function(
    525         inputs,
    526         cloned_outputs,

~/miniconda3/envs/aesara/lib/python3.8/site-packages/aesara/compile/function/types.py in orig_function(inputs, outputs, mode, accept_inplace, name, profile, on_unused_input, output_keys)
   1970     try:
   1971         Maker = getattr(mode, ""function_maker"", FunctionMaker)
-> 1972         m = Maker(
   1973             inputs,
   1974             outputs,

~/miniconda3/envs/aesara/lib/python3.8/site-packages/aesara/compile/function/types.py in __init__(self, inputs, outputs, mode, accept_inplace, function_builder, profile, on_unused_input, fgraph, output_keys, name)
   1615                         )
   1616                     else:
-> 1617                         optimizer_profile = optimizer(fgraph)
   1618 
   1619                     end_optimizer = time.time()

~/miniconda3/envs/aesara/lib/python3.8/site-packages/aesara/graph/opt.py in __call__(self, fgraph)
     91 
     92         """"""
---> 93         return self.optimize(fgraph)
     94 
     95     def add_requirements(self, fgraph):

~/miniconda3/envs/aesara/lib/python3.8/site-packages/aesara/graph/opt.py in optimize(self, fgraph, *args, **kwargs)
     82         """"""
     83         self.add_requirements(fgraph)
---> 84         ret = self.apply(fgraph, *args, **kwargs)
     85         return ret
     86 

~/miniconda3/envs/aesara/lib/python3.8/site-packages/aesara/graph/opt.py in apply(self, fgraph)
    255                 except Exception as e:
    256                     if self.failure_callback:
--> 257                         self.failure_callback(e, self, optimizer)
    258                         continue
    259                     else:

~/miniconda3/envs/aesara/lib/python3.8/site-packages/aesara/graph/opt.py in warn(exc, self, optimizer)
    185         _logger.error(traceback.format_exc())
    186         if config.on_opt_error == ""raise"":
--> 187             raise exc
    188         elif config.on_opt_error == ""pdb"":
    189             pdb.post_mortem(sys.exc_info()[2])

~/miniconda3/envs/aesara/lib/python3.8/site-packages/aesara/graph/opt.py in apply(self, fgraph)
    244                     nb_nodes_before = len(fgraph.apply_nodes)
    245                     t0 = time.time()
--> 246                     sub_prof = optimizer.optimize(fgraph)
    247                     l.append(float(time.time() - t0))
    248                     sub_profs.append(sub_prof)

~/miniconda3/envs/aesara/lib/python3.8/site-packages/aesara/graph/opt.py in optimize(self, fgraph, *args, **kwargs)
     82         """"""
     83         self.add_requirements(fgraph)
---> 84         ret = self.apply(fgraph, *args, **kwargs)
     85         return ret
     86 

~/miniconda3/envs/aesara/lib/python3.8/site-packages/aesara/graph/opt.py in apply(self, fgraph, start_from)
   2561                 nb = change_tracker.nb_imported
   2562                 t_opt = time.time()
-> 2563                 sub_prof = gopt.apply(fgraph)
   2564                 time_opts[gopt] += time.time() - t_opt
   2565                 sub_profs.append(sub_prof)

~/miniconda3/envs/aesara/lib/python3.8/site-packages/aesara/graph/opt.py in apply(self, fgraph, start_from)
   2124                     continue
   2125                 current_node = node
-> 2126                 nb += self.process_node(fgraph, node)
   2127             loop_t = time.time() - t0
   2128         finally:

~/miniconda3/envs/aesara/lib/python3.8/site-packages/aesara/graph/opt.py in process_node(self, fgraph, node, lopt)
   2016         except Exception as e:
   2017             if self.failure_callback is not None:
-> 2018                 self.failure_callback(
   2019                     e, self, [(x, None) for x in node.outputs], lopt, node
   2020                 )

~/miniconda3/envs/aesara/lib/python3.8/site-packages/aesara/graph/opt.py in warn_inplace(exc, nav, repl_pairs, local_opt, node)
   1914         if isinstance(exc, InconsistencyError):
   1915             return
-> 1916         return NavigatorOptimizer.warn(exc, nav, repl_pairs, local_opt, node)
   1917 
   1918     @staticmethod

~/miniconda3/envs/aesara/lib/python3.8/site-packages/aesara/graph/opt.py in warn(exc, nav, repl_pairs, local_opt, node)
   1900             # We always crash on AssertionError because something may be
   1901             # seriously wrong if such an exception is raised.
-> 1902             raise exc
   1903 
   1904     @staticmethod

~/miniconda3/envs/aesara/lib/python3.8/site-packages/aesara/graph/opt.py in process_node(self, fgraph, node, lopt)
   2013         lopt = lopt or self.local_opt
   2014         try:
-> 2015             replacements = lopt.transform(fgraph, node)
   2016         except Exception as e:
   2017             if self.failure_callback is not None:

~/miniconda3/envs/aesara/lib/python3.8/site-packages/aesara/graph/opt.py in transform(self, *args, **kwargs)
   1204 
   1205     def transform(self, *args, **kwargs):
-> 1206         return self.fn(*args, **kwargs)
   1207 
   1208     def add_requirements(self, fgraph):

~/miniconda3/envs/aesara/lib/python3.8/site-packages/aesara/tensor/basic_opt.py in constant_folding(fgraph, node)
   4353         compute_map[o] = [False]
   4354 
-> 4355     thunk = node.op.make_thunk(node, storage_map, compute_map, no_recycling=[])
   4356     required = thunk()
   4357     # A node whose inputs are all provided should always return successfully

~/miniconda3/envs/aesara/lib/python3.8/site-packages/aesara/graph/op.py in make_thunk(self, node, storage_map, compute_map, no_recycling, impl)
    654             )
    655             try:
--> 656                 return self.make_c_thunk(node, storage_map, compute_map, no_recycling)
    657             except (NotImplementedError, MethodNotDefined):
    658                 # We requested the c code, so don't catch the error.

~/miniconda3/envs/aesara/lib/python3.8/site-packages/aesara/graph/op.py in make_c_thunk(self, node, storage_map, compute_map, no_recycling)
    620                 print(f""Disabling C code for {self} due to unsupported float16"")
    621                 raise NotImplementedError(""float16"")
--> 622         outputs = cl.make_thunk(
    623             input_storage=node_input_storage, output_storage=node_output_storage
    624         )

~/miniconda3/envs/aesara/lib/python3.8/site-packages/aesara/link/c/basic.py in make_thunk(self, input_storage, output_storage, storage_map)
   1202         """"""
   1203         init_tasks, tasks = self.get_init_tasks()
-> 1204         cthunk, module, in_storage, out_storage, error_storage = self.__compile__(
   1205             input_storage, output_storage, storage_map
   1206         )

~/miniconda3/envs/aesara/lib/python3.8/site-packages/aesara/link/c/basic.py in __compile__(self, input_storage, output_storage, storage_map)
   1137         input_storage = tuple(input_storage)
   1138         output_storage = tuple(output_storage)
-> 1139         thunk, module = self.cthunk_factory(
   1140             error_storage,
   1141             input_storage,

~/miniconda3/envs/aesara/lib/python3.8/site-packages/aesara/link/c/basic.py in cthunk_factory(self, error_storage, in_storage, out_storage, storage_map)
   1633             for node in self.node_order:
   1634                 node.op.prepare_node(node, storage_map, None, ""c"")
-> 1635             module = get_module_cache().module_from_key(key=key, lnk=self)
   1636 
   1637         vars = self.inputs + self.outputs + self.orphans

~/miniconda3/envs/aesara/lib/python3.8/site-packages/aesara/link/c/cmodule.py in module_from_key(self, key, lnk)
   1196             try:
   1197                 location = dlimport_workdir(self.dirname)
-> 1198                 module = lnk.compile_cmodule(location)
   1199                 name = module.__file__
   1200                 assert name.startswith(location)

~/miniconda3/envs/aesara/lib/python3.8/site-packages/aesara/link/c/basic.py in compile_cmodule(self, location)
   1542             try:
   1543                 _logger.debug(f""LOCATION {location}"")
-> 1544                 module = c_compiler.compile_str(
   1545                     module_name=mod.code_hash,
   1546                     src_code=src_code,

~/miniconda3/envs/aesara/lib/python3.8/site-packages/aesara/link/c/cmodule.py in compile_str(module_name, src_code, location, include_dirs, lib_dirs, libs, preargs, py_module, hide_symbols)
   2551             # difficult to read.
   2552             # compile_stderr = compile_stderr.replace(""\\n"", "". "")
-> 2553             raise CompileError(
   2554                 f""Compilation failed (return status={status}):\\n{' '.join(cmd)}\\n{compile_stderr}""
   2555             )

CompileError: Compilation failed (return status=1):
/Users/sf/miniconda3/envs/aesara/bin/clang++ -dynamiclib -g -O3 -fno-math-errno -Wno-unused-label -Wno-unused-variable -Wno-write-strings -DNPY_NO_DEPRECATED_API=NPY_1_7_API_VERSION -m64 -fPIC -undefined dynamic_lookup -I/Users/sf/miniconda3/envs/aesara/lib/python3.8/site-packages/numpy/core/include -I/Users/sf/miniconda3/envs/aesara/include/python3.8 -I/Users/sf/miniconda3/envs/aesara/lib/python3.8/site-packages/aesara/link/c/c_code -L/Users/sf/miniconda3/envs/aesara/lib -fvisibility=hidden -o /Users/sf/.aesara/compiledir_macOS-11.0.1-x86_64-i386-64bit-i386-3.8.10-64/tmpwunj4hgp/m4b2ef76bb0f9461399355330a611097ca43c41e28c2d7fc7506e5efb00d6b0e5.so /Users/sf/.aesara/compiledir_macOS-11.0.1-x86_64-i386-64bit-i386-3.8.10-64/tmpwunj4hgp/mod.cpp
In file included from /Users/sf/.aesara/compiledir_macOS-11.0.1-x86_64-i386-64bit-i386-3.8.10-64/tmpwunj4hgp/mod.cpp:1:
In file included from /Users/sf/miniconda3/envs/aesara/include/python3.8/Python.h:25:
/Users/sf/miniconda3/envs/aesara/bin/../include/c++/v1/stdio.h:107:15: fatal error: 'stdio.h' file not found
#include_next <stdio.h>
              ^~~~~~~~~
1 error generated.
</details>

**Please provide any additional information below.**

I installed `mkl-service` in addition to `aesara`. Could I be missing something else?

## Versions and main components

* Aesara version: `2.0.10`
* Aesara config (`python -c ""import aesara; print(aesara.config)""`)
* Python version: `3.8.10`
* Operating system: `MacOS`
* How did you install Aesara: (conda/pip) `pip install git+https://…`
",,,2021-06-02 21:33:20,2021-06-03 17:08:35,"staticmethod mentioned 2021-06-02 23:26:47,staticmethod subscribed 2021-06-02 23:26:47,fanshi118 closed 2021-06-03 17:08:35",staticmethod fanshi118 brandonwillard,2
391,458,Duplicate argument while compiling `at.switch` via `numba` mode,fanshi118,"## Description of your problem or feature request

Ran into a duplicate argument error while trying to compile an `at.switch` function via `numba`.

**Please provide a minimal, self-contained, and reproducible example.**
```python
import aesara
import aesara.tensor as at


X = at.matrix(""X"")
x_switch = at.switch(X, 0, X)
x_switch_fn = aesara.function([X], x_switch, mode=""NUMBA"")
```

**Please provide the full traceback of any errors.**
<details>
  <summary>Click to expand traceback</summary>

```python
Traceback (most recent call last):

  File ""/Users/sf/miniconda3/envs/aesara/lib/python3.8/site-packages/IPython/core/interactiveshell.py"", line 3441, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)

  File ""<ipython-input-1-4311260e2989>"", line 7, in <module>
    x_switch_fn = aesara.function([X], x_switch, mode=""NUMBA"")

  File ""/Users/sf/miniconda3/envs/aesara/lib/python3.8/site-packages/aesara/compile/function/__init__.py"", line 337, in function
    fn = pfunc(

  File ""/Users/sf/miniconda3/envs/aesara/lib/python3.8/site-packages/aesara/compile/function/pfunc.py"", line 524, in pfunc
    return orig_function(

  File ""/Users/sf/miniconda3/envs/aesara/lib/python3.8/site-packages/aesara/compile/function/types.py"", line 1983, in orig_function
    fn = m.create(defaults)

  File ""/Users/sf/miniconda3/envs/aesara/lib/python3.8/site-packages/aesara/compile/function/types.py"", line 1838, in create
    _fn, _i, _o = self.linker.make_thunk(

  File ""/Users/sf/miniconda3/envs/aesara/lib/python3.8/site-packages/aesara/link/basic.py"", line 282, in make_thunk
    return self.make_all(

  File ""/Users/sf/miniconda3/envs/aesara/lib/python3.8/site-packages/aesara/link/basic.py"", line 739, in make_all
    thunks, nodes = self.create_jitable_thunk(

  File ""/Users/sf/miniconda3/envs/aesara/lib/python3.8/site-packages/aesara/link/basic.py"", line 683, in create_jitable_thunk
    converted_fgraph = self.fgraph_convert(

  File ""/Users/sf/miniconda3/envs/aesara/lib/python3.8/site-packages/aesara/link/numba/linker.py"", line 10, in fgraph_convert
    return numba_funcify(fgraph, **kwargs)

  File ""/Users/sf/miniconda3/envs/aesara/lib/python3.8/functools.py"", line 875, in wrapper
    return dispatch(args[0].__class__)(*args, **kw)

  File ""/Users/sf/miniconda3/envs/aesara/lib/python3.8/site-packages/aesara/link/numba/dispatch.py"", line 328, in numba_funcify_FunctionGraph
    return fgraph_to_python(

  File ""/Users/sf/miniconda3/envs/aesara/lib/python3.8/site-packages/aesara/link/utils.py"", line 685, in fgraph_to_python
    jax_func = op_conversion_fn(

  File ""/Users/sf/miniconda3/envs/aesara/lib/python3.8/functools.py"", line 875, in wrapper
    return dispatch(args[0].__class__)(*args, **kw)

  File ""/Users/sf/miniconda3/envs/aesara/lib/python3.8/site-packages/aesara/link/numba/dispatch.py"", line 371, in numba_funcify_Elemwise
    scalar_op_fn = numba_funcify(op.scalar_op, node, **kwargs)

  File ""/Users/sf/miniconda3/envs/aesara/lib/python3.8/functools.py"", line 875, in wrapper
    return dispatch(args[0].__class__)(*args, **kw)

  File ""/Users/sf/miniconda3/envs/aesara/lib/python3.8/site-packages/aesara/link/numba/dispatch.py"", line 364, in numba_funcify_ScalarOp
    scalar_op_fn = compile_function_src(scalar_op_src, scalar_op_fn_name, global_env)

  File ""/Users/sf/miniconda3/envs/aesara/lib/python3.8/site-packages/aesara/link/utils.py"", line 587, in compile_function_src
    mod_code = compile(src, filename, mode=""exec"")

  File ""/var/folders/gl/s96rmcrx5nnc6qy4crb_h2xc0000gp/T/tmpgrrv30jt"", line 2
    def where(auto_13, auto_4205, auto_13):
    ^
SyntaxError: duplicate argument 'auto_13' in function definition
```

</details>

**Please provide any additional information below.**


## Versions and main components

* Aesara version: `master` branch, latest
* Aesara config (`python -c ""import aesara; print(aesara.config)""`)
* Python version: `3.8.10`
* Operating system: `MacOS`, `Linux`
* How did you install Aesara: (conda/pip) `pip install git+https://…`
",bug important Numba,brandonwillard,2021-06-03 19:41:29,2021-06-04 00:17:20,"brandonwillard assigned 2021-06-03 19:56:07,brandonwillard labeled 2021-06-03 19:56:12,brandonwillard labeled 2021-06-03 19:56:12,brandonwillard labeled 2021-06-03 19:56:25,brandonwillard connected 2021-06-03 21:23:30,brandonwillard closed 2021-06-04 00:17:20",fanshi118 brandonwillard,0
394,461,LogSumExp of `-inf` returns `nan` instead of `-inf`,ricardoV94,"```python
x = at.vector('x')
res1 = at.log(at.sum(at.exp(x)))  # nan with sum
res2 = at.log(at.prod(at.exp(x)))  # but not with prod

fun = aesara.function([x], [res1, res2])
print(fun(np.array([-np.inf, -np.inf])))  # [array(nan), array(-inf)]
```

for reference in numpy we get `-inf`, together with a divide by zero encountered in log

```python
print(np.log(np.sum(np.exp([-np.inf, -np.inf]))))  # -inf
```

This showed up in the pymc3 `logsumexp` function which returns `nan` with this input:

```python
from pymc3.math import logsumexp

x = at.vector('x')
res = logsumexp(x)
fun = aesara.function([x], res) 

print(fun(np.array([-np.inf, -np.inf])))  # [nan]
```
Whereas the scipy reference works fine
```python
print(scipy.special.logsumexp([-np.inf, -np.inf]))  # -inf
```

Weirdly it happens with addition, but not subtraction or multiplication, so maybe the problem is not the sum but the addition?

```python
x = at.vector('x')
res1 = at.log(at.exp(x[0]) + at.exp(x[1]))
res2 = at.log(at.exp(x[0]) - at.exp(x[1]))
fun = aesara.function([x], [res1, res2])
print(fun(np.array([-np.inf, -np.inf])))  # [array(nan), array(-inf)]",bug help wanted important graph rewriting,,2021-06-04 10:08:48,2021-06-09 09:50:28,"ricardoV94 renamed 2021-06-04 10:08:56,ricardoV94 renamed 2021-06-04 10:17:13,brandonwillard labeled 2021-06-04 22:46:34,brandonwillard labeled 2021-06-04 22:46:34,brandonwillard labeled 2021-06-04 22:46:34,brandonwillard labeled 2021-06-05 00:29:49,ricardoV94 referenced 2021-06-06 13:14:25,ricardoV94 referenced 2021-06-07 08:52:11,ricardoV94 referenced 2021-06-07 16:54:15,ricardoV94 referenced 2021-06-07 16:54:15,ricardoV94 referenced 2021-06-07 16:55:45,ricardoV94 referenced 2021-06-07 16:55:45,ricardoV94 referenced 2021-06-09 07:02:41,ricardoV94 referenced 2021-06-09 07:02:41,ricardoV94 closed 2021-06-09 09:50:28,ricardoV94 referenced 2021-06-09 09:50:29,ricardoV94 referenced 2021-06-09 09:50:29",ricardoV94 brandonwillard,1
399,467,Add the `logaddexp` function to `aesara.tensor`,rlouf,"The function can be easily implemented, but for the sake of getting closer to the numpy API we should add the function to `aesara.tensor`:

 https://numpy.org/doc/stable/reference/generated/numpy.logaddexp.html",enhancement good first issue help wanted NumPy compatibility,,2021-06-07 14:17:32,2021-06-21 16:11:18,"brandonwillard labeled 2021-06-07 16:28:21,brandonwillard labeled 2021-06-07 16:28:21,brandonwillard labeled 2021-06-07 16:28:21,brandonwillard labeled 2021-06-07 16:28:21,ricardoV94 referenced 2021-06-21 10:28:26,brandonwillard closed 2021-06-21 16:11:18,brandonwillard referenced 2021-06-21 16:11:19,ricardoV94 referenced 2021-06-23 16:48:30",ricardoV94 rlouf brandonwillard,1
401,469,Prevent duplicate Actions runs,brandonwillard,It looks like we need to set up something that prevents duplicate/parallel workflow runs (e.g. caused by rebasing).  This might work: [skip-duplicate-actions](https://github.com/marketplace/actions/skip-duplicate-actions).,help wanted important CI,,2021-06-07 17:55:38,2021-06-15 19:57:23,"brandonwillard labeled 2021-06-07 17:55:38,brandonwillard labeled 2021-06-07 17:55:38,brandonwillard labeled 2021-06-07 17:55:38,brandonwillard closed 2021-06-15 19:57:23",brandonwillard,1
402,470,Stop using Numba's experimental first-class function type feature,brandonwillard,"While investigating #404, I noticed that we're implicitly using Numba's experimental first-class function type feature.  It appears to work, but it might not be the best for performance, and, perhaps more importantly, it shouldn't be necessary.

It's being used in [this line](https://github.com/aesara-devs/aesara/blob/3a3adaee3b8ceccff6d0208996f117fcd5e5eb6f/aesara/link/numba/dispatch.py#L553), so that's what needs to change.",important Numba performance concern,,2021-06-08 00:03:54,2021-06-08 17:13:08,"brandonwillard labeled 2021-06-08 00:03:54,brandonwillard labeled 2021-06-08 00:03:54,brandonwillard closed 2021-06-08 17:13:08,brandonwillard labeled 2022-07-31 19:56:59",brandonwillard,0
404,472,Create a Numba implementation for `BroadcastTo`,brandonwillard,"We might be able to create a Numba version of `BroadcastTo` using [`_broadcast_to_shape`](https://github.com/numba/numba/blob/6881dfe3883d1344014ea16185ed87de4b75b9a1/numba/np/arrayobj.py#L1283) (see [here](https://github.com/numba/numba/issues/7078#issuecomment-856962242)).  This would likely be a big improvement over the C backend, since even it needs to use Python for that `Op`.",enhancement important Numba,,2021-06-08 20:00:14,2022-01-20 03:57:22,"brandonwillard labeled 2021-06-08 20:00:14,brandonwillard labeled 2021-06-08 20:00:14,brandonwillard labeled 2021-06-08 20:00:14,kc611 closed 2022-01-20 03:57:22",kc611 brandonwillard,1
406,474,Make Numba `Elemwise` operate in-place,brandonwillard,"Currently, our Numba conversion for `Elemwise` `Op`s is not performing the in-place updates specified by `Elemwise.inplace_pattern`. 

The end result is that Numba converted graphs perform more costly copy/allocation operations than their C counterparts.  This is likely one of the reasons for the Numba/C performance discrepancy observed in #404.",bug important Numba performance concern,,2021-06-08 22:58:54,2021-06-10 04:54:11,"brandonwillard labeled 2021-06-08 22:58:54,brandonwillard labeled 2021-06-08 22:58:54,brandonwillard labeled 2021-06-08 22:58:54,brandonwillard renamed 2021-06-08 22:59:32,brandonwillard renamed 2021-06-08 23:11:32,brandonwillard connected 2021-06-09 02:19:45,brandonwillard closed 2021-06-10 04:54:11,brandonwillard labeled 2022-07-31 19:56:59",brandonwillard,0
409,477,"Local test failures in 2.0.11: TestGammaUBroadcast.test_good, TestGammaUInplaceBroadcast.test_good",mgorny,"## Description of your problem or feature request

**Please provide a minimal, self-contained, and reproducible example.**
The two following tests fail repeatedly on my system:
```
FAILED tests/tensor/test_math_scipy.py::TestGammaUBroadcast::test_good - AssertionError: Test Elemwise{gammau,no_inplace}::normal: Output 0 gav...
FAILED tests/tensor/test_math_scipy.py::TestGammaUInplaceBroadcast::test_good - AssertionError: Test Elemwise{gammau_inplace,inplace}::normal: ...
```

**Please provide the full traceback of any errors.**
```python
_________________________________________________________ TestGammaUBroadcast.test_good __________________________________________________________
[gw4] linux -- Python 3.8.10 /usr/bin/python3.8

self = <tests.tensor.utils.GammauTester object at 0x7f2401508b20>

    @pytest.mark.skipif(skip, reason=""Skipped"")
    def test_good(self):
        good = self.add_memmap_values(self.good)
    
        for testname, inputs in good.items():
            inputs = [copy(input) for input in inputs]
            inputrs = [
                TensorType(
                    dtype=input.dtype,
                    broadcastable=[shape_elem == 1 for shape_elem in input.shape],
                )()
                for input in inputs
            ]
            try:
                node = safe_make_node(self.op, *inputrs)
            except Exception as exc:
                err_msg = (
                    ""Test %s::%s: Error occurred while""
                    "" making a node with inputs %s""
                ) % (self.op, testname, inputs)
                exc.args += (err_msg,)
                raise
    
            try:
                f = inplace_func(inputrs, node.outputs, mode=mode, name=""test_good"")
            except Exception as exc:
                err_msg = (
                    ""Test %s::%s: Error occurred while"" "" trying to make a Function""
                ) % (self.op, testname)
                exc.args += (err_msg,)
                raise
            if isinstance(self.expected, dict) and testname in self.expected:
                expecteds = self.expected[testname]
                # with numpy version, when we print a number and read it
                # back, we don't get exactly the same result, so we accept
                # rounding error in that case.
                eps = 5e-9
            else:
                expecteds = self.expected(*inputs)
                eps = 1e-10
    
            if any(
                [i.dtype in (""float32"", ""int8"", ""uint8"", ""uint16"") for i in inputs]
            ):
                eps = 1e-6
            eps = np.max([eps, _eps])
    
            try:
                variables = f(*inputs)
            except Exception as exc:
                err_msg = (
                    ""Test %s::%s: Error occurred while calling""
                    "" the Function on the inputs %s""
                ) % (self.op, testname, inputs)
                exc.args += (err_msg,)
                raise
    
            if not isinstance(expecteds, (list, tuple)):
                expecteds = (expecteds,)
    
            for i, (variable, expected) in enumerate(zip(variables, expecteds)):
                condition = (
                    variable.dtype != expected.dtype
                    or variable.shape != expected.shape
                    or not np.allclose(variable, expected, atol=eps, rtol=eps)
                )
>               assert not condition, (
                    ""Test %s::%s: Output %s gave the wrong""
                    "" value. With inputs %s, expected %s (dtype %s),""
                    "" got %s (dtype %s). eps=%f""
                    "" np.allclose returns %s %s""
                ) % (
                    self.op,
                    testname,
                    i,
                    inputs,
                    expected,
                    expected.dtype,
                    variable,
                    variable.dtype,
                    eps,
                    np.allclose(variable, expected, atol=eps, rtol=eps),
                    np.allclose(variable, expected),
                )
E               AssertionError: Test Elemwise{gammau,no_inplace}::normal: Output 0 gave the wrong value. With inputs [array([[0.35236551, 8.53849422, 8.7427552 ],
E                        [0.76950255, 7.8087133 , 8.49094866]]), array([[9.09794555, 4.6555603 , 1.97892114],
E                        [0.65050043, 0.34918022, 2.26342246]])], expected [[2.51432298e-05 1.41710620e+04 2.33308494e+04]
E                  [4.83785761e-01 3.43596345e+03 1.37563075e+04]] (dtype float64), got [[2.51432298e-05 1.41710620e+04 2.33308494e+04]
E                  [4.83785761e-01 3.43596361e+03 1.37563075e+04]] (dtype float64). eps=0.000000 np.allclose returns False True
E               assert not True

_eps       = 2e-08
condition  = True
eps        = 2e-08
expected   = array([[2.51432298e-05, 1.41710620e+04, 2.33308494e+04],
       [4.83785761e-01, 3.43596345e+03, 1.37563075e+04]])
expecteds  = (array([[2.51432298e-05, 1.41710620e+04, 2.33308494e+04],
       [4.83785761e-01, 3.43596345e+03, 1.37563075e+04]]),)
f          = <aesara.compile.function.types.Function object at 0x7f24013395e0>
good       = {'empty': (array([], dtype=float64), array([], dtype=float64)),
 'empty_memmap': [array([], dtype=float64), array([], dtype=float64)],
 'int': (array([[1, 2, 9],
       [1, 5, 2]]),
         array([[10,  2,  9],
       [ 2,  9,  6]])),
 'normal': (array([[0.35236551, 8.53849422, 8.7427552 ],
       [0.76950255, 7.8087133 , 8.49094866]]),
            array([[9.09794555, 4.6555603 , 1.97892114],
       [0.65050043, 0.34918022, 2.26342246]])),
 'uint16': (array([[1, 6, 2],
       [3, 7, 9]], dtype=uint16),
            array([[10,  4,  7],
       [ 2,  8,  3]], dtype=uint16)),
 'uint64': (array([[8, 4, 5],
       [5, 1, 6]], dtype=uint64),
            array([[ 5,  8,  1],
       [ 1,  8, 10]], dtype=uint64)),
 'uint8': (array([[2, 5, 4],
       [2, 4, 5]], dtype=uint8),
           array([[2, 1, 4],
       [3, 3, 6]], dtype=uint8))}
i          = 0
inputrs    = [<TensorType(float64, matrix)>, <TensorType(float64, matrix)>]
inputs     = [array([[0.35236551, 8.53849422, 8.7427552 ],
       [0.76950255, 7.8087133 , 8.49094866]]),
 array([[9.09794555, 4.6555603 , 1.97892114],
       [0.65050043, 0.34918022, 2.26342246]])]
mode       = <aesara.compile.mode.Mode object at 0x7f2408119bb0>
node       = Elemwise{gammau,no_inplace}(<TensorType(float64, matrix)>, <TensorType(float64, matrix)>)
self       = <tests.tensor.utils.GammauTester object at 0x7f2401508b20>
testname   = 'normal'
variable   = array([[2.51432298e-05, 1.41710620e+04, 2.33308494e+04],
       [4.83785761e-01, 3.43596361e+03, 1.37563075e+04]])
variables  = [array([[2.51432298e-05, 1.41710620e+04, 2.33308494e+04],
       [4.83785761e-01, 3.43596361e+03, 1.37563075e+04]])]

tests/tensor/utils.py:542: AssertionError
______________________________________________________ TestGammaUInplaceBroadcast.test_good ______________________________________________________
[gw4] linux -- Python 3.8.10 /usr/bin/python3.8

self = <tests.tensor.utils.Elemwise{gammau_inplace,inplace}Tester object at 0x7f23fff88340>

    @pytest.mark.skipif(skip, reason=""Skipped"")
    def test_good(self):
        good = self.add_memmap_values(self.good)
    
        for testname, inputs in good.items():
            inputs = [copy(input) for input in inputs]
            inputrs = [
                TensorType(
                    dtype=input.dtype,
                    broadcastable=[shape_elem == 1 for shape_elem in input.shape],
                )()
                for input in inputs
            ]
            try:
                node = safe_make_node(self.op, *inputrs)
            except Exception as exc:
                err_msg = (
                    ""Test %s::%s: Error occurred while""
                    "" making a node with inputs %s""
                ) % (self.op, testname, inputs)
                exc.args += (err_msg,)
                raise
    
            try:
                f = inplace_func(inputrs, node.outputs, mode=mode, name=""test_good"")
            except Exception as exc:
                err_msg = (
                    ""Test %s::%s: Error occurred while"" "" trying to make a Function""
                ) % (self.op, testname)
                exc.args += (err_msg,)
                raise
            if isinstance(self.expected, dict) and testname in self.expected:
                expecteds = self.expected[testname]
                # with numpy version, when we print a number and read it
                # back, we don't get exactly the same result, so we accept
                # rounding error in that case.
                eps = 5e-9
            else:
                expecteds = self.expected(*inputs)
                eps = 1e-10
    
            if any(
                [i.dtype in (""float32"", ""int8"", ""uint8"", ""uint16"") for i in inputs]
            ):
                eps = 1e-6
            eps = np.max([eps, _eps])
    
            try:
                variables = f(*inputs)
            except Exception as exc:
                err_msg = (
                    ""Test %s::%s: Error occurred while calling""
                    "" the Function on the inputs %s""
                ) % (self.op, testname, inputs)
                exc.args += (err_msg,)
                raise
    
            if not isinstance(expecteds, (list, tuple)):
                expecteds = (expecteds,)
    
            for i, (variable, expected) in enumerate(zip(variables, expecteds)):
                condition = (
                    variable.dtype != expected.dtype
                    or variable.shape != expected.shape
                    or not np.allclose(variable, expected, atol=eps, rtol=eps)
                )
>               assert not condition, (
                    ""Test %s::%s: Output %s gave the wrong""
                    "" value. With inputs %s, expected %s (dtype %s),""
                    "" got %s (dtype %s). eps=%f""
                    "" np.allclose returns %s %s""
                ) % (
                    self.op,
                    testname,
                    i,
                    inputs,
                    expected,
                    expected.dtype,
                    variable,
                    variable.dtype,
                    eps,
                    np.allclose(variable, expected, atol=eps, rtol=eps),
                    np.allclose(variable, expected),
                )
E               AssertionError: Test Elemwise{gammau_inplace,inplace}::normal: Output 0 gave the wrong value. With inputs [array([[2.51432298e-05, 1.41710620e+04, 2.33308494e+04],
E                        [4.83785761e-01, 3.43596361e+03, 1.37563075e+04]]), array([[9.09794555, 4.6555603 , 1.97892114],
E                        [0.65050043, 0.34918022, 2.26342246]])], expected [[2.51432298e-05 1.41710620e+04 2.33308494e+04]
E                  [4.83785761e-01 3.43596345e+03 1.37563075e+04]] (dtype float64), got [[2.51432298e-05 1.41710620e+04 2.33308494e+04]
E                  [4.83785761e-01 3.43596361e+03 1.37563075e+04]] (dtype float64). eps=0.000000 np.allclose returns False True
E               assert not True

_eps       = 2e-08
condition  = True
eps        = 2e-08
expected   = array([[2.51432298e-05, 1.41710620e+04, 2.33308494e+04],
       [4.83785761e-01, 3.43596345e+03, 1.37563075e+04]])
expecteds  = (array([[2.51432298e-05, 1.41710620e+04, 2.33308494e+04],
       [4.83785761e-01, 3.43596345e+03, 1.37563075e+04]]),)
f          = <aesara.compile.function.types.Function object at 0x7f24001dfe80>
good       = {'empty': (array([], dtype=float64), array([], dtype=float64)),
 'empty_memmap': [array([], dtype=float64), array([], dtype=float64)],
 'int': (array([[1, 2, 9],
       [1, 5, 2]]),
         array([[10,  2,  9],
       [ 2,  9,  6]])),
 'normal': (array([[0.35236551, 8.53849422, 8.7427552 ],
       [0.76950255, 7.8087133 , 8.49094866]]),
            array([[9.09794555, 4.6555603 , 1.97892114],
       [0.65050043, 0.34918022, 2.26342246]])),
 'uint16': (array([[1, 6, 2],
       [3, 7, 9]], dtype=uint16),
            array([[10,  4,  7],
       [ 2,  8,  3]], dtype=uint16)),
 'uint64': (array([[8, 4, 5],
       [5, 1, 6]], dtype=uint64),
            array([[ 5,  8,  1],
       [ 1,  8, 10]], dtype=uint64)),
 'uint8': (array([[2, 5, 4],
       [2, 4, 5]], dtype=uint8),
           array([[2, 1, 4],
       [3, 3, 6]], dtype=uint8))}
i          = 0
inputrs    = [<TensorType(float64, matrix)>, <TensorType(float64, matrix)>]
inputs     = [array([[2.51432298e-05, 1.41710620e+04, 2.33308494e+04],
       [4.83785761e-01, 3.43596361e+03, 1.37563075e+04]]),
 array([[9.09794555, 4.6555603 , 1.97892114],
       [0.65050043, 0.34918022, 2.26342246]])]
mode       = <aesara.compile.mode.Mode object at 0x7f2408119bb0>
node       = Elemwise{gammau_inplace,inplace}(<TensorType(float64, matrix)>, <TensorType(float64, matrix)>)
self       = <tests.tensor.utils.Elemwise{gammau_inplace,inplace}Tester object at 0x7f23fff88340>
testname   = 'normal'
variable   = array([[2.51432298e-05, 1.41710620e+04, 2.33308494e+04],
       [4.83785761e-01, 3.43596361e+03, 1.37563075e+04]])
variables  = [array([[2.51432298e-05, 1.41710620e+04, 2.33308494e+04],
       [4.83785761e-01, 3.43596361e+03, 1.37563075e+04]])]

tests/tensor/utils.py:542: AssertionError
```

**Please provide any additional information below.**
IIUC this seems to be a precision problem but the output doesn't seem to include enough precision to tell it. I'm willing to debug it more if you tell me how ;-).

## Versions and main components

* Aesara version: 2.0.11
* Aesara config (`python -c ""import aesara; print(aesara.config)""`)  [config.txt](https://github.com/aesara-devs/aesara/files/6637787/config.txt)
* Python version: 3.8.10
* Operating system: Gentoo Linux
* How did you install Aesara: package manager",bug help wanted testing,,2021-06-11 10:42:23,2022-08-04 16:50:55,"brandonwillard labeled 2021-06-11 15:33:48,brandonwillard renamed 2021-06-25 20:10:09,brandonwillard unlabeled 2021-08-20 21:49:35,brandonwillard labeled 2021-08-20 21:49:35,brandonwillard labeled 2021-08-20 21:49:35,brandonwillard labeled 2021-08-20 21:49:35,brandonwillard closed 2022-08-04 16:50:55",dgerlanc ricardoV94 brandonwillard mgorny,8
410,478,Jax tutorial on `Extending Aesara` documentation should be under a single header,ricardoV94,"The Jax-tutorial sections span 7 headers in the `Extending Aesara` documentation. They should probably be nested withing a single header, similarly to how the sections inside `Implementing a double in C` are organized.

https://aesara.readthedocs.io/en/latest/extending/index.html

![image](https://user-images.githubusercontent.com/28983449/121852571-f8e5ec80-ccef-11eb-9f58-e7700f662d3d.png)
",documentation good first issue help wanted,,2021-06-14 07:10:22,2021-12-11 16:56:59,"ricardoV94 labeled 2021-06-14 07:10:22,ricardoV94 labeled 2021-06-14 07:10:22,brandonwillard labeled 2021-06-16 01:39:19,brandonwillard mentioned 2021-12-11 08:47:02,brandonwillard subscribed 2021-12-11 08:47:02,brandonwillard closed 2021-12-11 16:56:59,brandonwillard connected 2021-12-11 19:22:27",ricardoV94 brandonwillard,1
412,480,Replace `aet.abs_` with `aet.abs`,rlouf,The [documentation](https://aesara.readthedocs.io/en/latest/library/tensor/basic.html) does specify that `aet.abs` can be used instead of `aet.abs_` but this is actually not the case in 2.0.11.,good first issue NumPy compatibility,,2021-06-16 10:26:25,2021-06-21 04:56:46,"rlouf labeled 2021-06-16 10:26:25,rlouf labeled 2021-06-16 10:26:25,brandonwillard closed 2021-06-19 07:28:47,ricardoV94 reopened 2021-06-21 04:53:29,brandonwillard closed 2021-06-21 04:56:46",ricardoV94 rlouf twiecki brandonwillard,12
416,485,Running huge test suites on Windows causes `[WinError 206] The filename or extension is too long`,michaelosthege,"## Description

When running large test suites such as `test_distributions.py` from PyMC3, the following error can appear:

```
FileNotFoundError: [WinError 206] The filename or extension is too long: 'c:\\\\miniconda\\\\envs\\\\pymc3-dev-py38\\\\library\\\\mingw-w64\\\\bin'
```

It originates from `cmodule.py"", line 294, in dlimport`.

<details><summary>Full traceback</summary>

```
================================== FAILURES ===================================
____________________ test_orderedlogistic_dimensions[(1,)] ____________________

shape = (1,)

    @pytest.mark.parametrize(""shape"", [tuple(), (1,), (3, 1), (3, 2)], ids=str)
    def test_orderedlogistic_dimensions(shape):
        # Test for issue #3535
        loge = np.log10(np.exp(1))
        size = 7
        p = np.ones(shape + (10,)) / 10
        cutpoints = np.tile(logit(np.linspace(0, 1, 11)[1:-1]), shape + (1,))
        obs = np.random.randint(0, 2, size=(size,) + shape)
        with Model():
            ol = OrderedLogistic(
                ""ol"",
                eta=np.zeros(shape),
                cutpoints=cutpoints,
                observed=obs,
            )
            c = Categorical(
                ""c"",
                p=p,
                observed=obs,
            )
>       ologp = logpt_sum(ol, np.ones_like(obs)).eval() * loge

pymc3\\tests\\test_distributions.py:2891: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
C:\\Miniconda\\envs\\pymc3-dev-py38\\lib\\site-packages\\aesara\\graph\\basic.py:550: in eval
    self._fn_cache[inputs] = function(inputs, self)
C:\\Miniconda\\envs\\pymc3-dev-py38\\lib\\site-packages\\aesara\\compile\\function\\__init__.py:337: in function
    fn = pfunc(
C:\\Miniconda\\envs\\pymc3-dev-py38\\lib\\site-packages\\aesara\\compile\\function\\pfunc.py:524: in pfunc
    return orig_function(
C:\\Miniconda\\envs\\pymc3-dev-py38\\lib\\site-packages\\aesara\\compile\\function\\types.py:1983: in orig_function
    fn = m.create(defaults)
C:\\Miniconda\\envs\\pymc3-dev-py38\\lib\\site-packages\\aesara\\compile\\function\\types.py:1838: in create
    _fn, _i, _o = self.linker.make_thunk(
C:\\Miniconda\\envs\\pymc3-dev-py38\\lib\\site-packages\\aesara\\link\\basic.py:282: in make_thunk
    return self.make_all(
C:\\Miniconda\\envs\\pymc3-dev-py38\\lib\\site-packages\\aesara\\link\\vm.py:1133: in make_all
    node.op.make_thunk(node, storage_map, compute_map, [], impl=impl)
C:\\Miniconda\\envs\\pymc3-dev-py38\\lib\\site-packages\\aesara\\graph\\op.py:659: in make_thunk
    return self.make_c_thunk(node, storage_map, compute_map, no_recycling)
C:\\Miniconda\\envs\\pymc3-dev-py38\\lib\\site-packages\\aesara\\graph\\op.py:625: in make_c_thunk
    outputs = cl.make_thunk(
C:\\Miniconda\\envs\\pymc3-dev-py38\\lib\\site-packages\\aesara\\link\\c\\basic.py:1204: in make_thunk
    cthunk, module, in_storage, out_storage, error_storage = self.__compile__(
C:\\Miniconda\\envs\\pymc3-dev-py38\\lib\\site-packages\\aesara\\link\\c\\basic.py:1139: in __compile__
    thunk, module = self.cthunk_factory(
C:\\Miniconda\\envs\\pymc3-dev-py38\\lib\\site-packages\\aesara\\link\\c\\basic.py:1635: in cthunk_factory
    module = get_module_cache().module_from_key(key=key, lnk=self)
C:\\Miniconda\\envs\\pymc3-dev-py38\\lib\\site-packages\\aesara\\link\\c\\cmodule.py:1198: in module_from_key
    module = lnk.compile_cmodule(location)
C:\\Miniconda\\envs\\pymc3-dev-py38\\lib\\site-packages\\aesara\\link\\c\\basic.py:1544: in compile_cmodule
    module = c_compiler.compile_str(
C:\\Miniconda\\envs\\pymc3-dev-py38\\lib\\site-packages\\aesara\\link\\c\\cmodule.py:2564: in compile_str
    return dlimport(lib_filename)
C:\\Miniconda\\envs\\pymc3-dev-py38\\lib\\site-packages\\aesara\\link\\c\\cmodule.py:294: in dlimport
    os.add_dll_directory(os.path.dirname(gcc_path))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

path = 'c:\\\\miniconda\\\\envs\\\\pymc3-dev-py38\\\\library\\\\mingw-w64\\\\bin'

    def add_dll_directory(path):
        """"""Add a path to the DLL search path.
    
        This search path is used when resolving dependencies for imported
        extension modules (the module itself is resolved through sys.path),
        and also by ctypes.
    
        Remove the directory by calling close() on the returned object or
        using it in a with statement.
        """"""
        import nt
>       cookie = nt._add_dll_directory(path)
E       FileNotFoundError: [WinError 206] The filename or extension is too long: 'c:\\\\miniconda\\\\envs\\\\pymc3-dev-py38\\\\library\\\\mingw-w64\\\\bin'

C:\\Miniconda\\envs\\pymc3-dev-py38\\lib\\os.py:1109: FileNotFoundError
---------------------------- Captured stderr call -----------------------------
ERROR (aesara.link.c.cmodule): [WinError 206] The filename or extension is too long: 'c:\\\\miniconda\\\\envs\\\\pymc3-dev-py38\\\\library\\\\mingw-w64\\\\bin'
ERROR (aesara.graph.opt): Optimization failure due to: constant_folding
ERROR (aesara.graph.opt): node: All{1}(TensorConstant{(1, 10) of True})
ERROR (aesara.graph.opt): TRACEBACK:
ERROR (aesara.graph.opt): Traceback (most recent call last):
  File ""C:\\Miniconda\\envs\\pymc3-dev-py38\\lib\\site-packages\\aesara\\graph\\opt.py"", line 2014, in process_node
    replacements = lopt.transform(fgraph, node)
  File ""C:\\Miniconda\\envs\\pymc3-dev-py38\\lib\\site-packages\\aesara\\graph\\opt.py"", line 1187, in transform
    return self.fn(*args, **kwargs)
  File ""C:\\Miniconda\\envs\\pymc3-dev-py38\\lib\\site-packages\\aesara\\tensor\\basic_opt.py"", line 4355, in constant_folding
    thunk = node.op.make_thunk(node, storage_map, compute_map, no_recycling=[])
  File ""C:\\Miniconda\\envs\\pymc3-dev-py38\\lib\\site-packages\\aesara\\graph\\op.py"", line 659, in make_thunk
    return self.make_c_thunk(node, storage_map, compute_map, no_recycling)
  File ""C:\\Miniconda\\envs\\pymc3-dev-py38\\lib\\site-packages\\aesara\\graph\\op.py"", line 625, in make_c_thunk
    outputs = cl.make_thunk(
  File ""C:\\Miniconda\\envs\\pymc3-dev-py38\\lib\\site-packages\\aesara\\link\\c\\basic.py"", line 1204, in make_thunk
    cthunk, module, in_storage, out_storage, error_storage = self.__compile__(
  File ""C:\\Miniconda\\envs\\pymc3-dev-py38\\lib\\site-packages\\aesara\\link\\c\\basic.py"", line 1139, in __compile__
    thunk, module = self.cthunk_factory(
  File ""C:\\Miniconda\\envs\\pymc3-dev-py38\\lib\\site-packages\\aesara\\link\\c\\basic.py"", line 1635, in cthunk_factory
    module = get_module_cache().module_from_key(key=key, lnk=self)
  File ""C:\\Miniconda\\envs\\pymc3-dev-py38\\lib\\site-packages\\aesara\\link\\c\\cmodule.py"", line 1198, in module_from_key
    module = lnk.compile_cmodule(location)
  File ""C:\\Miniconda\\envs\\pymc3-dev-py38\\lib\\site-packages\\aesara\\link\\c\\basic.py"", line 1544, in compile_cmodule
    module = c_compiler.compile_str(
  File ""C:\\Miniconda\\envs\\pymc3-dev-py38\\lib\\site-packages\\aesara\\link\\c\\cmodule.py"", line 2564, in compile_str
    return dlimport(lib_filename)
  File ""C:\\Miniconda\\envs\\pymc3-dev-py38\\lib\\site-packages\\aesara\\link\\c\\cmodule.py"", line 294, in dlimport
    os.add_dll_directory(os.path.dirname(gcc_path))
  File ""C:\\Miniconda\\envs\\pymc3-dev-py38\\lib\\os.py"", line 1109, in add_dll_directory
    cookie = nt._add_dll_directory(path)
FileNotFoundError: [WinError 206] The filename or extension is too long: 'c:\\\\miniconda\\\\envs\\\\pymc3-dev-py38\\\\library\\\\mingw-w64\\\\bin'
```

</details>

The first thing one finds when researching solutions are problems with git bash not supporting long file paths.
But this error appears only when the entire tests file is run, and the path shown in the error message is clearly not too long.

When run individually these tests pass!

My best guess is that the `PATH` has a maximum length that is exceeded by the `add_dll_directory` operation when its executed over and over again in the test suite.

Not sure if there's an easy solution by removing DLL directories again?
Otherwise we can mark this as `won't fix`. I just wanted to make sure it's documented.

## Versions and main components

* Aesara version: 2.0.11
* Python version: 3.8
* Operating system: Windows, Windows in Docker
",wontfix Windows,,2021-06-20 20:34:25,2021-06-21 03:49:57,"brandonwillard mentioned 2021-06-20 20:34:56,brandonwillard subscribed 2021-06-20 20:34:56,michaelosthege referenced 2021-06-20 20:54:42,michaelosthege referenced 2021-06-20 21:57:10,brandonwillard labeled 2021-06-21 03:42:03,brandonwillard labeled 2021-06-21 03:42:03,brandonwillard closed 2021-06-21 03:49:57,twiecki referenced 2021-06-23 12:10:43",michaelosthege twiecki brandonwillard,2
419,491,Print sympy expression from Aesara,gkaissis,"Congratulations on the great project!
Is it possible to obtain a sympy expression for a given computational graph? Obviously not all aesara primitives map to sympy, but it would certainly be interesting to be able to ""parse"" the aesara expression tree to a regular ""formula"" to be able to reason about certain processes analytically, even for a restricted subset of aesara. In a way, I am looking for an ""inverse code generator"" ",,,2021-06-21 19:42:57,2021-06-22 16:11:21,"brandonwillard closed 2021-06-22 16:11:21,aesara-devs locked 2021-06-22 16:11:22",twiecki aesara-devs gkaissis brandonwillard,6
421,494,Test suite is not included in the tarball from PyPI,sergiopasra,"In the tarball from PyPI, the test directory is mostly empty, it doesn't contain any Python file. It would be nice to be able to run some tests on the released code.",question,,2021-06-23 02:53:30,2021-06-25 20:07:55,"brandonwillard labeled 2021-06-25 20:07:39,brandonwillard closed 2021-06-25 20:07:55",sergiopasra brandonwillard,4
429,508,`BroadcastTo`/`np.broadcast_to` zero-dimensional inconsistencies,brandonwillard,"`BroadcastTo` will not work with zero-dimensional arrays, because of its underlying `np.broadcast_to` function.

We run into problems with this due to the fact that it's not always possible to tell exactly what an array/tensor's shape is at the symbolic level, so, while a `BroadcastTo` in a graph is generally valid, it will still fail for some otherwise valid inputs.

As a result, optimizations or rewrites that reasonably introduce `BroadcastTo`s can also introduce inconsistencies.

```python
import numpy as np

import aesara
import aesara.tensor as at


X = at.matrix()
Y = at.broadcast_to(X, (2, 1))

Y_fn = aesara.function([X], Y)

# Valid
Y_fn(np.empty((1, 1)))

X_val = np.empty((1, 0))

# Invalid
Y_fn(X_val)
# ValueError: cannot reshape array of size 0 into shape (2,1)

# This is the equivalent NumPy graph, which fails the same way
np.broadcast_to(X_val, (2, 1))
# ValueError: cannot reshape array of size 0 into shape (2,1)
```

This NumPy behavior doesn't seem particularly consistent. Consider the following alternative&mdash;and less efficient&mdash;form of broadcasting:

```python
(np.ones((2, 1)) * X_val).shape
# (2, 0)
```

This example also demonstrates how an optimization/rewrite could introduce a problem when it converts a `at.ones(shape) * X` into a more efficient `at.broadcast_to(X, shape)`.

In general, `np.broadcast_to` can be imitated with multiplication, yet, in the zero-dimensional case, multiplication does not err, making the functional correspondence with `np.broadcast_to` inconsistent.

This alternative approach that allows broadcasting on zero-dimensional arrays is much more preferable, and it doesn't seem to conflict with the general broadcasting rules. Specifically, a dimension with size one is supposed to broadcast with _any_ other size, so why not include zero?

We can use this more consistent approach in `BroadcastTo` and avoid unnecessary issues and high-level workaround logic.
",bug important graph rewriting NumPy compatibility,,2021-07-06 22:44:47,2021-07-09 03:55:27,"brandonwillard labeled 2021-07-06 22:44:47,brandonwillard labeled 2021-07-06 22:44:47,brandonwillard labeled 2021-07-06 22:44:47,brandonwillard labeled 2021-07-06 22:44:47,brandonwillard closed 2021-07-09 03:55:28",brandonwillard,2
432,511,Create nightly releases,brandonwillard,We could really use nightly release packages.,help wanted CI,,2021-07-07 20:50:25,2021-12-13 21:45:20,"brandonwillard labeled 2021-07-07 20:50:25,brandonwillard labeled 2021-07-07 20:50:25,kc611 mentioned 2021-10-31 20:42:23,kc611 subscribed 2021-10-31 20:42:23,brandonwillard closed 2021-12-13 21:45:20",ricardoV94 kc611 dfm brandonwillard,4
435,518,Replace use of `take` in `_tensor_py_operators.__getitem__` with rewrites,brandonwillard,"`_tensor_py_operators.__getitem__` applies eager optimizations through its use of [`take`](https://github.com/aesara-devs/aesara/blob/main/aesara/tensor/var.py#L577).  Such rewrites should be applied during graph optimization (e.g. the ""specialization"" passes).  

The current approach unnecessarily complicates the canonical form of indexed `TensorVariable`s, because&mdash;for example&mdash;`var[idx]` will not always produce a `*Subtensor*`, so rewrites that apply to `*Subtensor*`s cannot be applied without also accounting for the types of graphs produced by `take`.  

Also, having these rewrites in the optimization process means we can apply them more often.

To put it another way, we need to convert [`aesara.tensor.subtensor.take`](https://github.com/aesara-devs/aesara/blob/95deb922b2a3a965e1d09308b6edde54358a8b67/aesara/tensor/subtensor.py#L2696) into an optimization&mdash;and perhaps a few other non-`*Subtensor*`-producing steps in [`_tensor_py_operators.__getitem__`](https://github.com/aesara-devs/aesara/blob/95deb922b2a3a965e1d09308b6edde54358a8b67/aesara/tensor/var.py#L444).  We need to do this because the graphs produced by `take` can be more efficiently computed than their equivalent `*Subtensor*`-only graphs, so we don't want to lose those optimizations, but we also don't want to complicate our rewrites by forcing them to deal with odd arrangements of `Dimshuffle`s, `Reshape`s, and `*Subtensor*`s instead of their equivalent simple, single `*Subtensor*` graphs.

Here is an illustration:
```python
import numpy as np

import aesara
import aesara.tensor as at


X = at.matrix(""X"")
y = at.lvector(""y"")
z = at.lmatrix(""z"")

aesara.dprint(X[y])
# AdvancedSubtensor1 [id A] ''
#  |X [id B]
#  |y [id C]


aesara.dprint(X[y, :])
# AdvancedSubtensor1 [id A] ''
#  |X [id B]
#  |y [id C]


aesara.dprint(X[z, 0])
# AdvancedSubtensor [id A] ''
#  |X [id B]
#  |z [id C]
#  |TensorFromScalar [id D] ''
#    |ScalarConstant{0} [id E]


# This result isn't a `*Subtensor*`, but it's not a real problem
aesara.dprint(X[..., None])
# InplaceDimShuffle{0,1,x} [id A] ''   
#  |X [id B]


# XXX: Bad graph form.
aesara.dprint(X[:, y])
# InplaceDimShuffle{1,0} [id A] ''
#  |AdvancedSubtensor1 [id B] ''
#    |InplaceDimShuffle{1,0} [id C] ''
#    | |X [id D]
#    |y [id E]


# XXX: Extremely bad graph form!
aesara.dprint(X[z, :])
# Reshape{3} [id A] ''
#  |AdvancedSubtensor1 [id B] ''
#  | |X [id C]
#  | |Reshape{1} [id D] ''
#  |   |z [id E]
#  |   |TensorConstant{(1,) of -1} [id F]
#  |Join [id G] ''
#    |TensorConstant{0} [id H]
#    |Shape [id I] ''
#    | |z [id E]
#    |Subtensor{int64::} [id J] ''
#      |Shape [id K] ''
#      | |X [id C]
#      |ScalarConstant{1} [id L]
```

The latter two should produce simple `AdvancedSubtensor*`s, and the graphs currently produced should be created by rewrites during the optimization passes. ",bug enhancement help wanted important graph rewriting,brandonwillard,2021-07-11 20:26:57,2021-07-17 00:18:51,"brandonwillard labeled 2021-07-11 20:26:57,brandonwillard labeled 2021-07-11 20:26:57,brandonwillard labeled 2021-07-11 20:26:57,brandonwillard labeled 2021-07-11 20:26:57,brandonwillard labeled 2021-07-11 21:34:06,brandonwillard assigned 2021-07-12 00:24:42,brandonwillard closed 2021-07-17 00:18:51",brandonwillard,0
437,522,Add `sum` to `_sparse_py_operators`,aerubanov,"When working on https://github.com/pymc-devs/pymc3/pull/4596, I needed the sum() method for sparse matrices for doing things like these:
```python
import numpy as np, aesara as as

 W = np.array(
            [[0.0, 1.0, 1.0, 0.0], [1.0, 0.0, 0.0, 1.0], [1.0, 0.0, 0.0, 1.0], [0.0, 1.0, 1.0, 0.0]]
        )
W = as.tensor.as_tensor_variable(W)
W = as.sparse.csr_from_dense(W)
D = W.sum(axis=0)
```
and as a result, D should be dense vector ```[2.0, 2.0,  2.0, 2.0]```.
",enhancement good first issue help wanted SciPy compatibility sparse tensors,,2021-07-12 15:16:01,2022-01-13 06:26:09,"twiecki labeled 2021-07-12 15:23:08,twiecki labeled 2021-07-12 15:23:08,aerubanov mentioned 2021-07-12 15:23:40,aerubanov subscribed 2021-07-12 15:23:40,aerubanov closed 2021-07-12 16:45:33,brandonwillard mentioned 2021-07-12 16:45:33,brandonwillard subscribed 2021-07-12 16:45:33,brandonwillard reopened 2021-07-12 16:46:44,brandonwillard renamed 2021-07-12 16:47:26,brandonwillard labeled 2021-07-12 16:47:56,brandonwillard labeled 2021-07-12 16:48:14,aerubanov closed 2022-01-13 06:26:09,brandonwillard connected 2022-01-13 06:30:22,brandonwillard labeled 2022-01-13 06:30:40",aerubanov twiecki brandonwillard,5
440,527,Make frequently used types available at the `aesara.*` level,michaelosthege,"Here's a list of submodules/types/functions that currently need their own import lines, but are so frequently used that I think we should aggregate them at higher package/submodule levels:

```python
from aesara.graph.basic import Constant, Variable
from aesara.tensor.var import TensorVariable
from aesara.compile.sharedvalue import SharedVariable
from aesara.tensor.sharedvar import ScalarSharedVariable
from aesara.graph.op import Op

import aesara.tensor as at
import aesara.tensor.random.basic as atr
```

While the above is not something I'd like to show others when demoing Aesara, this looks much less frightening:

```python
from aesara import Constant, Variable, TensorVariable, SharedVariable, ScalarSharedVariable, Op
import aesara.tensor as at
import aesara.tensor.random as atr
```

or simply this and rolling with `aesara.TensorVariable` and so on:

```python
import aesara
import aesara.tensor as at
import aesara.tensor.random as atr
```

Grouping the most important types at the package level also doesn't require in-depth knowledge of the codebase just to import things for an `isinstance` check.

Any concerns, or more things to add to the list?

@ricardoV94 @brandonwillard can someone add labels, please? I seem to have absolutely no permissions on this project any more :(",question wontfix,,2021-07-17 12:51:08,2021-08-24 00:10:36,"brandonwillard mentioned 2021-07-17 12:51:09,brandonwillard subscribed 2021-07-17 12:51:09,ricardoV94 mentioned 2021-07-17 12:51:09,ricardoV94 subscribed 2021-07-17 12:51:09,ricardoV94 labeled 2021-07-17 17:45:12,brandonwillard unlabeled 2021-08-23 23:14:53,brandonwillard labeled 2021-08-23 23:14:53,brandonwillard labeled 2021-08-23 23:14:53,aesara-devs locked 2021-08-24 00:10:35,brandonwillard closed 2021-08-24 00:10:36",ricardoV94 michaelosthege aesara-devs brandonwillard,1
441,528,Expose `standard_normal` in `RandomStream`.,zoj613,"The docs state that `RandomStream` is meant to function like `np.random.RandomState` but currently, we cannot use `standard_normal` via `RandomStream`.

```python
In [1]: from aesara.tensor.random.utils import RandomStream
   ...: 
   ...: rng = RandomStream()

In [2]: rng.standard_normal()
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)

```

**Please provide the full traceback of any errors.**
```
<ipython-input-2-400cf391cbda> in <module>
----> 1 rng.standard_normal()

~/.pyenv/versions/miniconda3-4.7.12/envs/pymc3/lib/python3.9/site-packages/aesara/tensor/random/utils.py in __getattr__(self, obj)
    170 
    171         if ns_obj is None:
--> 172             raise AttributeError(""No attribute {}."".format(obj))
    173 
    174         from aesara.tensor.random.op import RandomVariable

AttributeError: No attribute standard_normal.
```

",enhancement good first issue help wanted NumPy compatibility,,2021-07-19 16:12:59,2021-07-22 05:52:26,"brandonwillard labeled 2021-07-19 16:28:00,brandonwillard labeled 2021-07-19 16:28:00,brandonwillard labeled 2021-07-19 16:28:00,brandonwillard labeled 2021-07-19 16:28:00,brandonwillard mentioned 2021-07-19 23:03:04,brandonwillard subscribed 2021-07-19 23:03:04,twiecki closed 2021-07-22 05:52:26",ricardoV94 zoj613 twiecki brandonwillard,3
442,529,Fix Numba performance issues involving `CAReduce` `Op`s,brandonwillard,"The performance discussion in #404 demonstrates a disparity between our current C backend and Numba, with the latter not scaling as well as the former for `CAReduce` `Op`s (e.g. `max` with `axis` arguments given).

From [this discussion](https://numba.discourse.group/t/numba-performance-doesnt-scale-as-well-as-numpy-in-vectorized-max-function/782), it looks like we could generate our own `for`-loop(s) instead of using `vectorized`, if we want to get closer to C/NumPy performance with the Numba backend.  

At minimum, we need to do this for the conversion of `CAReduce` `Op`s, but it might help overall if we completely replaced [our use of `vectorized`](https://github.com/aesara-devs/aesara/blob/5248aa9ae9ab8b002b45cb78215c5772b3f25831/aesara/link/numba/dispatch.py#L450).",help wanted important Numba performance concern,kc611,2021-07-19 16:19:54,2022-01-20 17:54:57,"brandonwillard labeled 2021-07-19 16:19:54,brandonwillard labeled 2021-07-19 16:19:54,brandonwillard labeled 2021-07-19 16:19:54,gmarkall mentioned 2021-08-17 23:04:34,gmarkall subscribed 2021-08-17 23:04:34,fanshi118 mentioned 2021-09-16 18:52:28,fanshi118 subscribed 2021-09-16 18:52:28,brandonwillard connected 2021-09-24 19:30:55,brandonwillard renamed 2021-11-08 23:26:37,brandonwillard renamed 2021-11-08 23:26:50,brandonwillard milestoned 2021-11-08 23:32:20,kc611 assigned 2021-11-08 23:32:39,brandonwillard connected 2021-11-08 23:33:31,brandonwillard renamed 2021-11-08 23:37:01,brandonwillard closed 2022-01-20 17:54:57,brandonwillard labeled 2022-07-31 19:56:59",gmarkall kc611 brandonwillard fanshi118 twiecki,7
446,535,Extra `RandomVariable` in `Scan`,brandonwillard,"@kc611 noticed that there are extra `RandomVariable`s appearing in `Scan`s that use `RandomStream`:
```python
import numpy as np

import aesara
import aesara.tensor as at


srng = at.random.RandomStream(123)


def scan_fn():
    return srng.normal()


out, updates = aesara.scan(scan_fn, n_steps=10)


aesara.dprint(out)
# for{cpu,scan_fn}.0 [id A] ''
#  |TensorConstant{10} [id B]
#  |RandomGeneratorSharedVariable(<Generator(PCG64) at 0x7F4256D499B0>) [id C]
#  |TensorConstant{10} [id B]
#
# Inner graphs of the scan ops:
#
# for{cpu,scan_fn}.0 [id A] ''
#  >normal_rv{0, (0, 0), floatX, False}.1 [id D] ''
#  > |<RandomGeneratorType> [id E] -> [id C]
#  > |TensorConstant{[]} [id F]
#  > |TensorConstant{11} [id G]
#  > |TensorConstant{0.0} [id H]
#  > |TensorConstant{1.0} [id I]
#  >normal_rv{0, (0, 0), floatX, False}.0 [id J] ''
#  > |<RandomGeneratorType> [id E] -> [id C]
#  > |TensorConstant{[]} [id F]
#  > |TensorConstant{11} [id G]
#  > |TensorConstant{0.0} [id H]
#  > |TensorConstant{1.0} [id I]
```

It looks like this issue was present as far back as Theano-PyMC at the very least.",question Scan,,2021-07-27 16:37:55,2022-01-21 17:30:37,"brandonwillard labeled 2021-07-27 16:37:55,brandonwillard labeled 2021-07-27 16:37:55,kc611 mentioned 2021-07-27 16:38:13,kc611 subscribed 2021-07-27 16:38:13,kc611 connected 2021-07-30 17:51:17,brandonwillard labeled 2021-09-12 22:34:32,brandonwillard closed 2022-01-21 17:30:37,brandonwillard unlabeled 2022-01-21 17:30:57,brandonwillard labeled 2022-01-21 17:30:57,brandonwillard unlabeled 2022-01-21 17:31:02",kc611 brandonwillard,2
447,536,`RandomVariable` constant broadcastable is incorrect,brandonwillard,"The broadcastable property isn't being computed correctly by `RandomVariable` in certain cases:

```python
import numpy as np

import aesara
import aesara.tensor as at


# This works
assert at.random.normal(0, 1, size=1).broadcastable == (True,)

size = at.as_tensor(1)

# This fails
assert at.random.normal(0, 1, size=size).broadcastable == (True,)
```",bug important,brandonwillard,2021-07-29 01:02:28,2021-07-29 14:31:33,"brandonwillard labeled 2021-07-29 01:02:28,brandonwillard labeled 2021-07-29 01:02:28,brandonwillard assigned 2021-07-29 05:06:31,brandonwillard closed 2021-07-29 14:31:33",ricardoV94 brandonwillard,2
452,542,How should/do updates work in nested `Scan`s?,brandonwillard,"The `update` attribute investigated in #540 looks similar to the `default_update` feature, but the former seems `Scan`-specific.

In AeHMC, we had to set `default_update` on the shared variables of the updates in the inner-`Scan` of a nested `Scan` graph (see [here](https://github.com/aesara-devs/aehmc/blob/1311d4d5911f7420096899f78a0ccea4164a029f/aehmc/trajectory.py#L376)).  Without this, the code would raise missing input errors during compilation.  I don't believe we were able to reproduce the issue without recreating the exact graph constructed by the test in AeHMC, but it looks broadly related to the code we're investigating here.

_Originally posted by @brandonwillard in https://github.com/aesara-devs/aesara/issues/540#issuecomment-890262427_",help wanted question MWE needed Scan,,2021-07-31 00:20:32,2022-08-04 16:38:48,"brandonwillard mentioned 2021-07-31 00:20:32,brandonwillard subscribed 2021-07-31 00:20:32,brandonwillard renamed 2021-07-31 00:21:04,brandonwillard renamed 2021-07-31 00:21:11,brandonwillard labeled 2021-07-31 00:21:35,brandonwillard labeled 2021-07-31 00:21:35,brandonwillard labeled 2021-07-31 00:21:35,brandonwillard labeled 2021-09-12 22:34:33,brandonwillard closed 2022-08-04 16:38:48",brandonwillard,2
453,544,Enable in-place updates for `Advanced[Inc]Subtensor`,brandonwillard,"The `Advanced[Inc]Subtensor` `Op`s are very fundamental `Op`s, but they appear to have no support for in-placing.  Because of this, they're too slow to use for decently sized problems, so we need to update them (e.g. add correct `[view|destroy]_map`s) and create/update some optimization passes to set `inplace=True` on these `Op`s.

Without these changes `x[idx] = y` will perform an unnecessary `x.copy()` each time the `AdvancedIncSubtensor` `Op` for that statement is used.",enhancement help wanted important graph rewriting,,2021-08-02 21:25:27,2021-08-02 23:56:34,"brandonwillard labeled 2021-08-02 21:25:27,brandonwillard labeled 2021-08-02 21:25:27,brandonwillard labeled 2021-08-02 21:25:27,brandonwillard labeled 2021-08-02 21:25:27,brandonwillard pinned 2021-08-02 21:26:17,brandonwillard closed 2021-08-02 23:07:37,brandonwillard reopened 2021-08-02 23:20:59,brandonwillard closed 2021-08-02 23:56:34,brandonwillard unpinned 2021-08-03 19:47:44",brandonwillard,1
454,545,Move `*Subtensor*` rewrites from `basic_opt` into `subtensor_opt`,brandonwillard,The `aesara.tensor.subtensor_opt` module was created recently and we should move all the `*Subtensor*` rewrites that are still in `aesara.tensor.basic_opt` into that module.,good first issue help wanted graph rewriting refactor,,2021-08-02 23:58:27,2021-09-21 15:52:38,"brandonwillard labeled 2021-08-02 23:58:27,brandonwillard labeled 2021-08-02 23:58:27,brandonwillard labeled 2021-08-02 23:58:27,brandonwillard labeled 2021-08-02 23:58:27,brandonwillard labeled 2021-08-02 23:59:20,ricardoV94 unlabeled 2021-08-29 07:44:40,brandonwillard closed 2021-09-21 15:52:38",ricardoV94 brandonwillard,0
455,546,"Create canonicalization for `sub(x, neg(y))`",ricardoV94,"Seems like we are missing a canonicalization for `sub(x, neg(y)) -> add(x, y)` :

```python
import aesara
import aesara.tensor as at

x = at.scalar(""x"")
y = at.scalar(""y"")
z = x - (-y)

f = aesara.function([x, y], z)
aesara.dprint(f)
```
```
Elemwise{Composite{(i0 - (-i1))}} [id A] ''   0
 |x [id B]
 |y [id C]
```

It does work with constants
```python
z = 5 - (-x)

f = aesara.function([x], z)
aesara.dprint(f)
```
```
Elemwise{add,no_inplace} [id A] ''   0
 |TensorConstant{5.0} [id B]
 |x [id C]
```

And we have one for double negation:
```python
z = - (-x)

f = aesara.function([x], z)
aesara.dprint(f)
``` 
```
DeepCopyOp [id A] 'x'   0
 |x [id B]
```
_Originally posted by @ricardoV94 in https://github.com/aesara-devs/aesara/pull/473#r681526956_",enhancement graph rewriting,ricardoV94,2021-08-03 08:00:42,2022-10-18 20:12:11,"ricardoV94 mentioned 2021-08-03 08:00:43,ricardoV94 subscribed 2021-08-03 08:00:43,ricardoV94 renamed 2021-08-03 08:02:27,brandonwillard labeled 2021-08-03 19:43:38,brandonwillard labeled 2021-08-03 19:43:38,brandonwillard labeled 2021-08-03 19:43:38,ricardoV94 unlabeled 2021-08-06 08:26:26,ricardoV94 assigned 2021-08-06 08:26:29,rlouf closed 2022-10-18 20:12:12",rlouf ricardoV94 brandonwillard,1
461,554,Make `TensorConstant` a subclass of `TensorVariable`,brandonwillard,"Just as `aesara.graph.basic.Constant` is a subclass of `aesara.graph.basic.Variable`, `aesara.tensor.var.TensorConstant` should probably be a subclass of `aesara.tensor.var.TensorVariable` (or both should be subclasses of a more high-level tensor `Variable` type).

This would make it possible to use `isinstance(x, TensorVariable)` when `x` is either a `TensorVariable` or a `TensorConstant`.  Right now, one needs to include both in an `isinstance` expression.",enhancement good first issue help wanted important refactor graph objects,,2021-08-09 17:28:42,2021-08-10 01:12:16,"brandonwillard labeled 2021-08-09 17:28:43,brandonwillard labeled 2021-08-09 17:28:43,brandonwillard labeled 2021-08-09 17:28:43,brandonwillard labeled 2021-08-09 17:28:43,brandonwillard labeled 2021-08-09 17:28:43,brandonwillard closed 2021-08-10 01:12:16,brandonwillard labeled 2022-01-07 19:15:28",brandonwillard,0
464,557,Clean up `tracks` functionality,brandonwillard,"The `tracks` option is a setting available to `LocalOptimizer`s that limits their scope of applicability to certain `Op` classes and instances.  Currently, this feature is only used in select places (e.g. `LocalMetaOptimizer` and `EquilibriumOptimizer`) and with limited capabilities (see #51).

We need to clarify the scope of this feature by either generalizing it or restricting its scope by&mdash;for example&mdash;limiting its availability to only the (global) optimizers that use it.  

Either approach would make much clearer what it does and when it's relevant.  For instance, we could reduce the amount of redundant `isinstance` checks that appear in rewrites that use `local_optimizer`.


I'm partial to the first approach (i.e. generalize and improve the `tracks` functionality).  This could be accomplished by improving the inheritance checking for composite `Op` situations (e.g. `Elemwise` `Op`s that need to be filtered based on their scalar `Op`s) and by providing an automatic `isinstance` check derived from `tracks` for the resulting `LocalOptimizer` instance.  The latter would prevent the need for redundant `tracks` + manual `isinstance` checks and it could be done in a way that allows meta/`GlobalOptimizer`s to skip the automatic check and perform their own `track`s-based filtering/applications.",enhancement help wanted graph rewriting refactor,,2021-08-09 18:19:31,2021-12-06 21:06:23,"brandonwillard labeled 2021-08-09 18:19:31,brandonwillard labeled 2021-08-09 18:19:31,brandonwillard labeled 2021-08-09 18:19:31,brandonwillard labeled 2021-08-09 18:19:31,brandonwillard mentioned 2021-08-18 09:24:03,brandonwillard subscribed 2021-08-18 09:24:03,jeffreyenos mentioned 2021-08-18 09:41:28,jeffreyenos subscribed 2021-08-18 09:41:28,brandonwillard mentioned 2021-08-18 09:41:28,brandonwillard subscribed 2021-08-18 09:41:28,brandonwillard connected 2021-12-04 05:45:08,brandonwillard closed 2021-12-06 21:06:23",ricardoV94 jeffreyenos brandonwillard,5
465,558,Remove redundant erf(c) rewrites,ricardoV94,"There are many equivalent rewrites in `math_opt` that track how the same graph would look at different steps of `canonicalization` and `specialization`. 

It seems we can remove them from `canonicalization` so that a single `Patternsub` per rewrite will suffice",,,2021-08-10 05:01:37,2021-08-18 17:42:10,ricardoV94 closed 2021-08-18 17:42:10,ricardoV94,0
466,559,`RandomStateSharedVariable.__str__` does not use `name`,ricardoV94,"```python
import numpy as np
from aesara import shared

generator = shared(np.random.default_rng(), name='generator')
state = shared(np.random.RandomState(), name='state')

print(generator) 
print(state)
```
```
>>> generator
>>> RandomStateSharedVariable(<RandomState(MT19937) at 0x7FA145677240>)
```

```python
print(generator.name)
print(state.name)
```
```
>>> generator                                                                                                
>>> state
```",bug good first issue help wanted,,2021-08-10 16:10:17,2022-08-04 16:35:50,"brandonwillard labeled 2021-10-25 16:36:31,brandonwillard labeled 2021-10-25 16:36:31,brandonwillard labeled 2021-10-25 16:36:31,brandonwillard labeled 2021-10-25 16:37:03,brandonwillard unlabeled 2021-10-25 16:37:07,brandonwillard renamed 2022-08-04 16:30:09,brandonwillard closed 2022-08-04 16:35:50",ricardoV94 brandonwillard,1
470,564,Missing windows compile-chain requirements in Conda recipe: `m2w64-toolchain`,twiecki,"I'm surprised that `libblas`, `lapack` are not part of the dependencies.

Moreover, pymc3 listed `m2w64` as a dependency for compilation on windows, this might also need to be included here.

In an attempt to have clearer division, I'm proposing to remove these dependencies from PyMC3 https://github.com/pymc-devs/pymc3/pull/4932 and add them here. 

Related: https://github.com/conda-forge/aesara-feedstock/pull/27",C-backend backend compatibility Windows Conda,,2021-08-13 18:01:25,2022-08-04 16:53:36,"brandonwillard labeled 2021-08-23 23:11:02,brandonwillard labeled 2021-08-23 23:11:02,brandonwillard labeled 2021-08-23 23:11:02,brandonwillard closed 2021-08-23 23:11:25,twiecki reopened 2021-08-24 06:47:24,twiecki renamed 2021-08-24 06:47:48,brandonwillard renamed 2021-08-24 17:21:55,brandonwillard renamed 2021-08-24 17:22:02,brandonwillard labeled 2021-08-27 23:23:06,brandonwillard closed 2022-08-04 16:53:36",twiecki brandonwillard,5
474,568,`pm.sample_prior_predictive()` not working with Aesara v.2.1.3,larryshamalama,"`pm.sample_prior_predictive` doesn't seem to work when I upgrade Aesara to the latest version (2.1.3), but it works if I use Aesara v.2.0.12. Any ideas why this may be the case? See the example below and notebooks [here](https://github.com/larryshamalama/pymc3-playground/blob/master/notebooks/shortcomings/sample_prior_aesara_working.ipynb) and [here](https://github.com/larryshamalama/pymc3-playground/blob/master/notebooks/shortcomings/sample_prior_aesara_not_working.ipynb)

```{python}
with pm.Model() as model:
    norm = pm.Normal(""normal-dist"", mu=0., sigma=5.)
    
    prior = pm.sample_prior_predictive()
```

yields

```{python}
TypeError: ('Constants not allowed in param list', TensorConstant{[]})
```",,,2021-08-22 16:36:40,2021-08-22 17:44:37,larryshamalama closed 2021-08-22 17:44:37,larryshamalama ricardoV94,2
477,573,Remove `conda` directory and `meta.yaml`,dgerlanc,"The `conda/meta.yaml` file is duplicated by and out of sync with the `aesara-feedstock` version so we should delete the version in this repo and only have the `aesara-feedstock` [version](https://github.com/conda-forge/aesara-feedstock/blob/master/recipe/meta.yaml).

I checked the `numpy` and `pandas` repos and they don't have `conda/meta.yaml` files or even a `conda` directory, probably because this information is provided in the feedstock:

- https://github.com/numpy/numpy
- https://github.com/pandas-dev/pandas
",enhancement Conda,,2021-08-27 22:55:57,2021-08-30 17:38:35,"brandonwillard labeled 2021-08-27 23:22:17,brandonwillard labeled 2021-08-27 23:22:17,brandonwillard closed 2021-08-30 17:38:36",dgerlanc brandonwillard,0
480,576,Create a development `environment.yml`,dgerlanc,"[`numpy`](https://github.com/numpy/numpy/blob/main/environment.yml) and [`pandas`](https://github.com/pandas-dev/pandas/blob/master/environment.yml) both provide `conda` `environment.yml` files to set up a development environment for the respective projects. We should provide a similar `environment.yml` for `aesara`.
",enhancement good first issue Conda,,2021-08-27 23:19:39,2021-10-25 16:35:19,"dgerlanc labeled 2021-08-27 23:19:39,dgerlanc labeled 2021-08-27 23:19:39,brandonwillard labeled 2021-08-27 23:23:06,brandonwillard connected 2021-10-25 16:35:11,brandonwillard closed 2021-10-25 16:35:19",dgerlanc brandonwillard,0
483,579,`aesara.scan` produces cryptic error message when `updates` isn't given,brandonwillard,"An example:
```python
import aesara
import aesara.tensor as at


srng = at.random.RandomStream(seed=2398)

mus_tt = at.matrix(""mus"")
sigmas_tt = at.vector(""sigmas"")

Gamma_tt = at.matrix(""Gamma"")

S_0_tt = at.lscalar(""S_0"")


def scan_fn(mus_t, sigma_t, S_tm1, Gamma):
    S_t = srng.categorical(Gamma[S_tm1], name=""S_t"")
    Y_t = srng.normal(mus_t[S_t], sigma_t, name=""Y_t"")
    return S_t, Y_t


(S_rv, Y_rv), updates = aesara.scan(
    fn=scan_fn,
    sequences=[mus_tt, sigmas_tt],
    non_sequences=[Gamma_tt],
    outputs_info=[{""initial"": S_0_tt, ""taps"": [-1]}, {}],
    strict=True,
    name=""scan_rv"",
)
Y_rv.name = ""Y_rv""



S_Y_sampler_fn = aesara.function([Gamma_tt, S_0_tt, mus_tt, sigmas_tt], [S_rv, Y_rv])
# MissingInputError: Input 0 (S_0[t-1]) of the graph (indices start from 0), used to compute ScalarFromTensor(S_0[t-1]), was not provided and not given a value. Use the Aesara flag exception_verbosity='high', for more information on this error.

S_Y_sampler_fn = aesara.function([Gamma_tt, S_0_tt, mus_tt, sigmas_tt], [S_rv, Y_rv], updates=updates)
# No problem

```

There is some sort of cryptic dependency on the updates that isn't readily apparent.  Also, the variable name `""S_0[t-1]""` isn't correct, since the `""[t-1]""` suffix implies that it's somehow iterating over the scalar `S_0_tt`.
",bug help wanted important Scan,brandonwillard,2021-09-03 01:27:46,2022-04-18 22:27:04,"brandonwillard labeled 2021-09-03 01:27:46,brandonwillard labeled 2021-09-03 01:27:46,brandonwillard labeled 2021-09-03 01:27:46,rlouf mentioned 2021-09-03 01:30:31,rlouf subscribed 2021-09-03 01:30:31,brandonwillard labeled 2021-09-12 22:34:33,brandonwillard assigned 2022-04-09 21:28:53,brandonwillard closed 2022-04-18 22:27:04",rlouf ricardoV94 brandonwillard,7
488,588,Jax backend slowdown in jax==0.2.20,ricardoV94,"This snippet runs painfully slowly when using jax `0.2.20`, instead of `0.2.11`

```python
with pm.Model() as m:
    x = pm.Normal('x', initval=0)
    trace = pm.sampling_jax.sample_numpyro_nuts(chains=1)
```

There is a warning mentioning that omnistaging can no longer be disabled in the current version. Maybe related?",JAX,,2021-09-16 22:04:57,2021-10-21 13:14:25,"ricardoV94 labeled 2021-09-17 08:28:23,aseyboldt mentioned 2021-09-22 18:44:28,aseyboldt subscribed 2021-09-22 18:44:28,ricardoV94 closed 2021-10-21 13:14:25",ricardoV94 aseyboldt,1
490,590,Handling of `RandomVariable` parameters and `size` inconsistencies,brandonwillard,"Consider the following:
```python
import aesara
import aesara.tensor as at


at.vector().broadcastable
# (False,)

at.random.normal(size=1).broadcastable
# (True,)

(at.vector() + at.random.normal(size=1)).broadcastable
# (False,)

(at.random.normal(loc=at.vector(), size=1)).broadcastable
# (True,)
```

The last example has contradictory parameters, because the mean/`loc` parameter isn't broadcastable while the `size` parameter implies that it is.

We could rebroadcast the inputs when `size` specifies broadcastable dimensions that do not explicitly match the other parameters' `broadcastable`s, or we could attempt to raise an error/warning when such graphs are constructed.  The former sounds significantly better, though.

In general, any changes will likely involve the condition [here](https://github.com/aesara-devs/aesara/blob/5335e72932dde19e47b8539705e53dca762cb008/aesara/tensor/random/op.py#L185).

If we want to add more advanced symbolic error/consistency checking,  we might need add the `size` parameter in the rewrites/simplifications [here](https://github.com/aesara-devs/aesara/blob/5335e72932dde19e47b8539705e53dca762cb008/aesara/tensor/random/op.py#L285)&mdash;e.g. so that we could compare the parameters broadcast patterns and the ones specified by `size`.

_Originally posted by @brandonwillard in https://github.com/aesara-devs/aeppl/pull/26#issuecomment-921372945_",enhancement help wanted important,,2021-09-17 01:20:50,2022-08-04 16:42:52,"brandonwillard mentioned 2021-09-17 01:20:50,brandonwillard subscribed 2021-09-17 01:20:50,brandonwillard labeled 2021-09-17 01:21:07,brandonwillard labeled 2021-09-17 01:21:07,brandonwillard labeled 2021-09-17 01:35:35,brandonwillard renamed 2021-09-17 01:37:10,brandonwillard unlabeled 2021-09-17 01:37:14,brandonwillard labeled 2021-09-17 01:39:25,brandonwillard unlabeled 2021-09-17 01:39:33,brandonwillard labeled 2021-09-17 01:39:39,brandonwillard closed 2022-08-04 16:42:52",brandonwillard,1
493,593,Rewrite fails with float32 flag,ferrine,"## Description of your problem or feature request

Theano optimization fails in rewrite with the following flags

strict_float32=True + cast_policy='custom'

**Please provide a minimal, self-contained, and reproducible example.**
```python
import numpy as np
import aesara
import aesara.tensor as at
with aesara.config.change_flags(warn_float64='raise', floatX='float32'):
    a = at.constant(1000, dtype=""int16"")
    b = aesara.shared(np.asarray(3, dtype=""int64""))
    c = a / b
    f = aesara.function([], c)
```

**Please provide the full traceback of any errors.**
```python
ERROR (aesara.graph.opt): Optimization failure due to: local_upcast_elemwise_constant_inputs
ERROR (aesara.graph.opt): node: Elemwise{true_div,no_inplace}(TensorConstant{1000}, <TensorType(int64, scalar)>)
ERROR (aesara.graph.opt): TRACEBACK:
ERROR (aesara.graph.opt): Traceback (most recent call last):
  File ""/home/ferres/.pyenv/versions/aesara/lib/python3.8/site-packages/aesara/graph/opt.py"", line 2025, in process_node
    replacements = lopt.transform(fgraph, node)
  File ""/home/ferres/.pyenv/versions/aesara/lib/python3.8/site-packages/aesara/graph/opt.py"", line 1187, in transform
    return self.fn(*args, **kwargs)
  File ""/home/ferres/.pyenv/versions/aesara/lib/python3.8/site-packages/aesara/tensor/basic_opt.py"", line 2456, in local_upcast_elemwise_constant_inputs
    rval = [node.op(*new_inputs)]
  File ""/home/ferres/.pyenv/versions/aesara/lib/python3.8/site-packages/aesara/graph/op.py"", line 271, in __call__
    node = self.make_node(*inputs, **kwargs)
  File ""/home/ferres/.pyenv/versions/aesara/lib/python3.8/site-packages/aesara/tensor/elemwise.py"", line 489, in make_node
    outputs = [
  File ""/home/ferres/.pyenv/versions/aesara/lib/python3.8/site-packages/aesara/tensor/elemwise.py"", line 490, in <listcomp>
    TensorType(dtype=dtype, broadcastable=broadcastable)()
  File ""/home/ferres/.pyenv/versions/aesara/lib/python3.8/site-packages/aesara/graph/type.py"", line 196, in __call__
    return utils.add_tag_trace(self.make_variable(name))
  File ""/home/ferres/.pyenv/versions/aesara/lib/python3.8/site-packages/aesara/tensor/type.py"", line 356, in make_variable
    return self.Variable(self, name=name)
  File ""/home/ferres/.pyenv/versions/aesara/lib/python3.8/site-packages/aesara/tensor/var.py"", line 845, in __init__
    raise Exception(msg)
Exception: You are creating a TensorVariable with float64 dtype. You requested an action via the Aesara flag warn_float64={ignore,warn,raise,pdb}.

ERROR (aesara.graph.opt): Optimization failure due to: local_mul_canonizer
ERROR (aesara.graph.opt): node: Elemwise{true_div,no_inplace}(TensorConstant{1000}, <TensorType(int64, scalar)>)
ERROR (aesara.graph.opt): TRACEBACK:
ERROR (aesara.graph.opt): Traceback (most recent call last):
  File ""/home/ferres/.pyenv/versions/aesara/lib/python3.8/site-packages/aesara/graph/opt.py"", line 2025, in process_node
    replacements = lopt.transform(fgraph, node)
  File ""/home/ferres/.pyenv/versions/aesara/lib/python3.8/site-packages/aesara/tensor/math_opt.py"", line 997, in transform
    new = self.merge_num_denum(num, denum)
  File ""/home/ferres/.pyenv/versions/aesara/lib/python3.8/site-packages/aesara/tensor/math_opt.py"", line 773, in merge_num_denum
    return self.inverse(
  File ""/home/ferres/.pyenv/versions/aesara/lib/python3.8/site-packages/aesara/graph/op.py"", line 271, in __call__
    node = self.make_node(*inputs, **kwargs)
  File ""/home/ferres/.pyenv/versions/aesara/lib/python3.8/site-packages/aesara/tensor/elemwise.py"", line 489, in make_node
    outputs = [
  File ""/home/ferres/.pyenv/versions/aesara/lib/python3.8/site-packages/aesara/tensor/elemwise.py"", line 490, in <listcomp>
    TensorType(dtype=dtype, broadcastable=broadcastable)()
  File ""/home/ferres/.pyenv/versions/aesara/lib/python3.8/site-packages/aesara/graph/type.py"", line 196, in __call__
    return utils.add_tag_trace(self.make_variable(name))
  File ""/home/ferres/.pyenv/versions/aesara/lib/python3.8/site-packages/aesara/tensor/type.py"", line 356, in make_variable
    return self.Variable(self, name=name)
  File ""/home/ferres/.pyenv/versions/aesara/lib/python3.8/site-packages/aesara/tensor/var.py"", line 845, in __init__
    raise Exception(msg)
Exception: You are creating a TensorVariable with float64 dtype. You requested an action via the Aesara flag warn_float64={ignore,warn,raise,pdb}.
```

Should fail gracefully and not spawn Exceptions/Warnings

## Versions and main components

* Aesara version: master
* Aesara config (`python -c ""import aesara; print(aesara.config)""`)
* Python version:
* Operating system:
* How did you install Aesara: (conda/pip)
",,,2021-09-19 11:42:16,2021-10-25 17:38:43,"brandonwillard labeled 2021-09-19 19:43:31,brandonwillard unlabeled 2021-10-25 17:37:13,brandonwillard closed 2021-10-25 17:38:43",ferrine brandonwillard,3
497,597,Port tests to PyTest completely,ferrine,"There are a lot of tests that are mixed between unittests, pytest. Unifying them should benefit maintenance costs

The roadmap is to create many small PRs per file to be on the same page with main branch",testing refactor request discussion,,2021-09-26 17:47:03,2021-10-01 15:16:15,"brandonwillard labeled 2021-09-27 00:05:37,brandonwillard labeled 2021-09-30 21:13:43,brandonwillard labeled 2021-09-30 21:13:51,ferrine renamed 2021-10-01 15:15:11,ferrine renamed 2021-10-01 15:15:49,ferrine closed 2021-10-01 15:16:15",ferrine brandonwillard,3
500,600,`at.stack` fails with `NUMBA` backend,ricardoV94,"```python
import aesara
import aesara.tensor as at

x = at.scalar('x')
out = at.stack([x, 1.0])

fn_numba = aesara.function([x], out, mode=""NUMBA"")
fn_numba(5.0)
```

<details>

```python
---------------------------------------------------------------------------
TypingError                               Traceback (most recent call last)
~/miniconda3/envs/openlabs/lib/python3.9/site-packages/aesara/link/utils.py in streamline_default_f()
    187                 ):
--> 188                     thunk()
    189                     for old_s in old_storage:

~/miniconda3/envs/openlabs/lib/python3.9/site-packages/aesara/link/basic.py in thunk(fgraph, fgraph_jit, thunk_inputs, thunk_outputs)
    704         ):
--> 705             outputs = fgraph_jit(*[x[0] for x in thunk_inputs])
    706 

~/miniconda3/envs/openlabs/lib/python3.9/site-packages/numba/core/dispatcher.py in _compile_for_args(self, *args, **kws)
    481 
--> 482             error_rewrite(e, 'typing')
    483         except errors.UnsupportedError as e:

~/miniconda3/envs/openlabs/lib/python3.9/site-packages/numba/core/dispatcher.py in error_rewrite(e, issue_type)
    422             else:
--> 423                 raise e.with_traceback(None)
    424 

TypingError: Failed in nopython mode pipeline (step: nopython frontend)
Failed in nopython mode pipeline (step: nopython frontend)
Invalid use of getiter with parameters (StarArgTuple(array(float64, 0d, C), readonly array(float64, 0d, C)))

During: typing of intrinsic-call at /home/ricardo/miniconda3/envs/openlabs/lib/python3.9/site-packages/aesara/link/numba/dispatch/tensor_basic.py (175)

File ""miniconda3/envs/openlabs/lib/python3.9/site-packages/aesara/link/numba/dispatch/tensor_basic.py"", line 175:
    def makevector(*args):
        return np.array([a.item() for a in args], dtype=dtype)
        ^

During: resolving callee type: type(CPUDispatcher(<function numba_funcify_MakeVector.<locals>.makevector at 0x7fbf59d72040>))
During: typing of call at /tmp/tmp6g8qdul3 (3)

During: resolving callee type: type(CPUDispatcher(<function numba_funcify_MakeVector.<locals>.makevector at 0x7fbf59d72040>))
During: typing of call at /tmp/tmp6g8qdul3 (3)


File ""../../tmp/tmp6g8qdul3"", line 3:
def numba_funcified_fgraph(x):
    auto_1148 = makevector(x, auto_1159)
    ^


During handling of the above exception, another exception occurred:

TypingError                               Traceback (most recent call last)
/tmp/ipykernel_2951879/2625038068.py in <module>
----> 1 fn_numba(5.0)

~/miniconda3/envs/openlabs/lib/python3.9/site-packages/aesara/compile/function/types.py in __call__(self, *args, **kwargs)
    974         try:
    975             outputs = (
--> 976                 self.fn()
    977                 if output_subset is None
    978                 else self.fn(output_subset=output_subset)

~/miniconda3/envs/openlabs/lib/python3.9/site-packages/aesara/link/utils.py in streamline_default_f()
    190                         old_s[0] = None
    191             except Exception:
--> 192                 raise_with_op(fgraph, node, thunk)
    193 
    194         f = streamline_default_f

~/miniconda3/envs/openlabs/lib/python3.9/site-packages/aesara/link/utils.py in raise_with_op(fgraph, node, thunk, exc_info, storage_map)
    524         # Some exception need extra parameter in inputs. So forget the
    525         # extra long error message in that case.
--> 526     raise exc_value.with_traceback(exc_trace)
    527 
    528 

~/miniconda3/envs/openlabs/lib/python3.9/site-packages/aesara/link/utils.py in streamline_default_f()
    186                     thunks, order, post_thunk_old_storage
    187                 ):
--> 188                     thunk()
    189                     for old_s in old_storage:
    190                         old_s[0] = None

~/miniconda3/envs/openlabs/lib/python3.9/site-packages/aesara/link/basic.py in thunk(fgraph, fgraph_jit, thunk_inputs, thunk_outputs)
    703             thunk_outputs=thunk_outputs,
    704         ):
--> 705             outputs = fgraph_jit(*[x[0] for x in thunk_inputs])
    706 
    707             for o_node, o_storage, o_val in zip(fgraph.outputs, thunk_outputs, outputs):

~/miniconda3/envs/openlabs/lib/python3.9/site-packages/numba/core/dispatcher.py in _compile_for_args(self, *args, **kws)
    480                 e.patch_message(msg)
    481 
--> 482             error_rewrite(e, 'typing')
    483         except errors.UnsupportedError as e:
    484             # Something unsupported is present in the user code, add help info

~/miniconda3/envs/openlabs/lib/python3.9/site-packages/numba/core/dispatcher.py in error_rewrite(e, issue_type)
    421                 raise e
    422             else:
--> 423                 raise e.with_traceback(None)
    424 
    425         argtypes = []

TypingError: Failed in nopython mode pipeline (step: nopython frontend)
Failed in nopython mode pipeline (step: nopython frontend)
Invalid use of getiter with parameters (StarArgTuple(array(float64, 0d, C), readonly array(float64, 0d, C)))

During: typing of intrinsic-call at /home/ricardo/miniconda3/envs/openlabs/lib/python3.9/site-packages/aesara/link/numba/dispatch/tensor_basic.py (175)

File ""miniconda3/envs/openlabs/lib/python3.9/site-packages/aesara/link/numba/dispatch/tensor_basic.py"", line 175:
    def makevector(*args):
        return np.array([a.item() for a in args], dtype=dtype)
        ^

During: resolving callee type: type(CPUDispatcher(<function numba_funcify_MakeVector.<locals>.makevector at 0x7fbf59d72040>))
During: typing of call at /tmp/tmp6g8qdul3 (3)

During: resolving callee type: type(CPUDispatcher(<function numba_funcify_MakeVector.<locals>.makevector at 0x7fbf59d72040>))
During: typing of call at /tmp/tmp6g8qdul3 (3)


File ""../../tmp/tmp6g8qdul3"", line 3:
def numba_funcified_fgraph(x):
    auto_1148 = makevector(x, auto_1159)
    ^

Apply node that caused the error: MakeVector{dtype='float64'}(x, TensorConstant{1.0})
Toposort index: 0
Inputs types: [TensorType(float64, scalar), TensorType(float64, scalar)]
Inputs shapes: [()]
Inputs strides: [()]
Inputs values: [array(5.)]
Outputs clients: [['output']]

Backtrace when the node is created (use Aesara flag traceback__limit=N to make it longer):
  File ""/home/ricardo/miniconda3/envs/openlabs/lib/python3.9/site-packages/ipykernel/zmqshell.py"", line 533, in run_cell
    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)
  File ""/home/ricardo/miniconda3/envs/openlabs/lib/python3.9/site-packages/IPython/core/interactiveshell.py"", line 2901, in run_cell
    result = self._run_cell(
  File ""/home/ricardo/miniconda3/envs/openlabs/lib/python3.9/site-packages/IPython/core/interactiveshell.py"", line 2947, in _run_cell
    return runner(coro)
  File ""/home/ricardo/miniconda3/envs/openlabs/lib/python3.9/site-packages/IPython/core/async_helpers.py"", line 68, in _pseudo_sync_runner
    coro.send(None)
  File ""/home/ricardo/miniconda3/envs/openlabs/lib/python3.9/site-packages/IPython/core/interactiveshell.py"", line 3172, in run_cell_async
    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,
  File ""/home/ricardo/miniconda3/envs/openlabs/lib/python3.9/site-packages/IPython/core/interactiveshell.py"", line 3364, in run_ast_nodes
    if (await self.run_code(code, result,  async_=asy)):
  File ""/home/ricardo/miniconda3/envs/openlabs/lib/python3.9/site-packages/IPython/core/interactiveshell.py"", line 3444, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File ""/tmp/ipykernel_2951879/3844035012.py"", line 2, in <module>
    out = at.stack(x, 1.0)

HINT: Use the Aesara flag `exception_verbosity=high` for a debug print-out and storage map footprint of this Apply node.
```
</details>

This also fails:
```python
out = at.stack(x, at.exp(x))
```

But this works:
```python
out = at.stack(x, x) 
```",bug important Numba,,2021-09-29 15:40:46,2021-10-10 18:11:46,"ricardoV94 labeled 2021-09-29 15:40:46,ricardoV94 labeled 2021-09-29 15:40:58,ricardoV94 renamed 2021-09-29 15:47:46,brandonwillard labeled 2021-09-30 06:50:36,brandonwillard closed 2021-10-10 18:11:46",ricardoV94 brandonwillard,0
501,608,Fix `PushOutNonSeqScan` + `RandomVariable` shape inference error in `Scan`s,rlouf,"## Reproduce the warning

The following code produces the warning on my machine:

```python
import aesara
from aesara import tensor as at
from aesara.tensor.random.utils import RandomStream

from aeppl import joint_logprob

from aehmc import hmc
from aehmc.metrics import gaussian_metric


# A simple normal distribution
Y_rv = at.random.normal(0, 1, size=2, name=""Y"")

def logprob_fn(y):
  logprob = joint_logprob({Y_rv: y})
  return logprob


# Build the transition kernel
srng = RandomStream(seed=0)
kernel = hmc.kernel(
    srng,
    logprob_fn,
    step_size=1e-4,
    inverse_mass_matrix=at.ones(2),
    num_integration_steps=10,
)

# Compile a function that updates the chain
y_vv = Y_rv.clone()
y_vv.name = ""y_vv""

pe = -logprob_fn(y_vv)
pe.name=""potential energy""

peg = aesara.grad(pe, y_vv)
peg.name=""potential energy grad""

trajectory, updates = aesara.scan(
    fn=kernel,
    outputs_info=[
        {""initial"": y_vv},
        {""initial"": pe},
        {""initial"": peg},
    ],
    n_steps=100
)

generator = aesara.function((y_vv,), trajectory, updates=updates)
```
as well as if I set `Y_rv = at.random.normal(0,1)` and `inverse_mass_matrix = at.as_tensor(1.)` instead. Note that before your commit the scalar version produced a warning while the above snippet did not.

## Error

If I set `aesara.config.on_shape_error='raise'` I get the following stack trace:

<details>
  <summary>Stack trace</summary>

```
ERROR (aesara.graph.opt): SeqOptimizer apply <aesara.tensor.basic_opt.ShapeOptimizer object at 0x7f050c770550>
ERROR (aesara.graph.opt): Traceback:
ERROR (aesara.graph.opt): Traceback (most recent call last):
  File ""/home/remi/.virtualenvs/aehmc/lib/python3.9/site-packages/aesara/tensor/basic_opt.py"", line 894, in get_node_infer_shape
    o_shapes = shape_infer(
  File ""/home/remi/.virtualenvs/aehmc/lib/python3.9/site-packages/aesara/tensor/random/op.py"", line 299, in infer_shape
    shape = self._infer_shape(size, dist_params, param_shapes=param_shapes)
  File ""/home/remi/.virtualenvs/aehmc/lib/python3.9/site-packages/aesara/tensor/random/op.py"", line 183, in _infer_shape
    size_len = get_vector_length(size)
  File ""/home/remi/.virtualenvs/aehmc/lib/python3.9/site-packages/aesara/tensor/basic.py"", line 2837, in get_vector_length
    raise ValueError(f""Length of {v} cannot be determined"")
ValueError: Length of <TensorType(int64, vector)> cannot be determined

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/home/remi/.virtualenvs/aehmc/lib/python3.9/site-packages/aesara/tensor/basic_opt.py"", line 894, in get_node_infer_shape
    o_shapes = shape_infer(
  File ""/home/remi/.virtualenvs/aehmc/lib/python3.9/site-packages/aesara/scan/op.py"", line 1833, in infer_shape
    outs_shape = infer_shape(
  File ""/home/remi/.virtualenvs/aehmc/lib/python3.9/site-packages/aesara/compile/builders.py"", line 74, in infer_shape
    local_traverse(o)
  File ""/home/remi/.virtualenvs/aehmc/lib/python3.9/site-packages/aesara/compile/builders.py"", line 65, in local_traverse
    local_traverse(inp)
  File ""/home/remi/.virtualenvs/aehmc/lib/python3.9/site-packages/aesara/compile/builders.py"", line 65, in local_traverse
    local_traverse(inp)
  File ""/home/remi/.virtualenvs/aehmc/lib/python3.9/site-packages/aesara/compile/builders.py"", line 65, in local_traverse
    local_traverse(inp)
  [Previous line repeated 9 more times]
  File ""/home/remi/.virtualenvs/aehmc/lib/python3.9/site-packages/aesara/compile/builders.py"", line 70, in local_traverse
    shape_feature.on_import(dummy_fgraph, out.owner, reason=""dummy"")
  File ""/home/remi/.virtualenvs/aehmc/lib/python3.9/site-packages/aesara/tensor/basic_opt.py"", line 1304, in on_import
    o_shapes = self.get_node_infer_shape(node)
  File ""/home/remi/.virtualenvs/aehmc/lib/python3.9/site-packages/aesara/tensor/basic_opt.py"", line 916, in get_node_infer_shape
    raise Exception(msg).with_traceback(e.__traceback__)
  File ""/home/remi/.virtualenvs/aehmc/lib/python3.9/site-packages/aesara/tensor/basic_opt.py"", line 894, in get_node_infer_shape
    o_shapes = shape_infer(
  File ""/home/remi/.virtualenvs/aehmc/lib/python3.9/site-packages/aesara/tensor/random/op.py"", line 299, in infer_shape
    shape = self._infer_shape(size, dist_params, param_shapes=param_shapes)
  File ""/home/remi/.virtualenvs/aehmc/lib/python3.9/site-packages/aesara/tensor/random/op.py"", line 183, in _infer_shape
    size_len = get_vector_length(size)
  File ""/home/remi/.virtualenvs/aehmc/lib/python3.9/site-packages/aesara/tensor/basic.py"", line 2837, in get_vector_length
    raise ValueError(f""Length of {v} cannot be determined"")
Exception: Failed to infer_shape from Op normal_rv{0, (0, 0), floatX, False}.
Input shapes: [None, (TensorConstant{0},), (), (), ()]
Exception encountered during infer_shape: <class 'ValueError'>
Exception message: Length of <TensorType(int64, vector)> cannot be determined
Traceback: Traceback (most recent call last):
  File ""/home/remi/.virtualenvs/aehmc/lib/python3.9/site-packages/aesara/tensor/basic_opt.py"", line 894, in get_node_infer_shape
    o_shapes = shape_infer(
  File ""/home/remi/.virtualenvs/aehmc/lib/python3.9/site-packages/aesara/tensor/random/op.py"", line 299, in infer_shape
    shape = self._infer_shape(size, dist_params, param_shapes=param_shapes)
  File ""/home/remi/.virtualenvs/aehmc/lib/python3.9/site-packages/aesara/tensor/random/op.py"", line 183, in _infer_shape
    size_len = get_vector_length(size)
  File ""/home/remi/.virtualenvs/aehmc/lib/python3.9/site-packages/aesara/tensor/basic.py"", line 2837, in get_vector_length
    raise ValueError(f""Length of {v} cannot be determined"")
ValueError: Length of <TensorType(int64, vector)> cannot be determined


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/home/remi/.virtualenvs/aehmc/lib/python3.9/site-packages/aesara/graph/opt.py"", line 232, in apply
    sub_prof = optimizer.optimize(fgraph)
  File ""/home/remi/.virtualenvs/aehmc/lib/python3.9/site-packages/aesara/graph/opt.py"", line 83, in optimize
    self.add_requirements(fgraph)
  File ""/home/remi/.virtualenvs/aehmc/lib/python3.9/site-packages/aesara/tensor/basic_opt.py"", line 1474, in add_requirements
    fgraph.attach_feature(ShapeFeature())
  File ""/home/remi/.virtualenvs/aehmc/lib/python3.9/site-packages/aesara/graph/fg.py"", line 569, in attach_feature
    attach(self)
  File ""/home/remi/.virtualenvs/aehmc/lib/python3.9/site-packages/aesara/tensor/basic_opt.py"", line 1284, in on_attach
    self.on_import(fgraph, node, reason=""on_attach"")
  File ""/home/remi/.virtualenvs/aehmc/lib/python3.9/site-packages/aesara/tensor/basic_opt.py"", line 1304, in on_import
    o_shapes = self.get_node_infer_shape(node)
  File ""/home/remi/.virtualenvs/aehmc/lib/python3.9/site-packages/aesara/tensor/basic_opt.py"", line 916, in get_node_infer_shape
    raise Exception(msg).with_traceback(e.__traceback__)
  File ""/home/remi/.virtualenvs/aehmc/lib/python3.9/site-packages/aesara/tensor/basic_opt.py"", line 894, in get_node_infer_shape
    o_shapes = shape_infer(
  File ""/home/remi/.virtualenvs/aehmc/lib/python3.9/site-packages/aesara/scan/op.py"", line 1833, in infer_shape
    outs_shape = infer_shape(
  File ""/home/remi/.virtualenvs/aehmc/lib/python3.9/site-packages/aesara/compile/builders.py"", line 74, in infer_shape
    local_traverse(o)
  File ""/home/remi/.virtualenvs/aehmc/lib/python3.9/site-packages/aesara/compile/builders.py"", line 65, in local_traverse
    local_traverse(inp)
  File ""/home/remi/.virtualenvs/aehmc/lib/python3.9/site-packages/aesara/compile/builders.py"", line 65, in local_traverse
    local_traverse(inp)
  File ""/home/remi/.virtualenvs/aehmc/lib/python3.9/site-packages/aesara/compile/builders.py"", line 65, in local_traverse
    local_traverse(inp)
  [Previous line repeated 9 more times]
  File ""/home/remi/.virtualenvs/aehmc/lib/python3.9/site-packages/aesara/compile/builders.py"", line 70, in local_traverse
    shape_feature.on_import(dummy_fgraph, out.owner, reason=""dummy"")
  File ""/home/remi/.virtualenvs/aehmc/lib/python3.9/site-packages/aesara/tensor/basic_opt.py"", line 1304, in on_import
    o_shapes = self.get_node_infer_shape(node)
  File ""/home/remi/.virtualenvs/aehmc/lib/python3.9/site-packages/aesara/tensor/basic_opt.py"", line 916, in get_node_infer_shape
    raise Exception(msg).with_traceback(e.__traceback__)
  File ""/home/remi/.virtualenvs/aehmc/lib/python3.9/site-packages/aesara/tensor/basic_opt.py"", line 894, in get_node_infer_shape
    o_shapes = shape_infer(
  File ""/home/remi/.virtualenvs/aehmc/lib/python3.9/site-packages/aesara/tensor/random/op.py"", line 299, in infer_shape
    shape = self._infer_shape(size, dist_params, param_shapes=param_shapes)
  File ""/home/remi/.virtualenvs/aehmc/lib/python3.9/site-packages/aesara/tensor/random/op.py"", line 183, in _infer_shape
    size_len = get_vector_length(size)
  File ""/home/remi/.virtualenvs/aehmc/lib/python3.9/site-packages/aesara/tensor/basic.py"", line 2837, in get_vector_length
    raise ValueError(f""Length of {v} cannot be determined"")
Exception: Failed to infer_shape from Op for{cpu,scan_fn}.
Input shapes: [(), (Elemwise{add,no_inplace}.0,), (Elemwise{add,no_inplace}.0,), (Elemwise{add,no_inplace}.0,), None, None, (TensorConstant{0},), ()]
Exception encountered during infer_shape: <class 'Exception'>
Exception message: Failed to infer_shape from Op normal_rv{0, (0, 0), floatX, False}.
Input shapes: [None, (TensorConstant{0},), (), (), ()]
Exception encountered during infer_shape: <class 'ValueError'>
Exception message: Length of <TensorType(int64, vector)> cannot be determined
Traceback: Traceback (most recent call last):
  File ""/home/remi/.virtualenvs/aehmc/lib/python3.9/site-packages/aesara/tensor/basic_opt.py"", line 894, in get_node_infer_shape
    o_shapes = shape_infer(
  File ""/home/remi/.virtualenvs/aehmc/lib/python3.9/site-packages/aesara/tensor/random/op.py"", line 299, in infer_shape
    shape = self._infer_shape(size, dist_params, param_shapes=param_shapes)
  File ""/home/remi/.virtualenvs/aehmc/lib/python3.9/site-packages/aesara/tensor/random/op.py"", line 183, in _infer_shape
    size_len = get_vector_length(size)
  File ""/home/remi/.virtualenvs/aehmc/lib/python3.9/site-packages/aesara/tensor/basic.py"", line 2837, in get_vector_length
    raise ValueError(f""Length of {v} cannot be determined"")
ValueError: Length of <TensorType(int64, vector)> cannot be determined

Traceback: Traceback (most recent call last):
  File ""/home/remi/.virtualenvs/aehmc/lib/python3.9/site-packages/aesara/tensor/basic_opt.py"", line 894, in get_node_infer_shape
    o_shapes = shape_infer(
  File ""/home/remi/.virtualenvs/aehmc/lib/python3.9/site-packages/aesara/tensor/random/op.py"", line 299, in infer_shape
    shape = self._infer_shape(size, dist_params, param_shapes=param_shapes)
  File ""/home/remi/.virtualenvs/aehmc/lib/python3.9/site-packages/aesara/tensor/random/op.py"", line 183, in _infer_shape
    size_len = get_vector_length(size)
  File ""/home/remi/.virtualenvs/aehmc/lib/python3.9/site-packages/aesara/tensor/basic.py"", line 2837, in get_vector_length
    raise ValueError(f""Length of {v} cannot be determined"")
ValueError: Length of <TensorType(int64, vector)> cannot be determined

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/home/remi/.virtualenvs/aehmc/lib/python3.9/site-packages/aesara/tensor/basic_opt.py"", line 894, in get_node_infer_shape
    o_shapes = shape_infer(
  File ""/home/remi/.virtualenvs/aehmc/lib/python3.9/site-packages/aesara/scan/op.py"", line 1833, in infer_shape
    outs_shape = infer_shape(
  File ""/home/remi/.virtualenvs/aehmc/lib/python3.9/site-packages/aesara/compile/builders.py"", line 74, in infer_shape
    local_traverse(o)
  File ""/home/remi/.virtualenvs/aehmc/lib/python3.9/site-packages/aesara/compile/builders.py"", line 65, in local_traverse
    local_traverse(inp)
  File ""/home/remi/.virtualenvs/aehmc/lib/python3.9/site-packages/aesara/compile/builders.py"", line 65, in local_traverse
    local_traverse(inp)
  File ""/home/remi/.virtualenvs/aehmc/lib/python3.9/site-packages/aesara/compile/builders.py"", line 65, in local_traverse
    local_traverse(inp)
  [Previous line repeated 9 more times]
  File ""/home/remi/.virtualenvs/aehmc/lib/python3.9/site-packages/aesara/compile/builders.py"", line 70, in local_traverse
    shape_feature.on_import(dummy_fgraph, out.owner, reason=""dummy"")
  File ""/home/remi/.virtualenvs/aehmc/lib/python3.9/site-packages/aesara/tensor/basic_opt.py"", line 1304, in on_import
    o_shapes = self.get_node_infer_shape(node)
  File ""/home/remi/.virtualenvs/aehmc/lib/python3.9/site-packages/aesara/tensor/basic_opt.py"", line 916, in get_node_infer_shape
    raise Exception(msg).with_traceback(e.__traceback__)
  File ""/home/remi/.virtualenvs/aehmc/lib/python3.9/site-packages/aesara/tensor/basic_opt.py"", line 894, in get_node_infer_shape
    o_shapes = shape_infer(
  File ""/home/remi/.virtualenvs/aehmc/lib/python3.9/site-packages/aesara/tensor/random/op.py"", line 299, in infer_shape
    shape = self._infer_shape(size, dist_params, param_shapes=param_shapes)
  File ""/home/remi/.virtualenvs/aehmc/lib/python3.9/site-packages/aesara/tensor/random/op.py"", line 183, in _infer_shape
    size_len = get_vector_length(size)
  File ""/home/remi/.virtualenvs/aehmc/lib/python3.9/site-packages/aesara/tensor/basic.py"", line 2837, in get_vector_length
    raise ValueError(f""Length of {v} cannot be determined"")
Exception: Failed to infer_shape from Op normal_rv{0, (0, 0), floatX, False}.
Input shapes: [None, (TensorConstant{0},), (), (), ()]
Exception encountered during infer_shape: <class 'ValueError'>
Exception message: Length of <TensorType(int64, vector)> cannot be determined
Traceback: Traceback (most recent call last):
  File ""/home/remi/.virtualenvs/aehmc/lib/python3.9/site-packages/aesara/tensor/basic_opt.py"", line 894, in get_node_infer_shape
    o_shapes = shape_infer(
  File ""/home/remi/.virtualenvs/aehmc/lib/python3.9/site-packages/aesara/tensor/random/op.py"", line 299, in infer_shape
    shape = self._infer_shape(size, dist_params, param_shapes=param_shapes)
  File ""/home/remi/.virtualenvs/aehmc/lib/python3.9/site-packages/aesara/tensor/random/op.py"", line 183, in _infer_shape
    size_len = get_vector_length(size)
  File ""/home/remi/.virtualenvs/aehmc/lib/python3.9/site-packages/aesara/tensor/basic.py"", line 2837, in get_vector_length
    raise ValueError(f""Length of {v} cannot be determined"")
ValueError: Length of <TensorType(int64, vector)> cannot be determined



Traceback (most recent call last):
  File ""/home/remi/.virtualenvs/aehmc/lib/python3.9/site-packages/aesara/tensor/basic_opt.py"", line 894, in get_node_infer_shape
    o_shapes = shape_infer(
  File ""/home/remi/.virtualenvs/aehmc/lib/python3.9/site-packages/aesara/tensor/random/op.py"", line 299, in infer_shape
    shape = self._infer_shape(size, dist_params, param_shapes=param_shapes)
  File ""/home/remi/.virtualenvs/aehmc/lib/python3.9/site-packages/aesara/tensor/random/op.py"", line 183, in _infer_shape
    size_len = get_vector_length(size)
  File ""/home/remi/.virtualenvs/aehmc/lib/python3.9/site-packages/aesara/tensor/basic.py"", line 2837, in get_vector_length
    raise ValueError(f""Length of {v} cannot be determined"")
ValueError: Length of <TensorType(int64, vector)> cannot be determined

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/home/remi/.virtualenvs/aehmc/lib/python3.9/site-packages/aesara/tensor/basic_opt.py"", line 894, in get_node_infer_shape
    o_shapes = shape_infer(
  File ""/home/remi/.virtualenvs/aehmc/lib/python3.9/site-packages/aesara/scan/op.py"", line 1833, in infer_shape
    outs_shape = infer_shape(
  File ""/home/remi/.virtualenvs/aehmc/lib/python3.9/site-packages/aesara/compile/builders.py"", line 74, in infer_shape
    local_traverse(o)
  File ""/home/remi/.virtualenvs/aehmc/lib/python3.9/site-packages/aesara/compile/builders.py"", line 65, in local_traverse
    local_traverse(inp)
  File ""/home/remi/.virtualenvs/aehmc/lib/python3.9/site-packages/aesara/compile/builders.py"", line 65, in local_traverse
    local_traverse(inp)
  File ""/home/remi/.virtualenvs/aehmc/lib/python3.9/site-packages/aesara/compile/builders.py"", line 65, in local_traverse
    local_traverse(inp)
  [Previous line repeated 9 more times]
  File ""/home/remi/.virtualenvs/aehmc/lib/python3.9/site-packages/aesara/compile/builders.py"", line 70, in local_traverse
    shape_feature.on_import(dummy_fgraph, out.owner, reason=""dummy"")
  File ""/home/remi/.virtualenvs/aehmc/lib/python3.9/site-packages/aesara/tensor/basic_opt.py"", line 1304, in on_import
    o_shapes = self.get_node_infer_shape(node)
  File ""/home/remi/.virtualenvs/aehmc/lib/python3.9/site-packages/aesara/tensor/basic_opt.py"", line 916, in get_node_infer_shape
    raise Exception(msg).with_traceback(e.__traceback__)
  File ""/home/remi/.virtualenvs/aehmc/lib/python3.9/site-packages/aesara/tensor/basic_opt.py"", line 894, in get_node_infer_shape
    o_shapes = shape_infer(
  File ""/home/remi/.virtualenvs/aehmc/lib/python3.9/site-packages/aesara/tensor/random/op.py"", line 299, in infer_shape
    shape = self._infer_shape(size, dist_params, param_shapes=param_shapes)
  File ""/home/remi/.virtualenvs/aehmc/lib/python3.9/site-packages/aesara/tensor/random/op.py"", line 183, in _infer_shape
    size_len = get_vector_length(size)
  File ""/home/remi/.virtualenvs/aehmc/lib/python3.9/site-packages/aesara/tensor/basic.py"", line 2837, in get_vector_length
    raise ValueError(f""Length of {v} cannot be determined"")
Exception: Failed to infer_shape from Op normal_rv{0, (0, 0), floatX, False}.
Input shapes: [None, (TensorConstant{0},), (), (), ()]
Exception encountered during infer_shape: <class 'ValueError'>
Exception message: Length of <TensorType(int64, vector)> cannot be determined
Traceback: Traceback (most recent call last):
  File ""/home/remi/.virtualenvs/aehmc/lib/python3.9/site-packages/aesara/tensor/basic_opt.py"", line 894, in get_node_infer_shape
    o_shapes = shape_infer(
  File ""/home/remi/.virtualenvs/aehmc/lib/python3.9/site-packages/aesara/tensor/random/op.py"", line 299, in infer_shape
    shape = self._infer_shape(size, dist_params, param_shapes=param_shapes)
  File ""/home/remi/.virtualenvs/aehmc/lib/python3.9/site-packages/aesara/tensor/random/op.py"", line 183, in _infer_shape
    size_len = get_vector_length(size)
  File ""/home/remi/.virtualenvs/aehmc/lib/python3.9/site-packages/aesara/tensor/basic.py"", line 2837, in get_vector_length
    raise ValueError(f""Length of {v} cannot be determined"")
ValueError: Length of <TensorType(int64, vector)> cannot be determined


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/home/remi/projects/ampersand/aehmc/example.py"", line 58, in <module>
    generator = aesara.function((y_vv,), trajectory, updates=updates)
  File ""/home/remi/.virtualenvs/aehmc/lib/python3.9/site-packages/aesara/compile/function/__init__.py"", line 337, in function
    fn = pfunc(
  File ""/home/remi/.virtualenvs/aehmc/lib/python3.9/site-packages/aesara/compile/function/pfunc.py"", line 524, in pfunc
    return orig_function(
  File ""/home/remi/.virtualenvs/aehmc/lib/python3.9/site-packages/aesara/compile/function/types.py"", line 1972, in orig_function
    m = Maker(
  File ""/home/remi/.virtualenvs/aehmc/lib/python3.9/site-packages/aesara/compile/function/types.py"", line 1617, in __init__
    optimizer_profile = optimizer(fgraph)
  File ""/home/remi/.virtualenvs/aehmc/lib/python3.9/site-packages/aesara/graph/opt.py"", line 93, in __call__
    return self.optimize(fgraph)
  File ""/home/remi/.virtualenvs/aehmc/lib/python3.9/site-packages/aesara/graph/opt.py"", line 84, in optimize
    ret = self.apply(fgraph, *args, **kwargs)
  File ""/home/remi/.virtualenvs/aehmc/lib/python3.9/site-packages/aesara/graph/opt.py"", line 243, in apply
    self.failure_callback(e, self, optimizer)
  File ""/home/remi/.virtualenvs/aehmc/lib/python3.9/site-packages/aesara/graph/opt.py"", line 178, in warn
    raise exc
  File ""/home/remi/.virtualenvs/aehmc/lib/python3.9/site-packages/aesara/graph/opt.py"", line 232, in apply
    sub_prof = optimizer.optimize(fgraph)
  File ""/home/remi/.virtualenvs/aehmc/lib/python3.9/site-packages/aesara/graph/opt.py"", line 83, in optimize
    self.add_requirements(fgraph)
  File ""/home/remi/.virtualenvs/aehmc/lib/python3.9/site-packages/aesara/tensor/basic_opt.py"", line 1474, in add_requirements
    fgraph.attach_feature(ShapeFeature())
  File ""/home/remi/.virtualenvs/aehmc/lib/python3.9/site-packages/aesara/graph/fg.py"", line 569, in attach_feature
    attach(self)
  File ""/home/remi/.virtualenvs/aehmc/lib/python3.9/site-packages/aesara/tensor/basic_opt.py"", line 1284, in on_attach
    self.on_import(fgraph, node, reason=""on_attach"")
  File ""/home/remi/.virtualenvs/aehmc/lib/python3.9/site-packages/aesara/tensor/basic_opt.py"", line 1304, in on_import
    o_shapes = self.get_node_infer_shape(node)
  File ""/home/remi/.virtualenvs/aehmc/lib/python3.9/site-packages/aesara/tensor/basic_opt.py"", line 916, in get_node_infer_shape
    raise Exception(msg).with_traceback(e.__traceback__)
  File ""/home/remi/.virtualenvs/aehmc/lib/python3.9/site-packages/aesara/tensor/basic_opt.py"", line 894, in get_node_infer_shape
    o_shapes = shape_infer(
  File ""/home/remi/.virtualenvs/aehmc/lib/python3.9/site-packages/aesara/scan/op.py"", line 1833, in infer_shape
    outs_shape = infer_shape(
  File ""/home/remi/.virtualenvs/aehmc/lib/python3.9/site-packages/aesara/compile/builders.py"", line 74, in infer_shape
    local_traverse(o)
  File ""/home/remi/.virtualenvs/aehmc/lib/python3.9/site-packages/aesara/compile/builders.py"", line 65, in local_traverse
    local_traverse(inp)
  File ""/home/remi/.virtualenvs/aehmc/lib/python3.9/site-packages/aesara/compile/builders.py"", line 65, in local_traverse
    local_traverse(inp)
  File ""/home/remi/.virtualenvs/aehmc/lib/python3.9/site-packages/aesara/compile/builders.py"", line 65, in local_traverse
    local_traverse(inp)
  [Previous line repeated 9 more times]
  File ""/home/remi/.virtualenvs/aehmc/lib/python3.9/site-packages/aesara/compile/builders.py"", line 70, in local_traverse
    shape_feature.on_import(dummy_fgraph, out.owner, reason=""dummy"")
  File ""/home/remi/.virtualenvs/aehmc/lib/python3.9/site-packages/aesara/tensor/basic_opt.py"", line 1304, in on_import
    o_shapes = self.get_node_infer_shape(node)
  File ""/home/remi/.virtualenvs/aehmc/lib/python3.9/site-packages/aesara/tensor/basic_opt.py"", line 916, in get_node_infer_shape
    raise Exception(msg).with_traceback(e.__traceback__)
  File ""/home/remi/.virtualenvs/aehmc/lib/python3.9/site-packages/aesara/tensor/basic_opt.py"", line 894, in get_node_infer_shape
    o_shapes = shape_infer(
  File ""/home/remi/.virtualenvs/aehmc/lib/python3.9/site-packages/aesara/tensor/random/op.py"", line 299, in infer_shape
    shape = self._infer_shape(size, dist_params, param_shapes=param_shapes)
  File ""/home/remi/.virtualenvs/aehmc/lib/python3.9/site-packages/aesara/tensor/random/op.py"", line 183, in _infer_shape
    size_len = get_vector_length(size)
  File ""/home/remi/.virtualenvs/aehmc/lib/python3.9/site-packages/aesara/tensor/basic.py"", line 2837, in get_vector_length
    raise ValueError(f""Length of {v} cannot be determined"")
Exception: Failed to infer_shape from Op for{cpu,scan_fn}.
Input shapes: [(), (Elemwise{add,no_inplace}.0,), (Elemwise{add,no_inplace}.0,), (Elemwise{add,no_inplace}.0,), None, None, (TensorConstant{0},), ()]
Exception encountered during infer_shape: <class 'Exception'>
Exception message: Failed to infer_shape from Op normal_rv{0, (0, 0), floatX, False}.
Input shapes: [None, (TensorConstant{0},), (), (), ()]
Exception encountered during infer_shape: <class 'ValueError'>
Exception message: Length of <TensorType(int64, vector)> cannot be determined
Traceback: Traceback (most recent call last):
  File ""/home/remi/.virtualenvs/aehmc/lib/python3.9/site-packages/aesara/tensor/basic_opt.py"", line 894, in get_node_infer_shape
    o_shapes = shape_infer(
  File ""/home/remi/.virtualenvs/aehmc/lib/python3.9/site-packages/aesara/tensor/random/op.py"", line 299, in infer_shape
    shape = self._infer_shape(size, dist_params, param_shapes=param_shapes)
  File ""/home/remi/.virtualenvs/aehmc/lib/python3.9/site-packages/aesara/tensor/random/op.py"", line 183, in _infer_shape
    size_len = get_vector_length(size)
  File ""/home/remi/.virtualenvs/aehmc/lib/python3.9/site-packages/aesara/tensor/basic.py"", line 2837, in get_vector_length
    raise ValueError(f""Length of {v} cannot be determined"")
ValueError: Length of <TensorType(int64, vector)> cannot be determined

Traceback: Traceback (most recent call last):
  File ""/home/remi/.virtualenvs/aehmc/lib/python3.9/site-packages/aesara/tensor/basic_opt.py"", line 894, in get_node_infer_shape
    o_shapes = shape_infer(
  File ""/home/remi/.virtualenvs/aehmc/lib/python3.9/site-packages/aesara/tensor/random/op.py"", line 299, in infer_shape
    shape = self._infer_shape(size, dist_params, param_shapes=param_shapes)
  File ""/home/remi/.virtualenvs/aehmc/lib/python3.9/site-packages/aesara/tensor/random/op.py"", line 183, in _infer_shape
    size_len = get_vector_length(size)
  File ""/home/remi/.virtualenvs/aehmc/lib/python3.9/site-packages/aesara/tensor/basic.py"", line 2837, in get_vector_length
    raise ValueError(f""Length of {v} cannot be determined"")
ValueError: Length of <TensorType(int64, vector)> cannot be determined

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/home/remi/.virtualenvs/aehmc/lib/python3.9/site-packages/aesara/tensor/basic_opt.py"", line 894, in get_node_infer_shape
    o_shapes = shape_infer(
  File ""/home/remi/.virtualenvs/aehmc/lib/python3.9/site-packages/aesara/scan/op.py"", line 1833, in infer_shape
    outs_shape = infer_shape(
  File ""/home/remi/.virtualenvs/aehmc/lib/python3.9/site-packages/aesara/compile/builders.py"", line 74, in infer_shape
    local_traverse(o)
  File ""/home/remi/.virtualenvs/aehmc/lib/python3.9/site-packages/aesara/compile/builders.py"", line 65, in local_traverse
    local_traverse(inp)
  File ""/home/remi/.virtualenvs/aehmc/lib/python3.9/site-packages/aesara/compile/builders.py"", line 65, in local_traverse
    local_traverse(inp)
  File ""/home/remi/.virtualenvs/aehmc/lib/python3.9/site-packages/aesara/compile/builders.py"", line 65, in local_traverse
    local_traverse(inp)
  [Previous line repeated 9 more times]
  File ""/home/remi/.virtualenvs/aehmc/lib/python3.9/site-packages/aesara/compile/builders.py"", line 70, in local_traverse
    shape_feature.on_import(dummy_fgraph, out.owner, reason=""dummy"")
  File ""/home/remi/.virtualenvs/aehmc/lib/python3.9/site-packages/aesara/tensor/basic_opt.py"", line 1304, in on_import
    o_shapes = self.get_node_infer_shape(node)
  File ""/home/remi/.virtualenvs/aehmc/lib/python3.9/site-packages/aesara/tensor/basic_opt.py"", line 916, in get_node_infer_shape
    raise Exception(msg).with_traceback(e.__traceback__)
  File ""/home/remi/.virtualenvs/aehmc/lib/python3.9/site-packages/aesara/tensor/basic_opt.py"", line 894, in get_node_infer_shape
    o_shapes = shape_infer(
  File ""/home/remi/.virtualenvs/aehmc/lib/python3.9/site-packages/aesara/tensor/random/op.py"", line 299, in infer_shape
    shape = self._infer_shape(size, dist_params, param_shapes=param_shapes)
  File ""/home/remi/.virtualenvs/aehmc/lib/python3.9/site-packages/aesara/tensor/random/op.py"", line 183, in _infer_shape
    size_len = get_vector_length(size)
  File ""/home/remi/.virtualenvs/aehmc/lib/python3.9/site-packages/aesara/tensor/basic.py"", line 2837, in get_vector_length
    raise ValueError(f""Length of {v} cannot be determined"")
Exception: Failed to infer_shape from Op normal_rv{0, (0, 0), floatX, False}.
Input shapes: [None, (TensorConstant{0},), (), (), ()]
Exception encountered during infer_shape: <class 'ValueError'>
Exception message: Length of <TensorType(int64, vector)> cannot be determined
Traceback: Traceback (most recent call last):
  File ""/home/remi/.virtualenvs/aehmc/lib/python3.9/site-packages/aesara/tensor/basic_opt.py"", line 894, in get_node_infer_shape
    o_shapes = shape_infer(
  File ""/home/remi/.virtualenvs/aehmc/lib/python3.9/site-packages/aesara/tensor/random/op.py"", line 299, in infer_shape
    shape = self._infer_shape(size, dist_params, param_shapes=param_shapes)
  File ""/home/remi/.virtualenvs/aehmc/lib/python3.9/site-packages/aesara/tensor/random/op.py"", line 183, in _infer_shape
    size_len = get_vector_length(size)
  File ""/home/remi/.virtualenvs/aehmc/lib/python3.9/site-packages/aesara/tensor/basic.py"", line 2837, in get_vector_length
    raise ValueError(f""Length of {v} cannot be determined"")
ValueError: Length of <TensorType(int64, vector)> cannot be determined
```

</details>

and if I `dprint` the `trajectory` variable:

<details>
<summary>`trajectory` graph</summary>

```
Subtensor{int64::} [id A] ''   
 |for{cpu,scan_fn}.0 [id B] ''   
 | |TensorConstant{100} [id C]
 | |IncSubtensor{Set;:int64:} [id D] ''   
 | | |AllocEmpty{dtype='float64'} [id E] ''   
 | | | |Elemwise{add,no_inplace} [id F] ''   
 | | |   |TensorConstant{100} [id C]
 | | |   |Subtensor{int64} [id G] ''   
 | | |     |Shape [id H] ''   
 | | |     | |Rebroadcast{0} [id I] ''   
 | | |     |   |InplaceDimShuffle{x} [id J] ''   
 | | |     |     |y_vv [id K]
 | | |     |ScalarConstant{0} [id L]
 | | |Rebroadcast{0} [id I] ''   
 | | |ScalarFromTensor [id M] ''   
 | |   |Subtensor{int64} [id G] ''   
 | |IncSubtensor{Set;:int64:} [id N] ''   
 | | |AllocEmpty{dtype='float64'} [id O] ''   
 | | | |Elemwise{add,no_inplace} [id P] ''   
 | | |   |TensorConstant{100} [id C]
 | | |   |Subtensor{int64} [id Q] ''   
 | | |     |Shape [id R] ''   
 | | |     | |Rebroadcast{0} [id S] ''   
 | | |     |   |InplaceDimShuffle{x} [id T] ''   
 | | |     |     |Elemwise{neg,no_inplace} [id U] 'potential energy'   
 | | |     |       |Sum{acc_dtype=float64} [id V] ''   
 | | |     |         |Assert{msg='sigma > 0'} [id W] 'y_vv_logprob'   
 | | |     |           |Elemwise{sub,no_inplace} [id X] ''   
 | | |     |           | |Elemwise{sub,no_inplace} [id Y] ''   
 | | |     |           | | |Elemwise{mul,no_inplace} [id Z] ''   
 | | |     |           | | | |TensorConstant{-0.5} [id BA]
 | | |     |           | | | |Elemwise{pow,no_inplace} [id BB] ''   
 | | |     |           | | |   |Elemwise{true_div,no_inplace} [id BC] ''   
 | | |     |           | | |   | |Elemwise{sub,no_inplace} [id BD] ''   
 | | |     |           | | |   | | |y_vv [id K]
 | | |     |           | | |   | | |TensorConstant{0} [id BE]
 | | |     |           | | |   | |TensorConstant{1} [id BF]
 | | |     |           | | |   |TensorConstant{2} [id BG]
 | | |     |           | | |Elemwise{log,no_inplace} [id BH] ''   
 | | |     |           | |   |TensorConstant{2.5066282746310002} [id BI]
 | | |     |           | |Elemwise{log,no_inplace} [id BJ] ''   
 | | |     |           |   |TensorConstant{1} [id BF]
 | | |     |           |All [id BK] ''   
 | | |     |             |Elemwise{gt,no_inplace} [id BL] ''   
 | | |     |               |TensorConstant{1} [id BF]
 | | |     |               |TensorConstant{0.0} [id BM]
 | | |     |ScalarConstant{0} [id BN]
 | | |Rebroadcast{0} [id S] ''   
 | | |ScalarFromTensor [id BO] ''   
 | |   |Subtensor{int64} [id Q] ''   
 | |IncSubtensor{Set;:int64:} [id BP] ''   
 | | |AllocEmpty{dtype='float64'} [id BQ] ''   
 | | | |Elemwise{add,no_inplace} [id BR] ''   
 | | |   |TensorConstant{100} [id C]
 | | |   |Subtensor{int64} [id BS] ''   
 | | |     |Shape [id BT] ''   
 | | |     | |Rebroadcast{0} [id BU] ''   
 | | |     |   |InplaceDimShuffle{x} [id BV] ''   
 | | |     |     |Elemwise{true_div} [id BW] 'potential energy grad'   
 | | |     |       |Elemwise{mul} [id BX] ''   
 | | |     |       | |Elemwise{mul} [id BY] ''   
 | | |     |       | | |Elemwise{mul} [id BZ] ''   
 | | |     |       | | | |Elemwise{second} [id CA] '(dpotential energy/dy_vv_logprob)'   
 | | |     |       | | | | |Assert{msg='sigma > 0'} [id W] 'y_vv_logprob'   
 | | |     |       | | | | |InplaceDimShuffle{} [id CB] ''   
 | | |     |       | | | |   |Elemwise{neg} [id CC] ''   
 | | |     |       | | | |     |Elemwise{second,no_inplace} [id CD] ''   
 | | |     |       | | | |       |Elemwise{neg,no_inplace} [id U] 'potential energy'   
 | | |     |       | | | |       |TensorConstant{1.0} [id CE]
 | | |     |       | | | |TensorConstant{-0.5} [id BA]
 | | |     |       | | |TensorConstant{2} [id BG]
 | | |     |       | |Elemwise{pow} [id CF] ''   
 | | |     |       |   |Elemwise{true_div,no_inplace} [id BC] ''   
 | | |     |       |   |Elemwise{sub} [id CG] ''   
 | | |     |       |     |TensorConstant{2} [id BG]
 | | |     |       |     |InplaceDimShuffle{} [id CH] ''   
 | | |     |       |       |TensorConstant{1} [id CI]
 | | |     |       |TensorConstant{1} [id BF]
 | | |     |ScalarConstant{0} [id CJ]
 | | |Rebroadcast{0} [id BU] ''   
 | | |ScalarFromTensor [id CK] ''   
 | |   |Subtensor{int64} [id BS] ''   
 | |RandomGeneratorSharedVariable(<Generator(PCG64) at 0x7F05783B2820>) [id CL]
 | |RandomGeneratorSharedVariable(<Generator(PCG64) at 0x7F0578A45D60>) [id CM]
 |ScalarConstant{1} [id CN]
Subtensor{int64::} [id CO] ''   
 |for{cpu,scan_fn}.1 [id B] ''   
 |ScalarConstant{1} [id CP]
Subtensor{int64::} [id CQ] ''   
 |for{cpu,scan_fn}.2 [id B] ''   
 |ScalarConstant{1} [id CR]

Inner graphs of the scan ops:

for{cpu,scan_fn}.0 [id B] ''   
 >if{}.0 [id CS] ''   
 > |bernoulli_rv{0, (0,), int64, False}.1 [id CT] ''   
 > | |<RandomGeneratorType> [id CU] -> [id CL]
 > | |TensorConstant{[]} [id CV]
 > | |TensorConstant{4} [id CW]
 > | |Elemwise{clip,no_inplace} [id CX] ''   
 > |   |Elemwise{exp,no_inplace} [id CY] ''   
 > |   | |Elemwise{switch,no_inplace} [id CZ] ''   
 > |   |   |Elemwise{isnan,no_inplace} [id DA] ''   
 > |   |   | |Elemwise{sub,no_inplace} [id DB] ''   
 > |   |   |   |Elemwise{add,no_inplace} [id DC] ''   
 > |   |   |   | |potential energy[t-1] [id DD] -> [id N]
 > |   |   |   | |Elemwise{mul,no_inplace} [id DE] ''   
 > |   |   |   |   |TensorConstant{0.5} [id DF]
 > |   |   |   |   |Elemwise{mul,no_inplace} [id DG] ''   
 > |   |   |   |     |Elemwise{mul,no_inplace} [id DH] ''   
 > |   |   |   |     | |TensorConstant{1.0} [id DI]
 > |   |   |   |     | |Elemwise{mul,no_inplace} [id DJ] ''   
 > |   |   |   |     |   |Elemwise{sqrt,no_inplace} [id DK] ''   
 > |   |   |   |     |   | |Elemwise{reciprocal,no_inplace} [id DL] ''   
 > |   |   |   |     |   |   |TensorConstant{1.0} [id DI]
 > |   |   |   |     |   |normal_rv{0, (0, 0), floatX, False}.1 [id DM] 'momentum'   
 > |   |   |   |     |     |<RandomGeneratorType> [id DN] -> [id CM]
 > |   |   |   |     |     |Elemwise{Cast{int64}} [id DO] ''   
 > |   |   |   |     |     | |TensorConstant{[]} [id DP]
 > |   |   |   |     |     |TensorConstant{11} [id DQ]
 > |   |   |   |     |     |TensorConstant{0} [id DR]
 > |   |   |   |     |     |TensorConstant{1} [id DS]
 > |   |   |   |     |Elemwise{mul,no_inplace} [id DJ] ''   
 > |   |   |   |Elemwise{add,no_inplace} [id DT] ''   
 > |   |   |     |Subtensor{int64} [id DU] ''   
 > |   |   |     | |Subtensor{int64::} [id DV] ''   
 > |   |   |     | | |for{cpu,scan_fn}.2 [id DW] ''   
 > |   |   |     | | | |TensorConstant{10} [id DX]
 > |   |   |     | | | |IncSubtensor{Set;:int64:} [id DY] ''   
 > |   |   |     | | | | |AllocEmpty{dtype='float64'} [id DZ] ''   
 > |   |   |     | | | | | |Elemwise{add,no_inplace} [id EA] ''   
 > |   |   |     | | | | |   |TensorConstant{10} [id DX]
 > |   |   |     | | | | |   |Subtensor{int64} [id EB] ''   
 > |   |   |     | | | | |     |Shape [id EC] ''   
 > |   |   |     | | | | |     | |Rebroadcast{0} [id ED] ''   
 > |   |   |     | | | | |     |   |InplaceDimShuffle{x} [id EE] ''   
 > |   |   |     | | | | |     |     |y_vv[t-1] [id EF] -> [id D]
 > |   |   |     | | | | |     |ScalarConstant{0} [id EG]
 > |   |   |     | | | | |Rebroadcast{0} [id ED] ''   
 > |   |   |     | | | | |ScalarFromTensor [id EH] ''   
 > |   |   |     | | | |   |Subtensor{int64} [id EB] ''   
 > |   |   |     | | | |IncSubtensor{Set;:int64:} [id EI] ''   
 > |   |   |     | | | | |AllocEmpty{dtype='float64'} [id EJ] ''   
 > |   |   |     | | | | | |Elemwise{add,no_inplace} [id EK] ''   
 > |   |   |     | | | | |   |TensorConstant{10} [id DX]
 > |   |   |     | | | | |   |Subtensor{int64} [id EL] ''   
 > |   |   |     | | | | |     |Shape [id EM] ''   
 > |   |   |     | | | | |     | |Rebroadcast{0} [id EN] ''   
 > |   |   |     | | | | |     |   |InplaceDimShuffle{x} [id EO] ''   
 > |   |   |     | | | | |     |     |Elemwise{mul,no_inplace} [id DJ] ''   
 > |   |   |     | | | | |     |ScalarConstant{0} [id EP]
 > |   |   |     | | | | |Rebroadcast{0} [id EN] ''   
 > |   |   |     | | | | |ScalarFromTensor [id EQ] ''   
 > |   |   |     | | | |   |Subtensor{int64} [id EL] ''   
 > |   |   |     | | | |IncSubtensor{Set;:int64:} [id ER] ''   
 > |   |   |     | | | | |AllocEmpty{dtype='float64'} [id ES] ''   
 > |   |   |     | | | | | |Elemwise{add,no_inplace} [id ET] ''   
 > |   |   |     | | | | |   |TensorConstant{10} [id DX]
 > |   |   |     | | | | |   |Subtensor{int64} [id EU] ''   
 > |   |   |     | | | | |     |Shape [id EV] ''   
 > |   |   |     | | | | |     | |Rebroadcast{0} [id EW] ''   
 > |   |   |     | | | | |     |   |InplaceDimShuffle{x} [id EX] ''   
 > |   |   |     | | | | |     |     |potential energy[t-1] [id DD] -> [id N]
 > |   |   |     | | | | |     |ScalarConstant{0} [id EY]
 > |   |   |     | | | | |Rebroadcast{0} [id EW] ''   
 > |   |   |     | | | | |ScalarFromTensor [id EZ] ''   
 > |   |   |     | | | |   |Subtensor{int64} [id EU] ''   
 > |   |   |     | | | |IncSubtensor{Set;:int64:} [id FA] ''   
 > |   |   |     | | |   |AllocEmpty{dtype='float64'} [id FB] ''   
 > |   |   |     | | |   | |Elemwise{add,no_inplace} [id FC] ''   
 > |   |   |     | | |   |   |TensorConstant{10} [id DX]
 > |   |   |     | | |   |   |Subtensor{int64} [id FD] ''   
 > |   |   |     | | |   |     |Shape [id FE] ''   
 > |   |   |     | | |   |     | |Rebroadcast{0} [id FF] ''   
 > |   |   |     | | |   |     |   |InplaceDimShuffle{x} [id FG] ''   
 > |   |   |     | | |   |     |     |potential energy grad[t-1] [id FH] -> [id BP]
 > |   |   |     | | |   |     |ScalarConstant{0} [id FI]
 > |   |   |     | | |   |Rebroadcast{0} [id FF] ''   
 > |   |   |     | | |   |ScalarFromTensor [id FJ] ''   
 > |   |   |     | | |     |Subtensor{int64} [id FD] ''   
 > |   |   |     | | |ScalarConstant{1} [id FK]
 > |   |   |     | |ScalarConstant{-1} [id FL]
 > |   |   |     |Elemwise{mul,no_inplace} [id FM] ''   
 > |   |   |       |TensorConstant{0.5} [id FN]
 > |   |   |       |Elemwise{mul,no_inplace} [id FO] ''   
 > |   |   |         |Elemwise{mul,no_inplace} [id FP] ''   
 > |   |   |         | |TensorConstant{1.0} [id DI]
 > |   |   |         | |Elemwise{mul,no_inplace} [id FQ] ''   
 > |   |   |         |   |TensorConstant{-1.0} [id FR]
 > |   |   |         |   |Subtensor{int64} [id FS] ''   
 > |   |   |         |     |Subtensor{int64::} [id FT] ''   
 > |   |   |         |     | |for{cpu,scan_fn}.1 [id DW] ''   
 > |   |   |         |     | |ScalarConstant{1} [id FU]
 > |   |   |         |     |ScalarConstant{-1} [id FV]
 > |   |   |         |Elemwise{mul,no_inplace} [id FQ] ''   
 > |   |   |TensorConstant{-inf} [id FW]
 > |   |   |Elemwise{sub,no_inplace} [id DB] ''   
 > |   |TensorConstant{0} [id FX]
 > |   |TensorConstant{1.0} [id FY]
 > |Subtensor{int64} [id FZ] ''   
 > | |Subtensor{int64::} [id GA] ''   
 > | | |for{cpu,scan_fn}.0 [id DW] ''   
 > | | |ScalarConstant{1} [id GB]
 > | |ScalarConstant{-1} [id GC]
 > |Elemwise{mul,no_inplace} [id FQ] ''   
 > |Subtensor{int64} [id DU] ''   
 > |Subtensor{int64} [id GD] ''   
 > | |Subtensor{int64::} [id GE] ''   
 > | | |for{cpu,scan_fn}.3 [id DW] ''   
 > | | |ScalarConstant{1} [id GF]
 > | |ScalarConstant{-1} [id GG]
 > |y_vv[t-1] [id EF] -> [id D]
 > |Elemwise{mul,no_inplace} [id DJ] ''   
 > |potential energy[t-1] [id DD] -> [id N]
 > |potential energy grad[t-1] [id FH] -> [id BP]
 >if{}.2 [id CS] ''   
 >if{}.3 [id CS] ''   
 >bernoulli_rv{0, (0,), int64, False}.0 [id GH] ''   
 > |<RandomGeneratorType> [id CU] -> [id CL]
 > |TensorConstant{[]} [id CV]
 > |TensorConstant{4} [id CW]
 > |Elemwise{clip,no_inplace} [id GI] ''   
 >   |Elemwise{exp,no_inplace} [id GJ] ''   
 >   | |Elemwise{switch,no_inplace} [id GK] ''   
 >   |   |Elemwise{isnan,no_inplace} [id GL] ''   
 >   |   | |Elemwise{sub,no_inplace} [id GM] ''   
 >   |   |   |Elemwise{add,no_inplace} [id GN] ''   
 >   |   |   | |potential energy[t-1] [id DD] -> [id N]
 >   |   |   | |Elemwise{mul,no_inplace} [id GO] ''   
 >   |   |   |   |TensorConstant{0.5} [id DF]
 >   |   |   |   |Elemwise{mul,no_inplace} [id GP] ''   
 >   |   |   |     |Elemwise{mul,no_inplace} [id GQ] ''   
 >   |   |   |     | |TensorConstant{1.0} [id DI]
 >   |   |   |     | |Elemwise{mul,no_inplace} [id GR] ''   
 >   |   |   |     |   |Elemwise{sqrt,no_inplace} [id GS] ''   
 >   |   |   |     |   | |Elemwise{reciprocal,no_inplace} [id GT] ''   
 >   |   |   |     |   |   |TensorConstant{1.0} [id DI]
 >   |   |   |     |   |normal_rv{0, (0, 0), floatX, False}.1 [id GU] 'momentum'   
 >   |   |   |     |     |<RandomGeneratorType> [id DN] -> [id CM]
 >   |   |   |     |     |Elemwise{Cast{int64}} [id GV] ''   
 >   |   |   |     |     | |TensorConstant{[]} [id DP]
 >   |   |   |     |     |TensorConstant{11} [id DQ]
 >   |   |   |     |     |TensorConstant{0} [id DR]
 >   |   |   |     |     |TensorConstant{1} [id DS]
 >   |   |   |     |Elemwise{mul,no_inplace} [id GR] ''   
 >   |   |   |Elemwise{add,no_inplace} [id GW] ''   
 >   |   |     |Subtensor{int64} [id GX] ''   
 >   |   |     | |Subtensor{int64::} [id GY] ''   
 >   |   |     | | |for{cpu,scan_fn}.2 [id GZ] ''   
 >   |   |     | | | |TensorConstant{10} [id DX]
 >   |   |     | | | |IncSubtensor{Set;:int64:} [id HA] ''   
 >   |   |     | | | | |AllocEmpty{dtype='float64'} [id HB] ''   
 >   |   |     | | | | | |Elemwise{add,no_inplace} [id HC] ''   
 >   |   |     | | | | |   |TensorConstant{10} [id DX]
 >   |   |     | | | | |   |Subtensor{int64} [id HD] ''   
 >   |   |     | | | | |     |Shape [id HE] ''   
 >   |   |     | | | | |     | |Rebroadcast{0} [id HF] ''   
 >   |   |     | | | | |     |   |InplaceDimShuffle{x} [id HG] ''   
 >   |   |     | | | | |     |     |y_vv[t-1] [id EF] -> [id D]
 >   |   |     | | | | |     |ScalarConstant{0} [id EG]
 >   |   |     | | | | |Rebroadcast{0} [id HF] ''   
 >   |   |     | | | | |ScalarFromTensor [id HH] ''   
 >   |   |     | | | |   |Subtensor{int64} [id HD] ''   
 >   |   |     | | | |IncSubtensor{Set;:int64:} [id HI] ''   
 >   |   |     | | | | |AllocEmpty{dtype='float64'} [id HJ] ''   
 >   |   |     | | | | | |Elemwise{add,no_inplace} [id HK] ''   
 >   |   |     | | | | |   |TensorConstant{10} [id DX]
 >   |   |     | | | | |   |Subtensor{int64} [id HL] ''   
 >   |   |     | | | | |     |Shape [id HM] ''   
 >   |   |     | | | | |     | |Rebroadcast{0} [id HN] ''   
 >   |   |     | | | | |     |   |InplaceDimShuffle{x} [id HO] ''   
 >   |   |     | | | | |     |     |Elemwise{mul,no_inplace} [id GR] ''   
 >   |   |     | | | | |     |ScalarConstant{0} [id EP]
 >   |   |     | | | | |Rebroadcast{0} [id HN] ''   
 >   |   |     | | | | |ScalarFromTensor [id HP] ''   
 >   |   |     | | | |   |Subtensor{int64} [id HL] ''   
 >   |   |     | | | |IncSubtensor{Set;:int64:} [id HQ] ''   
 >   |   |     | | | | |AllocEmpty{dtype='float64'} [id HR] ''   
 >   |   |     | | | | | |Elemwise{add,no_inplace} [id HS] ''   
 >   |   |     | | | | |   |TensorConstant{10} [id DX]
 >   |   |     | | | | |   |Subtensor{int64} [id HT] ''   
 >   |   |     | | | | |     |Shape [id HU] ''   
 >   |   |     | | | | |     | |Rebroadcast{0} [id HV] ''   
 >   |   |     | | | | |     |   |InplaceDimShuffle{x} [id HW] ''   
 >   |   |     | | | | |     |     |potential energy[t-1] [id DD] -> [id N]
 >   |   |     | | | | |     |ScalarConstant{0} [id EY]
 >   |   |     | | | | |Rebroadcast{0} [id HV] ''   
 >   |   |     | | | | |ScalarFromTensor [id HX] ''   
 >   |   |     | | | |   |Subtensor{int64} [id HT] ''   
 >   |   |     | | | |IncSubtensor{Set;:int64:} [id HY] ''   
 >   |   |     | | |   |AllocEmpty{dtype='float64'} [id HZ] ''   
 >   |   |     | | |   | |Elemwise{add,no_inplace} [id IA] ''   
 >   |   |     | | |   |   |TensorConstant{10} [id DX]
 >   |   |     | | |   |   |Subtensor{int64} [id IB] ''   
 >   |   |     | | |   |     |Shape [id IC] ''   
 >   |   |     | | |   |     | |Rebroadcast{0} [id ID] ''   
 >   |   |     | | |   |     |   |InplaceDimShuffle{x} [id IE] ''   
 >   |   |     | | |   |     |     |potential energy grad[t-1] [id FH] -> [id BP]
 >   |   |     | | |   |     |ScalarConstant{0} [id FI]
 >   |   |     | | |   |Rebroadcast{0} [id ID] ''   
 >   |   |     | | |   |ScalarFromTensor [id IF] ''   
 >   |   |     | | |     |Subtensor{int64} [id IB] ''   
 >   |   |     | | |ScalarConstant{1} [id FK]
 >   |   |     | |ScalarConstant{-1} [id FL]
 >   |   |     |Elemwise{mul,no_inplace} [id IG] ''   
 >   |   |       |TensorConstant{0.5} [id FN]
 >   |   |       |Elemwise{mul,no_inplace} [id IH] ''   
 >   |   |         |Elemwise{mul,no_inplace} [id II] ''   
 >   |   |         | |TensorConstant{1.0} [id DI]
 >   |   |         | |Elemwise{mul,no_inplace} [id IJ] ''   
 >   |   |         |   |TensorConstant{-1.0} [id FR]
 >   |   |         |   |Subtensor{int64} [id IK] ''   
 >   |   |         |     |Subtensor{int64::} [id IL] ''   
 >   |   |         |     | |for{cpu,scan_fn}.1 [id GZ] ''   
 >   |   |         |     | |ScalarConstant{1} [id FU]
 >   |   |         |     |ScalarConstant{-1} [id FV]
 >   |   |         |Elemwise{mul,no_inplace} [id IJ] ''   
 >   |   |TensorConstant{-inf} [id FW]
 >   |   |Elemwise{sub,no_inplace} [id GM] ''   
 >   |TensorConstant{0} [id FX]
 >   |TensorConstant{1.0} [id FY]
 >normal_rv{0, (0, 0), floatX, False}.0 [id GU] ''   

for{cpu,scan_fn}.1 [id B] ''   
 >if{}.0 [id CS] ''   
 >if{}.2 [id CS] ''   
 >if{}.3 [id CS] ''   
 >bernoulli_rv{0, (0,), int64, False}.0 [id GH] ''   
 >normal_rv{0, (0, 0), floatX, False}.0 [id GU] ''   

for{cpu,scan_fn}.2 [id B] ''   
 >if{}.0 [id CS] ''   
 >if{}.2 [id CS] ''   
 >if{}.3 [id CS] ''   
 >bernoulli_rv{0, (0,), int64, False}.0 [id GH] ''   
 >normal_rv{0, (0, 0), floatX, False}.0 [id GU] ''   

for{cpu,scan_fn}.2 [id DW] ''   
 >ViewOp [id IM] ''   
 > |Elemwise{Composite{(i0 + (i1 * ((i2 * i3) + (i2 * i3))))}} [id IN] ''   
 >   |y_vv[t-1][t-1] [id IO] -> [id DY]
 >   |TensorConstant{0.0001} [id IP]
 >   |TensorConstant{0.5} [id IQ]
 >   |Elemwise{Composite{(i0 - (i1 * i2))}} [id IR] ''   
 >     |<TensorType(float64, scalar)> [id IS] -> [id EI]
 >     |TensorConstant{5e-05} [id IT]
 >     |potential energy grad[t-1][t-1] [id IU] -> [id FA]
 >Elemwise{Composite{(i0 - (i1 * i2))}} [id IV] ''   
 > |Elemwise{Composite{(i0 - (i1 * i2))}} [id IR] ''   
 > |TensorConstant{5e-05} [id IT]
 > |Elemwise{Composite{(i0 + (i1 * ((i2 * i3) + (i2 * i3))))}} [id IN] ''   
 >Elemwise{Composite{(-(i0 + (i1 * sqr(i2))))}} [id IW] ''   
 > |TensorConstant{-0.9189385332046727} [id IX]
 > |TensorConstant{-0.5} [id IY]
 > |Elemwise{Composite{(i0 + (i1 * ((i2 * i3) + (i2 * i3))))}} [id IN] ''   
 >Elemwise{Composite{(i0 + (i1 * ((i2 * i3) + (i2 * i3))))}} [id IN] ''   

for{cpu,scan_fn}.1 [id DW] ''   
 >ViewOp [id IM] ''   
 >Elemwise{Composite{(i0 - (i1 * i2))}} [id IV] ''   
 >Elemwise{Composite{(-(i0 + (i1 * sqr(i2))))}} [id IW] ''   
 >Elemwise{Composite{(i0 + (i1 * ((i2 * i3) + (i2 * i3))))}} [id IN] ''   

for{cpu,scan_fn}.0 [id DW] ''   
 >ViewOp [id IM] ''   
 >Elemwise{Composite{(i0 - (i1 * i2))}} [id IV] ''   
 >Elemwise{Composite{(-(i0 + (i1 * sqr(i2))))}} [id IW] ''   
 >Elemwise{Composite{(i0 + (i1 * ((i2 * i3) + (i2 * i3))))}} [id IN] ''   

for{cpu,scan_fn}.3 [id DW] ''   
 >ViewOp [id IM] ''   
 >Elemwise{Composite{(i0 - (i1 * i2))}} [id IV] ''   
 >Elemwise{Composite{(-(i0 + (i1 * sqr(i2))))}} [id IW] ''   
 >Elemwise{Composite{(i0 + (i1 * ((i2 * i3) + (i2 * i3))))}} [id IN] ''   

for{cpu,scan_fn}.2 [id GZ] ''   
 >ViewOp [id IM] ''   
 >Elemwise{Composite{(i0 - (i1 * i2))}} [id IV] ''   
 >Elemwise{Composite{(-(i0 + (i1 * sqr(i2))))}} [id IW] ''   
 >Elemwise{Composite{(i0 + (i1 * ((i2 * i3) + (i2 * i3))))}} [id IN] ''   

for{cpu,scan_fn}.1 [id GZ] ''   
 >ViewOp [id IM] ''   
 >Elemwise{Composite{(i0 - (i1 * i2))}} [id IV] ''   
 >Elemwise{Composite{(-(i0 + (i1 * sqr(i2))))}} [id IW] ''   
 >Elemwise{Composite{(i0 + (i1 * ((i2 * i3) + (i2 * i3))))}} [id IN] ''   

```

</details>

_Originally posted by @rlouf in https://github.com/aesara-devs/aehmc/issues/29#issuecomment-930897441_",bug help wanted important graph rewriting Scan shape inference,,2021-09-30 07:48:17,2022-04-10 23:12:55,"rlouf mentioned 2021-09-30 07:48:18,rlouf subscribed 2021-09-30 07:48:18,rlouf mentioned 2021-10-04 19:22:17,rlouf subscribed 2021-10-04 19:22:17,brandonwillard transferred 2021-10-04 19:22:18,brandonwillard renamed 2021-10-04 19:22:58,brandonwillard labeled 2021-10-04 19:23:19,brandonwillard labeled 2021-10-04 19:23:19,brandonwillard labeled 2021-10-04 19:23:19,brandonwillard labeled 2021-10-04 19:23:19,brandonwillard labeled 2021-10-04 19:23:19,brandonwillard renamed 2021-10-04 19:25:32,brandonwillard connected 2021-11-15 19:39:38,brandonwillard closed 2021-11-18 16:02:46,brandonwillard reopened 2021-12-28 20:44:23,brandonwillard labeled 2022-01-13 20:50:27,brandonwillard closed 2022-04-10 23:12:55",ricardoV94 rlouf brandonwillard,7
504,604,Remove gpuarray support,ferrine,"gpuarray is a legacy backend that we do not intend to support. In this issue I propose to do the following:

- [ ] delete the folder containing gpuarray related code
- [ ] delete all the tests that make use of gpuarray
- [ ] delete all the references of gpuarray in docs

This is a major influence on the codebase, requesting discussion",duplicate,,2021-10-01 15:01:10,2021-10-03 05:30:00,"ferrine renamed 2021-10-01 15:01:23,brandonwillard closed 2021-10-03 05:30:00,brandonwillard labeled 2021-10-03 05:30:18",ferrine ricardoV94 brandonwillard,2
507,611,Alloc fails with non scalar constants used as shape,ricardoV94,"```python
import aesara.tensor as at
size = at.constant([3])
size.eval()  # array([3])
y = at.zeros(size)
```
```
TypeError: ('Each shape dimension to Alloc must be a scalar, ', 'but dimension 0 have 1 dimensions for apply node: TensorConstant{(1,) of 3}')
```

But this works:
```python
x = at.vector(""x"")
size = x.shape
size.eval({x: [0, 0, 0]})  # array([3])
y = at.zeros(size)
y.eval({x: [0, 0, 0]})  # array([0., 0., 0.])
```

Also NumPy is fine
```python
np.zeros([3])  # array([0., 0., 0.])
```",bug,ricardoV94,2021-10-11 14:48:23,2021-10-11 15:10:03,"ricardoV94 referenced 2021-10-11 14:58:26,ricardoV94 assigned 2021-10-11 14:58:50,ricardoV94 labeled 2021-10-11 14:58:53,ricardoV94 closed 2021-10-11 15:10:03",ricardoV94 brandonwillard,3
511,616,`aet.switch` fails `local_useless_switch` optimizations when used with certain conditions. ,kc611,"MWE:
```python
import aesara
import aesara.tensor as aet
import numpy as np

aesara.config.floatX = 'float32'

cond = aet.as_tensor_variable(np.array([True]))
ift = aet.as_tensor_variable(np.array([0]))
iff = aet.as_tensor_variable(np.array([0]))

a = aet.switch(cond, ift, iff)
b = a.eval() # Throws Error while optimizations
```

The solutions also looks straightforward, the issue is probably that when `cond=True`, it fails to go through this particular branch:

https://github.com/aesara-devs/aesara/blob/29032f3483c8c36fa5575fdac07b62cea644320b/aesara/tensor/basic_opt.py#L2538-L2540

Adding an additional condition like `or cond in [True, False]` should fix this issue.

Note that this only happens when `floatX` is set to `float32`.",bug help wanted important,,2021-10-15 11:10:42,2021-10-18 17:51:37,"brandonwillard labeled 2021-10-15 15:52:41,brandonwillard labeled 2021-10-15 15:52:41,brandonwillard labeled 2021-10-15 15:52:41,brandonwillard closed 2021-10-18 17:51:37",kc611 brandonwillard,0
514,620,Numba's `Dot` implementation doesn't handle float typecasting. ,kc611,"## Description of your problem or feature request

We do have integer typecasts for `Dot` Op's implementation since Numba doesn't support `np.dot` on integral values, but the typecasting doesn't handle cases when both inputs are floats of different types. 

https://github.com/aesara-devs/aesara/blob/e6c2fbc9afbf82969fba3dc3fc194fa1df2710f4/aesara/link/numba/dispatch/basic.py#L556

MWE: 
```python
import numpy as np
import aesara
import aesara.tensor.math as aem
import aesara.tensor as aet
from aesara.graph.optdb import OptimizationQuery
from aesara.compile.mode import Mode
from aesara.link.numba.linker import NumbaLinker

opts = OptimizationQuery(include=[None], exclude=[""cxx_only"", ""BlasOpt""])
numba_mode = Mode(NumbaLinker(), opts)
py_mode = Mode(""py"", opts)

x = aet.vector().astype(""float32"")
y = aet.matrix().astype(""float64"")

x_val = np.random.rand(3).astype(""float32"")
y_val = np.random.rand(3,3).astype(""float64"")

g = aem.Dot()(x, y)
py_res = aesara.function(inputs=[x,y], outputs=[g], mode=py_mode)(x_val, y_val) # Passes
numba_res = aesara.function(inputs=[x,y], outputs=[g], mode=numba_mode)(x_val, y_val) # Fails
```
",bug Numba,,2021-10-19 13:15:54,2021-10-22 20:50:36,"brandonwillard labeled 2021-10-19 17:58:50,brandonwillard labeled 2021-10-19 17:58:50,brandonwillard closed 2021-10-22 20:50:36",kc611 brandonwillard,0
515,621,`DimShuffle`'s Numba implementation cannot handle cases when output is empty/scalar. ,kc611,"## Description of your problem or feature request

`DimShuffle`'s Numba implementation fails when output shape is `()` (i.e. no output when all dimensions are dropped). This happens because of the `n > 0` restriction placed on `create_tuple_creator`:
https://github.com/aesara-devs/aesara/blob/e6c2fbc9afbf82969fba3dc3fc194fa1df2710f4/aesara/link/numba/dispatch/basic.py#L218-L223

Which is used in the implementation of `DimShuffle` to create output 's shape tuple:
https://github.com/aesara-devs/aesara/blob/e6c2fbc9afbf82969fba3dc3fc194fa1df2710f4/aesara/link/numba/dispatch/elemwise.py#L327
**Please provide a minimal, self-contained, and reproducible example.**
```python
import numpy as np
import aesara
from aesara.graph.optdb import OptimizationQuery
from aesara.compile.mode import Mode
from aesara.link.numba.linker import NumbaLinker
from aesara.tensor.elemwise import DimShuffle
from aesara.tensor.type import TensorType

opts = OptimizationQuery(include=[None], exclude=[""cxx_only"", ""BlasOpt""])
numba_mode = Mode(NumbaLinker(), opts)
py_mode = Mode(""py"", opts)

orig_shape = (1, 1, 1)
dims = ()
out_shape = ()

broadcastables = [(entry == 1) for entry in orig_shape]
x = TensorType(aesara.config.floatX, broadcastables)(""x"")
out = DimShuffle(broadcastables, dims)(x)

py_res = aesara.function(inputs=[x], outputs=[out], mode=py_mode)(np.ones(orig_shape, dtype=aesara.config.floatX)) # Passes
numba_res = aesara.function(inputs=[x], outputs=[out], mode=numba_mode)(np.ones(orig_shape, dtype=aesara.config.floatX)) # Fails
pass
```
",bug important Numba,,2021-10-19 13:36:25,2021-10-22 20:50:21,"brandonwillard labeled 2021-10-19 18:02:14,brandonwillard labeled 2021-10-19 18:02:14,brandonwillard labeled 2021-10-19 18:02:14,brandonwillard closed 2021-10-22 20:50:21",kc611 brandonwillard,0
516,622,Support `updates` option for `aesara.function` in Numba,kc611,"## Description of your problem or feature request

This is more of a question than an issue, do we support the symbolic inputs and updates ?

Though it seems that in this case the graph generated is 'empty' and `NumbaLinker` isn't able to handle that, so this can be a whole another issue than the above question. This shouldn't be that common a case though. 

**Please provide a minimal, self-contained, and reproducible example.**
```python
import numpy as np
import aesara
import aesara.tensor as aet
from aesara.graph.optdb import OptimizationQuery
from aesara.compile.mode import Mode
from aesara.link.numba.linker import NumbaLinker
from aesara.compile.io import In

opts = OptimizationQuery(include=[None], exclude=[""cxx_only"", ""BlasOpt""])
numba_mode = Mode(NumbaLinker(), opts)
py_mode = Mode(""py"", opts)

a = aet.dscalar(""a"")
f = aesara.function([In(a, value=0.0, update=a + 1)], a, mode=py_mode) # Passes
f = aesara.function([In(a, value=0.0, update=a + 1)], a, mode=numba_mode) # Fails
# f = function([], [], updates=[(a, a), (b, (2 * b))]) # Also fails with same error
```
",enhancement help wanted important Numba,,2021-10-19 13:41:20,2022-10-05 18:25:07,"brandonwillard labeled 2021-10-19 17:59:47,brandonwillard labeled 2021-10-19 17:59:47,brandonwillard renamed 2021-10-20 22:45:50,brandonwillard labeled 2021-10-20 23:44:32,brandonwillard labeled 2021-11-08 23:25:27,brandonwillard milestoned 2021-11-08 23:25:31,brandonwillard closed 2022-10-05 18:25:07",ricardoV94 kc611 brandonwillard,4
517,623,Numba's `Scan` implementation fails when inner function's `nit_sot` outputs are not scalar.,kc611,"## Description of your problem or feature request

The way we currently handle `nit_sot` outputs that come from inner function of scan, is that we are provided a shape of the container as `nit_sot` inputs and we construct a empty `np.zeros` of the same shape corresponding to that particular input shape. 

https://github.com/aesara-devs/aesara/blob/e6c2fbc9afbf82969fba3dc3fc194fa1df2710f4/aesara/link/numba/dispatch/scan.py#L104-L111

However this only works in cases when the `nit_sot` outputs from inner function are scalars, since what we are given as shape for `nit_sot` is the shape of the 'container' array, not the resultant shape. We need to somehow append the output shape of each corresponding `nit_sot` output to this particular shape and construct the empty array out of this resulting final shape. 

**Please provide a minimal, self-contained, and reproducible example.**
```python
import numpy as np
import aesara
import aesara.tensor as aet
from aesara.graph.optdb import OptimizationQuery
from aesara.compile.mode import Mode
from aesara.link.numba.linker import NumbaLinker

opts = OptimizationQuery(include=[None], exclude=[""cxx_only"", ""BlasOpt""])
numba_mode = Mode(NumbaLinker(), opts)
py_mode = Mode(""py"", opts)


def step_fn():
    return aet.zeros((10,10)) # Multidimensional output with no input taps 

res, _ = aesara.scan(
    fn=step_fn,
    outputs_info=[None],
    n_steps=10
)

py_res = aesara.function(inputs=[], outputs=[res], mode=py_mode)() # Passes
numba_res = aesara.function(inputs=[], outputs=[res], mode=numba_mode)() # Fails
pass
```

**Please provide the full traceback of any errors.**
```python
numba.core.errors.TypingError: Failed in nopython mode pipeline (step: nopython frontend)
Failed in nopython mode pipeline (step: nopython frontend)
No implementation of function Function(<built-in function setitem>) found for signature:
 
 >>> setitem(array(float64, 1d, C), int32, array(float64, 2d, C))
 
There are 16 candidate implementations:
    - Of which 16 did not match due to:
    Overload of function 'setitem': File: <numerous>: Line N/A.
      With argument(s): '(array(float64, 1d, C), int32, array(float64, 2d, C))':
     No match.

During: typing of setitem at /tmp/tmpo2fpodxn (8)

File ""../../../../../../tmp/tmpo2fpodxn"", line 8:
def scan(n_steps, auto_33):
    <source elided>
        inner_args = ()
        (auto_33[i], ) = numba_aet_inner_func(*inner_args)
        ^

During: resolving callee type: type(CPUDispatcher(<function scan at 0x7fa4ae353290>))
During: typing of call at /tmp/tmpwmr5fyre (3)

During: resolving callee type: type(CPUDispatcher(<function scan at 0x7fa4ae353290>))
During: typing of call at /tmp/tmpwmr5fyre (3)

During: resolving callee type: type(CPUDispatcher(<function scan at 0x7fa4ae353290>))
During: typing of call at /tmp/tmpwmr5fyre (3)

During: resolving callee type: type(CPUDispatcher(<function scan at 0x7fa4ae353290>))
During: typing of call at /tmp/tmpwmr5fyre (3)


File ""../../../../../../tmp/tmpwmr5fyre"", line 3:
def numba_funcified_fgraph():
    auto_34 = scan(auto_33, auto_33)
    ^

Apply node that caused the error: for{cpu,scan_fn}(TensorConstant{10}, TensorConstant{10})
Toposort index: 0
Inputs types: [TensorType(int8, scalar), TensorType(int8, scalar)]
Inputs shapes: []
Inputs strides: []
Inputs values: []
Outputs clients: [['output']]
```

",bug important Numba,,2021-10-19 16:18:27,2021-10-22 20:50:57,"brandonwillard labeled 2021-10-19 18:05:54,brandonwillard labeled 2021-10-19 18:05:54,brandonwillard labeled 2021-10-19 18:05:54,brandonwillard pinned 2021-10-19 18:06:03,brandonwillard closed 2021-10-22 20:50:58,kc611 unpinned 2021-10-23 16:07:16",kc611 brandonwillard,0
518,624,Numba implementation of `ScalarOp` fails to promote dtype's properly in case of reciprocal,kc611,"## Description of your problem or feature request

This particular thing appears to be happening only in case of reciprocals currently not observed in any other similar `Scalar` `Op`s. (e.g. `sqrt` or `log`)

**Please provide a minimal, self-contained, and reproducible example.**
```python
import numpy as np
import aesara
import aesara.scalar.basic as aes
from aesara.graph.optdb import OptimizationQuery
from aesara.compile.mode import Mode
from aesara.link.numba.linker import NumbaLinker

opts = OptimizationQuery(include=[None], exclude=[""cxx_only"", ""BlasOpt""])
numba_mode = Mode(NumbaLinker(), opts)
py_mode = Mode(""py"", opts)

xi = aes.int32(""xi"")
ei = aes.reciprocal(xi)

py_res = aesara.function([xi], ei, mode=py_mode)(10) # 0.1 (float32)
numba_res = aesara.function([xi], ei, mode=numba_mode)(10) # 0 (int)

assert py_res == numba_res # Fails
```
",bug important Numba,,2021-10-20 16:18:57,2021-10-22 18:21:36,"brandonwillard labeled 2021-10-20 22:42:57,brandonwillard labeled 2021-10-20 22:42:57,brandonwillard labeled 2021-10-20 22:42:57,kc611 closed 2021-10-22 18:21:36",kc611 brandonwillard,0
523,629,Add a JAX conversion for `Assert` `Op`s,ricardoV94,https://github.com/pymc-devs/pymc/blob/8a72d37bacfcac0804a74207088bd5d58a0ea495/pymc/sampling_jax.py#L34-L43,enhancement help wanted JAX,rlouf,2021-10-25 13:03:42,2022-09-15 17:01:51,"ricardoV94 labeled 2021-10-25 13:03:50,ricardoV94 renamed 2021-10-25 13:04:21,brandonwillard renamed 2021-10-25 16:28:48,brandonwillard labeled 2021-10-25 16:28:57,brandonwillard labeled 2021-10-25 16:28:57,ricardoV94 mentioned 2021-10-25 16:31:08,ricardoV94 subscribed 2021-10-25 16:31:08,ricardoV94 mentioned 2021-10-25 16:51:39,ricardoV94 subscribed 2021-10-25 16:51:39,rlouf assigned 2022-02-01 16:31:14,brandonwillard closed 2022-02-07 19:34:10,rlouf reopened 2022-09-05 07:26:05,rlouf connected 2022-09-15 07:44:33,brandonwillard closed 2022-09-15 17:01:51",rlouf ricardoV94 kc611 brandonwillard,12
524,630,Concatenate fails with JAX backend,ricardoV94,"```python
import aesara.tensor as at

x = at.vector('x')
y = at.concatenate([x, x])

fn = aesara.function([x], y) 
fn([0.0, 0.0])
# array([0., 0., 0., 0.])

fn = aesara.function([x], y, mode=""JAX"")
fn([0.0, 0.0])
```

```python

---------------------------------------------------------------------------

ConcretizationTypeError                   Traceback (most recent call last)

~/Documents/Work/Labs/openlabs/venv/lib/python3.8/site-packages/aesara/link/utils.py in streamline_default_f()
    187                 ):
--> 188                     thunk()
    189                     for old_s in old_storage:

~/Documents/Work/Labs/openlabs/venv/lib/python3.8/site-packages/aesara/link/basic.py in thunk(fgraph, fgraph_jit, thunk_inputs, thunk_outputs)
    704         ):
--> 705             outputs = fgraph_jit(*[x[0] for x in thunk_inputs])
    706 

    [... skipping hidden 13 frame]

/tmp/tmprvqxtoqh in jax_funcified_fgraph(x)
      2 def jax_funcified_fgraph(x):
----> 3     auto_20135 = join(auto_20134, x, x)
      4     return (auto_20135,)

~/Documents/Work/Labs/openlabs/venv/lib/python3.8/site-packages/aesara/link/jax/dispatch.py in join(axis, *tensors)
    730             ndim = tensors[0].ndim
--> 731             if axis < -ndim:
    732                 raise IndexError(

    [... skipping hidden 1 frame]

~/Documents/Work/Labs/openlabs/venv/lib/python3.8/site-packages/jax/core.py in error(self, arg)
   1006   def error(self, arg):
-> 1007     raise ConcretizationTypeError(arg, fname_context)
   1008   return error

ConcretizationTypeError: Abstract tracer value encountered where concrete value is expected: Traced<ShapedArray(bool[])>with<DynamicJaxprTrace(level=0/1)>
The problem arose with the `bool` function. 
While tracing the function jax_funcified_fgraph at /tmp/tmprvqxtoqh:2 for jit, this value became a tracer due to JAX operations on these lines:

  operation a:i8[] = convert_element_type[new_dtype=int8 weak_type=False] b
    from line /home/ricardo/Documents/Work/Labs/openlabs/venv/lib/python3.8/site-packages/aesara/link/jax/dispatch.py:731 (join)

  operation a:bool[] = lt b c
    from line /home/ricardo/Documents/Work/Labs/openlabs/venv/lib/python3.8/site-packages/aesara/link/jax/dispatch.py:731 (join)

See https://jax.readthedocs.io/en/latest/errors.html#jax.errors.ConcretizationTypeError


During handling of the above exception, another exception occurred:

AttributeError                            Traceback (most recent call last)

/tmp/ipykernel_26964/1495567573.py in <module>
      1 fn = aesara.function([x], y, mode=""JAX"")
----> 2 fn(np.array([0.0, 0.0]))
      3 

~/Documents/Work/Labs/openlabs/venv/lib/python3.8/site-packages/aesara/compile/function/types.py in __call__(self, *args, **kwargs)
    974         try:
    975             outputs = (
--> 976                 self.fn()
    977                 if output_subset is None
    978                 else self.fn(output_subset=output_subset)

~/Documents/Work/Labs/openlabs/venv/lib/python3.8/site-packages/aesara/link/utils.py in streamline_default_f()
    190                         old_s[0] = None
    191             except Exception:
--> 192                 raise_with_op(fgraph, node, thunk)
    193 
    194         f = streamline_default_f

~/Documents/Work/Labs/openlabs/venv/lib/python3.8/site-packages/aesara/link/utils.py in raise_with_op(fgraph, node, thunk, exc_info, storage_map)
    515 
    516     try:
--> 517         exc_value = exc_type(
    518             str(exc_value) + detailed_err_msg + ""\\n"" + ""\\n"".join(hints)
    519         )

~/Documents/Work/Labs/openlabs/venv/lib/python3.8/site-packages/jax/_src/errors.py in __init__(self, tracer, context)
    148   def __init__(self, tracer: ""core.Tracer"", context: str = """"):
    149     super().__init__(
--> 150         ""Abstract tracer value encountered where concrete value is expected: ""
    151         f""{tracer}\\n{context}{tracer._origin_msg()}\\n"")
    152 

AttributeError: 'str' object has no attribute '_origin_msg'
```

Versions:
* Aesara: main
* JAX: 0.2.22",bug JAX,,2021-10-25 13:17:31,2021-10-27 04:45:40,"ricardoV94 labeled 2021-10-25 13:17:31,ricardoV94 labeled 2021-10-25 13:17:31,ricardoV94 closed 2021-10-27 04:45:40",ricardoV94,1
527,633,Adding a new dimension via `None` indexing fails when some dimensions are implied,ricardoV94,"```python
import aesara.tensor as at

x = at.matrix('x')
x[None, :] 
```
```
ValueError: ('You cannot drop a non-broadcastable dimension.', ([False, False], ['x']))
```
Same for:
```python
x[None]
x[:, None]
```

While any of these work:
```python
x[None, ...]
x[None, :, :]
at.shape_padleft(x)

x[:, None, :]
at.shape_padaxis(x, 1)

x[..., None]
x[:, :, None]
at.shape_padright(x)
```
",bug important NumPy compatibility,,2021-10-28 15:36:15,2021-10-29 06:07:33,"ricardoV94 renamed 2021-10-28 15:40:40,ricardoV94 labeled 2021-10-28 15:41:52,ricardoV94 labeled 2021-10-28 15:42:36,brandonwillard labeled 2021-10-29 03:56:18,brandonwillard referenced 2021-10-29 04:45:26,brandonwillard closed 2021-10-29 06:07:33,brandonwillard referenced 2021-10-29 06:07:34",ricardoV94 brandonwillard,0
530,638,RandomType pickling issues,ricardoV94,"This problem was seen in https://github.com/pymc-devs/pymc/issues/5090


```python
import numpy as np
import aesara
# import cloudpickle as pickle  # either library fails
import pickle

# same problem with aesara.shared(np.random.RandomState())
var1 = aesara.shared(np.random.default_rng())
var2 = aesara.shared(np.random.default_rng())
print(var2)  # RandomGeneratorSharedVariable(<Generator(PCG64) at 0x7FC441E6EAC0>)

res = var1.type.filter_variable(var2)
print(res)  # RandomGeneratorSharedVariable(<Generator(PCG64) at 0x7FC441E6EAC0>)

var2 = pickle.loads(pickle.dumps(var2))
print(var2)  # RandomGeneratorSharedVariable(<Generator(PCG64) at 0x7FC441E853C0>)

res = var1.type.filter_variable(var2)  # ! raises TypeError
```

```python
TypeError: Cannot convert Type RandomGeneratorType (of Variable <RandomGeneratorType>) into Type RandomGeneratorType. You can try to manually convert <RandomGeneratorType> into a RandomGeneratorType.
```",bug,brandonwillard,2021-10-29 12:28:10,2021-10-30 10:40:02,"ricardoV94 renamed 2021-10-29 12:29:51,ricardoV94 labeled 2021-10-29 12:35:53,brandonwillard unlabeled 2021-10-29 23:19:26,brandonwillard labeled 2021-10-29 23:19:26,brandonwillard assigned 2021-10-29 23:19:38,twiecki closed 2021-10-30 10:40:02",ricardoV94 twiecki brandonwillard,1
534,644,Fix flaky RNG usage in tests,brandonwillard,"All tests that use `tests.tensor.utils.test_rng`&mdash;and the helper functions that rely on it (e.g. `tests.tensor.utils.[random, random_nonzero, integers]`, etc.)&mdash;need to be refactor so that each individual unit test creates and uses its own RNG state from fixed seeds.

Currently, every test that relies on that RNG state is implicitly order dependent and flaky.  For example, the test results can differ when run individually, as a module, and all together, which makes localized work inconsistent and slow, because an entire run of the test suite is necessary to determine whether or not they will pass in CI.",bug good first issue help wanted testing important refactor,,2021-11-01 18:33:33,2022-02-01 23:45:05,"brandonwillard labeled 2021-11-01 18:33:33,brandonwillard labeled 2021-11-01 18:33:33,brandonwillard labeled 2021-11-01 18:33:33,brandonwillard labeled 2021-11-01 18:33:33,brandonwillard labeled 2021-11-01 18:33:33,brandonwillard labeled 2021-11-01 18:33:46,brandonwillard mentioned 2022-02-01 20:16:37,brandonwillard subscribed 2022-02-01 20:16:37,brandonwillard closed 2022-02-01 23:45:05",aerubanov brandonwillard,2
535,646,Address `local_useless_alloc`'s shape inconsistency,brandonwillard,"The optimization `aesara.tensor.basic_opt.local_useless_alloc` will remove `Alloc`s from the graph when their first inputs are equal in type to their outputs.

For example, from `TestLocalCanonicalizeAlloc.test_basic`:
```python
import numpy as np

import aesara
import aesara.tensor as at


x = aesara.shared(np.random.standard_normal((3, 7)))
a = at.alloc(x, 6, 7)

a_f = aesara.function([], a)
a_f().shape
# (3, 7)
```

This optimization is simply inconsistent.  We can remove it and put some narrower cases in its place (e.g. when it's possible to confirm equality of the shapes), or we could at least add `Assert`s that check the shapes at run-time.",bug help wanted graph rewriting,,2021-11-02 01:09:08,2021-11-02 16:41:10,"brandonwillard labeled 2021-11-02 01:09:08,brandonwillard labeled 2021-11-02 01:09:08,brandonwillard labeled 2021-11-02 01:09:08,brandonwillard closed 2021-11-02 16:41:10",brandonwillard,0
537,648,Remove `TensorConstant.tag.unique_value`,brandonwillard,"`TensorConstant.tag.unique_value` is an unnecessary specialized field used in only a few places.  This field also overlaps with the logic in `get_scalar_constant_value` and `extract_constant`, making for yet another redundancy.  Since this redundancy is attached to a core type, its removal has some priority over the other two functions.

In general, it can be easily replaced by a simple helper function&mdash;one that could also replace some of the functionality provided by `get_scalar_constant_value` and `extract_constant`, both of which are also slated for removal/replacement (see [here](https://github.com/aesara-devs/aesara/pull/643#issue-770029728)).",good first issue help wanted refactor,,2021-11-02 20:15:05,2021-11-28 01:41:48,"brandonwillard labeled 2021-11-02 20:15:05,brandonwillard labeled 2021-11-02 20:15:22,brandonwillard labeled 2021-11-02 20:15:22,brandonwillard connected 2021-11-02 22:55:33,brandonwillard connected 2021-11-14 23:24:18,brandonwillard closed 2021-11-28 01:41:48",Carlosbogo brandonwillard,2
544,659,The case `n_steps = 0` is not implemented in the `scan` Op,rlouf,"When we try to use the `scan` Op with `n_steps=0` or an empty array for `sequences`, as in the following example :

```python
import aesara
import aesara.tensor as at


def body_fn(index, cumul):
    return cumul + index


values, _ = aesara.scan(
    fn=body_fn,
    sequences=at.as_tensor([]),
    outputs_info=[at.constant(10.0, dtype=""float64"")],
)

print(values.eval())
```
we get the following traceback:

```python
Traceback (most recent call last):
  File ""/home/remi/.virtualenvs/aehmc/lib/python3.9/site-packages/aesara/compile/function/types.py"", line 976, in __call__
    self.fn()
  File ""/home/remi/.virtualenvs/aehmc/lib/python3.9/site-packages/aesara/scan/op.py"", line 1450, in rval
    r = p(n, [x[0] for x in i], o)
  File ""/home/remi/.virtualenvs/aehmc/lib/python3.9/site-packages/aesara/scan/op.py"", line 1409, in p
    return scan_perform_ext.perform(
  File ""scan_perform.pyx"", line 208, in aesara.scan.scan_perform.perform
NotImplementedError: n_steps == 0

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/home/remi/projects/ampersand/aehmc/example-isturning.py"", line 15, in <module>
    print(values.eval())
  File ""/home/remi/.virtualenvs/aehmc/lib/python3.9/site-packages/aesara/graph/basic.py"", line 553, in eval
    rval = self._fn_cache[inputs](*args)
  File ""/home/remi/.virtualenvs/aehmc/lib/python3.9/site-packages/aesara/compile/function/types.py"", line 989, in __call__
    raise_with_op(
  File ""/home/remi/.virtualenvs/aehmc/lib/python3.9/site-packages/aesara/link/utils.py"", line 526, in raise_with_op
    raise exc_value.with_traceback(exc_trace)
  File ""/home/remi/.virtualenvs/aehmc/lib/python3.9/site-packages/aesara/compile/function/types.py"", line 976, in __call__
    self.fn()
  File ""/home/remi/.virtualenvs/aehmc/lib/python3.9/site-packages/aesara/scan/op.py"", line 1450, in rval
    r = p(n, [x[0] for x in i], o)
  File ""/home/remi/.virtualenvs/aehmc/lib/python3.9/site-packages/aesara/scan/op.py"", line 1409, in p
    return scan_perform_ext.perform(
  File ""scan_perform.pyx"", line 208, in aesara.scan.scan_perform.perform
NotImplementedError: n_steps == 0
Apply node that caused the error: forall_inplace,cpu,scan_fn}(TensorConstant{0}, TensorConstant{[]}, IncSubtensor{InplaceSet;:int64:}.0)
Toposort index: 2
Inputs types: [TensorType(int64, scalar), TensorType(float64, vector), TensorType(float64, vector)]
Inputs shapes: [(), (0,), (1,)]
Inputs strides: [(), (8,), (8,)]
Inputs values: [array(0), array([], dtype=float64), array([10.])]
Outputs clients: [[Subtensor{int64::}(forall_inplace,cpu,scan_fn}.0, ScalarConstant{1})]]
```

where we would instead expect the code to return:

```
10.
```

It is very problematic for `aehmc` where we are not able to treat corner cases (that do appear often) properly.

## Versions and main components

* Aesara version: 2.2.6
* Python version: 3.9.5
* Operating system: Arch
* How did you install Aesara: pip",bug important Scan,,2021-11-09 09:21:07,2022-04-11 21:50:17,"rlouf labeled 2021-11-09 09:23:16,rlouf labeled 2021-11-09 09:23:16,ricardoV94 labeled 2021-11-09 11:54:50,brandonwillard closed 2022-04-11 21:50:17",ricardoV94 rlouf brandonwillard,0
545,660,tutorials mistake?,jguerra-astro,"## Hello all, i was reading through the example Ops (https://aesara.readthedocs.io/en/latest/extending/extending_aesara.html) and was confused by the grad() function in the example provided below.

```python
import aesara
from aesara.graph.op import Op
from aesara.graph.basic import Apply


class AXPBOp(Op):
    """"""
    This creates an Op that takes x to a*x+b.
    """"""
    __props__ = (""a"", ""b"")

    def __init__(self, a, b):
        self.a = a
        self.b = b
        super().__init__()

    def make_node(self, x):
        x = aesara.tensor.as_tensor_variable(x)
        return Apply(self, [x], [x.type()])

    def perform(self, node, inputs, output_storage):
        x = inputs[0]
        z = output_storage[0]
        z[0] = self.a * x + self.b

    def infer_shape(self, fgraph, node, i0_shapes):
        return i0_shapes

    def grad(self, inputs, output_grads):
        return [a * output_grads[0] + b]
```

 After asking on the pymc discourse forum It seems that this is a mistake and the grad function should be

```python
    def grad(self, inputs, output_grads):
        return [self.a * output_grads[0]]
```
",,,2021-11-09 16:48:28,2021-11-09 21:33:20,"jguerra-astro mentioned 2021-11-09 17:14:36,jguerra-astro subscribed 2021-11-09 17:14:36,brandonwillard closed 2021-11-09 21:33:20",jguerra-astro kc611 brandonwillard,2
546,661,Numba implementation of `RandomState` cannot update values in Graph. ,kc611,"## Description of your problem or feature request

**Please provide a minimal, self-contained, and reproducible example.**
```python
import aesara  
aesara.config.mode = ""NUMBA""  
import aesara.tensor as at  
import numpy as np

rng = aesara.shared(np.random.RandomState(1))
rng_new = aesara.shared(np.random.RandomState(2))
x = at.random.normal(rng=rng)
# This will first throw an error in DeepCopyOp but that can be bypassed by removing the conditional check.
aesara.function([],x,updates={rng:rng_new})()  
```
The issue is that we essentially replace the `RandomStateType` of any given graph with Numba's internal state array in:

https://github.com/aesara-devs/aesara/blob/7c32fc48cc971889242c8a124ea577d8618f0ef2/aesara/link/numba/dispatch/random.py#L25-L30

However this state array isn't recognized by `RandomStateType.filter()` as a valid value. Hence it cannot update the rng with a new one.",bug important Numba,,2021-11-09 16:57:00,2021-11-21 21:58:44,"brandonwillard labeled 2021-11-14 23:46:59,brandonwillard labeled 2021-11-14 23:47:06,brandonwillard labeled 2021-11-14 23:47:18,brandonwillard milestoned 2021-11-18 20:59:27,brandonwillard closed 2021-11-21 21:58:44",kc611 brandonwillard,0
547,662,`RandomVariable`s fail in Numba mode due to default `Generator`s,kc611,"## Description of your problem or feature request

```python
import aesara  
aesara.config.mode = ""NUMBA""  
import aesara.tensor as at  

at.random.normal().eval()  # Fails
```

```python
import aesara  
aesara.config.mode = ""NUMBA""  
import aesara.tensor as at  

rng = np.random.RandomState()
at.random.normal(rng=rng).eval()  # Works
```

Any user will try the first implementation which will inevitably fail until Numba implements support for Numpy's `Generator`s. We should be informing the users to use the `RandomState` API whenever working with Numba backend. Alternatively, we can some implicit conversion for `Generator` to Numba's internal state array conversion similar to how we do with `RandomState`s

https://github.com/aesara-devs/aesara/blob/7c32fc48cc971889242c8a124ea577d8618f0ef2/aesara/link/numba/dispatch/random.py#L25-L30",bug important Numba request discussion,,2021-11-09 17:04:39,2022-09-21 21:42:58,"brandonwillard labeled 2021-11-14 23:44:06,brandonwillard labeled 2021-11-14 23:44:06,brandonwillard labeled 2021-11-14 23:44:06,brandonwillard renamed 2021-11-14 23:45:27,brandonwillard labeled 2021-12-11 00:15:51,brandonwillard closed 2022-09-21 21:42:58",kc611 brandonwillard,2
550,665,Add a shape check to MvNormal when cov and mean are of different length,Sayam753,"## Description of your problem or feature request

When mean and cov parameters for MvNormal distribution are not of same length, a long traceback is shown. A shape check can be added to improve the error message, to replicate NumPy's behaviour.

**Please provide a minimal, self-contained, and reproducible example.**
```python
>>> import numpy as np
>>> np.__version__
'1.21.2'
>>> rng = np.random.default_rng()
>>> rng.multivariate_normal(mean=np.ones(1), cov=np.eye(3))
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""_generator.pyx"", line 3590, in numpy.random._generator.Generator.multivariate_normal
ValueError: mean and cov must have same length

>>> import aesara
>>> aesara.__version__
'2.2.6'
>>> from aesara.tensor.random.basic import multivariate_normal
>>> multivariate_normal(mean=np.zeros(1), cov=np.eye(3)).eval()
```

**Please provide the full traceback of any errors.**
<details>
<summary>Traceback</summary>
<p>

```python 
Traceback (most recent call last):
  File ""/Users/user/miniconda3/envs/pymc_v4/lib/python3.7/site-packages/aesara/compile/function/types.py"", line 977, in __call__
    if output_subset is None
  File ""/Users/user/miniconda3/envs/pymc_v4/lib/python3.7/site-packages/aesara/graph/op.py"", line 500, in rval
    r = p(n, [x[0] for x in i], o)
  File ""/Users/user/miniconda3/envs/pymc_v4/lib/python3.7/site-packages/aesara/tensor/random/op.py"", line 392, in perform
    smpl_val = self.rng_fn(rng, *(args + [size]))
  File ""/Users/user/miniconda3/envs/pymc_v4/lib/python3.7/site-packages/aesara/tensor/random/basic.py"", line 348, in rng_fn
    return safe_multivariate_normal(mean, cov, size=size, rng=rng)
  File ""/Users/user/miniconda3/envs/pymc_v4/lib/python3.7/site-packages/aesara/tensor/random/basic.py"", line 299, in safe_multivariate_normal
    stats.multivariate_normal(mean=mean, cov=cov, allow_singular=True).rvs(
  File ""/Users/user/miniconda3/envs/pymc_v4/lib/python3.7/site-packages/scipy/stats/_multivariate.py"", line 362, in __call__
    seed=seed)
  File ""/Users/user/miniconda3/envs/pymc_v4/lib/python3.7/site-packages/scipy/stats/_multivariate.py"", line 729, in __init__
    None, mean, cov)
  File ""/Users/user/miniconda3/envs/pymc_v4/lib/python3.7/site-packages/scipy/stats/_multivariate.py"", line 400, in _process_parameters
    cov.shape = (1, 1)
ValueError: cannot reshape array of size 9 into shape (1,1)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/Users/user/miniconda3/envs/pymc_v4/lib/python3.7/site-packages/aesara/graph/basic.py"", line 553, in eval
    rval = self._fn_cache[inputs](*args)
  File ""/Users/user/miniconda3/envs/pymc_v4/lib/python3.7/site-packages/aesara/compile/function/types.py"", line 993, in __call__
    storage_map=getattr(self.fn, ""storage_map"", None),
  File ""/Users/user/miniconda3/envs/pymc_v4/lib/python3.7/site-packages/aesara/link/utils.py"", line 526, in raise_with_op
    raise exc_value.with_traceback(exc_trace)
  File ""/Users/user/miniconda3/envs/pymc_v4/lib/python3.7/site-packages/aesara/compile/function/types.py"", line 977, in __call__
    if output_subset is None
  File ""/Users/user/miniconda3/envs/pymc_v4/lib/python3.7/site-packages/aesara/graph/op.py"", line 500, in rval
    r = p(n, [x[0] for x in i], o)
  File ""/Users/user/miniconda3/envs/pymc_v4/lib/python3.7/site-packages/aesara/tensor/random/op.py"", line 392, in perform
    smpl_val = self.rng_fn(rng, *(args + [size]))
  File ""/Users/user/miniconda3/envs/pymc_v4/lib/python3.7/site-packages/aesara/tensor/random/basic.py"", line 348, in rng_fn
    return safe_multivariate_normal(mean, cov, size=size, rng=rng)
  File ""/Users/user/miniconda3/envs/pymc_v4/lib/python3.7/site-packages/aesara/tensor/random/basic.py"", line 299, in safe_multivariate_normal
    stats.multivariate_normal(mean=mean, cov=cov, allow_singular=True).rvs(
  File ""/Users/user/miniconda3/envs/pymc_v4/lib/python3.7/site-packages/scipy/stats/_multivariate.py"", line 362, in __call__
    seed=seed)
  File ""/Users/user/miniconda3/envs/pymc_v4/lib/python3.7/site-packages/scipy/stats/_multivariate.py"", line 729, in __init__
    None, mean, cov)
  File ""/Users/user/miniconda3/envs/pymc_v4/lib/python3.7/site-packages/scipy/stats/_multivariate.py"", line 400, in _process_parameters
    cov.shape = (1, 1)
ValueError: cannot reshape array of size 9 into shape (1,1)
Apply node that caused the error: multivariate_normal_rv{1, (1, 2), floatX, False}(RandomGeneratorSharedVariable(<Generator(PCG64) at 0x7FD24FC89230>), TensorConstant{[]}, TensorConstant{11}, TensorConstant{(1,) of 0.0}, TensorConstant{[[1. 0. 0...0. 0. 1.]]})
Toposort index: 0
Inputs types: [RandomGeneratorType, TensorType(int64, vector), TensorType(int64, scalar), TensorType(float64, (True,)), TensorType(float64, matrix)]
Inputs shapes: ['No shapes', (0,), (), (1,), (3, 3)]
Inputs strides: ['No strides', (8,), (), (8,), (24, 8)]
Inputs values: [Generator(PCG64) at 0x7FD24FC89230, array([], dtype=int64), array(11), array([0.]), 'not shown']
Outputs clients: [[], ['output']]

Backtrace when the node is created (use Aesara flag traceback__limit=N to make it longer):
  File ""<stdin>"", line 1, in <module>

HINT: Use the Aesara flag `exception_verbosity=high` for a debug print-out and storage map footprint of this Apply node.

```

</p>
</details>


## Versions and main components

* Python version: 3.7.7
* Operating system: Mac OS 11.6
* How did you install Aesara: pip
",question request discussion,,2021-11-14 05:40:31,2021-11-19 12:33:24,"Sayam753 renamed 2021-11-14 05:42:05,brandonwillard labeled 2021-11-14 23:34:22,brandonwillard labeled 2021-11-14 23:41:39,aesara-devs locked 2021-11-19 12:33:24,brandonwillard closed 2021-11-19 12:33:24",ricardoV94 aesara-devs Sayam753 brandonwillard,7
552,668,`aesara.tensor.slinalg.Solve` doesn't match SciPy's `solve`,fshart,"## Change to aet.slinalg.Solve.perform to remove currently existing bug
The following mwe 
```python
import aesara.tensor as aet
import aesara
import numpy
import numpy as np


def main():
    b_dev = aet.matrix('b_dev')
    M_dev = aet.matrix('M_dev')
    sym_solve = aet.slinalg.Solve(assume_a='sym')
    sym_solve = sym_solve(M_dev, b_dev)
    sym_solve = aesara.function(
        inputs=[M_dev, b_dev],
        outputs=sym_solve,
    )
    Hc = numpy.array([[1, 0, 0], [0, 1, 1], [0, 1, 0]])
    g = numpy.array([[1], [1], [1]])
    dz = sym_solve(Hc, g)


if __name__ == ""__main__"":
    main()
```
results in an error
```python
 AESARA_FLAGS='exception_verbosity=high' python mwe_solve.py 
Traceback (most recent call last):
  File ""*/lib/python3.9/site-packages/aesara/compile/function/types.py"", line 976, in __call__
    self.fn()
  File ""*/lib/python3.9/site-packages/aesara/graph/op.py"", line 500, in rval
    r = p(n, [x[0] for x in i], o)
  File ""*/lib/python3.9/site-packages/aesara/tensor/slinalg.py"", line 317, in perform
    rval = scipy.linalg.solve_triangular(
  File ""*/lib/python3.9/site-packages/scipy/linalg/basic.py"", line 361, in solve_triangular
    raise LinAlgError(""singular matrix: resolution failed at diagonal %d"" %
numpy.linalg.LinAlgError: singular matrix: resolution failed at diagonal 2

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""**/mwe_solve.py"", line 22, in <module>
    main()
  File ""**/mwe_solve.py"", line 18, in main
    dz = sym_solve(Hc, g)
  File ""*/lib/python3.9/site-packages/aesara/compile/function/types.py"", line 989, in __call__
    raise_with_op(
  File ""*/lib/python3.9/site-packages/aesara/link/utils.py"", line 526, in raise_with_op
    raise exc_value.with_traceback(exc_trace)
  File ""*/lib/python3.9/site-packages/aesara/compile/function/types.py"", line 976, in __call__
    self.fn()
  File ""*/lib/python3.9/site-packages/aesara/graph/op.py"", line 500, in rval
    r = p(n, [x[0] for x in i], o)
  File ""*/lib/python3.9/site-packages/aesara/tensor/slinalg.py"", line 317, in perform
    rval = scipy.linalg.solve_triangular(
  File ""*/lib/python3.9/site-packages/scipy/linalg/basic.py"", line 361, in solve_triangular
    raise LinAlgError(""singular matrix: resolution failed at diagonal %d"" %
numpy.linalg.LinAlgError: singular matrix: resolution failed at diagonal 2
Apply node that caused the error: Solve{assume_a='sym', lower=False, check_finite=True}(M_dev, b_dev)
Toposort index: 0
Inputs types: [TensorType(float64, matrix), TensorType(float64, matrix)]
Inputs shapes: [(3, 3), (3, 1)]
Inputs strides: [(24, 8), (8, 8)]
Inputs values: ['not shown', array([[1.],
       [1.],
       [1.]])]
Inputs type_num: [12, 12]
Outputs clients: [['output']]

Backtrace when the node is created (use Aesara flag traceback__limit=N to make it longer):
  File ""**/mwe_solve.py"", line 22, in <module>
    main()
  File ""**/mwe_solve.py"", line 11, in main
    sym_solve = sym_solve(M_dev, b_dev)

Debug print of the apply node: 
Solve{assume_a='sym', lower=False, check_finite=True} [id A] <TensorType(float64, matrix)> ''   
 |M_dev [id B] <TensorType(float64, matrix)>
 |b_dev [id C] <TensorType(float64, matrix)>

Storage map footprint:
 - M_dev, Input, Shape: (3, 3), ElemSize: 8 Byte(s), TotalSize: 72 Byte(s)
 - b_dev, Input, Shape: (3, 1), ElemSize: 8 Byte(s), TotalSize: 24 Byte(s)
 TotalSize: 96 Byte(s) 0.000 GB
 TotalSize inputs: 96 Byte(s) 0.000 GB
```
where * is the path to the environment and ** the path to the mwe.

The matrix Hc is symmetric and non-degenerate, but has a zero on the main diagonal (and is negative definite).

I traced the error to the definition of aet.slinalg.Solve.perform, and the usage of 
```python
 rval = scipy.linalg.solve_triangular(
                A,
                b,
                lower=self.lower,
                check_finite=self.check_finite,
                # trans=trans
            )
```
directly on Hc and g gave the same error. Permuting the matrix, i.e. changing the position of the zero on the diagonal, corresponded to a change in the position where the function failed
```python
numpy.linalg.LinAlgError: singular matrix: resolution failed at diagonal 1
```

Shortening the whole perform function to
```python
    def perform(self, node, inputs, output_storage):
        A, b = inputs
        output_storage[0][0] = scipy.linalg.solve(
            A,
            b,
            assume_a=self.assume_a,
            lower=self.lower,
            check_finite=self.check_finite,
            # transposed=self.transposed,
        )
```
solved the error.

## Versions and main components

* Aesara version: 2.2.6
* Python version: 3.9.7
* How did you install Aesara: pip
",good first issue help wanted SciPy compatibility,,2021-11-17 19:26:03,2021-12-13 21:47:12,"brandonwillard closed 2021-11-18 04:37:21,brandonwillard labeled 2021-11-18 04:37:41,brandonwillard unlabeled 2021-11-18 14:26:26,brandonwillard labeled 2021-11-18 14:26:26,brandonwillard labeled 2021-11-18 14:26:26,brandonwillard labeled 2021-11-18 14:26:26,brandonwillard reopened 2021-11-18 14:26:35,brandonwillard renamed 2021-11-18 14:27:41,brandonwillard renamed 2021-11-18 14:28:05,fshart closed 2021-11-18 14:34:54,fshart reopened 2021-11-18 14:35:45,fshart referenced 2021-11-24 18:25:35,brandonwillard connected 2021-11-25 19:33:27,fshart referenced 2021-12-03 11:23:41,brandonwillard closed 2021-12-13 21:47:13",fshart brandonwillard,8
556,674,Remove/rename `aesara.tensor.nnet` sub-package,twiecki,"I think having `nnet` as a name is pretty confusing, these are general math functions not only used for neural nets, and by now Aesara isn't geared towards neural nets anyway. Maybe we can rename (to what?) or, better I think, just move it down to the regular name space.",help wanted refactor,,2021-11-24 15:04:03,2022-10-17 23:08:57,"twiecki labeled 2021-11-24 15:04:03,twiecki labeled 2021-11-24 15:04:03,brandonwillard unlabeled 2021-11-25 00:42:50,brandonwillard labeled 2021-11-25 00:42:50,brandonwillard renamed 2021-12-11 00:16:20,rlouf connected 2022-09-15 20:32:00,brandonwillard closed 2022-10-17 23:08:57",rth rlouf brandonwillard ricardoV94 canyon289 twiecki,8
559,677,Duplicate values in `CAReduce`'s `axis` argument not properly handled by Numba and C backends,ricardoV94,"```python
import aesara
import aesara.tensor as at

x = at.matrix('x')
y = x.sum(axis=(0, 0))

# Python
f = aesara.function([x], y, mode=""FAST_COMPILE"")
f(np.ones((3, 3)))  # ValueError: duplicate value in 'axis'

# C
f = aesara.function([x], y, mode=""FAST_RUN"")  # aesara.link.c.exceptions.CompileError: Compilation failed (return status=1):

# NUMBA
f = aesara.function([x], y, mode=""NUMBA"")
f(np.ones((3, 3)))  # 9.0

y = x.sum(axis=(1, 1))
f = aesara.function([x], y, mode=""NUMBA"")
f(np.ones((3, 3))) # 9.0

# JAX
f = aesara.function([x], y, mode=""JAX"")
f(np.ones((3, 3)))  # ValueError: duplicate value in 'axis': (1, 1)
```

Since `CAReduce` does not allow for symbolic axis, it should be easy to check for duplicates in `make_node` or `__init__`",bug help wanted C-backend Numba,ricardoV94,2021-11-25 17:08:45,2022-03-23 21:55:37,"ricardoV94 renamed 2021-11-25 17:08:59,ricardoV94 labeled 2021-11-25 17:12:32,brandonwillard labeled 2021-11-26 03:05:37,brandonwillard labeled 2021-11-26 03:05:37,brandonwillard labeled 2021-11-26 03:05:43,brandonwillard connected 2021-12-04 19:36:37,brandonwillard renamed 2021-12-11 00:18:31,ricardoV94 assigned 2022-02-24 11:58:56,ricardoV94 referenced 2022-02-24 12:09:48,ricardoV94 referenced 2022-02-24 12:27:28,ricardoV94 referenced 2022-02-25 10:29:18,ricardoV94 referenced 2022-02-25 10:34:01,ricardoV94 referenced 2022-02-25 10:36:11,ricardoV94 referenced 2022-03-02 16:26:38,ricardoV94 referenced 2022-03-02 19:17:52,ricardoV94 referenced 2022-03-03 10:32:48,ricardoV94 referenced 2022-03-13 14:57:59,ricardoV94 referenced 2022-03-14 17:57:02,ricardoV94 referenced 2022-03-16 17:25:52,ricardoV94 referenced 2022-03-17 08:51:40,ricardoV94 referenced 2022-03-21 18:00:22,ricardoV94 referenced 2022-03-23 11:34:13,ricardoV94 closed 2022-03-23 21:55:37",ricardoV94 brandonwillard,0
564,686,categorical does not validate probabilities,ricardoV94,"The categorical seems to do a simple draw from the inverse cdf, and is oblivious to invalid probabilities, 

```python
import numpy as np
import aesara.tensor.random as atr

atr.categorical([1, 1, 1], size=100)  # Only 0s
atr.categorical([0, 0, 0], size=100)  # Only 3s
```
",question,,2021-12-02 15:15:50,2021-12-03 07:43:37,"brandonwillard labeled 2021-12-02 17:16:10,brandonwillard closed 2021-12-02 17:23:55,ricardoV94 reopened 2021-12-03 07:07:13,aesara-devs locked 2021-12-03 07:43:37,brandonwillard converted_to_discussion 2021-12-03 07:43:37",ricardoV94 aesara-devs brandonwillard,4
566,689,Use constant names when specified,ricardoV94,"```python
import aesara.tensor as at

z = at.constant(1, name='z')

print(z)  
# TensorConstant{1}

aesara.dprint(z)  
# TensorConstant{1} [id A]
```

In contrast:
```python
z = at.scalar('z')

print(z)  
# z

aesara.dprint(z) 
# z [id A]
```

Or:
```python
z = at.constant(1, name='z')
y = z + 1; 
y.name = 'y'

print(y)  
# y

aesara.dprint(y)
# Elemwise{add,no_inplace} [id A] 'y'   
#  |TensorConstant{1} [id B] <-- We could show 'z' here
#  |TensorConstant{1} [id C]
```

",bug good first issue help wanted,,2021-12-05 12:00:12,2022-01-02 01:39:14,"brandonwillard labeled 2021-12-11 00:08:28,brandonwillard labeled 2021-12-11 00:08:28,brandonwillard labeled 2021-12-11 00:09:06,oscargus referenced 2022-01-01 23:41:01,oscargus referenced 2022-01-01 23:57:17,brandonwillard connected 2022-01-02 01:36:24,brandonwillard closed 2022-01-02 01:39:15",oscargus ricardoV94 brandonwillard,0
567,692,Wrong broadcastable output of Alloc when using SpecifyShape,ricardoV94,"```python
import aesara.tensor as at

constant_size = at.constant([1])
specify_size = at.specify_shape(constant_size, [1])

print(constant_size.type)  # TensorType(int32, (True,))
print(specify_size.type)  # TensorType(int32, (True,))

print(at.alloc(0, *constant_size).type)  # TensorType(int8, (True,))
print(at.alloc(0, *specify_size).type)  # TensorType(int8, vector)
```",bug,brandonwillard,2021-12-08 18:01:06,2021-12-09 22:36:39,"brandonwillard labeled 2021-12-08 19:43:55,brandonwillard referenced 2021-12-09 21:36:26,brandonwillard assigned 2021-12-09 21:37:59,brandonwillard closed 2021-12-09 22:36:39,brandonwillard referenced 2021-12-09 22:36:40",ricardoV94 brandonwillard,0
573,699,`DimShuffle`'s C code does not correctly handle `ndarray` views,brandonwillard,"Here's a simple example:
```python
import aesara.tensor as at


at.broadcast_to(0, (2, 3))[None].eval()
# array([[[  0, 122, -76],
#         [-88,  49,  86]]], dtype=int8)
```

Here's the same result, just computed more directly via the generated C thunk:
```python
import numpy as np
import aesara


x_at = at.vector()
x_fn = aesara.function([x_at], x_at[None])

x_fn.fn.fgraph.toposort()
# [InplaceDimShuffle{x,0}(<TensorType(float64, vector)>),
#  DeepCopyOp(InplaceDimShuffle{x,0}.0)]

dimshuffle_thunk = x_fn.fn.thunks[0]

dimshuffle_thunk
# <function aesara.graph.op.COp.make_c_thunk.<locals>.rval()>

dimshuffle_thunk.inputs[0][0] = np.broadcast_to(0, (6,))
dimshuffle_thunk()
dimshuffle_thunk.outputs[0][0]
# array([[              0,               0, 140009315547216,
#                      33,               0,               0]])

np.shares_memory(dimshuffle_thunk.inputs[0][0], dimshuffle_thunk.outputs[0][0])
# True
```",bug important C-backend,,2021-12-14 02:43:02,2021-12-15 21:29:15,"brandonwillard labeled 2021-12-14 02:43:02,brandonwillard labeled 2021-12-14 02:43:02,brandonwillard labeled 2021-12-14 02:43:02,brandonwillard closed 2021-12-15 21:29:16",brandonwillard,1
576,702,Errors with `jax.random.normal`'s `shape` parameter in version 0.2.26,brandonwillard,"Version 0.2.26 of JAX is causing new ""omnistaging""-like errors in `tests.link.test_jax:test_random_stats`.  Here's the relevant portion of the traceback:
```python
...
aesara/link/basic.py:705: in thunk
    outputs = fgraph_jit(*[x[0] for x in thunk_inputs])
/tmp/user/1000/tmprconxvmr:3: in jax_funcified_fgraph
    auto_41, auto_42 = random_variable(auto_36, auto_37, auto_38, auto_39, auto_40)
aesara/link/jax/dispatch.py:1054: in random_variable
    data = getattr(jax.random, name)(key=prng, shape=size)
../../../../apps/anaconda3/envs/aesara-3.7/lib/python3.7/site-packages/jax/_src/random.py:522: in normal
    shape = core.as_named_shape(shape)
E   TypeError: Shapes must be 1D sequences of concrete values of integer type, got (Traced<ShapedArray(int64[])>with<DynamicJaxprTrace(level=0/1)>,).
E   If using `jit`, try using `static_argnums` or applying `jit` to smaller subfunctions.
...
```
Oddly, in this case the actual `shape` value is `DeviceArray([10000], dtype=int64)`.  Converting it to back to a non-`DeviceArray` (e.g. `jax.device_get`) causes the failing code to work again.",bug help wanted JAX,,2021-12-15 06:03:33,2022-12-10 07:34:36,"brandonwillard labeled 2021-12-15 06:03:33,brandonwillard labeled 2021-12-15 06:03:33,brandonwillard labeled 2022-09-15 19:08:52,rlouf closed 2022-12-10 07:34:36",rlouf brandonwillard,2
577,703,Scan example uses old `T` alias,ricardoV94,"https://aesara.readthedocs.io/en/latest/library/scan.html#simple-loop-with-accumulation-computing-a-k

> outputs_info=T.ones_like(A)",documentation good first issue help wanted refactor,,2021-12-16 11:45:59,2022-01-03 18:53:10,"ricardoV94 labeled 2021-12-16 11:45:59,brandonwillard labeled 2021-12-16 16:53:52,brandonwillard labeled 2021-12-16 16:54:09,brandonwillard labeled 2021-12-16 16:54:09,oscargus referenced 2022-01-03 15:56:41,twiecki closed 2022-01-03 18:53:10,twiecki referenced 2022-01-03 18:53:13",oscargus ricardoV94 twiecki brandonwillard,0
579,705,Your cuDNN version is more recent than Theano,probit2011,"I have been trying to enable CUDA to run PyMC3 with the assistance of the GPU. Here are the specs of the machine/software I have been using:

Windows 10
Visual Studio Community 2019
Python 3.8.12
CUDA 10.2 (I tried 11.2 before that and obtained the same problem)
CuDNN 7.6.5 (I tried 8.1 with CUDA 11.2 and obtained the same problem)
TensorFlow 2.7.0
Theano-PyMC 1.1.2
Aesara 2.3.2 (the successor to Theano)
PyMC3 3.11.4
MKL 2.4.0
For the proper installation of Theano and CUDA in a Windows environment, I followed the advice provided on these web pages:

https://gist.github.com/ElefHead/93becdc9e99f2a9e4d2525a59f64b574

https://towardsdatascience.com/installing-tensorflow-with-cuda-cudnn-and-gpu-support-on-windows-10-60693e46e781

I have tested the installation against Tensorflow and it works. I have also used the tests provided on the Theano and Aesara ""Read the Docs"" sites (https://aesara.readthedocs.io/en/latest/tutorial/using_gpu.html#testing-the-gpu) and ran the check_blas test provided with Theano/Aesara (https://raw.githubusercontent.com/Theano/Theano/master/theano/misc/check_blas.py). After all this, I still get these disappointing error/warning messages:

`WARNING (aesara.tensor.blas): Using NumPy C-API based implementation for BLAS functions.`

`UserWarning: Your cuDNN version is more recent than Aesara. If you encounter problems, try updating Aesara or downgrading cuDNN to a version >= v5 and <= v7`
even though I have already downgraded cuDNN to 7.6.5 (and, obviously, can't use the GPU with Theano/Aesara/PyMC3).

With respect to the BLAS warning, I tried setting the blas__ldflags (Aesara) or blas.ldflags (Theano) as environment variables, assigning them the recommended MKL flags `-lmkl_intel_lp64 -lmkl_intel_thread -lmkl_core -lguide -liomp5 -lmkl_mc -lpthread`, still nothing works.

Can anybody please help me address these two issues?",invalid question GPU,,2021-12-18 06:06:55,2021-12-18 19:26:08,"brandonwillard closed 2021-12-18 19:26:08,brandonwillard labeled 2021-12-18 19:26:30,brandonwillard labeled 2021-12-18 19:26:30,brandonwillard labeled 2021-12-18 19:26:30",brandonwillard probit2011,1
580,706,Graphs fail to compile when aesara is installed on a non-conda environment,zoj613,"Graphs fail to compile when aesara is installed on a non-conda environment. I verified this using an environment created with pyenv using python 3.7.

**Please provide a minimal, self-contained, and reproducible example.**
```python
import aesara.tensor as at

A = at.arange(5)
print(A.eval())
```

**Please provide the full traceback of any errors.**
```python
You can find the C code in this temporary file: /tmp/aesara_compilation_error_uqqs1_ou
Traceback (most recent call last):
  File ""/home/zoj/.local/share/pyenv/versions/polyagamma/lib/python3.7/site-packages/aesara/link/vm.py"", line 1112, in make_all
    node.op.make_thunk(node, storage_map, compute_map, [], impl=impl)
  File ""/home/zoj/.local/share/pyenv/versions/polyagamma/lib/python3.7/site-packages/aesara/graph/op.py"", line 690, in make_thunk
    return self.make_c_thunk(node, storage_map, compute_map, no_recycling)
  File ""/home/zoj/.local/share/pyenv/versions/polyagamma/lib/python3.7/site-packages/aesara/graph/op.py"", line 657, in make_c_thunk
    input_storage=node_input_storage, output_storage=node_output_storage
  File ""/home/zoj/.local/share/pyenv/versions/polyagamma/lib/python3.7/site-packages/aesara/link/c/basic.py"", line 1205, in make_thunk
    input_storage, output_storage, storage_map
  File ""/home/zoj/.local/share/pyenv/versions/polyagamma/lib/python3.7/site-packages/aesara/link/c/basic.py"", line 1143, in __compile__
    storage_map,
  File ""/home/zoj/.local/share/pyenv/versions/polyagamma/lib/python3.7/site-packages/aesara/link/c/basic.py"", line 1635, in cthunk_factory
    module = get_module_cache().module_from_key(key=key, lnk=self)
  File ""/home/zoj/.local/share/pyenv/versions/polyagamma/lib/python3.7/site-packages/aesara/link/c/cmodule.py"", line 1198, in module_from_key
    module = lnk.compile_cmodule(location)
  File ""/home/zoj/.local/share/pyenv/versions/polyagamma/lib/python3.7/site-packages/aesara/link/c/basic.py"", line 1551, in compile_cmodule
    preargs=preargs,
  File ""/home/zoj/.local/share/pyenv/versions/polyagamma/lib/python3.7/site-packages/aesara/link/c/cmodule.py"", line 2554, in compile_str
    f""Compilation failed (return status={status}):\\n{' '.join(cmd)}\\n{compile_stderr}""
aesara.link.c.exceptions.CompileError: Compilation failed (return status=1):
/usr/bin/g++ -shared -g -O3 -fno-math-errno -Wno-unused-label -Wno-unused-variable -Wno-write-strings -march=ivybridge -mmmx -mpopcnt -msse -msse2 -msse3 -mssse3 -msse4.1 -msse4.2 -mavx -mno-avx2 -mno-sse4a -mno-fma4 -mno-xop -mno-fma -mno-avx512f -mno-bmi -mno-bmi2 -mno-aes -mpclmul -mno-avx512vl -mno-avx512bw -mno-avx512dq -mno-avx512cd -mno-avx512er -mno-avx512pf -mno-avx512vbmi -mno-avx512ifma -mno-avx5124vnniw -mno-avx5124fmaps -mno-avx512vpopcntdq -mno-avx512vbmi2 -mno-gfni -mno-vpclmulqdq -mno-avx512vnni -mno-avx512bitalg -mno-avx512bf16 -mno-avx512vp2intersect -mno-3dnow -mno-adx -mno-abm -mno-cldemote -mno-clflushopt -mno-clwb -mno-clzero -mcx16 -mno-enqcmd -mf16c -mfsgsbase -mfxsr -mno-hle -msahf -mno-lwp -mno-lzcnt -mno-movbe -mno-movdir64b -mno-movdiri -mno-mwaitx -mno-pconfig -mno-pku -mno-prefetchwt1 -mno-prfchw -mno-ptwrite -mno-rdpid -mno-rdrnd -mno-rdseed -mno-rtm -mno-serialize -mno-sgx -mno-sha -mno-shstk -mno-tbm -mno-tsxldtrk -mno-vaes -mno-waitpkg -mno-wbnoinvd -mxsave -mno-xsavec -mxsaveopt -mno-xsaves -mno-amx-tile -mno-amx-int8 -mno-amx-bf16 -mno-uintr -mno-hreset -mno-kl -mno-widekl -mno-avxvnni --param l1-cache-size=32 --param l1-cache-line-size=64 --param l2-cache-size=3072 -mtune=ivybridge -DNPY_NO_DEPRECATED_API=NPY_1_7_API_VERSION -m64 -fPIC -I/home/zoj/.local/share/pyenv/versions/polyagamma/lib/python3.7/site-packages/numpy/core/include -I/home/zoj/.local/share/pyenv/versions/3.7.12/include/python3.7m -I/home/zoj/.local/share/pyenv/versions/polyagamma/lib/python3.7/site-packages/aesara/link/c/c_code -L/home/zoj/.pyenv/versions/3.7.12/lib -L/home/zoj/.local/share/pyenv/versions/3.7.12/lib -fvisibility=hidden -o /home/zoj/.aesara/compiledir_Linux-5.10--lts-x86_64-with-arch--3.7.12-64/tmp8k09vdys/mf6fae8a01551c3ac2d9a0aaba0ff89f2ec61ceb4a936a76263a7c5d4ea33e204.so /home/zoj/.aesara/compiledir_Linux-5.10--lts-x86_64-with-arch--3.7.12-64/tmp8k09vdys/mod.cpp -lpython3.7m
/usr/bin/ld: /home/zoj/.local/share/pyenv/versions/3.7.12/lib/libpython3.7m.a(classobject.o): warning: relocation against `PyInstanceMethod_Type' in read-only section `.text'
/usr/bin/ld: /home/zoj/.local/share/pyenv/versions/3.7.12/lib/libpython3.7m.a(longobject.o): relocation R_X86_64_PC32 against symbol `PyExc_OverflowError' can not be used when making a shared object; recompile with -fPIC
/usr/bin/ld: final link failed: bad value
collect2: error: ld returned 1 exit status


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/zoj/.local/share/pyenv/versions/polyagamma/lib/python3.7/site-packages/aesara/graph/basic.py"", line 550, in eval
    self._fn_cache[inputs] = function(inputs, self)
  File ""/home/zoj/.local/share/pyenv/versions/polyagamma/lib/python3.7/site-packages/aesara/compile/function/__init__.py"", line 350, in function
    output_keys=output_keys,
  File ""/home/zoj/.local/share/pyenv/versions/polyagamma/lib/python3.7/site-packages/aesara/compile/function/pfunc.py"", line 532, in pfunc
    output_keys=output_keys,
  File ""/home/zoj/.local/share/pyenv/versions/polyagamma/lib/python3.7/site-packages/aesara/compile/function/types.py"", line 1984, in orig_function
    fn = m.create(defaults)
  File ""/home/zoj/.local/share/pyenv/versions/polyagamma/lib/python3.7/site-packages/aesara/compile/function/types.py"", line 1840, in create
    input_storage=input_storage_lists, storage_map=storage_map
  File ""/home/zoj/.local/share/pyenv/versions/polyagamma/lib/python3.7/site-packages/aesara/link/basic.py"", line 285, in make_thunk
    storage_map=storage_map,
  File ""/home/zoj/.local/share/pyenv/versions/polyagamma/lib/python3.7/site-packages/aesara/link/vm.py"", line 1121, in make_all
    raise_with_op(fgraph, node)
  File ""/home/zoj/.local/share/pyenv/versions/polyagamma/lib/python3.7/site-packages/aesara/link/utils.py"", line 526, in raise_with_op
    raise exc_value.with_traceback(exc_trace)
  File ""/home/zoj/.local/share/pyenv/versions/polyagamma/lib/python3.7/site-packages/aesara/link/vm.py"", line 1112, in make_all
    node.op.make_thunk(node, storage_map, compute_map, [], impl=impl)
  File ""/home/zoj/.local/share/pyenv/versions/polyagamma/lib/python3.7/site-packages/aesara/graph/op.py"", line 690, in make_thunk
    return self.make_c_thunk(node, storage_map, compute_map, no_recycling)
  File ""/home/zoj/.local/share/pyenv/versions/polyagamma/lib/python3.7/site-packages/aesara/graph/op.py"", line 657, in make_c_thunk
    input_storage=node_input_storage, output_storage=node_output_storage
  File ""/home/zoj/.local/share/pyenv/versions/polyagamma/lib/python3.7/site-packages/aesara/link/c/basic.py"", line 1205, in make_thunk
    input_storage, output_storage, storage_map
  File ""/home/zoj/.local/share/pyenv/versions/polyagamma/lib/python3.7/site-packages/aesara/link/c/basic.py"", line 1143, in __compile__
    storage_map,
  File ""/home/zoj/.local/share/pyenv/versions/polyagamma/lib/python3.7/site-packages/aesara/link/c/basic.py"", line 1635, in cthunk_factory
    module = get_module_cache().module_from_key(key=key, lnk=self)
  File ""/home/zoj/.local/share/pyenv/versions/polyagamma/lib/python3.7/site-packages/aesara/link/c/cmodule.py"", line 1198, in module_from_key
    module = lnk.compile_cmodule(location)
  File ""/home/zoj/.local/share/pyenv/versions/polyagamma/lib/python3.7/site-packages/aesara/link/c/basic.py"", line 1551, in compile_cmodule
    preargs=preargs,
  File ""/home/zoj/.local/share/pyenv/versions/polyagamma/lib/python3.7/site-packages/aesara/link/c/cmodule.py"", line 2554, in compile_str
    f""Compilation failed (return status={status}):\\n{' '.join(cmd)}\\n{compile_stderr}""
aesara.link.c.exceptions.CompileError: Compilation failed (return status=1):
/usr/bin/g++ -shared -g -O3 -fno-math-errno -Wno-unused-label -Wno-unused-variable -Wno-write-strings -march=ivybridge -mmmx -mpopcnt -msse -msse2 -msse3 -mssse3 -msse4.1 -msse4.2 -mavx -mno-avx2 -mno-sse4a -mno-fma4 -mno-xop -mno-fma -mno-avx512f -mno-bmi -mno-bmi2 -mno-aes -mpclmul -mno-avx512vl -mno-avx512bw -mno-avx512dq -mno-avx512cd -mno-avx512er -mno-avx512pf -mno-avx512vbmi -mno-avx512ifma -mno-avx5124vnniw -mno-avx5124fmaps -mno-avx512vpopcntdq -mno-avx512vbmi2 -mno-gfni -mno-vpclmulqdq -mno-avx512vnni -mno-avx512bitalg -mno-avx512bf16 -mno-avx512vp2intersect -mno-3dnow -mno-adx -mno-abm -mno-cldemote -mno-clflushopt -mno-clwb -mno-clzero -mcx16 -mno-enqcmd -mf16c -mfsgsbase -mfxsr -mno-hle -msahf -mno-lwp -mno-lzcnt -mno-movbe -mno-movdir64b -mno-movdiri -mno-mwaitx -mno-pconfig -mno-pku -mno-prefetchwt1 -mno-prfchw -mno-ptwrite -mno-rdpid -mno-rdrnd -mno-rdseed -mno-rtm -mno-serialize -mno-sgx -mno-sha -mno-shstk -mno-tbm -mno-tsxldtrk -mno-vaes -mno-waitpkg -mno-wbnoinvd -mxsave -mno-xsavec -mxsaveopt -mno-xsaves -mno-amx-tile -mno-amx-int8 -mno-amx-bf16 -mno-uintr -mno-hreset -mno-kl -mno-widekl -mno-avxvnni --param l1-cache-size=32 --param l1-cache-line-size=64 --param l2-cache-size=3072 -mtune=ivybridge -DNPY_NO_DEPRECATED_API=NPY_1_7_API_VERSION -m64 -fPIC -I/home/zoj/.local/share/pyenv/versions/polyagamma/lib/python3.7/site-packages/numpy/core/include -I/home/zoj/.local/share/pyenv/versions/3.7.12/include/python3.7m -I/home/zoj/.local/share/pyenv/versions/polyagamma/lib/python3.7/site-packages/aesara/link/c/c_code -L/home/zoj/.pyenv/versions/3.7.12/lib -L/home/zoj/.local/share/pyenv/versions/3.7.12/lib -fvisibility=hidden -o /home/zoj/.aesara/compiledir_Linux-5.10--lts-x86_64-with-arch--3.7.12-64/tmp8k09vdys/mf6fae8a01551c3ac2d9a0aaba0ff89f2ec61ceb4a936a76263a7c5d4ea33e204.so /home/zoj/.aesara/compiledir_Linux-5.10--lts-x86_64-with-arch--3.7.12-64/tmp8k09vdys/mod.cpp -lpython3.7m
/usr/bin/ld: /home/zoj/.local/share/pyenv/versions/3.7.12/lib/libpython3.7m.a(classobject.o): warning: relocation against `PyInstanceMethod_Type' in read-only section `.text'
/usr/bin/ld: /home/zoj/.local/share/pyenv/versions/3.7.12/lib/libpython3.7m.a(longobject.o): relocation R_X86_64_PC32 against symbol `PyExc_OverflowError' can not be used when making a shared object; recompile with -fPIC
/usr/bin/ld: final link failed: bad value
collect2: error: ld returned 1 exit status

Apply node that caused the error: DeepCopyOp(TensorConstant{[0 1 2 3 4]})
Toposort index: 0
Inputs types: [TensorType(int64, vector)]

HINT: Use a linker other than the C linker to print the inputs' shapes and strides.
HINT: Re-running with most Aesara optimizations disabled could provide a back-trace showing when this node was created. This can be done by setting the Aesara flag 'optimizer=fast_compile'. If that does not work, Aesara optimizations can be disabled with 'optimizer=None'.
HINT: Use the Aesara flag `exception_verbosity=high` for a debug print-out and storage map footprint of this Apply node.
```

**Please provide any additional information below.**


## Versions and main components

* Aesara version: 2.3.3
* Aesara config (`python -c ""import aesara; print(aesara.config)""`)
```
floatX ({'float32', 'float64', 'float16'})
    Doc:  Default floating-point precision for python casts.

Note: float16 support is experimental, use at your own risk.
    Value:  float64

warn_float64 ({'ignore', 'warn', 'raise', 'pdb'})
    Doc:  Do an action when a tensor variable with float64 dtype is created. They can't be run on the GPU with the current(old) gpu back-end and are slow with gamer GPUs.
    Value:  ignore

pickle_test_value (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f44cb76cdd0>>)
    Doc:  Dump test values while pickling model. If True, test values will be dumped with model.
    Value:  True

cast_policy ({'custom', 'numpy+floatX'})
    Doc:  Rules for implicit type casting
    Value:  custom

deterministic ({'default', 'more'})
    Doc:  If `more`, sometimes we will select some implementation that are more deterministic, but slower. In particular, on the GPU, we will avoid using AtomicAdd. Sometimes we will still use non-deterministic implementation, e.g. when we do not have a GPU implementation that is deterministic. Also see the dnn.conv.algo* flags to cover more cases.
    Value:  default

device (cpu, opencl*, cuda*)
    Doc:  Default device for computations. If cuda* or opencl*, change thedefault to try to move computation to the GPU. Do not use upper caseletters, only lower case even if NVIDIA uses capital letters. 'gpu' means let the driver select the gpu (needed for gpu in exclusive mode). 'gpuX' mean use the gpu number X.
    Value:  cpu

init_gpu_device (, opencl*, cuda*)
    Doc:  Initialize the gpu device to use, works only if device=cpu. Unlike 'device', setting this option will NOT move computations, nor shared variables, to the specified GPU. It can be used to run GPU-specific tests on a particular GPU.
    Value:

force_device (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f44cb76cf50>>)
    Doc:  Raise an error if we can't use the specified device
    Value:  False

conv__assert_shape (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f44cb76cb50>>)
    Doc:  If True, AbstractConv* ops will verify that user-provided shapes match the runtime shapes (debugging option, may slow down compilation)
    Value:  False

print_global_stats (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f44cb76cbd0>>)
    Doc:  Print some global statistics (time spent) at the end
    Value:  False

<aesara.configparser.ContextsParam object at 0x7f44cb76cc90>
    Doc:
        Context map for multi-gpu operation. Format is a
        semicolon-separated list of names and device names in the
        'name->dev_name' format. An example that would map name 'test' to
        device 'cuda0' and name 'test2' to device 'opencl0:0' follows:
        ""test->cuda0;test2->opencl0:0"".

        Invalid context names are 'cpu', 'cuda*' and 'opencl*'

    Value:

print_active_device (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f44cb76cad0>>)
    Doc:  Print active device at when the GPU device is initialized.
    Value:  True

gpuarray__preallocate (<class 'float'>)
    Doc:  If negative it disables the allocation cache. If
                 between 0 and 1 it enables the allocation cache and
                 preallocates that fraction of the total GPU memory.  If 1
                 or greater it will preallocate that amount of memory (in
                 megabytes).
    Value:  0.0

gpuarray__sched ({'default', 'multi', 'single'})
    Doc:  The sched parameter passed for context creation to pygpu.
                    With CUDA, using ""multi"" is equivalent to using the parameter
                    cudaDeviceScheduleBlockingSync. This is useful to lower the
                    CPU overhead when waiting for GPU. One user found that it
                    speeds up his other processes that was doing data augmentation.

    Value:  default

gpuarray__single_stream (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f44cb76ce50>>)
    Doc:
                 If your computations are mostly lots of small elements,
                 using single-stream will avoid the synchronization
                 overhead and usually be faster.  For larger elements it
                 does not make a difference yet.  In the future when true
                 multi-stream is enabled in libgpuarray, this may change.
                 If you want to make sure to have optimal performance,
                 check both options.

    Value:  True

cuda__root (<class 'str'>)
    Doc:  Location of the cuda installation
    Value:

cuda__include_path (<class 'str'>)
    Doc:  Location of the cuda includes
    Value:

assert_no_cpu_op ({'ignore', 'warn', 'raise', 'pdb'})
    Doc:  Raise an error/warning if there is a CPU op in the computational graph.
    Value:  ignore

unpickle_function (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f44cde10d50>>)
    Doc:  Replace unpickled Aesara functions with None. This is useful to unpickle old graphs that pickled them when it shouldn't
    Value:  True

reoptimize_unpickled_function (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f44cb7fe610>>)
    Doc:  Re-optimize the graph when an Aesara function is unpickled from the disk.
    Value:  False

dnn__conv__algo_fwd ({'none', 'time_on_shape_change', 'winograd_non_fused', 'small', 'fft_tiling', 'large', 'guess_once', 'guess_on_shape_change', 'winograd', 'fft', 'time_once'})
    Doc:  Default implementation to use for cuDNN forward convolution.
    Value:  small

dnn__conv__algo_bwd_data ({'none', 'time_on_shape_change', 'winograd_non_fused', 'deterministic', 'fft_tiling', 'guess_once', 'guess_on_shape_change', 'winograd', 'fft', 'time_once'})
    Doc:  Default implementation to use for cuDNN backward convolution to get the gradients of the convolution with regard to the inputs.
    Value:  none

dnn__conv__algo_bwd_filter ({'none', 'time_on_shape_change', 'winograd_non_fused', 'deterministic', 'small', 'fft_tiling', 'guess_once', 'guess_on_shape_change', 'fft', 'time_once'})
    Doc:  Default implementation to use for cuDNN backward convolution to get the gradients of the convolution with regard to the filters.
    Value:  none

dnn__conv__precision ({'float32', 'float64', 'float16', 'as_input', 'as_input_f32'})
    Doc:  Default data precision to use for the computation in cuDNN convolutions (defaults to the same dtype as the inputs of the convolutions, or float32 if inputs are float16).
    Value:  as_input_f32

dnn__base_path (<class 'str'>)
    Doc:  Install location of cuDNN.
    Value:

dnn__include_path (<class 'str'>)
    Doc:  Location of the cudnn header
    Value:

dnn__library_path (<class 'str'>)
    Doc:  Location of the cudnn link library.
    Value:

dnn__bin_path (<class 'str'>)
    Doc:  Location of the cuDNN load library (on non-windows platforms, this is the same as dnn__library_path)
    Value:

dnn__enabled ({'no_check', 'True', 'auto', 'False'})
    Doc:  'auto', use cuDNN if available, but silently fall back to not using it if not present. If True and cuDNN can not be used, raise an error. If False, disable cudnn even if present. If no_check, assume present and the version between header and library match (so less compilation at context init)
    Value:  auto

magma__include_path (<class 'str'>)
    Doc:  Location of the magma header
    Value:

magma__library_path (<class 'str'>)
    Doc:  Location of the magma library
    Value:

magma__enabled (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f44cb779690>>)
    Doc:   If True, use magma for matrix computation. If False, disable magma
    Value:  False

<aesara.configparser.ConfigParam object at 0x7f44cb779710>
    Doc:  Default compilation mode
    Value:  Mode

cxx (<class 'str'>)
    Doc:  The C++ compiler to use. Currently only g++ is supported, but supporting additional compilers should not be too difficult. If it is empty, no C++ code is compiled.
    Value:  /usr/bin/g++

linker ({'vm', 'cvm_nogc', 'c|py', 'c|py_nogc', 'c', 'py', 'vm_nogc', 'cvm'})
    Doc:  Default linker used if the aesara flags mode is Mode
    Value:  cvm

allow_gc (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f44cb779790>>)
    Doc:  Do we default to delete intermediate results during Aesara function calls? Doing so lowers the memory requirement, but asks that we reallocate memory at the next function call. This is implemented for the default linker, but may not work for all linkers.
    Value:  True

optimizer ({'o1', 'o2', 'fast_compile', 'merge', 'unsafe', 'fast_run', 'o4', 'o3', 'None'})
    Doc:  Default optimizer. If not None, will use this optimizer with the Mode
    Value:  o4

optimizer_verbose (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f44cb779990>>)
    Doc:  If True, we print all optimization being applied
    Value:  False

on_opt_error ({'ignore', 'warn', 'raise', 'pdb'})
    Doc:  What to do when an optimization crashes: warn and skip it, raise the exception, or fall into the pdb debugger.
    Value:  warn

nocleanup (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f44cb779a10>>)
    Doc:  Suppress the deletion of code files that did not compile cleanly
    Value:  False

on_unused_input ({'ignore', 'warn', 'raise'})
    Doc:  What to do if a variable in the 'inputs' list of  aesara.function() is not used in the graph.
    Value:  raise

gcc__cxxflags (<class 'str'>)
    Doc:  Extra compiler flags for gcc
    Value:

cmodule__warn_no_version (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f44cb7797d0>>)
    Doc:  If True, will print a warning when compiling one or more Op with C code that can't be cached because there is no c_code_cache_version() function associated to at least one of those Ops.
    Value:  False

cmodule__remove_gxx_opt (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f44cb779950>>)
    Doc:  If True, will remove the -O* parameter passed to g++.This is useful to debug in gdb modules compiled by Aesara.The parameter -g is passed by default to g++
    Value:  False

cmodule__compilation_warning (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f44cb779b50>>)
    Doc:  If True, will print compilation warnings.
    Value:  False

cmodule__preload_cache (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f44cb779bd0>>)
    Doc:  If set to True, will preload the C module cache at import time
    Value:  False

cmodule__age_thresh_use (<class 'int'>)
    Doc:  In seconds. The time after which Aesara won't reuse a compile c module.
    Value:  2073600

cmodule__debug (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f44cb779cd0>>)
    Doc:  If True, define a DEBUG macro (if not exists) for any compiled C code.
    Value:  False

compile__wait (<class 'int'>)
    Doc:  Time to wait before retrying to acquire the compile lock.
    Value:  5

compile__timeout (<class 'int'>)
    Doc:  In seconds, time that a process will wait before deciding to
    override an existing lock. An override only happens when the existing
    lock is held by the same owner *and* has not been 'refreshed' by this
    owner for more than this period. Refreshes are done every half timeout
    period for running processes.
    Value:  120

ctc__root (<class 'str'>)
    Doc:  Directory which contains the root of Baidu CTC library. It is assumed         that the compiled library is either inside the build, lib or lib64         subdirectory, and the header inside the include directory.
    Value:

tensor__cmp_sloppy (<class 'int'>)
    Doc:  Relax aesara.tensor.math._allclose (0) not at all, (1) a bit, (2) more
    Value:  0

tensor__local_elemwise_fusion (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f44cb779f90>>)
    Doc:  Enable or not in fast_run mode(fast_run optimization) the elemwise fusion optimization
    Value:  True

lib__amblibm (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f44cb77e090>>)
    Doc:  Use amd's amdlibm numerical library
    Value:  False

tensor__insert_inplace_optimizer_validate_nb (<class 'int'>)
    Doc:  -1: auto, if graph have less then 500 nodes 1, else 10
    Value:  -1

traceback__limit (<class 'int'>)
    Doc:  The number of stack to trace. -1 mean all.
    Value:  8

traceback__compile_limit (<class 'int'>)
    Doc:  The number of stack to trace to keep during compilation. -1 mean all. If greater then 0, will also make us save Aesara internal stack trace.
    Value:  0

experimental__unpickle_gpu_on_cpu (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f44cb77e310>>)
    Doc:  Allow unpickling of pickled GpuArrays as numpy.ndarrays.This is useful, if you want to open a GpuArray without having cuda installed.If you have cuda installed, this will force unpickling tobe done on the cpu to numpy.ndarray.Please be aware that this may get you access to the data,however, trying to unpicke gpu functions will not succeed.This flag is experimental and may be removed any time, whengpu<>cpu transparency is solved.
    Value:  False

experimental__local_alloc_elemwise (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f44cb77e3d0>>)
    Doc:  DEPRECATED: If True, enable the experimental optimization local_alloc_elemwise. Generates error if not True. Use optimizer_excluding=local_alloc_elemwise to disable.
    Value:  True

experimental__local_alloc_elemwise_assert (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f44cb77e450>>)
    Doc:  When the local_alloc_elemwise is applied, add an assert to highlight shape errors.
    Value:  True

warn__ignore_bug_before ({'all', '0.8.1', '0.10', '0.6', '1.0.4', '0.8', '0.7', '1.0.2', '0.4.1', '1.0.3', '0.9', '1.0', '1.0.1', '0.3', 'None', '0.4', '1.0.5', '0.5', '0.8.2'})
    Doc:  If 'None', we warn about all Aesara bugs found by default. If 'all', we don't warn about Aesara bugs found by default. If a version, we print only the warnings relative to Aesara bugs found after that version. Warning for specific bugs can be configured with specific [warn] flags.
    Value:  0.9

exception_verbosity ({'low', 'high'})
    Doc:  If 'low', the text of exceptions will generally refer to apply nodes with short names such as Elemwise{add_no_inplace}. If 'high', some exceptions will also refer to apply nodes with long descriptions  like:
        A. Elemwise{add_no_inplace}
                B. log_likelihood_v_given_h
                C. log_likelihood_h
    Value:  low

print_test_value (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f44cb77e5d0>>)
    Doc:  If 'True', the __eval__ of an Aesara variable will return its test_value when this is available. This has the practical conseguence that, e.g., in debugging `my_var` will print the same as `my_var.tag.test_value` when a test value is defined.
    Value:  False

compute_test_value ({'raise', 'pdb', 'ignore', 'warn', 'off'})
    Doc:  If 'True', Aesara will run each op at graph build time, using Constants, SharedVariables and the tag 'test_value' as inputs to the function. This helps the user track down problems in the graph before it gets optimized.
    Value:  off

compute_test_value_opt ({'raise', 'pdb', 'ignore', 'warn', 'off'})
    Doc:  For debugging Aesara optimization only. Same as compute_test_value, but is used during Aesara optimization
    Value:  off

check_input (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f44cb77e690>>)
    Doc:  Specify if types should check their input in their C code. It can be used to speed up compilation, reduce overhead (particularly for scalars) and reduce the number of generated C files.
    Value:  True

NanGuardMode__nan_is_error (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f44cb779c90>>)
    Doc:  Default value for nan_is_error
    Value:  True

NanGuardMode__inf_is_error (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f44cb77e710>>)
    Doc:  Default value for inf_is_error
    Value:  True

NanGuardMode__big_is_error (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f44cb77e7d0>>)
    Doc:  Default value for big_is_error
    Value:  True

NanGuardMode__action ({'warn', 'raise', 'pdb'})
    Doc:  What NanGuardMode does when it finds a problem
    Value:  raise

DebugMode__patience (<class 'int'>)
    Doc:  Optimize graph this many times to detect inconsistency
    Value:  10

DebugMode__check_c (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f44cb77e910>>)
    Doc:  Run C implementations where possible
    Value:  True

DebugMode__check_py (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f44cb77e9d0>>)
    Doc:  Run Python implementations where possible
    Value:  True

DebugMode__check_finite (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f44cb77ea50>>)
    Doc:  True -> complain about NaN/Inf results
    Value:  True

DebugMode__check_strides (<class 'int'>)
    Doc:  Check that Python- and C-produced ndarrays have same strides. On difference: (0) - ignore, (1) warn, or (2) raise error
    Value:  0

DebugMode__warn_input_not_reused (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f44cb77eb50>>)
    Doc:  Generate a warning when destroy_map or view_map says that an op works inplace, but the op did not reuse the input for its output.
    Value:  True

DebugMode__check_preallocated_output (<class 'str'>)
    Doc:  Test thunks with pre-allocated memory as output storage. This is a list of strings separated by "":"". Valid values are: ""initial"" (initial storage in storage map, happens with Scan),""previous"" (previously-returned memory), ""c_contiguous"", ""f_contiguous"", ""strided"" (positive and negative strides), ""wrong_size"" (larger and smaller dimensions), and ""ALL"" (all of the above).
    Value:

DebugMode__check_preallocated_output_ndim (<class 'int'>)
    Doc:  When testing with ""strided"" preallocated output memory, test all combinations of strides over that number of (inner-most) dimensions. You may want to reduce that number to reduce memory or time usage, but it is advised to keep a minimum of 2.
    Value:  4

profiling__time_thunks (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f44cb77ecd0>>)
    Doc:  Time individual thunks when profiling
    Value:  True

profiling__n_apply (<class 'int'>)
    Doc:  Number of Apply instances to print by default
    Value:  20

profiling__n_ops (<class 'int'>)
    Doc:  Number of Ops to print by default
    Value:  20

profiling__output_line_width (<class 'int'>)
    Doc:  Max line width for the profiling output
    Value:  512

profiling__min_memory_size (<class 'int'>)
    Doc:  For the memory profile, do not print Apply nodes if the size
                 of their outputs (in bytes) is lower than this threshold
    Value:  1024

profiling__min_peak_memory (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f44cb77ef90>>)
    Doc:  The min peak memory usage of the order
    Value:  False

profiling__destination (<class 'str'>)
    Doc:  File destination of the profiling output
    Value:  stderr

profiling__debugprint (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f44cb779250>>)
    Doc:  Do a debugprint of the profiled functions
    Value:  False

profiling__ignore_first_call (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f44cb7820d0>>)
    Doc:  Do we ignore the first call of an Aesara function.
    Value:  False

on_shape_error ({'warn', 'raise'})
    Doc:  warn: print a warning and use the default value. raise: raise an error
    Value:  warn

openmp (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f44cb782190>>)
    Doc:  Allow (or not) parallel computation on the CPU with OpenMP. This is the default value used when creating an Op that supports OpenMP parallelization. It is preferable to define it via the Aesara configuration file ~/.aesararc or with the environment variable AESARA_FLAGS. Parallelization is only done for some operations that implement it, and even for operations that implement parallelism, each operation is free to respect this flag or not. You can control the number of threads used with the environment variable OMP_NUM_THREADS. If it is set to 1, we disable openmp in Aesara by default.
    Value:  False

openmp_elemwise_minsize (<class 'int'>)
    Doc:  If OpenMP is enabled, this is the minimum size of vectors for which the openmp parallelization is enabled in element wise ops.
    Value:  200000

optimizer_excluding (<class 'str'>)
    Doc:  When using the default mode, we will remove optimizer with these tags. Separate tags with ':'.
    Value:

optimizer_including (<class 'str'>)
    Doc:  When using the default mode, we will add optimizer with these tags. Separate tags with ':'.
    Value:

optimizer_requiring (<class 'str'>)
    Doc:  When using the default mode, we will require optimizer with these tags. Separate tags with ':'.
    Value:

optdb__position_cutoff (<class 'float'>)
    Doc:  Where to stop eariler during optimization. It represent the position of the optimizer where to stop.
    Value:  inf

optdb__max_use_ratio (<class 'float'>)
    Doc:  A ratio that prevent infinite loop in EquilibriumOptimizer.
    Value:  8.0

cycle_detection ({'fast', 'regular'})
    Doc:  If cycle_detection is set to regular, most inplaces are allowed,but it is slower. If cycle_detection is set to faster, less inplacesare allowed, but it makes the compilation faster.The interaction of which one give the lower peak memory usage iscomplicated and not predictable, so if you are close to the peakmemory usage, triyng both could give you a small gain.
    Value:  regular

check_stack_trace ({'warn', 'raise', 'off', 'log'})
    Doc:  A flag for checking the stack trace during the optimization process. default (off): does not check the stack trace of any optimization log: inserts a dummy stack trace that identifies the optimizationthat inserted the variable that had an empty stack trace.warn: prints a warning if a stack trace is missing and also a dummystack trace is inserted that indicates which optimization insertedthe variable that had an empty stack trace.raise: raises an exception if a stack trace is missing
    Value:  off

metaopt__verbose (<class 'int'>)
    Doc:  0 for silent, 1 for only warnings, 2 for full output withtimings and selected implementation
    Value:  0

metaopt__optimizer_excluding (<class 'str'>)
    Doc:  exclude optimizers with these tags. Separate tags with ':'.
    Value:

metaopt__optimizer_including (<class 'str'>)
    Doc:  include optimizers with these tags. Separate tags with ':'.
    Value:

profile (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f44cb782650>>)
    Doc:  If VM should collect profile information
    Value:  False

profile_optimizer (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f44cb782690>>)
    Doc:  If VM should collect optimizer profile information
    Value:  False

profile_memory (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f44cb7826d0>>)
    Doc:  If VM should collect memory profile information and print it
    Value:  False

<aesara.configparser.ConfigParam object at 0x7f44cb782710>
    Doc:  Useful only for the vm linkers. When lazy is None, auto detect if lazy evaluation is needed and use the appropriate version. If lazy is True/False, force the version used between Loop/LoopGC and Stack.
    Value:  None

cache_optimizations (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f44cb782790>>)
    Doc:  WARNING: work in progress, does not work yet. Specify if the optimization cache should be used. This cache will any optimized graph and its optimization. Actually slow downs a lot the first optimization, and could possibly still contains some bugs. Use at your own risks.
    Value:  False

unittests__rseed (<class 'str'>)
    Doc:  Seed to use for randomized unit tests. Special value 'random' means using a seed of None.
    Value:  666

warn__round (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f44cb782850>>)
    Doc:  Warn when using `tensor.round` with the default mode. Round changed its default from `half_away_from_zero` to `half_to_even` to have the same default as NumPy.
    Value:  False

compiledir_format (<class 'str'>)
    Doc:  Format string for platform-dependent compiled module subdirectory
(relative to base_compiledir). Available keys: aesara_version, device,
gxx_version, hostname, numpy_version, platform, processor,
python_bitwidth, python_int_bitwidth, python_version, short_platform.
Defaults to compiledir_%(short_platform)s-%(processor)s-%(python_versi
on)s-%(python_bitwidth)s.
    Value:  compiledir_%(short_platform)s-%(processor)s-%(python_version)s-%(python_bitwidth)s

<aesara.configparser.ConfigParam object at 0x7f44d82c1e10>
    Doc:  platform-independent root directory for compiled modules
    Value:  /home/zoj/.aesara

<aesara.configparser.ConfigParam object at 0x7f44cb782e10>
    Doc:  platform-dependent cache directory for compiled modules
    Value:  /home/zoj/.aesara/compiledir_Linux-5.10--lts-x86_64-with-arch--3.7.12-64

<aesara.configparser.ConfigParam object at 0x7f44cb782e50>
    Doc:  Directory to cache pre-compiled kernels for the gpuarray backend.
    Value:  /home/zoj/.aesara/compiledir_Linux-5.10--lts-x86_64-with-arch--3.7.12-64/gpuarray_kernels

blas__ldflags (<class 'str'>)
    Doc:  lib[s] to include for [Fortran] level-3 blas implementation
    Value:  -lblas

blas__check_openmp (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f44cb2bd690>>)
    Doc:  Check for openmp library conflict.
WARNING: Setting this to False leaves you open to wrong results in blas-related operations.
    Value:  True

scan__allow_gc (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f44bda4f050>>)
    Doc:  Allow/disallow gc inside of Scan (default: False)
    Value:  False

scan__allow_output_prealloc (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f44bda4f0d0>>)
    Doc:  Allow/disallow memory preallocation for outputs inside of scan (default: True)
    Value:  True
```
* Python version: 3.7.12
* Operating system: Arch
* How did you install Aesara: pip
",help wanted C-backend backend compatibility MWE needed Conda,,2021-12-22 13:39:39,2022-01-20 18:38:52,"brandonwillard labeled 2021-12-24 19:59:05,brandonwillard labeled 2021-12-24 19:59:05,brandonwillard labeled 2021-12-24 19:59:05,brandonwillard labeled 2021-12-24 19:59:05,brandonwillard labeled 2022-01-18 05:37:30,brandonwillard closed 2022-01-20 18:38:52",zoj613 brandonwillard,3
581,707,New DimShuffle C-code fails on Windows,ricardoV94,"This was first seen in https://github.com/pymc-devs/pymc/pull/5279

The following tests are failing on my Windows machine:
* `test_elemwise.py::TestDimShuffle::test_infer_shape`
* `test_elemwise.py::TestDimShuffle::test_too_big_rank`
* `test_elemwise.py::TestDimShuffle::test_c_views`

The first two tests which precede #701 pass before the relevant commit: https://github.com/aesara-devs/aesara/commit/e593b0ac57a0d56d4f6ffdd08d52c3be78ebf961 and fail after.

For sanity check, all tests in ` test_elemwise.py::TestBroadcast` work fine in main.

# Traceback 

When running `test_elemwise.py::TestDimShuffle::test_c_views`:

```python
C:\\Users\\ricar\\miniconda3\\envs\\aesara-dev-custom\\python.exe ""C:\\Program Files\\JetBrains\\PyCharm Community Edition 2020.1.1\\plugins\\python-ce\\helpers\\pycharm\\_jb_pytest_runner.py"" --target test_elemwise.py::TestDimShuffle.test_c_views
Launching pytest with arguments test_elemwise.py::TestDimShuffle::test_c_views in C:\\Users\\ricar\\Documents\\aesara\\tests\\tensor

============================= test session starts =============================
platform win32 -- Python 3.9.9, pytest-6.2.5, py-1.11.0, pluggy-1.0.0 -- C:\\Users\\ricar\\miniconda3\\envs\\aesara-dev-custom\\python.exe
cachedir: .pytest_cache
rootdir: C:\\Users\\ricar\\Documents\\aesara, configfile: setup.cfg
collecting ... collected 1 item

test_elemwise.py::TestDimShuffle::test_c_views Windows fatal exception: code 0xc0000374

Current thread 0x00002704 (most recent call first):
  File ""C:\\Users\\ricar\\Documents\\aesara\\aesara\\link\\c\\basic.py"", line 1747 in __call__
  File ""C:\\Users\\ricar\\Documents\\aesara\\tests\\tensor\\test_elemwise.py"", line 135 in test_c_views
  File ""C:\\Users\\ricar\\miniconda3\\envs\\aesara-dev-custom\\lib\\site-packages\\_pytest\\python.py"", line 183 in pytest_pyfunc_call
  File ""C:\\Users\\ricar\\miniconda3\\envs\\aesara-dev-custom\\lib\\site-packages\\pluggy\\_callers.py"", line 39 in _multicall
  File ""C:\\Users\\ricar\\miniconda3\\envs\\aesara-dev-custom\\lib\\site-packages\\pluggy\\_manager.py"", line 80 in _hookexec
  File ""C:\\Users\\ricar\\miniconda3\\envs\\aesara-dev-custom\\lib\\site-packages\\pluggy\\_hooks.py"", line 265 in __call__
  File ""C:\\Users\\ricar\\miniconda3\\envs\\aesara-dev-custom\\lib\\site-packages\\_pytest\\python.py"", line 1641 in runtest
  File ""C:\\Users\\ricar\\miniconda3\\envs\\aesara-dev-custom\\lib\\site-packages\\_pytest\\runner.py"", line 162 in pytest_runtest_call
  File ""C:\\Users\\ricar\\miniconda3\\envs\\aesara-dev-custom\\lib\\site-packages\\pluggy\\_callers.py"", line 39 in _multicall
  File ""C:\\Users\\ricar\\miniconda3\\envs\\aesara-dev-custom\\lib\\site-packages\\pluggy\\_manager.py"", line 80 in _hookexec
  File ""C:\\Users\\ricar\\miniconda3\\envs\\aesara-dev-custom\\lib\\site-packages\\pluggy\\_hooks.py"", line 265 in __call__
  File ""C:\\Users\\ricar\\miniconda3\\envs\\aesara-dev-custom\\lib\\site-packages\\_pytest\\runner.py"", line 255 in <lambda>
  File ""C:\\Users\\ricar\\miniconda3\\envs\\aesara-dev-custom\\lib\\site-packages\\_pytest\\runner.py"", line 311 in from_call
  File ""C:\\Users\\ricar\\miniconda3\\envs\\aesara-dev-custom\\lib\\site-packages\\_pytest\\runner.py"", line 254 in call_runtest_hook
  File ""C:\\Users\\ricar\\miniconda3\\envs\\aesara-dev-custom\\lib\\site-packages\\_pytest\\runner.py"", line 215 in call_and_report
  File ""C:\\Users\\ricar\\miniconda3\\envs\\aesara-dev-custom\\lib\\site-packages\\_pytest\\runner.py"", line 126 in runtestprotocol
  File ""C:\\Users\\ricar\\miniconda3\\envs\\aesara-dev-custom\\lib\\site-packages\\_pytest\\runner.py"", line 109 in pytest_runtest_protocol
  File ""C:\\Users\\ricar\\miniconda3\\envs\\aesara-dev-custom\\lib\\site-packages\\pluggy\\_callers.py"", line 39 in _multicall
  File ""C:\\Users\\ricar\\miniconda3\\envs\\aesara-dev-custom\\lib\\site-packages\\pluggy\\_manager.py"", line 80 in _hookexec
  File ""C:\\Users\\ricar\\miniconda3\\envs\\aesara-dev-custom\\lib\\site-packages\\pluggy\\_hooks.py"", line 265 in __call__
  File ""C:\\Users\\ricar\\miniconda3\\envs\\aesara-dev-custom\\lib\\site-packages\\_pytest\\main.py"", line 348 in pytest_runtestloop
  File ""C:\\Users\\ricar\\miniconda3\\envs\\aesara-dev-custom\\lib\\site-packages\\pluggy\\_callers.py"", line 39 in _multicall
  File ""C:\\Users\\ricar\\miniconda3\\envs\\aesara-dev-custom\\lib\\site-packages\\pluggy\\_manager.py"", line 80 in _hookexec
  File ""C:\\Users\\ricar\\miniconda3\\envs\\aesara-dev-custom\\lib\\site-packages\\pluggy\\_hooks.py"", line 265 in __call__
  File ""C:\\Users\\ricar\\miniconda3\\envs\\aesara-dev-custom\\lib\\site-packages\\_pytest\\main.py"", line 323 in _main
  File ""C:\\Users\\ricar\\miniconda3\\envs\\aesara-dev-custom\\lib\\site-packages\\_pytest\\main.py"", line 269 in wrap_session
  File ""C:\\Users\\ricar\\miniconda3\\envs\\aesara-dev-custom\\lib\\site-packages\\_pytest\\main.py"", line 316 in pytest_cmdline_main
  File ""C:\\Users\\ricar\\miniconda3\\envs\\aesara-dev-custom\\lib\\site-packages\\pluggy\\_callers.py"", line 39 in _multicall
  File ""C:\\Users\\ricar\\miniconda3\\envs\\aesara-dev-custom\\lib\\site-packages\\pluggy\\_manager.py"", line 80 in _hookexec
  File ""C:\\Users\\ricar\\miniconda3\\envs\\aesara-dev-custom\\lib\\site-packages\\pluggy\\_hooks.py"", line 265 in __call__
  File ""C:\\Users\\ricar\\miniconda3\\envs\\aesara-dev-custom\\lib\\site-packages\\_pytest\\config\\__init__.py"", line 162 in main
  File ""C:\\Program Files\\JetBrains\\PyCharm Community Edition 2020.1.1\\plugins\\python-ce\\helpers\\pycharm\\_jb_pytest_runner.py"", line 43 in <module>

Process finished with exit code -1073740940 (0xC0000374)
```

## Versions and main components

* Aesara version: main
* Aesara config (`python -c ""import aesara; print(aesara.config)""`)
* Python version: 3.9.9
* Operating system: Windows 10
* How did you install Aesara: used the following conda enviroment.yml:

```yml
name: aesara-dev-custom
channels:
- conda-forge
- defaults
dependencies:
 # base dependencies (see install guide for Windows)
- aesara=2.3.3
- pip
- python=3.9
# Extra stuff for dev, testing and docs build
- ipython
- pre-commit
- pytest
```

And then removed aesara with `conda remove --force aesara` to use the local branch",bug help wanted C-backend Windows,,2021-12-23 10:01:16,2022-01-18 05:21:01,"ricardoV94 labeled 2021-12-23 10:01:16,ricardoV94 labeled 2021-12-23 10:01:16,brandonwillard labeled 2022-01-07 19:19:38,brandonwillard labeled 2022-01-07 19:19:53,brandonwillard closed 2022-01-18 05:21:02",ricardoV94 twiecki brandonwillard,8
582,709,Improve `Scan`'s compilation performance by improving its handling of inner graphs,brandonwillard,"Code that uses a lot of `Scan`s takes too long to optimize, and the reason appears to involve unnecessary compilations of the inner functions via `op.fn` calls.  

At least one of the invocations of `op.fn` is for the purpose of obtaining inner function inputs, which can be obtained without compilation.  Removing these compilation steps, or at least reducing them to non-C compilation, should fix the latency.",bug help wanted important MWE needed Scan,,2021-12-28 01:20:55,2022-01-23 02:24:21,"brandonwillard labeled 2021-12-28 01:20:56,brandonwillard labeled 2021-12-28 01:20:56,brandonwillard labeled 2021-12-28 01:20:56,brandonwillard labeled 2022-01-07 19:19:17,brandonwillard renamed 2022-01-10 21:22:19,brandonwillard renamed 2022-01-10 21:22:30,brandonwillard labeled 2022-01-21 18:45:30,brandonwillard referenced 2022-01-21 23:20:07,brandonwillard connected 2022-01-21 23:20:20,brandonwillard referenced 2022-01-22 01:39:26,brandonwillard referenced 2022-01-22 02:29:18,brandonwillard referenced 2022-01-23 00:59:12,brandonwillard closed 2022-01-23 02:24:21,brandonwillard referenced 2022-01-23 02:24:22",brandonwillard,2
585,715,AttributeError on import with new NumPy 1.22,michaelosthege,"## Description of your problem or feature request

Yesterdays NumPy 1.22.0 release broke some blas config settings.

I first saw it in a CI run today, and it was easily reproduced locally by installing the latest NumPy.

**Please provide the full traceback of any errors.**

```python
>>> import aesara
Traceback (most recent call last):
  File ""C:\\Users\\zufal\\miniconda3\\envs\\aesara-testing\\lib\\site-packages\\aesara\\configparser.py"", line 239, in fetch_val_for_key
    return self._aesara_cfg.get(section, option)
  File ""C:\\Users\\zufal\\miniconda3\\envs\\aesara-testing\\lib\\configparser.py"", line 781, in get
    d = self._unify_values(section, vars)
  File ""C:\\Users\\zufal\\miniconda3\\envs\\aesara-testing\\lib\\configparser.py"", line 1152, in _unify_values
    raise NoSectionError(section) from None
configparser.NoSectionError: No section: 'blas'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\\Users\\zufal\\miniconda3\\envs\\aesara-testing\\lib\\site-packages\\aesara\\configparser.py"", line 355, in __get__
    val_str = cls.fetch_val_for_key(self.name, delete_key=delete_key)
  File ""C:\\Users\\zufal\\miniconda3\\envs\\aesara-testing\\lib\\site-packages\\aesara\\configparser.py"", line 243, in fetch_val_for_key
    raise KeyError(key)
KeyError: 'blas__ldflags'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\\Users\\zufal\\miniconda3\\envs\\aesara-testing\\lib\\site-packages\\aesara\\__init__.py"", line 79, in <module>
    from aesara import scalar, tensor
  File ""C:\\Users\\zufal\\miniconda3\\envs\\aesara-testing\\lib\\site-packages\\aesara\\tensor\\__init__.py"", line 96, in <module>
    from aesara.tensor import (  # noqa
  File ""C:\\Users\\zufal\\miniconda3\\envs\\aesara-testing\\lib\\site-packages\\aesara\\tensor\\blas.py"", line 166, in <module>
    from aesara.tensor.blas_headers import blas_header_text, blas_header_version
  File ""C:\\Users\\zufal\\miniconda3\\envs\\aesara-testing\\lib\\site-packages\\aesara\\tensor\\blas_headers.py"", line 1015, in <module>
    if not config.blas__ldflags:
  File ""C:\\Users\\zufal\\miniconda3\\envs\\aesara-testing\\lib\\site-packages\\aesara\\configparser.py"", line 359, in __get__
    val_str = self.default()
  File ""C:\\Users\\zufal\\miniconda3\\envs\\aesara-testing\\lib\\site-packages\\aesara\\link\\c\\cmodule.py"", line 2628, in default_blas_ldflags
    blas_info = numpy.distutils.__config__.blas_opt_info
AttributeError: module 'numpy.distutils.__config__' has no attribute 'blas_opt_info'
```

## Versions and main components

* Aesara version: 2.3.3
* ~Aesara config (`python -c ""import aesara; print(aesara.config)""`)~
* Python version: confirmed for 3.8, 3.9
* Operating system: confirmed for Linux, Windows
* How did you install Aesara: pip
",bug important NumPy compatibility,,2022-01-01 12:55:03,2022-01-01 23:02:41,"michaelosthege referenced 2022-01-01 14:27:06,michaelosthege referenced 2022-01-01 14:30:35,brandonwillard labeled 2022-01-01 19:54:11,brandonwillard labeled 2022-01-01 19:54:11,brandonwillard labeled 2022-01-01 19:54:15,brandonwillard closed 2022-01-01 23:02:41",michaelosthege brandonwillard,0
587,718,Add benchmark reporting to CI,brandonwillard,I recently noticed that SymPy is making great use of [`asv`](https://github.com/airspeed-velocity/asv) in [their CI routine](https://github.com/sympy/sympy/blob/b0b66d5511575eda9f0e3c7cc048b7bf63a85706/.github/workflows/runtests.yml#L294).  We could really use a similar type of reporting in order to consistently track performance changes.,help wanted testing important CI performance concern tooling,,2022-01-01 20:03:14,2022-12-12 17:35:06,"brandonwillard labeled 2022-01-01 20:03:14,brandonwillard labeled 2022-01-01 20:03:14,brandonwillard labeled 2022-01-01 20:03:14,brandonwillard labeled 2022-01-01 20:03:14,brandonwillard labeled 2022-10-08 00:15:54,brandonwillard labeled 2022-10-08 00:16:02,brandonwillard closed 2022-12-12 17:35:06",brandonwillard,0
594,731,Rename `FunctionGraph.change_input` to `change_node_input`,brandonwillard,"The method name `FunctionGraph.change_input` is unnecessarily confusing, because it implies that one is changing an input of the `FunctionGraph` and not an input of a specific node in the `FunctionGraph`.  Let's rename this method to `change_node_input`.",good first issue help wanted refactor,,2022-01-07 22:45:29,2022-01-08 18:14:24,"brandonwillard labeled 2022-01-07 22:45:29,brandonwillard labeled 2022-01-07 22:45:29,brandonwillard labeled 2022-01-07 22:45:29,brandonwillard closed 2022-01-08 18:14:24",brandonwillard,0
595,733,Remove `FunctionMaker.optimize_graph_with_cache` and related code,brandonwillard,"We need to remove `FunctionMaker.optimize_graph_with_cache`; it's unnecessary, not used, and its function can be handled much better using a standard/established caching/memoization library&mdash;and consistent, effective object hashing, of course.",good first issue help wanted refactor,,2022-01-08 04:23:23,2022-01-08 22:40:15,"brandonwillard labeled 2022-01-08 04:23:23,brandonwillard labeled 2022-01-08 04:23:23,brandonwillard labeled 2022-01-08 04:23:23,brandonwillard renamed 2022-01-08 04:23:52,MridulS referenced 2022-01-08 19:46:11,brandonwillard milestoned 2022-01-08 20:31:32,brandonwillard closed 2022-01-08 22:40:15,brandonwillard referenced 2022-01-08 22:40:16",MridulS brandonwillard,0
599,737,Add a rewrite to remove unnecessary `BroadcastTo`s,brandonwillard,"It looks like we're missing a canonicalization step for some useless `BroadcastTo`s:
```python
import aesara
import aesara.tensor as at


x = at.scalar()
y = at.broadcast_to(x, [])

y_fn = aesara.function([x], y)

aesara.dprint(y_fn)
# DeepCopyOp [id A] ''   1
#  |BroadcastTo [id B] ''   0
#    |<TensorType(float64, scalar)> [id C]
```
",enhancement help wanted graph rewriting,,2022-01-10 21:39:04,2022-01-11 03:48:53,"brandonwillard labeled 2022-01-10 21:39:04,brandonwillard labeled 2022-01-10 21:39:04,brandonwillard labeled 2022-01-10 21:39:04,brandonwillard closed 2022-01-11 03:48:53",brandonwillard,0
600,739,Create a `RandomState` `Variable` constructor `Op`,brandonwillard,"We need an `Op` that will construct `RandomState` `Variable`s with the seed values as inputs.  

We should need only one such `Op` that handles all `RandomState` types (e.g. by having a fixed `Op`-level property that determines which types the `Op` generates).

This&mdash;combined with #738&mdash;will largely remove the need to use shared variables for RNG states.",enhancement help wanted important random variables,,2022-01-10 22:57:28,2022-01-30 21:30:58,"brandonwillard labeled 2022-01-10 22:57:28,brandonwillard labeled 2022-01-10 22:57:28,brandonwillard labeled 2022-01-10 22:57:28,brandonwillard labeled 2022-01-10 22:57:28,brandonwillard labeled 2022-01-10 22:57:38,brandonwillard closed 2022-01-30 21:30:58",ricardoV94 kc611 brandonwillard,6
603,742,Clarify `Type` interface and/or its documentation,brandonwillard,"[The documentation](https://aesara.readthedocs.io/en/latest/extending/type.html#Type) states that `size` and `may_share_memory` are methods of `Type`, but they're not part of the `Type` definition.  These discrepancies need to be fixed either in the code or the documentation.
",bug documentation,,2022-01-11 18:31:29,2022-01-14 23:54:03,"brandonwillard labeled 2022-01-11 18:31:29,brandonwillard labeled 2022-01-11 18:31:29,brandonwillard renamed 2022-01-11 18:31:44,brandonwillard connected 2022-01-14 23:54:02,brandonwillard closed 2022-01-14 23:54:03",brandonwillard,0
607,746,Add `aesara.as_symbolic` to the documentation,brandonwillard,#743 introduced an `aesara.as_symbolic` dispatch function that converts any Python object into its symbolic Aesara equivalent.  We should add this information to the documentation somewhere.,documentation good first issue help wanted,,2022-01-12 17:05:48,2022-10-17 23:31:20,"brandonwillard labeled 2022-01-12 17:05:48,brandonwillard labeled 2022-01-12 17:05:48,brandonwillard labeled 2022-01-12 17:05:48,anirudhacharya mentioned 2022-10-17 07:24:09,anirudhacharya subscribed 2022-10-17 07:24:09,brandonwillard closed 2022-10-17 23:31:20",anirudhacharya rlouf brandonwillard,2
608,750,Remove primers/documentation for external packages,brandonwillard,"We can't afford to write or maintain documentation for external libraries like NumPy or even Python itself&mdash;no matter how big or small the material may be.  

The two most egregious instances comprise the entirety of [the ""Prerequisites"" section in the tutorial](https://aesara.readthedocs.io/en/latest/tutorial/index.html#prerequisites).  

We should remove those and any other similar pages/sections and simply focus the documentation for the system we're developing.
",documentation good first issue help wanted refactor,,2022-01-15 00:03:48,2022-10-14 22:46:55,"brandonwillard labeled 2022-01-15 00:03:48,brandonwillard labeled 2022-01-15 00:03:48,brandonwillard labeled 2022-01-15 00:03:48,brandonwillard labeled 2022-01-15 00:03:48,brandonwillard milestoned 2022-01-15 00:07:30,brandonwillard renamed 2022-08-04 16:13:05,rlouf connected 2022-10-14 09:09:44,brandonwillard closed 2022-10-14 22:46:55",Carlosbogo rlouf brandonwillard,2
609,753,"Remove ""Requirements"" and ""Updating Aesara"" sections from the documentation",brandonwillard,"The [""Requirements""](https://aesara.readthedocs.io/en/latest/requirements.html) section is largely out of date, unnecessary, and not worth maintaining, so we should remove it.

Information regarding the installation requirements should be in the [""Installing Aesara""](https://aesara.readthedocs.io/en/latest/install.html) section, especially since the requirements will depend almost exclusively on what and how Aesara is installed.

Likewise, any useful information in [""Updating Aesara""](https://aesara.readthedocs.io/en/latest/updating.html#updating-aesara) should be moved to ""Installing Aesara"" and provided as additional information under each of the relevant installation scenarios.",documentation good first issue help wanted,,2022-01-15 00:55:57,2022-10-14 22:46:55,"brandonwillard labeled 2022-01-15 00:55:57,brandonwillard labeled 2022-01-15 00:55:57,brandonwillard labeled 2022-01-15 00:55:57,brandonwillard milestoned 2022-01-15 00:55:57,brandonwillard connected 2022-01-15 19:59:52,brandonwillard mentioned 2022-01-16 11:25:18,brandonwillard subscribed 2022-01-16 11:25:18,brandonwillard mentioned 2022-01-17 02:24:04,brandonwillard subscribed 2022-01-17 02:24:04,redlif345 mentioned 2022-01-17 02:52:44,redlif345 subscribed 2022-01-17 02:52:44,SaarthakMaini mentioned 2022-01-17 02:52:44,SaarthakMaini subscribed 2022-01-17 02:52:44,brandonwillard closed 2022-10-14 22:46:55",SaarthakMaini redlif345 adil14788 brandonwillard,7
613,758,Replace usage of `numpy.distutils`,oscarbenjamin,"NumPy is deprecating numpy.distutils:
https://mail.python.org/archives/list/numpy-discussion@python.org/thread/PMU4P4YRP2FZA2Z6Z6Z74ZFYD6PCRXQ5/

According to @rgommers:

[Aesara] is the only real consumer of numpy.distutils BLAS/LAPACK support,
which they should vendor if they don't want to switch build systems.

[Aesara has] one nontrivial use to query for BLAS config:
https://github.com/aesara-devs/aesara/blob/4468ba601f96d82a387ab0d8ab09bdedbf9bf49a/aesara/link/c/cmodule.py#L2634
",important C-backend NumPy compatibility,,2022-01-17 11:01:16,2022-07-12 20:33:55,"rgommers mentioned 2022-01-17 11:01:16,rgommers subscribed 2022-01-17 11:01:16,brandonwillard labeled 2022-01-17 20:00:08,brandonwillard labeled 2022-01-17 20:00:08,brandonwillard milestoned 2022-01-17 20:00:16,brandonwillard labeled 2022-05-09 00:28:16,brandonwillard renamed 2022-07-12 17:03:20,brandonwillard connected 2022-07-12 17:03:25,brandonwillard closed 2022-07-12 20:33:55",rgommers oscarbenjamin brandonwillard,2
619,764,Move Softmax and related Ops out of nnet module,ricardoV94,"This was meant to be done after https://github.com/aesara-devs/aesara/pull/673

We should move Softmax/ LogSoftmax and SoftmaxGrad (and their rewrites) out of the `nnet` submodule and into the `tensor` module. We should also create an alias for `logsoftmax` -> `log_softmax` to match the scipy name.",good first issue help wanted refactor SciPy compatibility,,2022-01-18 09:24:17,2022-10-17 23:08:58,"ricardoV94 labeled 2022-01-18 09:24:24,ricardoV94 labeled 2022-01-18 09:24:28,ricardoV94 labeled 2022-01-18 09:24:31,ricardoV94 labeled 2022-01-18 09:24:47,rlouf connected 2022-09-15 20:34:09,brandonwillard closed 2022-10-17 23:08:58",rlouf ricardoV94 brandonwillard,0
622,768,Implement Op for `linspace` and similar numpy functions,ricardoV94,"The following numpy functions don't have to counterpart in Aesara:
* [linspace](https://numpy.org/doc/stable/reference/generated/numpy.linspace.html)
* [logspace](https://numpy.org/doc/stable/reference/generated/numpy.logspace.html)
* [geomspace](https://numpy.org/doc/stable/reference/generated/numpy.geomspace.html)

I am not familiar with all of them, so some might not make sense, or they might be low priority.
`linspace` would definitely be nice to have.",enhancement help wanted NumPy compatibility,,2022-01-19 16:29:14,2022-03-18 19:42:28,"ricardoV94 labeled 2022-01-19 16:31:24,ricardoV94 labeled 2022-01-19 16:31:24,ricardoV94 labeled 2022-01-19 16:31:24,brandonwillard labeled 2022-01-19 22:34:54,brandonwillard closed 2022-03-18 19:42:28",ricardoV94 brandonwillard,0
625,775,`tensor.zeros` fails with symbolic scalar values,ricardoV94,"```python
import aesara.tensor as at

at.zeros([5]).eval()
at.zeros(5).eval()
at.zeros(at.constant([5])).eval()
at.zeros(at.constant(5)).eval()  # <- This fails
```
```python
Traceback (most recent call last):
  File ""/home/ricardo/Documents/Projects/aesara/aesara/tensor/var.py"", line 609, in __iter__
    for i in range(at.basic.get_vector_length(self)):
  File ""/home/ricardo/Documents/Projects/aesara/aesara/tensor/__init__.py"", line 74, in get_vector_length
    raise TypeError(f""Argument must be a vector; got {v.type}"")
TypeError: Argument must be a vector; got TensorType(int8, ())
During handling of the above exception, another exception occurred:
Traceback (most recent call last):
  File ""/home/ricardo/Documents/Projects/aesara/venv/lib/python3.8/site-packages/IPython/core/interactiveshell.py"", line 3441, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File ""<ipython-input-7-b11de4c0b2ab>"", line 6, in <module>
    at.zeros(at.constant(5)).eval()
  File ""/home/ricardo/Documents/Projects/aesara/aesara/tensor/basic.py"", line 993, in zeros
    return alloc(np.array(0, dtype=dtype), *shape)
  File ""/home/ricardo/Documents/Projects/aesara/aesara/tensor/var.py"", line 613, in __iter__
    raise TypeError(
TypeError: TensorType does not support iteration. Maybe you are using builtins.sum instead of aesara.tensor.math.sum? (Maybe .max?)

```
",bug good first issue help wanted NumPy compatibility,,2022-01-21 16:13:44,2022-04-28 01:40:49,"brandonwillard labeled 2022-01-21 17:04:17,brandonwillard labeled 2022-01-21 17:04:17,brandonwillard labeled 2022-01-21 17:04:17,brandonwillard labeled 2022-01-21 17:04:17,ricardoV94 mentioned 2022-04-05 13:04:34,ricardoV94 subscribed 2022-04-05 13:04:34,brandonwillard connected 2022-04-28 01:40:24,brandonwillard closed 2022-04-28 01:40:49",ricardoV94 brandonwillard danhphan,2
627,780,Graph Optimizer fails after graph transform,ferrine,"## Description of your problem or feature request
local optimizer breaks with meaningless error after failing to optimize a graph

**Please provide a minimal, self-contained, and reproducible example.**
```python
import aesara
import aesara.tensor as at
import numpy as np

lopt = aesara.tensor.math_opt.local_add_canonizer

mu = aesara.shared(np.asarray([0.]))
dim_shuffle_mu = mu.dimshuffle(""x"", 0)
zero_mat = aesara.tensor.constant([[0.]])
zero_mat0 = aesara.tensor.TensorType(""float64"", broadcastable=(False, False))()
node = zero_mat0 + dim_shuffle_mu
node_n = aesara.clone_replace(node, {zero_mat0: zero_mat})


fgraph = aesara.graph.FunctionGraph([mu], [node_n])
lopt.transform(fgraph, list(aesara.graph.ancestors(fgraph.outputs))[0].owner)
```

**Please provide the full traceback of any errors.**
```python
TypeError                                 Traceback (most recent call last)
/tmp/ipykernel_1147883/2645146651.py in <module>
----> 1 lopt.transform(fgraph, list(aesara.graph.ancestors(fgraph.outputs))[0].owner)

~/.miniconda3/envs/pymc/lib/python3.9/site-packages/aesara/tensor/math_opt.py in transform(self, fgraph, node)
   1034         else:
   1035             _logger.warning(
-> 1036                 "" "".join(
   1037                     (
   1038                         ""CANONIZE FAILED: new, out = "",

TypeError: sequence item 1: expected str instance, TensorVariable found
```

**Please provide any additional information below.**
* I am not sure if this optimization should pass, but it certainly should raise no errors.
* blocks https://github.com/pymc-devs/pymc/pull/4582

## Versions and main components

* Aesara version: '2.3.6'
* Aesara config (`python -c ""import aesara; print(aesara.config)""`)
<details>

```
floatX ({'float64', 'float16', 'float32'}) 
    Doc:  Default floating-point precision for python casts.

Note: float16 support is experimental, use at your own risk.
    Value:  float64

warn_float64 ({'pdb', 'raise', 'warn', 'ignore'}) 
    Doc:  Do an action when a tensor variable with float64 dtype is created. They can't be run on the GPU with the current(old) gpu back-end and are slow with gamer GPUs.
    Value:  ignore

pickle_test_value (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7ff86c5dad90>>) 
    Doc:  Dump test values while pickling model. If True, test values will be dumped with model.
    Value:  True

cast_policy ({'numpy+floatX', 'custom'}) 
    Doc:  Rules for implicit type casting
    Value:  custom

deterministic ({'more', 'default'}) 
    Doc:  If `more`, sometimes we will select some implementation that are more deterministic, but slower. In particular, on the GPU, we will avoid using AtomicAdd. Sometimes we will still use non-deterministic implementation, e.g. when we do not have a GPU implementation that is deterministic. Also see the dnn.conv.algo* flags to cover more cases.
    Value:  default

device (cpu, opencl*, cuda*) 
    Doc:  Default device for computations. If cuda* or opencl*, change thedefault to try to move computation to the GPU. Do not use upper caseletters, only lower case even if NVIDIA uses capital letters. 'gpu' means let the driver select the gpu (needed for gpu in exclusive mode). 'gpuX' mean use the gpu number X.
    Value:  cpu

init_gpu_device (, opencl*, cuda*) 
    Doc:  Initialize the gpu device to use, works only if device=cpu. Unlike 'device', setting this option will NOT move computations, nor shared variables, to the specified GPU. It can be used to run GPU-specific tests on a particular GPU.
    Value:  

force_device (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7ff86c5dad30>>) 
    Doc:  Raise an error if we can't use the specified device
    Value:  False

conv__assert_shape (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7ff808118280>>) 
    Doc:  If True, AbstractConv* ops will verify that user-provided shapes match the runtime shapes (debugging option, may slow down compilation)
    Value:  False

print_global_stats (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7ff8081182e0>>) 
    Doc:  Print some global statistics (time spent) at the end
    Value:  False

<aesara.configparser.ContextsParam object at 0x7ff808118460>
    Doc:  
        Context map for multi-gpu operation. Format is a
        semicolon-separated list of names and device names in the
        'name->dev_name' format. An example that would map name 'test' to
        device 'cuda0' and name 'test2' to device 'opencl0:0' follows:
        ""test->cuda0;test2->opencl0:0"".

        Invalid context names are 'cpu', 'cuda*' and 'opencl*'
        
    Value:  

print_active_device (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7ff8081184c0>>) 
    Doc:  Print active device at when the GPU device is initialized.
    Value:  True

gpuarray__preallocate (<class 'float'>) 
    Doc:  If negative it disables the allocation cache. If
                 between 0 and 1 it enables the allocation cache and
                 preallocates that fraction of the total GPU memory.  If 1
                 or greater it will preallocate that amount of memory (in
                 megabytes).
    Value:  0.0

gpuarray__sched ({'multi', 'single', 'default'}) 
    Doc:  The sched parameter passed for context creation to pygpu.
                    With CUDA, using ""multi"" is equivalent to using the parameter
                    cudaDeviceScheduleBlockingSync. This is useful to lower the
                    CPU overhead when waiting for GPU. One user found that it
                    speeds up his other processes that was doing data augmentation.
                 
    Value:  default

gpuarray__single_stream (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7ff808124ee0>>) 
    Doc:  
                 If your computations are mostly lots of small elements,
                 using single-stream will avoid the synchronization
                 overhead and usually be faster.  For larger elements it
                 does not make a difference yet.  In the future when true
                 multi-stream is enabled in libgpuarray, this may change.
                 If you want to make sure to have optimal performance,
                 check both options.
                 
    Value:  True

cuda__root (<class 'str'>) 
    Doc:  Location of the cuda installation
    Value:  /opt/cuda

cuda__include_path (<class 'str'>) 
    Doc:  Location of the cuda includes
    Value:  /opt/cuda/include

assert_no_cpu_op ({'pdb', 'raise', 'warn', 'ignore'}) 
    Doc:  Raise an error/warning if there is a CPU op in the computational graph.
    Value:  ignore

unpickle_function (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7ff808124fa0>>) 
    Doc:  Replace unpickled Aesara functions with None. This is useful to unpickle old graphs that pickled them when it shouldn't
    Value:  True

reoptimize_unpickled_function (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7ff808124f70>>) 
    Doc:  Re-optimize the graph when an Aesara function is unpickled from the disk.
    Value:  False

dnn__conv__algo_fwd ({'time_once', 'none', 'small', 'guess_on_shape_change', 'guess_once', 'fft', 'fft_tiling', 'winograd', 'time_on_shape_change', 'large', 'winograd_non_fused'}) 
    Doc:  Default implementation to use for cuDNN forward convolution.
    Value:  small

dnn__conv__algo_bwd_data ({'time_once', 'none', 'guess_on_shape_change', 'guess_once', 'fft', 'fft_tiling', 'winograd', 'deterministic', 'time_on_shape_change', 'winograd_non_fused'}) 
    Doc:  Default implementation to use for cuDNN backward convolution to get the gradients of the convolution with regard to the inputs.
    Value:  none

dnn__conv__algo_bwd_filter ({'time_once', 'none', 'small', 'guess_on_shape_change', 'guess_once', 'fft', 'fft_tiling', 'deterministic', 'time_on_shape_change', 'winograd_non_fused'}) 
    Doc:  Default implementation to use for cuDNN backward convolution to get the gradients of the convolution with regard to the filters.
    Value:  none

dnn__conv__precision ({'float64', 'as_input', 'float16', 'float32', 'as_input_f32'}) 
    Doc:  Default data precision to use for the computation in cuDNN convolutions (defaults to the same dtype as the inputs of the convolutions, or float32 if inputs are float16).
    Value:  as_input_f32

dnn__base_path (<class 'str'>) 
    Doc:  Install location of cuDNN.
    Value:  

dnn__include_path (<class 'str'>) 
    Doc:  Location of the cudnn header
    Value:  

dnn__library_path (<class 'str'>) 
    Doc:  Location of the cudnn link library.
    Value:  

dnn__bin_path (<class 'str'>) 
    Doc:  Location of the cuDNN load library (on non-windows platforms, this is the same as dnn__library_path)
    Value:  

dnn__enabled ({'no_check', 'True', 'auto', 'False'}) 
    Doc:  'auto', use cuDNN if available, but silently fall back to not using it if not present. If True and cuDNN can not be used, raise an error. If False, disable cudnn even if present. If no_check, assume present and the version between header and library match (so less compilation at context init)
    Value:  auto

magma__include_path (<class 'str'>) 
    Doc:  Location of the magma header
    Value:  

magma__library_path (<class 'str'>) 
    Doc:  Location of the magma library
    Value:  

magma__enabled (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7ff8081353d0>>) 
    Doc:   If True, use magma for matrix computation. If False, disable magma
    Value:  False

<aesara.configparser.ConfigParam object at 0x7ff808135400>
    Doc:  Default compilation mode
    Value:  Mode

cxx (<class 'str'>) 
    Doc:  The C++ compiler to use. Currently only g++ is supported, but supporting additional compilers should not be too difficult. If it is empty, no C++ code is compiled.
    Value:  /bin/g++

linker ({'c', 'c|py_nogc', 'py', 'cvm_nogc', 'vm_nogc', 'cvm', 'vm', 'c|py'}) 
    Doc:  Default linker used if the aesara flags mode is Mode
    Value:  cvm

allow_gc (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7ff808135790>>) 
    Doc:  Do we default to delete intermediate results during Aesara function calls? Doing so lowers the memory requirement, but asks that we reallocate memory at the next function call. This is implemented for the default linker, but may not work for all linkers.
    Value:  True

optimizer ({'unsafe', 'None', 'fast_run', 'fast_compile', 'merge', 'o1', 'o3', 'o4', 'o2'}) 
    Doc:  Default optimizer. If not None, will use this optimizer with the Mode
    Value:  o4

optimizer_verbose (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7ff8081358b0>>) 
    Doc:  If True, we print all optimization being applied
    Value:  False

on_opt_error ({'pdb', 'raise', 'warn', 'ignore'}) 
    Doc:  What to do when an optimization crashes: warn and skip it, raise the exception, or fall into the pdb debugger.
    Value:  warn

nocleanup (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7ff808135910>>) 
    Doc:  Suppress the deletion of code files that did not compile cleanly
    Value:  False

on_unused_input ({'raise', 'warn', 'ignore'}) 
    Doc:  What to do if a variable in the 'inputs' list of  aesara.function() is not used in the graph.
    Value:  raise

gcc__cxxflags (<class 'str'>) 
    Doc:  Extra compiler flags for gcc
    Value:  

cmodule__warn_no_version (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7ff8081359a0>>) 
    Doc:  If True, will print a warning when compiling one or more Op with C code that can't be cached because there is no c_code_cache_version() function associated to at least one of those Ops.
    Value:  False

cmodule__remove_gxx_opt (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7ff808135a30>>) 
    Doc:  If True, will remove the -O* parameter passed to g++.This is useful to debug in gdb modules compiled by Aesara.The parameter -g is passed by default to g++
    Value:  False

cmodule__compilation_warning (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7ff808135ac0>>) 
    Doc:  If True, will print compilation warnings.
    Value:  False

cmodule__preload_cache (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7ff808135af0>>) 
    Doc:  If set to True, will preload the C module cache at import time
    Value:  False

cmodule__age_thresh_use (<class 'int'>) 
    Doc:  In seconds. The time after which Aesara won't reuse a compile c module.
    Value:  2073600

cmodule__debug (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7ff808135b50>>) 
    Doc:  If True, define a DEBUG macro (if not exists) for any compiled C code.
    Value:  False

compile__wait (<class 'int'>) 
    Doc:  Time to wait before retrying to acquire the compile lock.
    Value:  5

compile__timeout (<class 'int'>) 
    Doc:  In seconds, time that a process will wait before deciding to
    override an existing lock. An override only happens when the existing
    lock is held by the same owner *and* has not been 'refreshed' by this
    owner for more than this period. Refreshes are done every half timeout
    period for running processes.
    Value:  120

ctc__root (<class 'str'>) 
    Doc:  Directory which contains the root of Baidu CTC library. It is assumed         that the compiled library is either inside the build, lib or lib64         subdirectory, and the header inside the include directory.
    Value:  

tensor__cmp_sloppy (<class 'int'>) 
    Doc:  Relax aesara.tensor.math._allclose (0) not at all, (1) a bit, (2) more
    Value:  0

tensor__local_elemwise_fusion (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7ff808135d00>>) 
    Doc:  Enable or not in fast_run mode(fast_run optimization) the elemwise fusion optimization
    Value:  True

lib__amblibm (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7ff808135d90>>) 
    Doc:  Use amd's amdlibm numerical library
    Value:  False

tensor__insert_inplace_optimizer_validate_nb (<class 'int'>) 
    Doc:  -1: auto, if graph have less then 500 nodes 1, else 10
    Value:  -1

traceback__limit (<class 'int'>) 
    Doc:  The number of stack to trace. -1 mean all.
    Value:  8

traceback__compile_limit (<class 'int'>) 
    Doc:  The number of stack to trace to keep during compilation. -1 mean all. If greater then 0, will also make us save Aesara internal stack trace.
    Value:  0

experimental__unpickle_gpu_on_cpu (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7ff808135f10>>) 
    Doc:  Allow unpickling of pickled GpuArrays as numpy.ndarrays.This is useful, if you want to open a GpuArray without having cuda installed.If you have cuda installed, this will force unpickling tobe done on the cpu to numpy.ndarray.Please be aware that this may get you access to the data,however, trying to unpicke gpu functions will not succeed.This flag is experimental and may be removed any time, whengpu<>cpu transparency is solved.
    Value:  False

experimental__local_alloc_elemwise (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7ff808135f40>>) 
    Doc:  DEPRECATED: If True, enable the experimental optimization local_alloc_elemwise. Generates error if not True. Use optimizer_excluding=local_alloc_elemwise to disable.
    Value:  True

experimental__local_alloc_elemwise_assert (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7ff808135fd0>>) 
    Doc:  When the local_alloc_elemwise is applied, add an assert to highlight shape errors.
    Value:  True

warn__ignore_bug_before ({'0.8.2', '0.5', '0.3', '0.7', '1.0', '0.9', '1.0.3', '0.6', '0.8', '0.10', 'None', '0.4', '1.0.1', '1.0.5', '0.4.1', '1.0.4', 'all', '0.8.1', '1.0.2'}) 
    Doc:  If 'None', we warn about all Aesara bugs found by default. If 'all', we don't warn about Aesara bugs found by default. If a version, we print only the warnings relative to Aesara bugs found after that version. Warning for specific bugs can be configured with specific [warn] flags.
    Value:  0.9

exception_verbosity ({'high', 'low'}) 
    Doc:  If 'low', the text of exceptions will generally refer to apply nodes with short names such as Elemwise{add_no_inplace}. If 'high', some exceptions will also refer to apply nodes with long descriptions  like:
        A. Elemwise{add_no_inplace}
                B. log_likelihood_v_given_h
                C. log_likelihood_h
    Value:  low

print_test_value (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7ff80813a100>>) 
    Doc:  If 'True', the __eval__ of an Aesara variable will return its test_value when this is available. This has the practical conseguence that, e.g., in debugging `my_var` will print the same as `my_var.tag.test_value` when a test value is defined.
    Value:  False

compute_test_value ({'raise', 'off', 'pdb', 'warn', 'ignore'}) 
    Doc:  If 'True', Aesara will run each op at graph build time, using Constants, SharedVariables and the tag 'test_value' as inputs to the function. This helps the user track down problems in the graph before it gets optimized.
    Value:  off

compute_test_value_opt ({'raise', 'off', 'pdb', 'warn', 'ignore'}) 
    Doc:  For debugging Aesara optimization only. Same as compute_test_value, but is used during Aesara optimization
    Value:  off

check_input (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7ff80813a190>>) 
    Doc:  Specify if types should check their input in their C code. It can be used to speed up compilation, reduce overhead (particularly for scalars) and reduce the number of generated C files.
    Value:  True

NanGuardMode__nan_is_error (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7ff80813a1c0>>) 
    Doc:  Default value for nan_is_error
    Value:  True

NanGuardMode__inf_is_error (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7ff80813a1f0>>) 
    Doc:  Default value for inf_is_error
    Value:  True

NanGuardMode__big_is_error (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7ff80813a280>>) 
    Doc:  Default value for big_is_error
    Value:  True

NanGuardMode__action ({'pdb', 'raise', 'warn'}) 
    Doc:  What NanGuardMode does when it finds a problem
    Value:  raise

DebugMode__patience (<class 'int'>) 
    Doc:  Optimize graph this many times to detect inconsistency
    Value:  10

DebugMode__check_c (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7ff80813a310>>) 
    Doc:  Run C implementations where possible
    Value:  True

DebugMode__check_py (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7ff80813a3a0>>) 
    Doc:  Run Python implementations where possible
    Value:  True

DebugMode__check_finite (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7ff80813a3d0>>) 
    Doc:  True -> complain about NaN/Inf results
    Value:  True

DebugMode__check_strides (<class 'int'>) 
    Doc:  Check that Python- and C-produced ndarrays have same strides. On difference: (0) - ignore, (1) warn, or (2) raise error
    Value:  0

DebugMode__warn_input_not_reused (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7ff80813a430>>) 
    Doc:  Generate a warning when destroy_map or view_map says that an op works inplace, but the op did not reuse the input for its output.
    Value:  True

DebugMode__check_preallocated_output (<class 'str'>) 
    Doc:  Test thunks with pre-allocated memory as output storage. This is a list of strings separated by "":"". Valid values are: ""initial"" (initial storage in storage map, happens with Scan),""previous"" (previously-returned memory), ""c_contiguous"", ""f_contiguous"", ""strided"" (positive and negative strides), ""wrong_size"" (larger and smaller dimensions), and ""ALL"" (all of the above).
    Value:  

DebugMode__check_preallocated_output_ndim (<class 'int'>) 
    Doc:  When testing with ""strided"" preallocated output memory, test all combinations of strides over that number of (inner-most) dimensions. You may want to reduce that number to reduce memory or time usage, but it is advised to keep a minimum of 2.
    Value:  4

profiling__time_thunks (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7ff80813a4c0>>) 
    Doc:  Time individual thunks when profiling
    Value:  True

profiling__n_apply (<class 'int'>) 
    Doc:  Number of Apply instances to print by default
    Value:  20

profiling__n_ops (<class 'int'>) 
    Doc:  Number of Ops to print by default
    Value:  20

profiling__output_line_width (<class 'int'>) 
    Doc:  Max line width for the profiling output
    Value:  512

profiling__min_memory_size (<class 'int'>) 
    Doc:  For the memory profile, do not print Apply nodes if the size
                 of their outputs (in bytes) is lower than this threshold
    Value:  1024

profiling__min_peak_memory (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7ff80813a610>>) 
    Doc:  The min peak memory usage of the order
    Value:  False

profiling__destination (<class 'str'>) 
    Doc:  File destination of the profiling output
    Value:  stderr

profiling__debugprint (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7ff80813a670>>) 
    Doc:  Do a debugprint of the profiled functions
    Value:  False

profiling__ignore_first_call (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7ff80813a6a0>>) 
    Doc:  Do we ignore the first call of an Aesara function.
    Value:  False

on_shape_error ({'raise', 'warn'}) 
    Doc:  warn: print a warning and use the default value. raise: raise an error
    Value:  warn

openmp (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7ff80813a700>>) 
    Doc:  Allow (or not) parallel computation on the CPU with OpenMP. This is the default value used when creating an Op that supports OpenMP parallelization. It is preferable to define it via the Aesara configuration file ~/.aesararc or with the environment variable AESARA_FLAGS. Parallelization is only done for some operations that implement it, and even for operations that implement parallelism, each operation is free to respect this flag or not. You can control the number of threads used with the environment variable OMP_NUM_THREADS. If it is set to 1, we disable openmp in Aesara by default.
    Value:  False

openmp_elemwise_minsize (<class 'int'>) 
    Doc:  If OpenMP is enabled, this is the minimum size of vectors for which the openmp parallelization is enabled in element wise ops.
    Value:  200000

optimizer_excluding (<class 'str'>) 
    Doc:  When using the default mode, we will remove optimizer with these tags. Separate tags with ':'.
    Value:  

optimizer_including (<class 'str'>) 
    Doc:  When using the default mode, we will add optimizer with these tags. Separate tags with ':'.
    Value:  

optimizer_requiring (<class 'str'>) 
    Doc:  When using the default mode, we will require optimizer with these tags. Separate tags with ':'.
    Value:  

optdb__position_cutoff (<class 'float'>) 
    Doc:  Where to stop eariler during optimization. It represent the position of the optimizer where to stop.
    Value:  inf

optdb__max_use_ratio (<class 'float'>) 
    Doc:  A ratio that prevent infinite loop in EquilibriumOptimizer.
    Value:  8.0

cycle_detection ({'fast', 'regular'}) 
    Doc:  If cycle_detection is set to regular, most inplaces are allowed,but it is slower. If cycle_detection is set to faster, less inplacesare allowed, but it makes the compilation faster.The interaction of which one give the lower peak memory usage iscomplicated and not predictable, so if you are close to the peakmemory usage, triyng both could give you a small gain.
    Value:  regular

check_stack_trace ({'raise', 'warn', 'off', 'log'}) 
    Doc:  A flag for checking the stack trace during the optimization process. default (off): does not check the stack trace of any optimization log: inserts a dummy stack trace that identifies the optimizationthat inserted the variable that had an empty stack trace.warn: prints a warning if a stack trace is missing and also a dummystack trace is inserted that indicates which optimization insertedthe variable that had an empty stack trace.raise: raises an exception if a stack trace is missing
    Value:  off

metaopt__verbose (<class 'int'>) 
    Doc:  0 for silent, 1 for only warnings, 2 for full output withtimings and selected implementation
    Value:  0

metaopt__optimizer_excluding (<class 'str'>) 
    Doc:  exclude optimizers with these tags. Separate tags with ':'.
    Value:  

metaopt__optimizer_including (<class 'str'>) 
    Doc:  include optimizers with these tags. Separate tags with ':'.
    Value:  

profile (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7ff80813aa00>>) 
    Doc:  If VM should collect profile information
    Value:  False

profile_optimizer (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7ff80813aa30>>) 
    Doc:  If VM should collect optimizer profile information
    Value:  False

profile_memory (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7ff80813aa60>>) 
    Doc:  If VM should collect memory profile information and print it
    Value:  False

<aesara.configparser.ConfigParam object at 0x7ff80813aa90>
    Doc:  Useful only for the vm linkers. When lazy is None, auto detect if lazy evaluation is needed and use the appropriate version. If lazy is True/False, force the version used between Loop/LoopGC and Stack.
    Value:  None

unittests__rseed (<class 'str'>) 
    Doc:  Seed to use for randomized unit tests. Special value 'random' means using a seed of None.
    Value:  666

warn__round (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7ff80813ab50>>) 
    Doc:  Warn when using `tensor.round` with the default mode. Round changed its default from `half_away_from_zero` to `half_to_even` to have the same default as NumPy.
    Value:  False

numba__vectorize_target ({'parallel', 'cpu', 'cuda'}) 
    Doc:  Default target for numba.vectorize.
    Value:  cpu

numba__fastmath (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7ff80813ac10>>) 
    Doc:  If True, use Numba's fastmath mode.
    Value:  True

numba__cache (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7ff80813aca0>>) 
    Doc:  If True, use Numba's file based caching.
    Value:  True

compiledir_format (<class 'str'>) 
    Doc:  Format string for platform-dependent compiled module subdirectory
(relative to base_compiledir). Available keys: aesara_version, device,
gxx_version, hostname, numpy_version, platform, processor,
python_bitwidth, python_int_bitwidth, python_version, short_platform.
Defaults to compiledir_%(short_platform)s-%(processor)s-%(python_versi
on)s-%(python_bitwidth)s.
    Value:  compiledir_%(short_platform)s-%(processor)s-%(python_version)s-%(python_bitwidth)s

<aesara.configparser.ConfigParam object at 0x7ff86c0d1370>
    Doc:  platform-independent root directory for compiled modules
    Value:  /home/ferres/.aesara

<aesara.configparser.ConfigParam object at 0x7ff86c0d1940>
    Doc:  platform-dependent cache directory for compiled modules
    Value:  /home/ferres/.aesara/compiledir_Linux-5.16-arch1-1-x86_64-with-glibc2.33--3.9.7-64

<aesara.configparser.ConfigParam object at 0x7ff86c62e2e0>
    Doc:  Directory to cache pre-compiled kernels for the gpuarray backend.
    Value:  /home/ferres/.aesara/compiledir_Linux-5.16-arch1-1-x86_64-with-glibc2.33--3.9.7-64/gpuarray_kernels

blas__ldflags (<class 'str'>) 
    Doc:  lib[s] to include for [Fortran] level-3 blas implementation
    Value:  -L/home/ferres/.miniconda3/envs/pymc/lib -lmkl_rt -lpthread -lm -lm -Wl,-rpath,/home/ferres/.miniconda3/envs/pymc/lib

blas__check_openmp (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7ff871462d90>>) 
    Doc:  Check for openmp library conflict.
WARNING: Setting this to False leaves you open to wrong results in blas-related operations.
    Value:  True

scan__allow_gc (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7ff80813ad90>>) 
    Doc:  Allow/disallow gc inside of Scan (default: False)
    Value:  False

scan__allow_output_prealloc (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7ff834fd0e50>>) 
    Doc:  Allow/disallow memory preallocation for outputs inside of scan (default: True)
    Value:  True
```

<details/>
* Python version: 3.9
* Operating system: Archlinux
* How did you install Aesara: pip
",invalid question,,2022-01-23 16:11:30,2022-01-23 17:17:49,"brandonwillard labeled 2022-01-23 17:16:32,brandonwillard labeled 2022-01-23 17:16:32,brandonwillard closed 2022-01-23 17:17:49,brandonwillard connected 2022-01-23 18:21:52,brandonwillard mentioned 2022-01-24 08:13:43,brandonwillard subscribed 2022-01-24 08:13:43",ferrine ricardoV94 brandonwillard,4
630,784,Constant folding overrides broadcastable pattern,ferrine,"## Description of your problem or feature request

**Please provide a minimal, self-contained, and reproducible example.**
```python
from aesara.graph.opt_utils import optimize_graph
v = at.patternbroadcast(at.ones((1, 1)), (False, False))
assert optimize_graph(v).broadcastable == v.broadcastable
```

**Please provide the full traceback of any errors.**
```python
AssertionError                            Traceback (most recent call last)
/tmp/ipykernel_1243842/1172206837.py in <module>
      1 from aesara.graph.opt_utils import optimize_graph
      2 v = at.patternbroadcast(at.ones((1, 1)), (False, False))
----> 3 assert optimize_graph(v).broadcastable == v.broadcastable

AssertionError: 
```

**Please provide any additional information below.**

## Versions and main components

* Aesara version: '2.3.6'
* Aesara config (`python -c ""import aesara; print(aesara.config)""`) not applicable
* Python version:
* Operating system:
* How did you install Aesara: (conda/pip) pip
",question,,2022-01-24 10:15:52,2022-01-24 16:27:08,"ferrine mentioned 2022-01-24 11:34:56,ferrine subscribed 2022-01-24 11:34:56,brandonwillard labeled 2022-01-24 16:26:45,aesara-devs locked 2022-01-24 16:27:08,brandonwillard converted_to_discussion 2022-01-24 16:27:08",ferrine ricardoV94 aesara-devs brandonwillard,6
631,785,Implement `tri[l|u]_indices`,ricardoV94,There is currently no equivalent to the numpy [tril_indices](https://numpy.org/doc/stable/reference/generated/numpy.tril_indices.html) and [triu_indices](https://numpy.org/doc/stable/reference/generated/numpy.triu_indices.html),help wanted NumPy compatibility,zoj613,2022-01-24 11:41:52,2022-01-30 15:16:59,"ricardoV94 labeled 2022-01-24 11:41:53,ricardoV94 renamed 2022-01-25 08:31:03,zoj613 assigned 2022-01-26 10:44:18,brandonwillard labeled 2022-01-26 18:48:35,ricardoV94 closed 2022-01-30 15:16:59",ricardoV94 zoj613 brandonwillard,0
632,786,`tensor.take` bug,ricardoV94,"```python
import aesara.tensor as at
import numpy as np

x = np.zeros((3, 3))
at.take(x, np.eye(3, dtype=int))
```

```
Traceback (most recent call last):
  File ""/home/ricardo/Documents/Projects/aesara/venv/lib/python3.8/site-packages/IPython/core/interactiveshell.py"", line 3441, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File ""<ipython-input-7-0de512d75688>"", line 4, in <module>
    at.take(x, np.eye(3, dtype=int))
  File ""/home/ricardo/Documents/Projects/aesara/aesara/tensor/subtensor.py"", line 2777, in take
    elif axis < 0:
TypeError: '<' not supported between instances of 'NoneType' and 'int'
```",bug,,2022-01-24 12:02:45,2022-01-24 20:39:56,"ricardoV94 labeled 2022-01-24 12:02:52,brandonwillard connected 2022-01-24 20:34:24,brandonwillard closed 2022-01-24 20:39:56",ricardoV94 kc611 brandonwillard,2
635,790,Build documentation in CI ,brandonwillard,"We need to add a GitHub Actions workflow that builds the documentation.  

This is necessary for streamlined reviewing of documentation-based PRs, since it will help prevent unnoticed formatting issues&mdash;among other things.
",documentation help wanted important CI,,2022-01-24 17:29:24,2022-02-07 17:52:59,"brandonwillard labeled 2022-01-24 17:29:24,brandonwillard labeled 2022-01-24 17:29:24,brandonwillard labeled 2022-01-24 17:29:24,brandonwillard labeled 2022-01-24 17:29:24,brandonwillard closed 2022-02-07 17:52:59",brandonwillard,0
637,794,Add type hints to `aesara.tensor.random.op.default_shape_from_params`,canyon289,"## Description of your problem or feature request
Add type hints to `default_shape_from_params`. I believe this would help newer developers quickly figure how this function is supposed to work.

https://github.com/aesara-devs/aesara/blob/main/aesara/tensor/random/op.py#L27

If anyone disagrees though feel free to close this ticket!

P.S I can't add labels but this would be intended as a beginner issue
",documentation good first issue help wanted,danhphan,2022-01-27 01:19:37,2022-02-10 18:10:50,"brandonwillard renamed 2022-01-27 01:25:45,brandonwillard labeled 2022-01-27 01:26:11,brandonwillard labeled 2022-01-27 01:26:11,brandonwillard labeled 2022-01-27 01:26:11,brandonwillard mentioned 2022-02-06 12:00:24,brandonwillard subscribed 2022-02-06 12:00:24,canyon289 mentioned 2022-02-06 12:00:24,canyon289 subscribed 2022-02-06 12:00:24,danhphan assigned 2022-02-07 19:40:36,ricardoV94 closed 2022-02-10 18:10:50",ricardoV94 canyon289 brandonwillard danhphan,2
639,797,Propagate variable shape information across Ops,ricardoV94,"Now that Aesara variable types can have fixed shapes beyond broadcastable/non broadcastable we should try to propagate this information across operators.

```python
x = at.TensorType(""float64"", (3, 3))()
print(x.type)  # tensortype(float64, (3, 3))
y = x + 1
print(y.type)  # tensortype(float64, (none, none))
```

In this trivial example, the `Add` `op` could have easily created an output type with `shape=(3, 3)`",duplicate,,2022-01-29 16:57:39,2022-02-06 17:43:39,"brandonwillard labeled 2022-02-06 17:43:06,brandonwillard closed 2022-02-06 17:43:39",ricardoV94 brandonwillard,1
640,798,`at.broadcast_shape` function does not work with tensor inputs,Sayam753,"**Please provide a minimal, self-contained, and reproducible example.**
```python
>>> import aesara.tensor as at
>>> shape1 = at.as_tensor_variable([3, 4])
>>> shape2 = at.as_tensor_variable([1, 4])
>>> at.broadcast_shape(shape1, shape2, arrays_are_shapes=True)
```

**Please provide the full traceback of any errors.**
```python-traceback
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/Users/sayam/miniconda3/envs/pymc_dev/lib/python3.7/site-packages/aesara/tensor/extra_ops.py"", line 1479, in broadcast_shape
    return broadcast_shape_iter(arrays, **kwargs)
  File ""/Users/sayam/miniconda3/envs/pymc_dev/lib/python3.7/site-packages/aesara/tensor/extra_ops.py"", line 1510, in broadcast_shape_iter
    max_dims = max(len(a) for a in arrays)
  File ""/Users/sayam/miniconda3/envs/pymc_dev/lib/python3.7/site-packages/aesara/tensor/extra_ops.py"", line 1510, in <genexpr>
    max_dims = max(len(a) for a in arrays)
TypeError: object of type 'TensorConstant' has no len()
```

**Please provide any additional information below.**

If the inputs to `at.broadcast_shape` are lists, it works correctly -

```python
>>> at.broadcast_shape([3, 4], [1, 4], arrays_are_shapes=True)
(3, 4)
```

## Versions and main components

* Aesara version: 2.3.6
* Aesara config (`python -c ""import aesara; print(aesara.config)""`)
* Python version: python 3.7.7
* Operating system: MacOS
* How did you install Aesara: (conda/pip) pip
",,,2022-01-30 13:46:05,2022-01-30 17:51:25,brandonwillard closed 2022-01-30 17:51:25,Sayam753 brandonwillard,1
642,800,Add `mypy` checking to `pre-commit` hooks,brandonwillard,"We need to set up a `mypy` to check the subset of type hints that we currently have (and will continue to add per #200).

A big part of this will undoubtedly involve corrections to the informal type annotations we currently have, but another key part of this work is determining good `mypy` settings given the size of the library, partial information, and external dependencies.  It might be best to start by establishing the latter first.",help wanted testing important CI,,2022-01-30 23:30:50,2022-04-06 20:21:12,"brandonwillard labeled 2022-01-30 23:30:50,brandonwillard labeled 2022-01-30 23:30:50,brandonwillard labeled 2022-01-30 23:30:50,brandonwillard labeled 2022-01-30 23:30:50,brandonwillard connected 2022-04-06 20:21:04,brandonwillard closed 2022-04-06 20:21:12",brandonwillard,0
643,801,The Numba backend does not support `RandomStream`,rlouf,"The following code uses a `RandomStream` to define random variables and buid a simple logprob function with `joint_logprob`:

```python
from aesara.graph.fg import FunctionGraph
from aesara.link.numba.dispatch import numba_funcify

import aesara.tensor as at

from aeppl import joint_logprob

srng = at.random.RandomStream(seed=0)
Y_rv = srng.normal(1, 2)

def logprob_fn(y):
    logprob = joint_logprob({Y_rv: y})
    return logprob

yrv_fgraph = FunctionGraph(outputs=[Y_rv], clone=False)
fn = numba_funcify(yrv_fgraph)
```

```python
  File ""<stdin>"", line 20, in <module>
  File ""<stdin>"", line 18, in main
  File ""/usr/lib/python3.9/functools.py"", line 877, in wrapper
    return dispatch(args[0].__class__)(*args, **kw)
  File ""/home/remi/projects/aesara/aesara/link/numba/dispatch/basic.py"", line 354, in numba_funcify_FunctionGraph
    return fgraph_to_python(
  File ""/home/remi/projects/aesara/aesara/link/utils.py"", line 723, in fgraph_to_python
    compiled_func = op_conversion_fn(
  File ""/usr/lib/python3.9/functools.py"", line 877, in wrapper
    return dispatch(args[0].__class__)(*args, **kw)
  File ""/home/remi/projects/aesara/aesara/link/numba/dispatch/random.py"", line 221, in numba_funcify_RandomVariable
    raise TypeError(""Numba does not support NumPy `Generator`s"")
TypeError: Numba does not support NumPy `Generator`s
```

It is problematic since the current implementation of HMC/NUTS relies on `RandomStreams` and its autoupdate feature inside of `Scan` ops. Is there any fundamental incompatibility or is it just not implemented yet?",duplicate backend compatibility Numba random variables NumPy compatibility,,2022-01-31 16:11:37,2022-09-21 21:48:12,"brandonwillard labeled 2022-08-04 16:09:51,brandonwillard labeled 2022-08-04 16:09:51,brandonwillard labeled 2022-08-04 16:09:51,brandonwillard renamed 2022-08-04 16:10:01,brandonwillard labeled 2022-08-04 16:10:09,brandonwillard labeled 2022-09-21 21:46:58,brandonwillard closed 2022-09-21 21:48:12",rlouf kc611 brandonwillard,4
645,804,"Failed to import aesara, cblas.h not found",grundew,"## Importing aesara fails, unable to locate cblas.h


```python
import aesara
```

**Please provide the full traceback of any errors.**
```python
n [1]: import aesara
/home/gw/.pyenv/versions/anaconda3-2021.05/envs/aesara/bin/../lib/gcc/x86_64-conda-linux-gnu/9.3.0/../../../../x86_64-conda-linux-gnu/bin/ld: /usr/lib/x86_64-linux-gnu/libopenblas.so: undefined reference to `logf@GLIBC_2.27'
/home/gw/.pyenv/versions/anaconda3-2021.05/envs/aesara/bin/../lib/gcc/x86_64-conda-linux-gnu/9.3.0/../../../../x86_64-conda-linux-gnu/bin/ld: /home/gw/.pyenv/versions/anaconda3-2021.05/envs/aesara/bin/../x86_64-conda-linux-gnu/sysroot/lib64/libpthread.so.0: undefined reference to `h_errno@GLIBC_PRIVATE'
/home/gw/.pyenv/versions/anaconda3-2021.05/envs/aesara/bin/../lib/gcc/x86_64-conda-linux-gnu/9.3.0/../../../../x86_64-conda-linux-gnu/bin/ld: /usr/lib/x86_64-linux-gnu/libopenblas.so: undefined reference to `log@GLIBC_2.29'
/home/gw/.pyenv/versions/anaconda3-2021.05/envs/aesara/bin/../lib/gcc/x86_64-conda-linux-gnu/9.3.0/../../../../x86_64-conda-linux-gnu/bin/ld: /home/gw/.pyenv/versions/anaconda3-2021.05/envs/aesara/bin/../x86_64-conda-linux-gnu/sysroot/lib64/libpthread.so.0: undefined reference to `__libc_dl_error_tsd@GLIBC_PRIVATE'
/home/gw/.pyenv/versions/anaconda3-2021.05/envs/aesara/bin/../lib/gcc/x86_64-conda-linux-gnu/9.3.0/../../../../x86_64-conda-linux-gnu/bin/ld: /usr/lib/x86_64-linux-gnu/libopenblas.so: undefined reference to `powf@GLIBC_2.27'
/home/gw/.pyenv/versions/anaconda3-2021.05/envs/aesara/bin/../lib/gcc/x86_64-conda-linux-gnu/9.3.0/../../../../x86_64-conda-linux-gnu/bin/ld: /usr/lib/x86_64-linux-gnu/libopenblas.so: undefined reference to `pow@GLIBC_2.29'
/home/gw/.pyenv/versions/anaconda3-2021.05/envs/aesara/bin/../lib/gcc/x86_64-conda-linux-gnu/9.3.0/../../../../x86_64-conda-linux-gnu/bin/ld: /home/gw/.pyenv/versions/anaconda3-2021.05/envs/aesara/bin/../x86_64-conda-linux-gnu/sysroot/lib64/libpthread.so.0: undefined reference to `__vdso_clock_gettime@GLIBC_PRIVATE'
collect2: error: ld returned 1 exit status
/tmp/tmpem1dex2r/source.c:1:10: fatal error: cblas.h: No such file or directory
    1 | #include <cblas.h>
      |          ^~~~~~~~~
compilation terminated.

```


## Versions and main components

* Aesara version: 2.3.8
* Aesara config (`python -c ""import aesara; print(aesara.config)""`)
* Python version: 3.8.12
* Operating system: ubuntu 
* How did you install Aesara: conda
",question MWE needed,,2022-02-03 14:17:41,2022-02-04 03:06:08,"brandonwillard labeled 2022-02-04 03:03:42,brandonwillard labeled 2022-02-04 03:03:42,aesara-devs locked 2022-02-04 03:06:08,brandonwillard converted_to_discussion 2022-02-04 03:06:08",aesara-devs brandonwillard grundew,0
648,810,Add support for SciPy  1.8.0,bsaintjo,"## Scipy version 1.8.0 causes import error
Ran into the issue trying to install pymc4

```python
>>> import aesara
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/dondon/.local/share/virtualenvs/Programming-OzyK_nMv/lib/python3.10/site-packages/aesara/__init__.py"", line 126, in <module>
    from aesara import scalar, tensor
  File ""/home/dondon/.local/share/virtualenvs/Programming-OzyK_nMv/lib/python3.10/site-packages/aesara/tensor/__init__.py"", line 98, in <module>
    from aesara.tensor import (  # noqa
  File ""/home/dondon/.local/share/virtualenvs/Programming-OzyK_nMv/lib/python3.10/site-packages/aesara/tensor/nnet/__init__.py"", line 3, in <module>
    import aesara.tensor.nnet.opt
  File ""/home/dondon/.local/share/virtualenvs/Programming-OzyK_nMv/lib/python3.10/site-packages/aesara/tensor/nnet/opt.py"", line 17, in <module>
    from aesara.tensor.nnet.abstract_conv import (
  File ""/home/dondon/.local/share/virtualenvs/Programming-OzyK_nMv/lib/python3.10/site-packages/aesara/tensor/nnet/abstract_conv.py"", line 18, in <module>
    from scipy.signal.signaltools import _bvalfromboundary, _valfrommode, convolve
ImportError: cannot import name '_bvalfromboundary' from 'scipy.signal.signaltools' (/home/dondon/.local/share/virtualenvs/Programming-OzyK_nMv/lib/python3.10/site-packages/scipy/signal/signaltools.py)
```

**Please provide any additional information below.**

- Downgrading to scipy==1.7.3 fixes the issue.
- the `scipy.signal.signaltools` module was deprecated for version 1.8.0 and now everything is in `scipy.signal._signaltools`
- `scipy.signal.signaltools` doesn't export `_bvalfromboundary` or `_valfrommode`

On `scipy==1.8.0`
```python
>>> import scipy.signal.signaltools
>>> scipy.signal.signaltools._bvalfromboundary
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/dondon/.local/share/virtualenvs/Programming-OzyK_nMv/lib/python3.10/site-packages/scipy/signal/signaltools.py"", line 29, in __getattr__
    raise AttributeError(
AttributeError: scipy.signal.signaltools is deprecated and has no attribute _bvalfromboundary. Try looking in scipy.signal instead.
>>> import scipy.signal._signaltools
>>> scipy.signal._signaltools._bvalfromboundary
<function _bvalfromboundary at 0x7f3bef644310>

```

## Versions and main components

* Aesara version: 2.3.8
* Aesara config (`python -c ""import aesara; print(aesara.config)""`) N/A
* Python version: 3.10.1
* Operating system: Linux
* How did you install Aesara: (conda/pip) pip, pipenv
",good first issue help wanted SciPy compatibility,,2022-02-06 01:44:26,2022-02-07 11:08:35,"michaelosthege referenced 2022-02-06 12:04:07,brandonwillard labeled 2022-02-06 17:46:19,brandonwillard labeled 2022-02-06 17:46:55,brandonwillard labeled 2022-02-06 17:46:55,brandonwillard labeled 2022-02-06 17:46:55,brandonwillard connected 2022-02-06 17:47:49,brandonwillard disconnected 2022-02-06 18:04:54,brandonwillard referenced 2022-02-06 18:05:24,brandonwillard renamed 2022-02-06 18:05:59,brandonwillard unlabeled 2022-02-06 18:06:12,twiecki closed 2022-02-07 11:08:35",michaelosthege bsaintjo twiecki brandonwillard,1
652,814,Use Numba's new `RandomState` and `Generator` support,kc611,"As an effort towards #801 , We want to implement a new class based `RandomState` in our Numba backend. It'll allow us to curcumvent Numba's limitation of a global random state and will give the ability to have multiple independent random states just like Numpy's new `RandomState` interface. 

The current implementation for this is at: https://gist.github.com/kc611/821b41413d0f6cc0ad34d842a25aa7ea

The general idea is to synchronize Numba's global random state with the current `RandomStateNumbaType`'s state key before each random draw is made. The new global random state can then be put back into the `RandomStateNumbaType` object, allowing local state based draws. ",enhancement important Numba NumPy compatibility,,2022-02-09 16:22:50,2023-05-09 05:45:44,"brandonwillard labeled 2022-02-09 16:35:40,brandonwillard labeled 2022-02-09 16:35:40,brandonwillard labeled 2022-02-09 16:35:40,brandonwillard labeled 2022-02-09 16:35:40,brandonwillard renamed 2022-09-21 21:41:14,brandonwillard milestoned 2022-10-05 16:33:16,brandonwillard closed 2023-05-09 05:45:44,soraros unsubscribed 2023-05-11 08:46:46",rlouf kc611 soraros brandonwillard,5
653,815,Use numpy.linalg.cholesky instead of scipy,lucianopaz,"## Description of your problem or feature request

Numpy's [Cholesky decomposition](https://numpy.org/doc/stable/reference/generated/numpy.linalg.cholesky.html) can handle arrays of any dimension, as long as their last two dimensions have the same length. Scipy's [Cholesky decomposition](https://docs.scipy.org/doc/scipy/reference/generated/scipy.linalg.cholesky.html) is restricted to using only 2-D arrays.

**Please provide a minimal, self-contained, and reproducible example.**
```python
import numpy as np
from scipy import linalg as sa

a = np.random.randn(10, 3, 3)
b = (np.moveaxis(a, -1, -2) @ a)
c = np.linalg.cholesky(b)  # Returns a (10, 3, 3) array
d = sa.cholesky(b)  # Raises ValueError: Input array needs to be 2D but received a 3d-array.
```

I would like to add an Op that wraps numpy's `cholesky` function and add it to `aesara.tensor.nlinalg`. Luckily, both [jax](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.linalg.cholesky.html) and [numba](https://numba.pydata.org/numba-doc/latest/developer/autogen_numpy_listing.html) already support `numpy.linalg.cholesky`, so we can register those for dispatching.",,,2022-02-10 09:40:09,2022-02-10 09:54:02,"lucianopaz closed 2022-02-10 09:54:02,ricardoV94 mentioned 2022-02-10 09:54:02,ricardoV94 subscribed 2022-02-10 09:54:02",ricardoV94 lucianopaz,2
654,816,One should be able to specify the dtype when using `identity_like`,rlouf,"The [current implementation](https://github.com/aesara-devs/aesara/blob/630b55741946a9aa7c33343a866ca920f0891211/aesara/tensor/basic.py#L1439-L1440) of `identity_like` does not allow to specify the dtype, unlike the current implementation of `ones_like`.",enhancement good first issue,,2022-02-10 13:27:26,2022-06-30 05:21:55,"rlouf labeled 2022-02-10 13:27:26,ricardoV94 labeled 2022-04-08 19:25:52,guyrt referenced 2022-06-28 15:39:31,guyrt referenced 2022-06-28 19:32:14,guyrt referenced 2022-06-29 15:05:47,ricardoV94 closed 2022-06-30 05:21:55,ricardoV94 referenced 2022-06-30 05:21:57",ricardoV94 rlouf guyrt,0
656,818,Not able to import aesara in colab notebooks. ,kc611,"## Description of your problem or feature request

Importing `aesara` in colab environment throws the following error: 
 


**Please provide the full traceback of any errors.**
```python
---------------------------------------------------------------------------
NoSectionError                            Traceback (most recent call last)
/usr/local/lib/python3.7/dist-packages/aesara/configparser.py in fetch_val_for_key(self, key, delete_key)
    238             try:
--> 239                 return self._aesara_cfg.get(section, option)
    240             except InterpolationError:

12 frames
/usr/lib/python3.7/configparser.py in get(self, section, option, raw, vars, fallback)
    779         try:
--> 780             d = self._unify_values(section, vars)
    781         except NoSectionError:

/usr/lib/python3.7/configparser.py in _unify_values(self, section, vars)
   1145             if section != self.default_section:
-> 1146                 raise NoSectionError(section) from None
   1147         # Update with the entry specific variables

NoSectionError: No section: 'blas'

During handling of the above exception, another exception occurred:

KeyError                                  Traceback (most recent call last)
/usr/local/lib/python3.7/dist-packages/aesara/configparser.py in __get__(self, cls, type_, delete_key)
    354             try:
--> 355                 val_str = cls.fetch_val_for_key(self.name, delete_key=delete_key)
    356                 self.is_default = False

/usr/local/lib/python3.7/dist-packages/aesara/configparser.py in fetch_val_for_key(self, key, delete_key)
    242         except (NoOptionError, NoSectionError):
--> 243             raise KeyError(key)
    244 

KeyError: 'blas__ldflags'

During handling of the above exception, another exception occurred:

ModuleNotFoundError                       Traceback (most recent call last)
/usr/local/lib/python3.7/dist-packages/aesara/link/c/cmodule.py in check_mkl_openmp()
   2587     try:
-> 2588         import mkl
   2589 

ModuleNotFoundError: No module named 'mkl'

During handling of the above exception, another exception occurred:

RuntimeError                              Traceback (most recent call last)
<ipython-input-11-de56996b6d0a> in <module>()
----> 1 import aesara

/usr/local/lib/python3.7/dist-packages/aesara/__init__.py in <module>()
    124 
    125 # isort: off
--> 126 from aesara import scalar, tensor
    127 from aesara.compile import (
    128     In,

/usr/local/lib/python3.7/dist-packages/aesara/tensor/__init__.py in <module>()
     96 # adds shared-variable constructors
     97 from aesara.tensor import sharedvar  # noqa
---> 98 from aesara.tensor import (  # noqa
     99     basic_opt,
    100     blas,

/usr/local/lib/python3.7/dist-packages/aesara/tensor/blas.py in <module>()
    164 from aesara.tensor import basic as at
    165 from aesara.tensor.basic_opt import local_dimshuffle_lift
--> 166 from aesara.tensor.blas_headers import blas_header_text, blas_header_version
    167 from aesara.tensor.elemwise import DimShuffle, Elemwise
    168 from aesara.tensor.exceptions import NotScalarConstantError

/usr/local/lib/python3.7/dist-packages/aesara/tensor/blas_headers.py in <module>()
   1013 
   1014 
-> 1015 if not config.blas__ldflags:
   1016     _logger.warning(""Using NumPy C-API based implementation for BLAS functions."")
   1017 

/usr/local/lib/python3.7/dist-packages/aesara/configparser.py in __get__(self, cls, type_, delete_key)
    357             except KeyError:
    358                 if callable(self.default):
--> 359                     val_str = self.default()
    360                 else:
    361                     val_str = self.default

/usr/local/lib/python3.7/dist-packages/aesara/link/c/cmodule.py in default_blas_ldflags()
   2794         if res:
   2795             if ""mkl"" in res:
-> 2796                 check_mkl_openmp()
   2797             return res
   2798 

/usr/local/lib/python3.7/dist-packages/aesara/link/c/cmodule.py in check_mkl_openmp()
   2607 you set this flag and don't set the appropriate environment or make
   2608 sure you have the right version you *will* get wrong results.
-> 2609 """"""
   2610         )
   2611 

RuntimeError: 
Could not import 'mkl'.  If you are using conda, update the numpy
packages to the latest build otherwise, set MKL_THREADING_LAYER=GNU in
your environment for MKL 2018.

If you have MKL 2017 install and are not in a conda environment you
can set the Aesara flag blas__check_openmp to False.  Be warned that if
you set this flag and don't set the appropriate environment or make
sure you have the right version you *will* get wrong results.
```

Link to notebook: https://colab.research.google.com/drive/1bPq3mcIlcw6mJQE8Gf8bcEdVhJdJSUpB#scrollTo=4v5AGEZjmLc2

## Versions and main components

* Operating Environment: Google Colab (default settings and packages)
* Aesara version: 2.4.0
* Python version: 3.7.12
",question,,2022-02-15 18:40:08,2022-07-08 20:32:05,"brandonwillard labeled 2022-02-15 19:58:48,brandonwillard closed 2022-07-08 20:32:05",ricardoV94 kc611 brandonwillard,1
657,819,random_make_inplace assumes that RandomVariable Ops cannot have extra props,lucianopaz,"## Description of your problem or feature request

I'm writing a new random variable Op, and I need to pass some shape related information to the op when it is created. This information should be stored in the `__props__` of the Op and that works fine. The problem is that when aesara tries to optimize the graph, and call `random_make_inplace` assumes that there are not any extra props, and raises an error.

**Please provide a minimal, self-contained, and reproducible example.**
```python
class Test(RandomVariable):
    name = ""test""
    ndim_supp = 0
    ndims_params = [0]
    __props__ = (""name"", ""ndim_supp"", ""ndims_params"", ""dtype"", ""inplace"", ""extra"")
    dtype = ""floatX""
    _print_name = (""Test"", ""\\\\operatorname{Test}"")
    
    def __init__(self, extra, *args, **kwargs):
        self.extra = extra
        super().__init__(*args, **kwargs)

    def make_node(self, rng, size, dtype, sigma):
        return super().make_node(rng, size, dtype, sigma)

    def rng_fn(self, rng, sigma, size):
        return rng.normal(scale=sigma, size=size)

Test(extra=""a"")(sigma=1).eval()
```

**Please provide the full traceback of any errors.**
```python
ERROR (aesara.graph.opt): Optimization failure due to: random_make_inplace
ERROR (aesara.graph.opt): node: Test{name='test', ndim_supp=0, ndims_params=(0,), dtype='floatX', inplace=False, extra='a'}(RandomGeneratorSharedVariable(<Generator(PCG64) at 0x7FEB1976DE40>), TensorConstant{[]}, TensorConstant{11}, TensorConstant{1})
ERROR (aesara.graph.opt): TRACEBACK:
ERROR (aesara.graph.opt): Traceback (most recent call last):
  File ""/home/lpaz/anaconda3/lib/python3.8/site-packages/aesara/graph/opt.py"", line 1994, in process_node
    replacements = lopt.transform(fgraph, node)
  File ""/home/lpaz/anaconda3/lib/python3.8/site-packages/aesara/graph/opt.py"", line 1204, in transform
    return self.fn(fgraph, node)
  File ""/home/lpaz/anaconda3/lib/python3.8/site-packages/aesara/tensor/random/opt.py"", line 47, in random_make_inplace
    name, ndim_supp, ndims_params, dtype, _ = op._props()
ValueError: too many values to unpack (expected 5)
```

**Please provide any additional information below.**

I spoke with @ricardoV94 and it seems like it should be easy to patch [this opt](https://github.com/aesara-devs/aesara/blob/main/aesara/tensor/random/opt.py#L42-L51) to consume any extra `__props__` and keep them when creating the inplace version of the Op

## Versions and main components

* Aesara version: 2.3.8
* Aesara config (`python -c ""import aesara; print(aesara.config)""`)
* Python version: 3.8
* Operating system: Ubuntu 18.04
* How did you install Aesara: pip
",refactor random variables,,2022-02-16 14:55:31,2022-02-17 06:25:42,"ricardoV94 mentioned 2022-02-16 14:55:32,ricardoV94 subscribed 2022-02-16 14:55:32,brandonwillard labeled 2022-02-16 21:53:46,brandonwillard labeled 2022-02-16 21:53:46,ricardoV94 closed 2022-02-17 06:25:42",ricardoV94 lucianopaz brandonwillard,0
661,823,`DiffOp` output type is wrong,ricardoV94,"The DiffOp uses `input.type()` as the output type, which is wrong when we know more about the shape of the inputs:

```python
y = at.diff([1, 2, 3])
print(y.type)  # TensorType(int64, (3,))
y.eval()
```

Which leads to a TypeError during constant folding:
```
TypeError: Cannot convert Type TensorType(int64, (2,)) (of Variable TensorConstant{(2,) of 1}) into Type TensorType(int64, (3,)). You can try to manually convert TensorConstant{(2,) of 1} into a TensorType(int64, (3,)).
```

Related to #732 and #821",bug shape inference,,2022-02-17 12:57:48,2022-03-14 06:18:46,"ricardoV94 labeled 2022-02-17 12:57:48,ricardoV94 labeled 2022-02-17 12:57:48,brandonwillard renamed 2022-02-17 16:53:22,ricardoV94 closed 2022-03-14 06:18:46",ricardoV94 brandonwillard,0
667,830,Bug in `squeeze` when using negative `axis`,ricardoV94,"Squeeze does not seem to do anything when axis is negative:
```python
import aesara.tensor as at
at.squeeze(at.as_tensor_variable([1]), axis=-1).eval()
# array([1])
at.squeeze(at.as_tensor_variable([[1]]), axis=-1).eval()
# array([[1]])
```
This doesn't happen with positive axis:
```python
at.squeeze(at.as_tensor_variable([1]), axis=0).eval()
# array(1)
at.squeeze(at.as_tensor_variable([[1]]), axis=0).eval()
# array([1])
```",bug good first issue,ricardoV94,2022-02-21 14:36:07,2022-03-23 21:55:37,"ricardoV94 labeled 2022-02-21 14:36:07,kc611 labeled 2022-02-21 17:24:22,ricardoV94 assigned 2022-02-23 21:19:55,ricardoV94 referenced 2022-02-24 12:09:48,ricardoV94 referenced 2022-02-24 12:21:34,ricardoV94 referenced 2022-02-24 12:27:28,ricardoV94 referenced 2022-02-25 10:29:18,ricardoV94 referenced 2022-02-25 10:34:01,ricardoV94 referenced 2022-02-25 10:36:11,ricardoV94 referenced 2022-03-02 16:26:38,ricardoV94 referenced 2022-03-02 19:17:52,ricardoV94 referenced 2022-03-03 10:32:48,ricardoV94 referenced 2022-03-13 14:57:59,ricardoV94 referenced 2022-03-14 17:57:02,ricardoV94 referenced 2022-03-16 17:25:52,ricardoV94 referenced 2022-03-16 17:29:06,ricardoV94 referenced 2022-03-17 08:51:40,ricardoV94 referenced 2022-03-21 18:00:22,ricardoV94 referenced 2022-03-23 11:34:13,ricardoV94 closed 2022-03-23 21:55:37",ricardoV94 kc611,2
673,838,Error where `ones_like` variable passed to `max`,ghost,"## Description of your problem or feature request

I've just started playing around with aesara, and was trying to reproduce this numpy code, and generated the traceback below.

```
import numpy as np

x = np.array([[1, 2, 3], [0, 2, 0]])
np.maximum(x, np.ones_like(x))
```


**Please provide a minimal, self-contained, and reproducible example.**

```python
import aesara
import aesara.tensor as at

x = at.dmatrix()
z = at.max(x, at.ones_like(x))
f = aesara.function([x], z)
```

**Please provide the full traceback of any errors.**

```
$ python -i aesara_test.py
Traceback (most recent call last):
  File ""/home/user/.local/lib/python3.9/site-packages/aesara/tensor/math.py"", line 671, in max
    out = max_and_argmax(x, axis)[0]
  File ""/home/user/.local/lib/python3.9/site-packages/aesara/tensor/math.py"", line 582, in max_and_argmax
    axis = check_and_normalize_axes(a, axis)
  File ""/home/user/.local/lib/python3.9/site-packages/aesara/tensor/math.py"", line 537, in check_and_normalize_axes
    raise TypeError(f""Computation needs a constant axis. Got {axis}"")
TypeError: Computation needs a constant axis. Got Elemwise{second,no_inplace}.0

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/home/user/.local/lib/python3.9/site-packages/aesara/tensor/var.py"", line 609, in __iter__
    for i in range(at.basic.get_vector_length(self)):
  File ""/home/user/.local/lib/python3.9/site-packages/aesara/tensor/__init__.py"", line 74, in get_vector_length
    raise TypeError(f""Argument must be a vector; got {v.type}"")
TypeError: Argument must be a vector; got TensorType(float64, (None, None))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/home/user/aesara_test.py"", line 5, in <module>
    z = at.max(x, at.ones_like(x))
  File ""/home/user/.local/lib/python3.9/site-packages/aesara/tensor/math.py"", line 673, in max
    out = Max(axis)(x)
  File ""/home/user/.local/lib/python3.9/site-packages/aesara/tensor/math.py"", line 628, in __init__
    super().__init__(aes.scalar_maximum, axis)
  File ""/home/user/.local/lib/python3.9/site-packages/aesara/tensor/elemwise.py"", line 1284, in __init__
    self.axis = tuple(axis)
  File ""/home/user/.local/lib/python3.9/site-packages/aesara/tensor/var.py"", line 613, in __iter__
    raise TypeError(
TypeError: TensorType does not support iteration. Maybe you are using builtins.sum instead of aesara.tensor.math.sum? (Maybe .max?)
```

## Versions and main components

* Aesara version: 2.4.0
* Python version: 3.9.9
* Operating system: Linux
* How did you install Aesara: pip

<details><summary>aesara config</summary>

    $ python --version
    Python 3.9.9
    $ python -c ""import aesara; print(aesara.config)""
    floatX ({'float16', 'float64', 'float32'})
        Doc:  Default floating-point precision for python casts.
    
    Note: float16 support is experimental, use at your own risk.
        Value:  float64
    
    warn_float64 ({'pdb', 'raise', 'ignore', 'warn'})
        Doc:  Do an action when a tensor variable with float64 dtype is created. They can't be run on the GPU with the current(old) gpu back-end and are slow with gamer GPUs.
        Value:  ignore
    
    pickle_test_value (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f40c180e160>>)
        Doc:  Dump test values while pickling model. If True, test values will be dumped with model.
        Value:  True
    
    cast_policy ({'numpy+floatX', 'custom'})
        Doc:  Rules for implicit type casting
        Value:  custom
    
    deterministic ({'more', 'default'})
        Doc:  If `more`, sometimes we will select some implementation that are more deterministic, but slower. In particular, on the GPU, we will avoid using AtomicAdd. Sometimes we will still use non-deterministic implementation, e.g. when we do not have a GPU implementation that is deterministic. Also see the dnn.conv.algo* flags to cover more cases.
        Value:  default
    
    device (cpu, opencl*, cuda*)
        Doc:  Default device for computations. If cuda* or opencl*, change thedefault to try to move computation to the GPU. Do not use upper caseletters, only lower case even if NVIDIA uses capital letters. 'gpu' means let the driver select the gpu (needed for gpu in exclusive mode). 'gpuX' mean use the gpu number X.
        Value:  cpu
    
    init_gpu_device (, opencl*, cuda*)
        Doc:  Initialize the gpu device to use, works only if device=cpu. Unlike 'device', setting this option will NOT move computations, nor shared variables, to the specified GPU. It can be used to run GPU-specific tests on a particular GPU.
        Value:
    
    force_device (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f40d6c9b580>>)
        Doc:  Raise an error if we can't use the specified device
        Value:  False
    
    conv__assert_shape (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f40c18233d0>>)
        Doc:  If True, AbstractConv* ops will verify that user-provided shapes match the runtime shapes (debugging option, may slow down compilation)
        Value:  False
    
    print_global_stats (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f40c18230a0>>)
        Doc:  Print some global statistics (time spent) at the end
        Value:  False
    
    <aesara.configparser.ContextsParam object at 0x7f40c1823160>
        Doc:
            Context map for multi-gpu operation. Format is a
            semicolon-separated list of names and device names in the
            'name->dev_name' format. An example that would map name 'test' to
            device 'cuda0' and name 'test2' to device 'opencl0:0' follows:
            ""test->cuda0;test2->opencl0:0"".
    
            Invalid context names are 'cpu', 'cuda*' and 'opencl*'
    
        Value:
    
    print_active_device (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f40c1823280>>)
        Doc:  Print active device at when the GPU device is initialized.
        Value:  True
    
    gpuarray__preallocate (<class 'float'>)
        Doc:  If negative it disables the allocation cache. If
                     between 0 and 1 it enables the allocation cache and
                     preallocates that fraction of the total GPU memory.  If 1
                     or greater it will preallocate that amount of memory (in
                     megabytes).
        Value:  0.0
    
    gpuarray__sched ({'single', 'multi', 'default'})
        Doc:  The sched parameter passed for context creation to pygpu.
                        With CUDA, using ""multi"" is equivalent to using the parameter
                        cudaDeviceScheduleBlockingSync. This is useful to lower the
                        CPU overhead when waiting for GPU. One user found that it
                        speeds up his other processes that was doing data augmentation.
    
        Value:  default
    
    gpuarray__single_stream (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f40c17c12b0>>)
        Doc:
                     If your computations are mostly lots of small elements,
                     using single-stream will avoid the synchronization
                     overhead and usually be faster.  For larger elements it
                     does not make a difference yet.  In the future when true
                     multi-stream is enabled in libgpuarray, this may change.
                     If you want to make sure to have optimal performance,
                     check both options.
    
        Value:  True
    
    cuda__root (<class 'str'>)
        Doc:  Location of the cuda installation
        Value:
    
    cuda__include_path (<class 'str'>)
        Doc:  Location of the cuda includes
        Value:
    
    assert_no_cpu_op ({'pdb', 'raise', 'ignore', 'warn'})
        Doc:  Raise an error/warning if there is a CPU op in the computational graph.
        Value:  ignore
    
    unpickle_function (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f40c17c1400>>)
        Doc:  Replace unpickled Aesara functions with None. This is useful to unpickle old graphs that pickled them when it shouldn't
        Value:  True
    
    reoptimize_unpickled_function (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f40c17c1430>>)
        Doc:  Re-optimize the graph when an Aesara function is unpickled from the disk.
        Value:  False
    
    dnn__conv__algo_fwd ({'fft', 'winograd_non_fused', 'small', 'guess_on_shape_change', 'none', 'large', 'time_once', 'time_on_shape_change', 'fft_tiling', 'winograd', 'guess_once'})
        Doc:  Default implementation to use for cuDNN forward convolution.
        Value:  small
    
    dnn__conv__algo_bwd_data ({'fft', 'deterministic', 'winograd_non_fused', 'guess_on_shape_change', 'none', 'time_once', 'time_on_shape_change', 'fft_tiling', 'winograd', 'guess_once'})
        Doc:  Default implementation to use for cuDNN backward convolution to get the gradients of the convolution with regard to the inputs.
        Value:  none
    
    dnn__conv__algo_bwd_filter ({'fft', 'deterministic', 'small', 'winograd_non_fused', 'none', 'time_once', 'time_on_shape_change', 'fft_tiling', 'guess_on_shape_change', 'guess_once'})
        Doc:  Default implementation to use for cuDNN backward convolution to get the gradients of the convolution with regard to the filters.
        Value:  none
    
    dnn__conv__precision ({'as_input_f32', 'float64', 'float16', 'float32', 'as_input'})
        Doc:  Default data precision to use for the computation in cuDNN convolutions (defaults to the same dtype as the inputs of the convolutions, or float32 if inputs are float16).
        Value:  as_input_f32
    
    dnn__base_path (<class 'str'>)
        Doc:  Install location of cuDNN.
        Value:
    
    dnn__include_path (<class 'str'>)
        Doc:  Location of the cudnn header
        Value:
    
    dnn__library_path (<class 'str'>)
        Doc:  Location of the cudnn link library.
        Value:
    
    dnn__bin_path (<class 'str'>)
        Doc:  Location of the cuDNN load library (on non-windows platforms, this is the same as dnn__library_path)
        Value:
    
    dnn__enabled ({'no_check', 'False', 'auto', 'True'})
        Doc:  'auto', use cuDNN if available, but silently fall back to not using it if not present. If True and cuDNN can not be used, raise an error. If False, disable cudnn even if present. If no_check, assume present and the version between header and library match (so less compilation at context init)
        Value:  auto
    
    magma__include_path (<class 'str'>)
        Doc:  Location of the magma header
        Value:
    
    magma__library_path (<class 'str'>)
        Doc:  Location of the magma library
        Value:
    
    magma__enabled (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f40c17c1790>>)
        Doc:   If True, use magma for matrix computation. If False, disable magma
        Value:  False
    
    <aesara.configparser.ConfigParam object at 0x7f40c17c17c0>
        Doc:  Default compilation mode
        Value:  Mode
    
    cxx (<class 'str'>)
        Doc:  The C++ compiler to use. Currently only g++ is supported, but supporting additional compilers should not be too difficult. If it is empty, no C++ code is compiled.
        Value:  /usr/bin/g++
    
    linker ({'py', 'vm', 'c|py_nogc', 'cvm_nogc', 'c|py', 'vm_nogc', 'c', 'cvm'})
        Doc:  Default linker used if the aesara flags mode is Mode
        Value:  cvm
    
    allow_gc (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f40c17c1730>>)
        Doc:  Do we default to delete intermediate results during Aesara function calls? Doing so lowers the memory requirement, but asks that we reallocate memory at the next function call. This is implemented for the default linker, but may not work for all linkers.
        Value:  True
    
    optimizer ({'None', 'o4', 'o1', 'o2', 'merge', 'fast_run', 'fast_compile', 'o3', 'unsafe'})
        Doc:  Default optimizer. If not None, will use this optimizer with the Mode
        Value:  o4
    
    optimizer_verbose (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f40c17c19a0>>)
        Doc:  If True, we print all optimization being applied
        Value:  False
    
    on_opt_error ({'pdb', 'raise', 'ignore', 'warn'})
        Doc:  What to do when an optimization crashes: warn and skip it, raise the exception, or fall into the pdb debugger.
        Value:  warn
    
    nocleanup (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f40c17c1a00>>)
        Doc:  Suppress the deletion of code files that did not compile cleanly
        Value:  False
    
    on_unused_input ({'raise', 'ignore', 'warn'})
        Doc:  What to do if a variable in the 'inputs' list of  aesara.function() is not used in the graph.
        Value:  raise
    
    gcc__cxxflags (<class 'str'>)
        Doc:  Extra compiler flags for gcc
        Value:
    
    cmodule__warn_no_version (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f40c17c18b0>>)
        Doc:  If True, will print a warning when compiling one or more Op with C code that can't be cached because there is no c_code_cache_version() function associated to at least one of those Ops.
        Value:  False
    
    cmodule__remove_gxx_opt (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f40c17c1940>>)
        Doc:  If True, will remove the -O* parameter passed to g++.This is useful to debug in gdb modules compiled by Aesara.The parameter -g is passed by default to g++
        Value:  False
    
    cmodule__compilation_warning (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f40c17c1ac0>>)
        Doc:  If True, will print compilation warnings.
        Value:  False
    
    cmodule__preload_cache (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f40c17c1af0>>)
        Doc:  If set to True, will preload the C module cache at import time
        Value:  False
    
    cmodule__age_thresh_use (<class 'int'>)
        Doc:  In seconds. The time after which Aesara won't reuse a compile c module.
        Value:  2073600
    
    cmodule__debug (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f40c17c1b50>>)
        Doc:  If True, define a DEBUG macro (if not exists) for any compiled C code.
        Value:  False
    
    compile__wait (<class 'int'>)
        Doc:  Time to wait before retrying to acquire the compile lock.
        Value:  5
    
    compile__timeout (<class 'int'>)
        Doc:  In seconds, time that a process will wait before deciding to
        override an existing lock. An override only happens when the existing
        lock is held by the same owner *and* has not been 'refreshed' by this
        owner for more than this period. Refreshes are done every half timeout
        period for running processes.
        Value:  120
    
    ctc__root (<class 'str'>)
        Doc:  Directory which contains the root of Baidu CTC library. It is assumed         that the compiled library is either inside the build, lib or lib64         subdirectory, and the header inside the include directory.
        Value:
    
    tensor__cmp_sloppy (<class 'int'>)
        Doc:  Relax aesara.tensor.math._allclose (0) not at all, (1) a bit, (2) more
        Value:  0
    
    tensor__local_elemwise_fusion (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f40c17c1d00>>)
        Doc:  Enable or not in fast_run mode(fast_run optimization) the elemwise fusion optimization
        Value:  True
    
    lib__amblibm (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f40c17c1d90>>)
        Doc:  Use amd's amdlibm numerical library
        Value:  False
    
    tensor__insert_inplace_optimizer_validate_nb (<class 'int'>)
        Doc:  -1: auto, if graph have less then 500 nodes 1, else 10
        Value:  -1
    
    traceback__limit (<class 'int'>)
        Doc:  The number of stack to trace. -1 mean all.
        Value:  8
    
    traceback__compile_limit (<class 'int'>)
        Doc:  The number of stack to trace to keep during compilation. -1 mean all. If greater then 0, will also make us save Aesara internal stack trace.
        Value:  0
    
    experimental__unpickle_gpu_on_cpu (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f40c17c1f10>>)
        Doc:  Allow unpickling of pickled GpuArrays as numpy.ndarrays.This is useful, if you want to open a GpuArray without having cuda installed.If you have cuda installed, this will force unpickling tobe done on the cpu to numpy.ndarray.Please be aware that this may get you access to the data,however, trying to unpicke gpu functions will not succeed.This flag is experimental and may be removed any time, whengpu<>cpu transparency is solved.
        Value:  False
    
    experimental__local_alloc_elemwise (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f40c17c1f40>>)
        Doc:  DEPRECATED: If True, enable the experimental optimization local_alloc_elemwise. Generates error if not True. Use optimizer_excluding=local_alloc_elemwise to disable.
        Value:  True
    
    experimental__local_alloc_elemwise_assert (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f40c17c1fd0>>)
        Doc:  When the local_alloc_elemwise is applied, add an assert to highlight shape errors.
        Value:  True
    
    warn__ignore_bug_before ({'0.7', '1.0.4', '1.0.3', '0.4.1', '1.0.1', '0.10', '1.0.5', '0.8.2', '0.6', '1.0', '0.3', 'None', '0.8', '1.0.2', 'all', '0.4', '0.5', '0.8.1', '0.9'})
        Doc:  If 'None', we warn about all Aesara bugs found by default. If 'all', we don't warn about Aesara bugs found by default. If a version, we print only the warnings relative to Aesara bugs found after that version. Warning for specific bugs can be configured with specific [warn] flags.
        Value:  0.9
    
    exception_verbosity ({'low', 'high'})
        Doc:  If 'low', the text of exceptions will generally refer to apply nodes with short names such as Elemwise{add_no_inplace}. If 'high', some exceptions will also refer to apply nodes with long descriptions  like:
            A. Elemwise{add_no_inplace}
                    B. log_likelihood_v_given_h
                    C. log_likelihood_h
        Value:  low
    
    print_test_value (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f40c17cf100>>)
        Doc:  If 'True', the __eval__ of an Aesara variable will return its test_value when this is available. This has the practical conseguence that, e.g., in debugging `my_var` will print the same as `my_var.tag.test_value` when a test value is defined.
        Value:  False
    
    compute_test_value ({'pdb', 'warn', 'raise', 'ignore', 'off'})
        Doc:  If 'True', Aesara will run each op at graph build time, using Constants, SharedVariables and the tag 'test_value' as inputs to the function. This helps the user track down problems in the graph before it gets optimized.
        Value:  off
    
    compute_test_value_opt ({'pdb', 'warn', 'raise', 'ignore', 'off'})
        Doc:  For debugging Aesara optimization only. Same as compute_test_value, but is used during Aesara optimization
        Value:  off
    
    check_input (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f40c17cf190>>)
        Doc:  Specify if types should check their input in their C code. It can be used to speed up compilation, reduce overhead (particularly for scalars) and reduce the number of generated C files.
        Value:  True
    
    NanGuardMode__nan_is_error (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f40c17cf1c0>>)
        Doc:  Default value for nan_is_error
        Value:  True
    
    NanGuardMode__inf_is_error (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f40c17cf1f0>>)
        Doc:  Default value for inf_is_error
        Value:  True
    
    NanGuardMode__big_is_error (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f40c17cf280>>)
        Doc:  Default value for big_is_error
        Value:  True
    
    NanGuardMode__action ({'pdb', 'raise', 'warn'})
        Doc:  What NanGuardMode does when it finds a problem
        Value:  raise
    
    DebugMode__patience (<class 'int'>)
        Doc:  Optimize graph this many times to detect inconsistency
        Value:  10
    
    DebugMode__check_c (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f40c17cf310>>)
        Doc:  Run C implementations where possible
        Value:  True
    
    DebugMode__check_py (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f40c17cf3a0>>)
        Doc:  Run Python implementations where possible
        Value:  True
    
    DebugMode__check_finite (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f40c17cf3d0>>)
        Doc:  True -> complain about NaN/Inf results
        Value:  True
    
    DebugMode__check_strides (<class 'int'>)
        Doc:  Check that Python- and C-produced ndarrays have same strides. On difference: (0) - ignore, (1) warn, or (2) raise error
        Value:  0
    
    DebugMode__warn_input_not_reused (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f40c17cf430>>)
        Doc:  Generate a warning when destroy_map or view_map says that an op works inplace, but the op did not reuse the input for its output.
        Value:  True
    
    DebugMode__check_preallocated_output (<class 'str'>)
        Doc:  Test thunks with pre-allocated memory as output storage. This is a list of strings separated by "":"". Valid values are: ""initial"" (initial storage in storage map, happens with Scan),""previous"" (previously-returned memory), ""c_contiguous"", ""f_contiguous"", ""strided"" (positive and negative strides), ""wrong_size"" (larger and smaller dimensions), and ""ALL"" (all of the above).
        Value:
    
    DebugMode__check_preallocated_output_ndim (<class 'int'>)
        Doc:  When testing with ""strided"" preallocated output memory, test all combinations of strides over that number of (inner-most) dimensions. You may want to reduce that number to reduce memory or time usage, but it is advised to keep a minimum of 2.
        Value:  4
    
    profiling__time_thunks (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f40c17cf4c0>>)
        Doc:  Time individual thunks when profiling
        Value:  True
    
    profiling__n_apply (<class 'int'>)
        Doc:  Number of Apply instances to print by default
        Value:  20
    
    profiling__n_ops (<class 'int'>)
        Doc:  Number of Ops to print by default
        Value:  20
    
    profiling__output_line_width (<class 'int'>)
        Doc:  Max line width for the profiling output
        Value:  512
    
    profiling__min_memory_size (<class 'int'>)
        Doc:  For the memory profile, do not print Apply nodes if the size
                     of their outputs (in bytes) is lower than this threshold
        Value:  1024
    
    profiling__min_peak_memory (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f40c17cf610>>)
        Doc:  The min peak memory usage of the order
        Value:  False
    
    profiling__destination (<class 'str'>)
        Doc:  File destination of the profiling output
        Value:  stderr
    
    profiling__debugprint (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f40c17cf670>>)
        Doc:  Do a debugprint of the profiled functions
        Value:  False
    
    profiling__ignore_first_call (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f40c17cf6a0>>)
        Doc:  Do we ignore the first call of an Aesara function.
        Value:  False
    
    on_shape_error ({'raise', 'warn'})
        Doc:  warn: print a warning and use the default value. raise: raise an error
        Value:  warn
    
    openmp (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f40c17cf700>>)
        Doc:  Allow (or not) parallel computation on the CPU with OpenMP. This is the default value used when creating an Op that supports OpenMP parallelization. It is preferable to define it via the Aesara configuration file ~/.aesararc or with the environment variable AESARA_FLAGS. Parallelization is only done for some operations that implement it, and even for operations that implement parallelism, each operation is free to respect this flag or not. You can control the number of threads used with the environment variable OMP_NUM_THREADS. If it is set to 1, we disable openmp in Aesara by default.
        Value:  False
    
    openmp_elemwise_minsize (<class 'int'>)
        Doc:  If OpenMP is enabled, this is the minimum size of vectors for which the openmp parallelization is enabled in element wise ops.
        Value:  200000
    
    optimizer_excluding (<class 'str'>)
        Doc:  When using the default mode, we will remove optimizer with these tags. Separate tags with ':'.
        Value:
    
    optimizer_including (<class 'str'>)
        Doc:  When using the default mode, we will add optimizer with these tags. Separate tags with ':'.
        Value:
    
    optimizer_requiring (<class 'str'>)
        Doc:  When using the default mode, we will require optimizer with these tags. Separate tags with ':'.
        Value:
    
    optdb__position_cutoff (<class 'float'>)
        Doc:  Where to stop eariler during optimization. It represent the position of the optimizer where to stop.
        Value:  inf
    
    optdb__max_use_ratio (<class 'float'>)
        Doc:  A ratio that prevent infinite loop in EquilibriumOptimizer.
        Value:  8.0
    
    cycle_detection ({'fast', 'regular'})
        Doc:  If cycle_detection is set to regular, most inplaces are allowed,but it is slower. If cycle_detection is set to faster, less inplacesare allowed, but it makes the compilation faster.The interaction of which one give the lower peak memory usage iscomplicated and not predictable, so if you are close to the peakmemory usage, triyng both could give you a small gain.
        Value:  regular
    
    check_stack_trace ({'off', 'raise', 'log', 'warn'})
        Doc:  A flag for checking the stack trace during the optimization process. default (off): does not check the stack trace of any optimization log: inserts a dummy stack trace that identifies the optimizationthat inserted the variable that had an empty stack trace.warn: prints a warning if a stack trace is missing and also a dummystack trace is inserted that indicates which optimization insertedthe variable that had an empty stack trace.raise: raises an exception if a stack trace is missing
        Value:  off
    
    metaopt__verbose (<class 'int'>)
        Doc:  0 for silent, 1 for only warnings, 2 for full output withtimings and selected implementation
        Value:  0
    
    metaopt__optimizer_excluding (<class 'str'>)
        Doc:  exclude optimizers with these tags. Separate tags with ':'.
        Value:
    
    metaopt__optimizer_including (<class 'str'>)
        Doc:  include optimizers with these tags. Separate tags with ':'.
        Value:
    
    profile (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f40c17cfa00>>)
        Doc:  If VM should collect profile information
        Value:  False
    
    profile_optimizer (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f40c17cfa30>>)
        Doc:  If VM should collect optimizer profile information
        Value:  False
    
    profile_memory (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f40c17cfa60>>)
        Doc:  If VM should collect memory profile information and print it
        Value:  False
    
    <aesara.configparser.ConfigParam object at 0x7f40c17cfa90>
        Doc:  Useful only for the vm linkers. When lazy is None, auto detect if lazy evaluation is needed and use the appropriate version. If lazy is True/False, force the version used between Loop/LoopGC and Stack.
        Value:  None
    
    unittests__rseed (<class 'str'>)
        Doc:  Seed to use for randomized unit tests. Special value 'random' means using a seed of None.
        Value:  666
    
    warn__round (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f40c17cfb50>>)
        Doc:  Warn when using `tensor.round` with the default mode. Round changed its default from `half_away_from_zero` to `half_to_even` to have the same default as NumPy.
        Value:  False
    
    numba__vectorize_target ({'cpu', 'parallel', 'cuda'})
        Doc:  Default target for numba.vectorize.
        Value:  cpu
    
    numba__fastmath (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f40c17cfc10>>)
        Doc:  If True, use Numba's fastmath mode.
        Value:  True
    
    numba__cache (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f40c17cfca0>>)
        Doc:  If True, use Numba's file based caching.
        Value:  True
    
    compiledir_format (<class 'str'>)
        Doc:  Format string for platform-dependent compiled module subdirectory
    (relative to base_compiledir). Available keys: aesara_version, device,
    gxx_version, hostname, numpy_version, platform, processor,
    python_bitwidth, python_int_bitwidth, python_version, short_platform.
    Defaults to compiledir_%(short_platform)s-%(processor)s-%(python_versi
    on)s-%(python_bitwidth)s.
        Value:  compiledir_%(short_platform)s-%(processor)s-%(python_version)s-%(python_bitwidth)s
    
    <aesara.configparser.ConfigParam object at 0x7f40c17cfd60>
        Doc:  platform-independent root directory for compiled modules
        Value:  /home/user/.aesara
    
    <aesara.configparser.ConfigParam object at 0x7f40c17cfd00>
        Doc:  platform-dependent cache directory for compiled modules
        Value:  /home/user/.aesara/compiledir_Linux-5.15-gentoo-rob-2-x86_64-Intel-R-_Core-TM-_i7-3632QM_CPU_@_2.20GHz-with-glibc2.33-Intel_R_Core_TM_i7-3632QM_CPU_@_2.20GHz-3.9.9-64
    
    <aesara.configparser.ConfigParam object at 0x7f40c17cfd30>
        Doc:  Directory to cache pre-compiled kernels for the gpuarray backend.
        Value:  /home/user/.aesara/compiledir_Linux-5.15-gentoo-rob-2-x86_64-Intel-R-_Core-TM-_i7-3632QM_CPU_@_2.20GHz-with-glibc2.33-Intel_R_Core_TM_i7-3632QM_CPU_@_2.20GHz-3.9.9-64/gpuarray_kernels
    
    blas__ldflags (<class 'str'>)
        Doc:  lib[s] to include for [Fortran] level-3 blas implementation
        Value:  -L/usr/lib64 -lopenblas -lopenblas
    
    blas__check_openmp (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f40d3799370>>)
        Doc:  Check for openmp library conflict.
    WARNING: Setting this to False leaves you open to wrong results in blas-related operations.
        Value:  True
    
    scan__allow_gc (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f40c17dd2e0>>)
        Doc:  Allow/disallow gc inside of Scan (default: False)
        Value:  False
    
    scan__allow_output_prealloc (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f40c8f35940>>)
        Doc:  Allow/disallow memory preallocation for outputs inside of scan (default: True)
        Value:  True
    
    $ pip show aesara
    Name: aesara
    Version: 2.4.0
    


</details>",,,2022-03-02 14:33:37,2022-03-02 15:01:03,,ricardoV94 ghost,2
678,845,warning about shared variables in using sampling_jax ,fbarfi,"## Description of your problem or feature request
Getting error message :
Graph contains shared variables with default_update which cannot be safely replaced.

I am not using any shared variables in my code/model

**Please provide a minimal, self-contained, and reproducible example.**
```python
pm.sampling_jax.sample_numpyro_nuts()
```

**Please provide the full traceback of any errors.**
```python
[ValueError                                Traceback (most recent call last)
Input In [7], in <module>
      1 SEED =123456
      3 with SAR_model:
----> 4         idata = pm.sampling_jax.sample_numpyro_nuts(draws=100,tune=100)

File /opt/homebrew/Caskroom/miniforge/base/envs/pymc-dev-py39/lib/python3.9/site-packages/pymc/sampling_jax.py:230, in sample_numpyro_nuts(draws, tune, chains, target_accept, random_seed, initvals, model, var_names, progress_bar, keep_untransformed, chain_method, idata_kwargs, nuts_kwargs)
    221 print(""Compiling..."", file=sys.stdout)
    223 init_params = _get_batched_jittered_initial_points(
    224     model=model,
    225     chains=chains,
    226     initvals=initvals,
    227     random_seed=random_seed,
    228 )
--> 230 logp_fn = get_jaxified_logp(model)
    232 if nuts_kwargs is None:
    233     nuts_kwargs = {}

File /opt/homebrew/Caskroom/miniforge/base/envs/pymc-dev-py39/lib/python3.9/site-packages/pymc/sampling_jax.py:102, in get_jaxified_logp(model)
    100 def get_jaxified_logp(model: Model) -> Callable:
--> 102     logp_fn = get_jaxified_graph(inputs=model.value_vars, outputs=[model.logpt()])
    104     def logp_fn_wrap(x):
    105         # NumPyro expects a scalar potential with the opposite sign of model.logpt
    106         res = logp_fn(*x)[0]

File /opt/homebrew/Caskroom/miniforge/base/envs/pymc-dev-py39/lib/python3.9/site-packages/pymc/sampling_jax.py:79, in get_jaxified_graph(inputs, outputs)
     73 def get_jaxified_graph(
     74     inputs: Optional[List[TensorVariable]] = None,
     75     outputs: Optional[List[TensorVariable]] = None,
     76 ) -> List[TensorVariable]:
     77     """"""Compile an Aesara graph into an optimized JAX function""""""
---> 79     graph = _replace_shared_variables(outputs)
     81     fgraph = FunctionGraph(inputs=inputs, outputs=graph, clone=True)
     82     # We need to add a Supervisor to the fgraph to be able to run the
     83     # JAX sequential optimizer without warnings. We made sure there
     84     # are no mutable input variables, so we only need to check for
     85     # ""destroyers"". This should be automatically handled by Aesara
     86     # once https://github.com/aesara-devs/aesara/issues/637 is fixed.

File /opt/homebrew/Caskroom/miniforge/base/envs/pymc-dev-py39/lib/python3.9/site-packages/pymc/sampling_jax.py:62, in _replace_shared_variables(graph)
     59 shared_variables = [var for var in graph_inputs(graph) if isinstance(var, SharedVariable)]
     61 if any(hasattr(var, ""default_update"") for var in shared_variables):
---> 62     raise ValueError(
     63         ""Graph contains shared variables with default_update which cannot ""
     64         ""be safely replaced.""
     65     )
     67 replacements = {var: at.constant(var.get_value(borrow=True)) for var in shared_variables}
     69 new_graph = clone_replace(graph, replace=replacements)

ValueError: Graph contains shared variables with default_update which cannot be safely replaced.
```

**Please provide any additional information below.**


## Versions and main components

* Aesara version: latest
* Aesara config (`python -c ""import aesara; print(aesara.config)""`)
* Python version: 3.9.10
* Operating system: Apple M1
* How did you install Aesara: (conda/pip) Conda 
",MWE needed,,2022-03-05 22:32:24,2022-03-06 06:38:56,"brandonwillard closed 2022-03-06 06:38:56,brandonwillard labeled 2022-03-06 06:39:17",fbarfi brandonwillard,2
679,846,Implement JAX dispatch for `SolveTriangular` `Op`,ricardoV94,,good first issue help wanted JAX,,2022-03-07 17:59:16,2022-03-31 18:06:26,"ricardoV94 labeled 2022-03-07 17:59:16,ricardoV94 labeled 2022-03-07 17:59:16,ricardoV94 labeled 2022-03-07 17:59:16,ricardoV94 renamed 2022-03-07 17:59:23,ricardoV94 renamed 2022-03-07 18:10:27,brandonwillard renamed 2022-03-23 21:55:37,zaxtax referenced 2022-03-30 23:23:39,zaxtax referenced 2022-03-31 09:44:47,ricardoV94 closed 2022-03-31 18:06:26,ricardoV94 referenced 2022-03-31 18:06:28",ricardoV94 zaxtax twiecki brandonwillard,1
682,849,"Memory leak with `grad(sum(M @ v), v)` where `M` is shared.",mattearllongshot,"## Description of your problem or feature request

**Please provide a minimal, self-contained, and reproducible example.**
```python
import os

import aesara
import aesara.tensor as at
import numpy as np
import psutil

# Set up the computation graph.  This is just the gradient of sum(M @ v) wrt v.
v = at.vector()
M = aesara.shared(np.zeros((100, 100)), borrow=False)
y = at.sum(M @ v)
f = aesara.function([v], at.grad(y, v))

# call the function 10,000 times, recording memory every 100 iterations.
rss_list = []
for _ in range(10):
    for _ in range(100):
        M.set_value(np.random.random((100, 100)).astype('float32'), borrow=False)
        f(np.random.random((100,)).astype('float32'))
    rss_list.append(psutil.Process(os.getpid()).memory_info().rss)
    
# Show the memory increase per iteration.
np.diff(np.array(rss_list)) / 100
```

This prints:
```
array([78725.12, 79503.36, 79421.44, 82534.4 , 78438.4 , 79953.92,
       79831.04, 80076.8 , 82657.28])
```

The amount increased is proportional to the size of the array passed in the set_value call.

**Please provide any additional information below.**

It doesn't happen if `M + v` is done instead of `M @ v`.  Nor does it happen if f calculates just `sum(M @ v)` rather than `grad(sum(M @ v), v)`

## Versions and main components

* Aesara version: 2.5.0
* Aesara config (`python -c ""import aesara; print(aesara.config)""`)

```
floatX ({'float64', 'float16', 'float32'}) 
    Doc:  Default floating-point precision for python casts.

Note: float16 support is experimental, use at your own risk.
    Value:  float64

warn_float64 ({'pdb', 'raise', 'warn', 'ignore'}) 
    Doc:  Do an action when a tensor variable with float64 dtype is created. They can't be run on the GPU with the current(old) gpu back-end and are slow with gamer GPUs.
    Value:  ignore

pickle_test_value (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f7be1d95460>>) 
    Doc:  Dump test values while pickling model. If True, test values will be dumped with model.
    Value:  True

cast_policy ({'custom', 'numpy+floatX'}) 
    Doc:  Rules for implicit type casting
    Value:  custom

deterministic ({'default', 'more'}) 
    Doc:  If `more`, sometimes we will select some implementation that are more deterministic, but slower. In particular, on the GPU, we will avoid using AtomicAdd. Sometimes we will still use non-deterministic implementation, e.g. when we do not have a GPU implementation that is deterministic. Also see the dnn.conv.algo* flags to cover more cases.
    Value:  default

device (cpu, opencl*, cuda*) 
    Doc:  Default device for computations. If cuda* or opencl*, change thedefault to try to move computation to the GPU. Do not use upper caseletters, only lower case even if NVIDIA uses capital letters. 'gpu' means let the driver select the gpu (needed for gpu in exclusive mode). 'gpuX' mean use the gpu number X.
    Value:  cpu

init_gpu_device (, opencl*, cuda*) 
    Doc:  Initialize the gpu device to use, works only if device=cpu. Unlike 'device', setting this option will NOT move computations, nor shared variables, to the specified GPU. It can be used to run GPU-specific tests on a particular GPU.
    Value:  

force_device (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f7be05f9ac0>>) 
    Doc:  Raise an error if we can't use the specified device
    Value:  False

conv__assert_shape (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f7be05f9b20>>) 
    Doc:  If True, AbstractConv* ops will verify that user-provided shapes match the runtime shapes (debugging option, may slow down compilation)
    Value:  False

print_global_stats (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f7be05f9bb0>>) 
    Doc:  Print some global statistics (time spent) at the end
    Value:  False

<aesara.configparser.ContextsParam object at 0x7f7be05f9d90>
    Doc:  
        Context map for multi-gpu operation. Format is a
        semicolon-separated list of names and device names in the
        'name->dev_name' format. An example that would map name 'test' to
        device 'cuda0' and name 'test2' to device 'opencl0:0' follows:
        ""test->cuda0;test2->opencl0:0"".

        Invalid context names are 'cpu', 'cuda*' and 'opencl*'
        
    Value:  

print_active_device (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f7be05f9ca0>>) 
    Doc:  Print active device at when the GPU device is initialized.
    Value:  True

gpuarray__preallocate (<class 'float'>) 
    Doc:  If negative it disables the allocation cache. If
                 between 0 and 1 it enables the allocation cache and
                 preallocates that fraction of the total GPU memory.  If 1
                 or greater it will preallocate that amount of memory (in
                 megabytes).
    Value:  0.0

gpuarray__sched ({'default', 'multi', 'single'}) 
    Doc:  The sched parameter passed for context creation to pygpu.
                    With CUDA, using ""multi"" is equivalent to using the parameter
                    cudaDeviceScheduleBlockingSync. This is useful to lower the
                    CPU overhead when waiting for GPU. One user found that it
                    speeds up his other processes that was doing data augmentation.
                 
    Value:  default

gpuarray__single_stream (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f7bd3938eb0>>) 
    Doc:  
                 If your computations are mostly lots of small elements,
                 using single-stream will avoid the synchronization
                 overhead and usually be faster.  For larger elements it
                 does not make a difference yet.  In the future when true
                 multi-stream is enabled in libgpuarray, this may change.
                 If you want to make sure to have optimal performance,
                 check both options.
                 
    Value:  True

cuda__root (<class 'str'>) 
    Doc:  Location of the cuda installation
    Value:  

cuda__include_path (<class 'str'>) 
    Doc:  Location of the cuda includes
    Value:  

assert_no_cpu_op ({'pdb', 'raise', 'warn', 'ignore'}) 
    Doc:  Raise an error/warning if there is a CPU op in the computational graph.
    Value:  ignore

unpickle_function (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f7bd3938fd0>>) 
    Doc:  Replace unpickled Aesara functions with None. This is useful to unpickle old graphs that pickled them when it shouldn't
    Value:  True

reoptimize_unpickled_function (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f7bd3938f40>>) 
    Doc:  Re-optimize the graph when an Aesara function is unpickled from the disk.
    Value:  False

dnn__conv__algo_fwd ({'none', 'winograd_non_fused', 'time_once', 'large', 'fft_tiling', 'fft', 'guess_on_shape_change', 'guess_once', 'time_on_shape_change', 'winograd', 'small'}) 
    Doc:  Default implementation to use for cuDNN forward convolution.
    Value:  small

dnn__conv__algo_bwd_data ({'none', 'deterministic', 'winograd_non_fused', 'time_once', 'fft_tiling', 'fft', 'guess_on_shape_change', 'guess_once', 'time_on_shape_change', 'winograd'}) 
    Doc:  Default implementation to use for cuDNN backward convolution to get the gradients of the convolution with regard to the inputs.
    Value:  none

dnn__conv__algo_bwd_filter ({'none', 'deterministic', 'winograd_non_fused', 'time_once', 'fft_tiling', 'fft', 'guess_on_shape_change', 'guess_once', 'time_on_shape_change', 'small'}) 
    Doc:  Default implementation to use for cuDNN backward convolution to get the gradients of the convolution with regard to the filters.
    Value:  none

dnn__conv__precision ({'float64', 'float16', 'as_input_f32', 'float32', 'as_input'}) 
    Doc:  Default data precision to use for the computation in cuDNN convolutions (defaults to the same dtype as the inputs of the convolutions, or float32 if inputs are float16).
    Value:  as_input_f32

dnn__base_path (<class 'str'>) 
    Doc:  Install location of cuDNN.
    Value:  

dnn__include_path (<class 'str'>) 
    Doc:  Location of the cudnn header
    Value:  

dnn__library_path (<class 'str'>) 
    Doc:  Location of the cudnn link library.
    Value:  

dnn__bin_path (<class 'str'>) 
    Doc:  Location of the cuDNN load library (on non-windows platforms, this is the same as dnn__library_path)
    Value:  

dnn__enabled ({'False', 'no_check', 'True', 'auto'}) 
    Doc:  'auto', use cuDNN if available, but silently fall back to not using it if not present. If True and cuDNN can not be used, raise an error. If False, disable cudnn even if present. If no_check, assume present and the version between header and library match (so less compilation at context init)
    Value:  auto

magma__include_path (<class 'str'>) 
    Doc:  Location of the magma header
    Value:  

magma__library_path (<class 'str'>) 
    Doc:  Location of the magma library
    Value:  

magma__enabled (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f7bd394d3a0>>) 
    Doc:   If True, use magma for matrix computation. If False, disable magma
    Value:  False

<aesara.configparser.ConfigParam object at 0x7f7bd394d3d0>
    Doc:  Default compilation mode
    Value:  Mode

cxx (<class 'str'>) 
    Doc:  The C++ compiler to use. Currently only g++ is supported, but supporting additional compilers should not be too difficult. If it is empty, no C++ code is compiled.
    Value:  /usr/bin/g++

linker ({'py', 'cvm_nogc', 'vm', 'c|py_nogc', 'vm_nogc', 'c|py', 'cvm', 'c'}) 
    Doc:  Default linker used if the aesara flags mode is Mode
    Value:  cvm

allow_gc (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f7bd394d340>>) 
    Doc:  Do we default to delete intermediate results during Aesara function calls? Doing so lowers the memory requirement, but asks that we reallocate memory at the next function call. This is implemented for the default linker, but may not work for all linkers.
    Value:  True

optimizer ({'None', 'o3', 'fast_compile', 'merge', 'o1', 'o2', 'unsafe', 'fast_run', 'o4'}) 
    Doc:  Default optimizer. If not None, will use this optimizer with the Mode
    Value:  o4

optimizer_verbose (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f7bd394d5b0>>) 
    Doc:  If True, we print all optimization being applied
    Value:  False

on_opt_error ({'pdb', 'raise', 'warn', 'ignore'}) 
    Doc:  What to do when an optimization crashes: warn and skip it, raise the exception, or fall into the pdb debugger.
    Value:  warn

nocleanup (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f7bd394d610>>) 
    Doc:  Suppress the deletion of code files that did not compile cleanly
    Value:  False

on_unused_input ({'raise', 'warn', 'ignore'}) 
    Doc:  What to do if a variable in the 'inputs' list of  aesara.function() is not used in the graph.
    Value:  raise

gcc__cxxflags (<class 'str'>) 
    Doc:  Extra compiler flags for gcc
    Value:  

cmodule__warn_no_version (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f7bd394d490>>) 
    Doc:  If True, will print a warning when compiling one or more Op with C code that can't be cached because there is no c_code_cache_version() function associated to at least one of those Ops.
    Value:  False

cmodule__remove_gxx_opt (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f7bd394d4c0>>) 
    Doc:  If True, will remove the -O* parameter passed to g++.This is useful to debug in gdb modules compiled by Aesara.The parameter -g is passed by default to g++
    Value:  False

cmodule__compilation_warning (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f7bd394d6d0>>) 
    Doc:  If True, will print compilation warnings.
    Value:  False

cmodule__preload_cache (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f7bd394d700>>) 
    Doc:  If set to True, will preload the C module cache at import time
    Value:  False

cmodule__age_thresh_use (<class 'int'>) 
    Doc:  In seconds. The time after which Aesara won't reuse a compile c module.
    Value:  2073600

cmodule__debug (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f7bd394d760>>) 
    Doc:  If True, define a DEBUG macro (if not exists) for any compiled C code.
    Value:  False

compile__wait (<class 'int'>) 
    Doc:  Time to wait before retrying to acquire the compile lock.
    Value:  5

compile__timeout (<class 'int'>) 
    Doc:  In seconds, time that a process will wait before deciding to
    override an existing lock. An override only happens when the existing
    lock is held by the same owner *and* has not been 'refreshed' by this
    owner for more than this period. Refreshes are done every half timeout
    period for running processes.
    Value:  120

ctc__root (<class 'str'>) 
    Doc:  Directory which contains the root of Baidu CTC library. It is assumed         that the compiled library is either inside the build, lib or lib64         subdirectory, and the header inside the include directory.
    Value:  

tensor__cmp_sloppy (<class 'int'>) 
    Doc:  Relax aesara.tensor.math._allclose (0) not at all, (1) a bit, (2) more
    Value:  0

tensor__local_elemwise_fusion (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f7bd394d910>>) 
    Doc:  Enable or not in fast_run mode(fast_run optimization) the elemwise fusion optimization
    Value:  True

lib__amblibm (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f7bd394d9a0>>) 
    Doc:  Use amd's amdlibm numerical library
    Value:  False

tensor__insert_inplace_optimizer_validate_nb (<class 'int'>) 
    Doc:  -1: auto, if graph have less then 500 nodes 1, else 10
    Value:  -1

traceback__limit (<class 'int'>) 
    Doc:  The number of stack to trace. -1 mean all.
    Value:  8

traceback__compile_limit (<class 'int'>) 
    Doc:  The number of stack to trace to keep during compilation. -1 mean all. If greater then 0, will also make us save Aesara internal stack trace.
    Value:  0

experimental__unpickle_gpu_on_cpu (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f7bd394db20>>) 
    Doc:  Allow unpickling of pickled GpuArrays as numpy.ndarrays.This is useful, if you want to open a GpuArray without having cuda installed.If you have cuda installed, this will force unpickling tobe done on the cpu to numpy.ndarray.Please be aware that this may get you access to the data,however, trying to unpicke gpu functions will not succeed.This flag is experimental and may be removed any time, whengpu<>cpu transparency is solved.
    Value:  False

experimental__local_alloc_elemwise (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f7bd394db50>>) 
    Doc:  DEPRECATED: If True, enable the experimental optimization local_alloc_elemwise. Generates error if not True. Use optimizer_excluding=local_alloc_elemwise to disable.
    Value:  True

experimental__local_alloc_elemwise_assert (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f7bd394dbe0>>) 
    Doc:  When the local_alloc_elemwise is applied, add an assert to highlight shape errors.
    Value:  True

warn__ignore_bug_before ({'1.0', '0.6', '0.10', '0.8', '0.8.2', '1.0.1', '0.4', 'None', '1.0.3', '0.8.1', '0.5', '1.0.2', '1.0.5', '0.9', '0.4.1', '0.7', 'all', '1.0.4', '0.3'}) 
    Doc:  If 'None', we warn about all Aesara bugs found by default. If 'all', we don't warn about Aesara bugs found by default. If a version, we print only the warnings relative to Aesara bugs found after that version. Warning for specific bugs can be configured with specific [warn] flags.
    Value:  0.9

exception_verbosity ({'high', 'low'}) 
    Doc:  If 'low', the text of exceptions will generally refer to apply nodes with short names such as Elemwise{add_no_inplace}. If 'high', some exceptions will also refer to apply nodes with long descriptions  like:
        A. Elemwise{add_no_inplace}
                B. log_likelihood_v_given_h
                C. log_likelihood_h
    Value:  low

print_test_value (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f7bd394dcd0>>) 
    Doc:  If 'True', the __eval__ of an Aesara variable will return its test_value when this is available. This has the practical conseguence that, e.g., in debugging `my_var` will print the same as `my_var.tag.test_value` when a test value is defined.
    Value:  False

compute_test_value ({'pdb', 'off', 'warn', 'ignore', 'raise'}) 
    Doc:  If 'True', Aesara will run each op at graph build time, using Constants, SharedVariables and the tag 'test_value' as inputs to the function. This helps the user track down problems in the graph before it gets optimized.
    Value:  off

compute_test_value_opt ({'pdb', 'off', 'warn', 'ignore', 'raise'}) 
    Doc:  For debugging Aesara optimization only. Same as compute_test_value, but is used during Aesara optimization
    Value:  off

check_input (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f7bd394dd60>>) 
    Doc:  Specify if types should check their input in their C code. It can be used to speed up compilation, reduce overhead (particularly for scalars) and reduce the number of generated C files.
    Value:  True

NanGuardMode__nan_is_error (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f7bd394dd90>>) 
    Doc:  Default value for nan_is_error
    Value:  True

NanGuardMode__inf_is_error (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f7bd394ddc0>>) 
    Doc:  Default value for inf_is_error
    Value:  True

NanGuardMode__big_is_error (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f7bd394de50>>) 
    Doc:  Default value for big_is_error
    Value:  True

NanGuardMode__action ({'pdb', 'raise', 'warn'}) 
    Doc:  What NanGuardMode does when it finds a problem
    Value:  raise

DebugMode__patience (<class 'int'>) 
    Doc:  Optimize graph this many times to detect inconsistency
    Value:  10

DebugMode__check_c (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f7bd394dee0>>) 
    Doc:  Run C implementations where possible
    Value:  True

DebugMode__check_py (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f7bd394df70>>) 
    Doc:  Run Python implementations where possible
    Value:  True

DebugMode__check_finite (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f7bd394dfa0>>) 
    Doc:  True -> complain about NaN/Inf results
    Value:  True

DebugMode__check_strides (<class 'int'>) 
    Doc:  Check that Python- and C-produced ndarrays have same strides. On difference: (0) - ignore, (1) warn, or (2) raise error
    Value:  0

DebugMode__warn_input_not_reused (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f7bd394df40>>) 
    Doc:  Generate a warning when destroy_map or view_map says that an op works inplace, but the op did not reuse the input for its output.
    Value:  True

DebugMode__check_preallocated_output (<class 'str'>) 
    Doc:  Test thunks with pre-allocated memory as output storage. This is a list of strings separated by "":"". Valid values are: ""initial"" (initial storage in storage map, happens with Scan),""previous"" (previously-returned memory), ""c_contiguous"", ""f_contiguous"", ""strided"" (positive and negative strides), ""wrong_size"" (larger and smaller dimensions), and ""ALL"" (all of the above).
    Value:  

DebugMode__check_preallocated_output_ndim (<class 'int'>) 
    Doc:  When testing with ""strided"" preallocated output memory, test all combinations of strides over that number of (inner-most) dimensions. You may want to reduce that number to reduce memory or time usage, but it is advised to keep a minimum of 2.
    Value:  4

profiling__time_thunks (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f7bd39550d0>>) 
    Doc:  Time individual thunks when profiling
    Value:  True

profiling__n_apply (<class 'int'>) 
    Doc:  Number of Apply instances to print by default
    Value:  20

profiling__n_ops (<class 'int'>) 
    Doc:  Number of Ops to print by default
    Value:  20

profiling__output_line_width (<class 'int'>) 
    Doc:  Max line width for the profiling output
    Value:  512

profiling__min_memory_size (<class 'int'>) 
    Doc:  For the memory profile, do not print Apply nodes if the size
                 of their outputs (in bytes) is lower than this threshold
    Value:  1024

profiling__min_peak_memory (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f7bd3955220>>) 
    Doc:  The min peak memory usage of the order
    Value:  False

profiling__destination (<class 'str'>) 
    Doc:  File destination of the profiling output
    Value:  stderr

profiling__debugprint (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f7bd3955280>>) 
    Doc:  Do a debugprint of the profiled functions
    Value:  False

profiling__ignore_first_call (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f7bd39552b0>>) 
    Doc:  Do we ignore the first call of an Aesara function.
    Value:  False

on_shape_error ({'raise', 'warn'}) 
    Doc:  warn: print a warning and use the default value. raise: raise an error
    Value:  warn

openmp (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f7bd3955310>>) 
    Doc:  Allow (or not) parallel computation on the CPU with OpenMP. This is the default value used when creating an Op that supports OpenMP parallelization. It is preferable to define it via the Aesara configuration file ~/.aesararc or with the environment variable AESARA_FLAGS. Parallelization is only done for some operations that implement it, and even for operations that implement parallelism, each operation is free to respect this flag or not. You can control the number of threads used with the environment variable OMP_NUM_THREADS. If it is set to 1, we disable openmp in Aesara by default.
    Value:  False

openmp_elemwise_minsize (<class 'int'>) 
    Doc:  If OpenMP is enabled, this is the minimum size of vectors for which the openmp parallelization is enabled in element wise ops.
    Value:  200000

optimizer_excluding (<class 'str'>) 
    Doc:  When using the default mode, we will remove optimizer with these tags. Separate tags with ':'.
    Value:  

optimizer_including (<class 'str'>) 
    Doc:  When using the default mode, we will add optimizer with these tags. Separate tags with ':'.
    Value:  

optimizer_requiring (<class 'str'>) 
    Doc:  When using the default mode, we will require optimizer with these tags. Separate tags with ':'.
    Value:  

optdb__position_cutoff (<class 'float'>) 
    Doc:  Where to stop eariler during optimization. It represent the position of the optimizer where to stop.
    Value:  inf

optdb__max_use_ratio (<class 'float'>) 
    Doc:  A ratio that prevent infinite loop in EquilibriumOptimizer.
    Value:  8.0

cycle_detection ({'regular', 'fast'}) 
    Doc:  If cycle_detection is set to regular, most inplaces are allowed,but it is slower. If cycle_detection is set to faster, less inplacesare allowed, but it makes the compilation faster.The interaction of which one give the lower peak memory usage iscomplicated and not predictable, so if you are close to the peakmemory usage, triyng both could give you a small gain.
    Value:  regular

check_stack_trace ({'raise', 'warn', 'log', 'off'}) 
    Doc:  A flag for checking the stack trace during the optimization process. default (off): does not check the stack trace of any optimization log: inserts a dummy stack trace that identifies the optimizationthat inserted the variable that had an empty stack trace.warn: prints a warning if a stack trace is missing and also a dummystack trace is inserted that indicates which optimization insertedthe variable that had an empty stack trace.raise: raises an exception if a stack trace is missing
    Value:  off

metaopt__verbose (<class 'int'>) 
    Doc:  0 for silent, 1 for only warnings, 2 for full output withtimings and selected implementation
    Value:  0

metaopt__optimizer_excluding (<class 'str'>) 
    Doc:  exclude optimizers with these tags. Separate tags with ':'.
    Value:  

metaopt__optimizer_including (<class 'str'>) 
    Doc:  include optimizers with these tags. Separate tags with ':'.
    Value:  

profile (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f7bd3955610>>) 
    Doc:  If VM should collect profile information
    Value:  False

profile_optimizer (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f7bd3955640>>) 
    Doc:  If VM should collect optimizer profile information
    Value:  False

profile_memory (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f7bd3955670>>) 
    Doc:  If VM should collect memory profile information and print it
    Value:  False

<aesara.configparser.ConfigParam object at 0x7f7bd39556a0>
    Doc:  Useful only for the vm linkers. When lazy is None, auto detect if lazy evaluation is needed and use the appropriate version. If lazy is True/False, force the version used between Loop/LoopGC and Stack.
    Value:  None

unittests__rseed (<class 'str'>) 
    Doc:  Seed to use for randomized unit tests. Special value 'random' means using a seed of None.
    Value:  666

warn__round (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f7bd3955760>>) 
    Doc:  Warn when using `tensor.round` with the default mode. Round changed its default from `half_away_from_zero` to `half_to_even` to have the same default as NumPy.
    Value:  False

numba__vectorize_target ({'parallel', 'cuda', 'cpu'}) 
    Doc:  Default target for numba.vectorize.
    Value:  cpu

numba__fastmath (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f7bd3955820>>) 
    Doc:  If True, use Numba's fastmath mode.
    Value:  True

numba__cache (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f7bd39558b0>>) 
    Doc:  If True, use Numba's file based caching.
    Value:  True

compiledir_format (<class 'str'>) 
    Doc:  Format string for platform-dependent compiled module subdirectory
(relative to base_compiledir). Available keys: aesara_version, device,
gxx_version, hostname, numpy_version, platform, processor,
python_bitwidth, python_int_bitwidth, python_version, short_platform.
Defaults to compiledir_%(short_platform)s-%(processor)s-%(python_versi
on)s-%(python_bitwidth)s.
    Value:  compiledir_%(short_platform)s-%(processor)s-%(python_version)s-%(python_bitwidth)s

<aesara.configparser.ConfigParam object at 0x7f7bd39559a0>
    Doc:  platform-independent root directory for compiled modules
    Value:  /home/matthew/.aesara

<aesara.configparser.ConfigParam object at 0x7f7bd3955970>
    Doc:  platform-dependent cache directory for compiled modules
    Value:  /home/matthew/.aesara/compiledir_Linux-4.15--generic-x86_64-with-glibc2.10-x86_64-3.8.12-64

<aesara.configparser.ConfigParam object at 0x7f7bd3955880>
    Doc:  Directory to cache pre-compiled kernels for the gpuarray backend.
    Value:  /home/matthew/.aesara/compiledir_Linux-4.15--generic-x86_64-with-glibc2.10-x86_64-3.8.12-64/gpuarray_kernels

blas__ldflags (<class 'str'>) 
    Doc:  lib[s] to include for [Fortran] level-3 blas implementation
    Value:  -L/mnt/data/conda/envs/mbt-py38-light/lib -lmkl_core -lmkl_intel_thread -lmkl_rt -Wl,-rpath,/mnt/data/conda/envs/mbt-py38-light/lib

blas__check_openmp (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f7bd35b1eb0>>) 
    Doc:  Check for openmp library conflict.
WARNING: Setting this to False leaves you open to wrong results in blas-related operations.
    Value:  True

scan__allow_gc (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f7bd3955c10>>) 
    Doc:  Allow/disallow gc inside of Scan (default: False)
    Value:  False

scan__allow_output_prealloc (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f7bcfe95700>>) 
    Doc:  Allow/disallow memory preallocation for outputs inside of scan (default: True)
    Value:  True
```

* Python version:  3.8.12
* Operating system:  Ubuntu 18.04.6 LTS
* How did you install Aesara: conda (mamba)
",bug important C-backend performance concern,,2022-03-10 14:41:15,2022-03-11 04:25:55,"brandonwillard connected 2022-03-11 02:04:17,brandonwillard labeled 2022-03-11 02:04:37,brandonwillard labeled 2022-03-11 02:06:38,brandonwillard labeled 2022-03-11 02:06:38,brandonwillard labeled 2022-03-11 02:06:38,brandonwillard closed 2022-03-11 04:25:55",brandonwillard mattearllongshot,2
683,850,Create Code Of Conduct,logankilpatrick,"Hey folks! I wanted to suggest you all create a CoC. You are welcome to adopt the NumFOCUS https://numfocus.org/code-of-conduct so long as you add a clear way for folks to report CoC violations to the team.
",,,2022-03-10 19:23:49,2022-03-15 15:55:54,"brandonwillard mentioned 2022-03-10 19:34:01,brandonwillard subscribed 2022-03-10 19:34:01,michaelosthege mentioned 2022-03-10 19:34:01,michaelosthege subscribed 2022-03-10 19:34:01,ricardoV94 mentioned 2022-03-10 19:34:01,ricardoV94 subscribed 2022-03-10 19:34:01,kc611 mentioned 2022-03-10 19:34:02,kc611 subscribed 2022-03-10 19:34:02,aesara-devs locked 2022-03-15 15:55:53,brandonwillard converted_to_discussion 2022-03-15 15:55:54",aesara-devs kc611 brandonwillard ricardoV94 logankilpatrick michaelosthege,1
684,851,Memory leak in reshape,lucianopaz,"## Description of your problem or feature request

**Please provide a minimal, self-contained, and reproducible example.**
```python
import os
import psutil
import aesara
from aesara import tensor as at


process = psutil.Process(os.getpid())
old_rss = None
n = 100_000

x = at.sharedvar.TensorSharedVariable(
    ""test"", at.type.dvector, np.ones(n), strict=True
)
y = at.reshape(x, [-1, 1])
f = aesara.function([], y)
for i in range(100):
    x.set_value(np.ones(n))
    f()
    if i % 10 == 0:
        new_rss = process.memory_info().rss
        if old_rss is not None:
            print(f""Python process increased RSS by {round((new_rss - old_rss) / 1024**2, 2)} MB"")
        old_rss = new_rss
```

**Please provide the full traceback of any errors.**
```python
Python process increased RSS by 6.62 MB
Python process increased RSS by 7.74 MB
Python process increased RSS by 7.64 MB
Python process increased RSS by 7.68 MB
Python process increased RSS by 7.64 MB
Python process increased RSS by 7.68 MB
Python process increased RSS by 7.41 MB
Python process increased RSS by 7.63 MB
Python process increased RSS by 7.85 MB
```

**Please provide any additional information below.**

This happens when the reshape operation is compiled to C. When I run the same code setting `aesara.config.cxx = """"`, the Python process does not increase RSS.

## Versions and main components

* Aesara version: 2.4.0
* Aesara config (`python -c ""import aesara; print(aesara.config)""`)
* Python version: 3.9
* Operating system: Ubuntu 18.04
* How did you install Aesara: (conda/pip) pip
",bug duplicate important C-backend performance concern,,2022-03-10 22:04:17,2022-03-11 00:09:08,"brandonwillard labeled 2022-03-10 22:08:52,brandonwillard labeled 2022-03-10 22:08:52,brandonwillard labeled 2022-03-10 22:08:52,brandonwillard labeled 2022-03-10 22:08:52,brandonwillard labeled 2022-03-11 00:07:24,brandonwillard closed 2022-03-11 00:09:08",lucianopaz brandonwillard,1
685,852,Memory leak in `DimShuffle`,lucianopaz,"## Description of your problem or feature request

**Please provide a minimal, self-contained, and reproducible example.**
```python
import os
import psutil
import aesara
from aesara import tensor as at


process = psutil.Process(os.getpid())
old_rss = None
n = 100_000

x = at.sharedvar.TensorSharedVariable(
    ""test"", at.type.dvector, np.ones(n), strict=True
)
y = x.dimshuffle([0, ""x""])
f = aesara.function([], y)
for i in range(100):
    x.set_value(np.ones(n))
    f()
    if i % 10 == 0:
        new_rss = process.memory_info().rss
        if old_rss is not None:
            print(f""Python process increased RSS by {round((new_rss - old_rss) / 1024**2, 2)} MB"")
        old_rss = new_rss
```

**Please provide the full traceback of any errors.**
```python
Python process increased RSS by 6.62 MB
Python process increased RSS by 7.74 MB
Python process increased RSS by 7.64 MB
Python process increased RSS by 7.68 MB
Python process increased RSS by 7.64 MB
Python process increased RSS by 7.68 MB
Python process increased RSS by 7.41 MB
Python process increased RSS by 7.63 MB
Python process increased RSS by 7.85 MB
```

**Please provide any additional information below.**

This happens when the reshape operation is compiled to C. When I run the same code setting `aesara.config.cxx = """"`, the Python process does not increase RSS.

## Versions and main components

* Aesara version: 2.4.0
* Aesara config (`python -c ""import aesara; print(aesara.config)""`)
* Python version: 3.9
* Operating system: Ubuntu 18.04
* How did you install Aesara: (conda/pip) pip",bug duplicate important C-backend performance concern,,2022-03-10 22:04:52,2022-03-11 04:25:55,"brandonwillard labeled 2022-03-10 22:09:18,brandonwillard labeled 2022-03-10 22:09:18,brandonwillard labeled 2022-03-10 22:09:18,brandonwillard labeled 2022-03-10 22:09:18,brandonwillard renamed 2022-03-11 02:02:17,brandonwillard labeled 2022-03-11 02:06:25,brandonwillard closed 2022-03-11 04:25:55",lucianopaz andrejmuhic twiecki brandonwillard,6
688,858,Errors in cache updating logic with concurrent graph compiles,mattearllongshot,"## Description of your problem or feature request

Hello, we're developing an application which consists of multiple worker processes which start at the same time, each of which compiles an identical Aesara graph.  Here is a short example which reproduces the problem: 

```python
import multiprocessing

import aesara
import aesara.tensor as at
import numpy as np

def f(factor, barrier):
    a = at.vector()
    b = at.vector()
    barrier.wait()   # optional, try with or without
    f = aesara.function([a, b], at.sum(factor * at.dot(a, b)))
    return f(np.array([1, 2, 3]), np.array([4, 5, 6]))

num_procs = 20

while True:
    factor = np.random.random()   # random factor to prevent caching between runs
    print(f'{factor=}')
    barrier = multiprocessing.Barrier(num_procs)
    procs = [multiprocessing.Process(target=f, args=(factor, barrier)) for i in range(num_procs)]
    for proc in procs:
        proc.start()
    for proc in procs:
        proc.join()

    exit_codes = [proc.exitcode for proc in procs]
    print(f'{exit_codes=}')
    if any(exit_code != 0 for exit_code in exit_codes):
        break
```

Each loop will run 20 parallel compilations of an identical graph (the graph is different for each loop).  Here is some example output:

```
factor=0.8232652038645821
exit_codes=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
factor=0.6012850336942555
exit_codes=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
factor=0.3702983019245454
exit_codes=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
factor=0.10300371685537746
WARNING (aesara.link.c.cmodule): Deleting (broken cache directory [EOF]): /home/matthew/.aesara/compiledir_Linux-4.15--generic-x86_64-with-glibc2.10-x86_64-3.8.12-64/tmp74cpm5pp
WARNING (aesara.link.c.cmodule): Deleting (broken cache directory [EOF]): /home/matthew/.aesara/compiledir_Linux-4.15--generic-x86_64-with-glibc2.10-x86_64-3.8.12-64/tmp74cpm5pp
Process Process-67:
Traceback (most recent call last):
  File ""/mnt/data/conda/envs/mbt-py38-light/lib/python3.8/site-packages/aesara/link/vm.py"", line 1112, in make_all
    node.op.make_thunk(node, storage_map, compute_map, [], impl=impl)
  File ""/mnt/data/conda/envs/mbt-py38-light/lib/python3.8/site-packages/aesara/link/c/op.py"", line 131, in make_thunk
    return self.make_c_thunk(node, storage_map, compute_map, no_recycling)
  File ""/mnt/data/conda/envs/mbt-py38-light/lib/python3.8/site-packages/aesara/link/c/op.py"", line 96, in make_c_thunk
    outputs = cl.make_thunk(
  File ""/mnt/data/conda/envs/mbt-py38-light/lib/python3.8/site-packages/aesara/link/c/basic.py"", line 1198, in make_thunk
    cthunk, module, in_storage, out_storage, error_storage = self.__compile__(
  File ""/mnt/data/conda/envs/mbt-py38-light/lib/python3.8/site-packages/aesara/link/c/basic.py"", line 1133, in __compile__
    thunk, module = self.cthunk_factory(
  File ""/mnt/data/conda/envs/mbt-py38-light/lib/python3.8/site-packages/aesara/link/c/basic.py"", line 1629, in cthunk_factory
    module = get_module_cache().module_from_key(key=key, lnk=self)
  File ""/mnt/data/conda/envs/mbt-py38-light/lib/python3.8/site-packages/aesara/link/c/cmodule.py"", line 1185, in module_from_key
    module = self._get_from_hash(module_hash, key)
  File ""/mnt/data/conda/envs/mbt-py38-light/lib/python3.8/site-packages/aesara/link/c/cmodule.py"", line 1088, in _get_from_hash
    key_data.add_key(key, save_pkl=bool(key[0]))
  File ""/mnt/data/conda/envs/mbt-py38-light/lib/python3.8/site-packages/aesara/link/c/cmodule.py"", line 528, in add_key
    self.save_pkl()
  File ""/mnt/data/conda/envs/mbt-py38-light/lib/python3.8/site-packages/aesara/link/c/cmodule.py"", line 549, in save_pkl
    with open(self.key_pkl, ""wb"") as f:
FileNotFoundError: [Errno 2] No such file or directory: '/home/matthew/.aesara/compiledir_Linux-4.15--generic-x86_64-with-glibc2.10-x86_64-3.8.12-64/tmp74cpm5pp/key.pkl'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/mnt/data/conda/envs/mbt-py38-light/lib/python3.8/multiprocessing/process.py"", line 315, in _bootstrap
    self.run()
  File ""/mnt/data/conda/envs/mbt-py38-light/lib/python3.8/multiprocessing/process.py"", line 108, in run
    self._target(*self._args, **self._kwargs)
  File ""/home/matthew/junk/aesara_race.py"", line 11, in f
    f = aesara.function([a, b], at.sum(factor * at.dot(a, b)))
  File ""/mnt/data/conda/envs/mbt-py38-light/lib/python3.8/site-packages/aesara/compile/function/__init__.py"", line 337, in function
    fn = pfunc(
  File ""/mnt/data/conda/envs/mbt-py38-light/lib/python3.8/site-packages/aesara/compile/function/pfunc.py"", line 363, in pfunc
    return orig_function(
  File ""/mnt/data/conda/envs/mbt-py38-light/lib/python3.8/site-packages/aesara/compile/function/types.py"", line 1743, in orig_function
    fn = m.create(defaults)
  File ""/mnt/data/conda/envs/mbt-py38-light/lib/python3.8/site-packages/aesara/compile/function/types.py"", line 1638, in create
    _fn, _i, _o = self.linker.make_thunk(
  File ""/mnt/data/conda/envs/mbt-py38-light/lib/python3.8/site-packages/aesara/link/basic.py"", line 254, in make_thunk
    return self.make_all(
  File ""/mnt/data/conda/envs/mbt-py38-light/lib/python3.8/site-packages/aesara/link/vm.py"", line 1121, in make_all
    raise_with_op(fgraph, node)
  File ""/mnt/data/conda/envs/mbt-py38-light/lib/python3.8/site-packages/aesara/link/utils.py"", line 538, in raise_with_op
    raise exc_value.with_traceback(exc_trace)
  File ""/mnt/data/conda/envs/mbt-py38-light/lib/python3.8/site-packages/aesara/link/vm.py"", line 1112, in make_all
    node.op.make_thunk(node, storage_map, compute_map, [], impl=impl)
  File ""/mnt/data/conda/envs/mbt-py38-light/lib/python3.8/site-packages/aesara/link/c/op.py"", line 131, in make_thunk
    return self.make_c_thunk(node, storage_map, compute_map, no_recycling)
  File ""/mnt/data/conda/envs/mbt-py38-light/lib/python3.8/site-packages/aesara/link/c/op.py"", line 96, in make_c_thunk
    outputs = cl.make_thunk(
  File ""/mnt/data/conda/envs/mbt-py38-light/lib/python3.8/site-packages/aesara/link/c/basic.py"", line 1198, in make_thunk
    cthunk, module, in_storage, out_storage, error_storage = self.__compile__(
  File ""/mnt/data/conda/envs/mbt-py38-light/lib/python3.8/site-packages/aesara/link/c/basic.py"", line 1133, in __compile__
    thunk, module = self.cthunk_factory(
  File ""/mnt/data/conda/envs/mbt-py38-light/lib/python3.8/site-packages/aesara/link/c/basic.py"", line 1629, in cthunk_factory
    module = get_module_cache().module_from_key(key=key, lnk=self)
  File ""/mnt/data/conda/envs/mbt-py38-light/lib/python3.8/site-packages/aesara/link/c/cmodule.py"", line 1185, in module_from_key
    module = self._get_from_hash(module_hash, key)
  File ""/mnt/data/conda/envs/mbt-py38-light/lib/python3.8/site-packages/aesara/link/c/cmodule.py"", line 1088, in _get_from_hash
    key_data.add_key(key, save_pkl=bool(key[0]))
  File ""/mnt/data/conda/envs/mbt-py38-light/lib/python3.8/site-packages/aesara/link/c/cmodule.py"", line 528, in add_key
    self.save_pkl()
  File ""/mnt/data/conda/envs/mbt-py38-light/lib/python3.8/site-packages/aesara/link/c/cmodule.py"", line 549, in save_pkl
    with open(self.key_pkl, ""wb"") as f:
FileNotFoundError: [Errno 2] No such file or directory: '/home/matthew/.aesara/compiledir_Linux-4.15--generic-x86_64-with-glibc2.10-x86_64-3.8.12-64/tmp74cpm5pp/key.pkl'
Apply node that caused the error: Elemwise{Mul}[(0, 1)](TensorConstant{0.10300371685537746}, InplaceDimShuffle{}.0)
Toposort index: 4
Inputs types: [TensorType(float64, ()), TensorType(float64, ())]

HINT: Use a linker other than the C linker to print the inputs' shapes and strides.
HINT: Re-running with most Aesara optimizations disabled could provide a back-trace showing when this node was created. This can be done by setting the Aesara flag 'optimizer=fast_compile'. If that does not work, Aesara optimizations can be disabled with 'optimizer=None'.
HINT: Use the Aesara flag `exception_verbosity=high` for a debug print-out and storage map footprint of this Apply node.
exit_codes=[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
```

The number of failing processes seems to vary.  To reproduce you might need to increase `num_procs` or perhaps make the function being compiled more complex, as it seems to be timing dependent.

## Versions and main components

* Aesara version:  2.5.1
* Aesara config (`python -c ""import aesara; print(aesara.config)""`)  [aesara_config.txt](https://github.com/aesara-devs/aesara/files/8233404/aesara_config.txt)
* Python version: 3.8.12
* Operating system: Ubuntu 18.04.6 LTS
* How did you install Aesara: conda (mamba)",bug important C-backend,,2022-03-11 15:56:24,2022-05-21 02:24:12,"brandonwillard labeled 2022-03-12 22:02:37,brandonwillard labeled 2022-03-12 22:02:37,brandonwillard labeled 2022-05-09 00:32:21,mattearllongshot mentioned 2022-05-09 00:35:48,mattearllongshot subscribed 2022-05-09 00:35:48,brandonwillard closed 2022-05-21 02:24:12",brandonwillard mattearllongshot,16
689,859,Add JAX implementation for `BroadcastTo`,twiecki,Seems like `BroadcastTo` has no JAX implementation: https://github.com/pymc-devs/pymc/issues/5468#issuecomment-1056041513,good first issue help wanted JAX,,2022-03-14 10:13:43,2022-04-07 18:54:22,"twiecki labeled 2022-03-14 10:13:43,twiecki labeled 2022-03-14 10:13:43,ricardoV94 unlabeled 2022-03-14 10:48:05,ricardoV94 labeled 2022-03-14 10:48:05,ricardoV94 unlabeled 2022-03-14 14:50:05,ricardoV94 labeled 2022-03-14 14:50:05,brandonwillard renamed 2022-03-23 21:55:51,brandonwillard labeled 2022-03-23 21:56:00,brandonwillard mentioned 2022-04-03 09:45:28,brandonwillard subscribed 2022-04-03 09:45:28,danhphan mentioned 2022-04-03 09:45:28,danhphan subscribed 2022-04-03 09:45:28,ricardoV94 mentioned 2022-04-03 09:45:28,ricardoV94 subscribed 2022-04-03 09:45:28,brandonwillard closed 2022-04-07 18:54:22",ricardoV94 brandonwillard twiecki danhphan,6
690,860,Remove `Diff` `Op`,ricardoV94,"I wonder if we should just remove the `Diff` `Op` altogether.

Since it doesn't have a `C` implementation, slicing + subtraction seems to be faster, even for large `n`. See gist here: 

https://gist.github.com/ricardoV94/6e4c59fd312b9e7f9352b2fe07cab0c9 

This also overcomes the gradient bug and limited implementation to vector inputs.

Not sure about the speed implications for the Numba and JAX backends.

Only downsides are 1) a DiffOp might be a useful representation when it comes to graph manipulation and 2) graph can become quite large for large `n`, but I don't think many people use this, and the advantage of having a grad probably wins

_Originally posted by @ricardoV94 in https://github.com/aesara-devs/aesara/issues/855#issuecomment-1065061223_",enhancement refactor,,2022-03-14 17:56:14,2022-04-22 18:32:02,"ricardoV94 mentioned 2022-03-14 17:56:15,ricardoV94 subscribed 2022-03-14 17:56:15,brandonwillard labeled 2022-03-15 15:55:23,brandonwillard labeled 2022-03-15 15:55:23,brandonwillard closed 2022-04-22 18:32:02",larryshamalama ricardoV94 brandonwillard,2
700,878,What is Aerasa?,Knibbaz,"# Question
I was wandering what Aerasa is and if it is a form of Machine-  or Deep Learning. On the [machinelearnigmastery](https://machinelearningmastery.com/introduction-python-deep-learning-library-theano/) and [geeksforgeeks](https://www.geeksforgeeks.org/theano-in-python/) websites I red the the predecessor of Aerasa, Theano, was more like Deep Learning. Does Aerasa have Neural Networks or did they mention it completely wrong?

I was searching on the GitHub of Theano and Aerasa and saw that you can execute mathamatical expressions on both the CPU and GPU. But not much is mentioned about Machine- or Deep Learning, is it even one of these methods?",question,,2022-03-30 09:40:09,2022-03-30 18:35:24,"brandonwillard labeled 2022-03-30 18:35:14,aesara-devs locked 2022-03-30 18:35:24,brandonwillard converted_to_discussion 2022-03-30 18:35:24",aesara-devs brandonwillard Knibbaz,0
705,886,Continue cleaning up the sparse/dense tensor types,brandonwillard,"Currently, `TensorType`s aren't an abstract class, and direct instances of `TensorType`s represent dense tensors, as do instances of the new `DenseTensorType`.

Instead, it would be better to have and use a single abstract tensor `Type` and distinct sparse and dense tensor `Type`s.  In this case, we could remove all the meta classes.

One of the issues I can see with this change: external libraries that use `TensorType` and/or `TensorVariable`s directly will need to change these to `DenseTensorType` and `DenseTensorVariable`, unless we continue to keep these names reserved for dense tensors and use new `AbstractTensor[Type|Variable]` classes as the shared base.

_Originally posted by @brandonwillard in https://github.com/aesara-devs/aesara/pull/766#pullrequestreview-894459515_",important refactor sparse tensors,,2022-04-05 21:38:18,2022-09-17 17:44:38,"brandonwillard mentioned 2022-04-05 21:38:18,brandonwillard subscribed 2022-04-05 21:38:18,brandonwillard labeled 2022-04-05 21:38:30,brandonwillard labeled 2022-04-05 21:38:30,brandonwillard labeled 2022-04-05 21:38:30,rlouf connected 2022-09-17 05:46:19,brandonwillard closed 2022-09-17 17:44:38",rlouf ricardoV94 danhphan brandonwillard,5
708,890,Scan Gibbs example is outdated and wrong,ricardoV94,"This section of the Scan documentation has several issues:

https://aesara.readthedocs.io/en/latest/library/scan.html#using-shared-variables-gibbs-sampling

1. Binomial no longer accepts `n`, `p` as keyword arguments 
2. Outputs info is wrong because it uses a `float` vector, but the returned output type is `int64`
3. `aesara.dot` is no longer a thing
4. updates are not actually optional, the function will not compile without them #579
",documentation good first issue help wanted Scan,,2022-04-07 12:04:55,2022-09-23 21:56:26,"ricardoV94 labeled 2022-04-07 17:02:24,ricardoV94 labeled 2022-04-07 17:02:24,ricardoV94 labeled 2022-04-07 17:02:24,brandonwillard labeled 2022-04-07 18:52:58,brandonwillard closed 2022-09-23 21:56:26",ricardoV94 LegrandNico brandonwillard,1
710,893,Gradient with respect to scan logprob raises TypeError,ricardoV94,"
```python
import aesara
import aesara.tensor as at
from aeppl import joint_logprob

x, _ = aesara.scan(
    fn=lambda: at.random.normal(0, 1),
    n_steps=10,
)

x_v = x.clone()
x_logp = joint_logprob({x: x_v})

at.grad(x_logp, x_v)
```

**Please provide the full traceback of any errors.**
<details>
 <summary>traceback</summary>

```python
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-4-5d51258474f1> in <module>
----> 1 at.grad(x_logp, x_v)

~/Documents/Projects/aeppl/venv/lib/python3.8/site-packages/aesara/gradient.py in grad(cost, wrt, consider_constant, disconnected_inputs, add_names, known_grads, return_disconnected, null_gradients)
    628             assert g.type.dtype in aesara.tensor.type.float_dtypes
    629 
--> 630     rval = _populate_grad_dict(var_to_app_to_idx, grad_dict, wrt, cost_name)
    631 
    632     for i in range(len(rval)):

~/Documents/Projects/aeppl/venv/lib/python3.8/site-packages/aesara/gradient.py in _populate_grad_dict(var_to_app_to_idx, grad_dict, wrt, cost_name)
   1432         return grad_dict[var]
   1433 
-> 1434     rval = [access_grad_cache(elem) for elem in wrt]
   1435 
   1436     return rval

~/Documents/Projects/aeppl/venv/lib/python3.8/site-packages/aesara/gradient.py in <listcomp>(.0)
   1432         return grad_dict[var]
   1433 
-> 1434     rval = [access_grad_cache(elem) for elem in wrt]
   1435 
   1436     return rval

~/Documents/Projects/aeppl/venv/lib/python3.8/site-packages/aesara/gradient.py in access_grad_cache(var)
   1385                     for idx in node_to_idx[node]:
   1386 
-> 1387                         term = access_term_cache(node)[idx]
   1388 
   1389                         if not isinstance(term, Variable):

~/Documents/Projects/aeppl/venv/lib/python3.8/site-packages/aesara/gradient.py in access_term_cache(node)
   1057             inputs = node.inputs
   1058 
-> 1059             output_grads = [access_grad_cache(var) for var in node.outputs]
   1060 
   1061             # list of bools indicating if each output is connected to the cost

~/Documents/Projects/aeppl/venv/lib/python3.8/site-packages/aesara/gradient.py in <listcomp>(.0)
   1057             inputs = node.inputs
   1058 
-> 1059             output_grads = [access_grad_cache(var) for var in node.outputs]
   1060 
   1061             # list of bools indicating if each output is connected to the cost

~/Documents/Projects/aeppl/venv/lib/python3.8/site-packages/aesara/gradient.py in access_grad_cache(var)
   1385                     for idx in node_to_idx[node]:
   1386 
-> 1387                         term = access_term_cache(node)[idx]
   1388 
   1389                         if not isinstance(term, Variable):

~/Documents/Projects/aeppl/venv/lib/python3.8/site-packages/aesara/gradient.py in access_term_cache(node)
   1057             inputs = node.inputs
   1058 
-> 1059             output_grads = [access_grad_cache(var) for var in node.outputs]
   1060 
   1061             # list of bools indicating if each output is connected to the cost

~/Documents/Projects/aeppl/venv/lib/python3.8/site-packages/aesara/gradient.py in <listcomp>(.0)
   1057             inputs = node.inputs
   1058 
-> 1059             output_grads = [access_grad_cache(var) for var in node.outputs]
   1060 
   1061             # list of bools indicating if each output is connected to the cost

~/Documents/Projects/aeppl/venv/lib/python3.8/site-packages/aesara/gradient.py in access_grad_cache(var)
   1385                     for idx in node_to_idx[node]:
   1386 
-> 1387                         term = access_term_cache(node)[idx]
   1388 
   1389                         if not isinstance(term, Variable):

~/Documents/Projects/aeppl/venv/lib/python3.8/site-packages/aesara/gradient.py in access_term_cache(node)
   1212                             )
   1213 
-> 1214                 input_grads = node.op.L_op(inputs, node.outputs, new_output_grads)
   1215 
   1216                 if input_grads is None:

~/Documents/Projects/aeppl/venv/lib/python3.8/site-packages/aesara/scan/op.py in L_op(self, inputs, outs, dC_douts)
   2302         # self.truncate_gradient
   2303         if self.truncate_gradient != -1:
-> 2304             grad_steps = minimum(grad_steps, self.truncate_gradient)
   2305 
   2306         self_inputs = self.inputs

~/Documents/Projects/aeppl/venv/lib/python3.8/site-packages/aesara/graph/op.py in __call__(self, *inputs, **kwargs)
    292         """"""
    293         return_list = kwargs.pop(""return_list"", False)
--> 294         node = self.make_node(*inputs, **kwargs)
    295 
    296         if config.compute_test_value != ""off"":

~/Documents/Projects/aeppl/venv/lib/python3.8/site-packages/aesara/tensor/elemwise.py in make_node(self, *inputs)
    460         using DimShuffle.
    461         """"""
--> 462         inputs = [as_tensor_variable(i) for i in inputs]
    463         out_dtypes, out_broadcastables, inputs = self.get_output_info(
    464             DimShuffle, *inputs

~/Documents/Projects/aeppl/venv/lib/python3.8/site-packages/aesara/tensor/elemwise.py in <listcomp>(.0)
    460         using DimShuffle.
    461         """"""
--> 462         inputs = [as_tensor_variable(i) for i in inputs]
    463         out_dtypes, out_broadcastables, inputs = self.get_output_info(
    464             DimShuffle, *inputs

~/Documents/Projects/aeppl/venv/lib/python3.8/site-packages/aesara/tensor/__init__.py in as_tensor_variable(x, name, ndim, **kwargs)
     40 
     41     """"""
---> 42     return _as_tensor_variable(x, name, ndim, **kwargs)
     43 
     44 

/usr/lib/python3.8/functools.py in wrapper(*args, **kw)
    873                             '1 positional argument')
    874 
--> 875         return dispatch(args[0].__class__)(*args, **kw)
    876 
    877     funcname = getattr(func, '__name__', 'singledispatch function')

~/Documents/Projects/aeppl/venv/lib/python3.8/site-packages/aesara/tensor/basic.py in _as_tensor_bool(x, name, ndim, **kwargs)
    174 @_as_tensor_variable.register(bool)
    175 def _as_tensor_bool(x, name, ndim, **kwargs):
--> 176     raise TypeError(
    177         ""Cannot cast True or False as a tensor variable. Please use ""
    178         ""np.array(True) or np.array(False) if you need these constants. ""

TypeError: Cannot cast True or False as a tensor variable. Please use np.array(True) or np.array(False) if you need these constants. This error might be caused by using the == operator on Variables. v == w does not do what you think it does, use aesara.tensor.eq(v, w) instead.
```
</details>",bug Scan,,2022-04-07 19:08:16,2022-04-07 20:40:55,"brandonwillard transferred 2022-04-07 20:39:21,brandonwillard labeled 2022-04-07 20:40:07,brandonwillard labeled 2022-04-07 20:40:07,brandonwillard connected 2022-04-07 20:40:18,brandonwillard closed 2022-04-07 20:40:56",ricardoV94 brandonwillard,1
715,900,`OpFromGraph.make_node`'s expected inputs and actual inputs are inconsistent,brandonwillard,"`OpFromGraph.make_node` expects inputs corresponding to each of its inner-graph inputs, except for the shared variables; however, `OpFromGraph.make_node` also constructs `Apply` nodes such that `Apply.inputs` _contains_ the shared variable inputs of the inner-graph.  This leads to an inherent inconsistency that prevents rewrites from reconstructing nodes via `Op.make_node(*Apply.inputs)`.

Here's an example in which this issue results in an optimization failure:
```python
import aesara
import aesara.tensor as at


from aesara.compile.builders import OpFromGraph


y = aesara.shared(1.0, name=""y"")

test_ofg = OpFromGraph([], [1 + y])

test_ofg.inputs
# []
test_ofg.shared_inputs
# [y]


def inner_func():
    return test_ofg()


out, out_updates = aesara.scan(inner_func, n_steps=10)

with aesara.config.change_flags(on_opt_error=""raise""):
    out_fn = aesara.function([], out)
# ERROR (aesara.graph.opt): Optimization failure due to: push_out_non_seq_scan
# ERROR (aesara.graph.opt): node: for{cpu,scan_fn}(TensorConstant{10}, TensorConstant{10}, y)
# ERROR (aesara.graph.opt): TRACEBACK:
# ERROR (aesara.graph.opt): Traceback (most recent call last):
#   File "".../Aesara/aesara/graph/opt.py"", line 1850, in process_node
#     replacements = lopt.transform(fgraph, node)
#   File "".../Aesara/aesara/graph/opt.py"", line 1055, in transform
#     return self.fn(fgraph, node)
#   File "".../Aesara/aesara/scan/opt.py"", line 282, in push_out_non_seq_scan
#     nw_outer_node = nd.op.make_node(*outside_ins)
#   File "".../Aesara/aesara/compile/builders.py"", line 738, in make_node
#     raise ValueError(
# ValueError: Expected 1 inputs, got 2
```",bug Scan,,2022-04-12 21:31:00,2022-04-14 13:21:56,"brandonwillard labeled 2022-04-12 21:31:00,brandonwillard labeled 2022-04-12 21:31:00,brandonwillard closed 2022-04-14 13:21:56",brandonwillard,0
723,909,Intel MKL FATAL ERROR upon importing Aesara on M1 Mac ,larryshamalama,"Importing Aesara yields the following error due to the `mkl` package.

<img width=""833"" alt=""Screen Shot 2022-04-16 at 4 18 04 PM"" src=""https://user-images.githubusercontent.com/24764859/163692568-f5603d45-39be-455a-ba3a-4603eb7f92d7.png"">

Recreating the conda environment in [`environment.yml`](https://github.com/aesara-devs/aesara/blob/main/environment.yml) minus the [`mkl` dependencies](https://github.com/aesara-devs/aesara/blob/main/environment.yml#L20-L22) allows Aesara to be imported.

Given that `mkl` is an optional dependency, its removal altogether can be considered. Another option would be to replace it by [blip](https://github.com/flame/blis) as suggested by @dgerlanc 

CC @brandonwillard 

* Aesara version: 2.4.0
* Python version: 3.10.4
* Operating system: Mac Monterey (M1)
* How did you install Aesara: conda",request discussion Conda,,2022-04-17 00:25:18,2023-01-30 19:31:17,"dgerlanc mentioned 2022-04-17 00:25:18,dgerlanc subscribed 2022-04-17 00:25:18,brandonwillard mentioned 2022-04-17 00:25:18,brandonwillard subscribed 2022-04-17 00:25:18,brandonwillard labeled 2022-04-17 00:40:49,brandonwillard labeled 2022-04-17 00:40:49,brandonwillard mentioned 2022-04-25 20:33:16,brandonwillard subscribed 2022-04-25 20:33:16,brandonwillard closed 2023-01-30 19:31:17",larryshamalama dgerlanc twiecki brandonwillard,5
726,913,Error with scalar inputs in CAReduce and squeeze,ricardoV94,"```python
import aesara.tensor as at
import numpy as np

np.squeeze(5, axis=-1)  # Fine
at.squeeze(at.constant(5), axis=-1)  # Or axis=0
```
```
numpy.AxisError: axis 0 is out of bounds for array of dimension 0
```
Same happens with CAReduce. Bug was introduced in https://github.com/aesara-devs/aesara/pull/834",bug,,2022-04-19 08:41:58,2022-04-19 20:06:19,"ricardoV94 labeled 2022-04-19 08:41:58,ricardoV94 renamed 2022-04-19 08:47:16,ricardoV94 renamed 2022-04-19 09:44:30,ricardoV94 closed 2022-04-19 20:06:20",ricardoV94,0
730,918,Missing compiler executables on import,hectormz,"Importing `aeppl` (or importing `pymc4`) results in a list of executables that cannot be found:

```
Could not locate executable g77
Could not locate executable f77
Could not locate executable ifort
Could not locate executable ifl
Could not locate executable f90
Could not locate executable efl
Could not locate executable gfortran
Could not locate executable f95
Could not locate executable g95
Could not locate executable efort
Could not locate executable efc
Could not locate executable flang
don't know how to compile Fortran code on platform 'nt'
```
Do you know which dependency is producing these messages and if they are warnings or errors?

## Versions and main components

* Aesara version: 2.5.1
* Aesara config 

```
WARNING (aesara.tensor.blas): Using NumPy C-API based implementation for BLAS functions.
floatX ({'float16', 'float64', 'float32'})
    Doc:  Default floating-point precision for python casts.

Note: float16 support is experimental, use at your own risk.
    Value:  float64

warn_float64 ({'warn', 'ignore', 'pdb', 'raise'})
    Doc:  Do an action when a tensor variable with float64 dtype is created. They can't be run on the GPU with the current(old) gpu back-end and are slow with gamer GPUs.
    Value:  ignore

pickle_test_value (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x00000288E0C39820>>)
    Doc:  Dump test values while pickling model. If True, test values will be dumped with model.
    Value:  True

cast_policy ({'custom', 'numpy+floatX'})
    Doc:  Rules for implicit type casting
    Value:  custom

deterministic ({'default', 'more'})
    Doc:  If `more`, sometimes we will select some implementation that are more deterministic, but slower. In particular, on the GPU, we will avoid using AtomicAdd. Sometimes we will still use non-deterministic implementation, e.g. when we do not have a GPU implementation that is deterministic. Also see the dnn.conv.algo* flags to cover more cases.
    Value:  default

device (cpu, opencl*, cuda*)
    Doc:  Default device for computations. If cuda* or opencl*, change thedefault to try to move computation to the GPU. Do not use upper caseletters, only lower case even if NVIDIA uses capital letters. 'gpu' means let the driver select the gpu (needed for gpu in exclusive mode). 'gpuX' mean use the gpu number X.
    Value:  cpu

init_gpu_device (, opencl*, cuda*)
    Doc:  Initialize the gpu device to use, works only if device=cpu. Unlike 'device', setting this option will NOT move computations, nor shared variables, to the specified GPU. It can be used to run GPU-specific tests on a particular GPU.
    Value:

force_device (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x00000288F8F6BDF0>>)
    Doc:  Raise an error if we can't use the specified device
    Value:  False

conv__assert_shape (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x00000288F8F6B6A0>>)
    Doc:  If True, AbstractConv* ops will verify that user-provided shapes match the runtime shapes (debugging option, may slow down compilation)
    Value:  False

print_global_stats (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x00000288F8F6BA60>>)
    Doc:  Print some global statistics (time spent) at the end
    Value:  False

<aesara.configparser.ContextsParam object at 0x00000288F8F6BB80>
    Doc:
        Context map for multi-gpu operation. Format is a
        semicolon-separated list of names and device names in the
        'name->dev_name' format. An example that would map name 'test' to
        device 'cuda0' and name 'test2' to device 'opencl0:0' follows:
        ""test->cuda0;test2->opencl0:0"".

        Invalid context names are 'cpu', 'cuda*' and 'opencl*'

    Value:

print_active_device (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x00000288F8F6BB20>>)
    Doc:  Print active device at when the GPU device is initialized.
    Value:  True

gpuarray__preallocate (<class 'float'>)
    Doc:  If negative it disables the allocation cache. If
                 between 0 and 1 it enables the allocation cache and
                 preallocates that fraction of the total GPU memory.  If 1
                 or greater it will preallocate that amount of memory (in
                 megabytes).
    Value:  0.0

gpuarray__sched ({'default', 'single', 'multi'})
    Doc:  The sched parameter passed for context creation to pygpu.
                    With CUDA, using ""multi"" is equivalent to using the parameter
                    cudaDeviceScheduleBlockingSync. This is useful to lower the
                    CPU overhead when waiting for GPU. One user found that it
                    speeds up his other processes that was doing data augmentation.

    Value:  default

gpuarray__single_stream (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x00000288F8FA7BB0>>)
    Doc:
                 If your computations are mostly lots of small elements,
                 using single-stream will avoid the synchronization
                 overhead and usually be faster.  For larger elements it
                 does not make a difference yet.  In the future when true
                 multi-stream is enabled in libgpuarray, this may change.
                 If you want to make sure to have optimal performance,
                 check both options.

    Value:  True

cuda__root (<class 'str'>)
    Doc:  Location of the cuda installation
    Value:

cuda__include_path (<class 'str'>)
    Doc:  Location of the cuda includes
    Value:

assert_no_cpu_op ({'warn', 'ignore', 'pdb', 'raise'})
    Doc:  Raise an error/warning if there is a CPU op in the computational graph.
    Value:  ignore

unpickle_function (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x00000288F8FA7CD0>>)
    Doc:  Replace unpickled Aesara functions with None. This is useful to unpickle old graphs that pickled them when it shouldn't
    Value:  True

reoptimize_unpickled_function (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x00000288F8FA7D30>>)
    Doc:  Re-optimize the graph when an Aesara function is unpickled from the disk.
    Value:  False

dnn__conv__algo_fwd ({'small', 'winograd', 'guess_on_shape_change', 'winograd_non_fused', 'guess_once', 'time_on_shape_change', 'large', 'fft', 'none', 'time_once', 'fft_tiling'})
    Doc:  Default implementation to use for cuDNN forward convolution.
    Value:  small

dnn__conv__algo_bwd_data ({'winograd', 'guess_on_shape_change', 'time_once', 'winograd_non_fused', 'guess_once', 'time_on_shape_change', 'fft', 'none', 'deterministic', 'fft_tiling'})
    Doc:  Default implementation to use for cuDNN backward convolution to get the gradients of the convolution with regard to the inputs.
    Value:  none

dnn__conv__algo_bwd_filter ({'small', 'guess_on_shape_change', 'winograd_non_fused', 'guess_once', 'time_on_shape_change', 'fft', 'none', 'time_once', 'fft_tiling', 'deterministic'})
    Doc:  Default implementation to use for cuDNN backward convolution to get the gradients of the convolution with regard to the filters.
    Value:  none

dnn__conv__precision ({'as_input', 'as_input_f32', 'float64', 'float16', 'float32'})
    Doc:  Default data precision to use for the computation in cuDNN convolutions (defaults to the same dtype as the inputs of the convolutions, or float32 if inputs are float16).
    Value:  as_input_f32

dnn__base_path (<class 'str'>)
    Doc:  Install location of cuDNN.
    Value:

dnn__include_path (<class 'str'>)
    Doc:  Location of the cudnn header
    Value:

dnn__library_path (<class 'str'>)
    Doc:  Location of the cudnn link library.
    Value:

dnn__bin_path (<class 'str'>)
    Doc:  Location of the cuDNN load library (on non-windows platforms, this is the same as dnn__library_path)
    Value:

dnn__enabled ({'auto', 'False', 'no_check', 'True'})
    Doc:  'auto', use cuDNN if available, but silently fall back to not using it if not present. If True and cuDNN can not be used, raise an error. If False, disable cudnn even if present. If no_check, assume present and the version between header and library match (so less compilation at context init)
    Value:  auto

magma__include_path (<class 'str'>)
    Doc:  Location of the magma header
    Value:

magma__library_path (<class 'str'>)
    Doc:  Location of the magma library
    Value:

magma__enabled (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x00000288F8FB9100>>)
    Doc:   If True, use magma for matrix computation. If False, disable magma
    Value:  False

<aesara.configparser.ConfigParam object at 0x00000288F8FB90D0>
    Doc:  Default compilation mode
    Value:  Mode

cxx (<class 'str'>)
    Doc:  The C++ compiler to use. Currently only g++ is supported, but supporting additional compilers should not be too difficult. If it is empty, no C++ code is compiled.
    Value:  ""C:\\Users\\XXX\\scoop\\apps\\gcc\\current\\bin\\g++.exe""

linker ({'cvm', 'c', 'c|py', 'cvm_nogc', 'vm_nogc', 'c|py_nogc', 'vm', 'py'})
    Doc:  Default linker used if the aesara flags mode is Mode
    Value:  cvm

allow_gc (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x00000288F8FB9460>>)
    Doc:  Do we default to delete intermediate results during Aesara function calls? Doing so lowers the memory requirement, but asks that we reallocate memory at the next function call. This is implemented for the default linker, but may not work for all linkers.
    Value:  True

optimizer ({'o1', 'unsafe', 'fast_run', 'merge', 'None', 'o2', 'o3', 'o4', 'fast_compile'})
    Doc:  Default optimizer. If not None, will use this optimizer with the Mode
    Value:  o4

optimizer_verbose (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x00000288F8FB9190>>)
    Doc:  If True, we print all optimization being applied
    Value:  False

on_opt_error ({'warn', 'ignore', 'pdb', 'raise'})
    Doc:  What to do when an optimization crashes: warn and skip it, raise the exception, or fall into the pdb debugger.
    Value:  warn

nocleanup (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x00000288F8FB9310>>)
    Doc:  Suppress the deletion of code files that did not compile cleanly
    Value:  False

on_unused_input ({'raise', 'ignore', 'warn'})
    Doc:  What to do if a variable in the 'inputs' list of  aesara.function() is not used in the graph.
    Value:  raise

gcc__cxxflags (<class 'str'>)
    Doc:  Extra compiler flags for gcc
    Value:

cmodule__warn_no_version (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x00000288F8FB9430>>)
    Doc:  If True, will print a warning when compiling one or more Op with C code that can't be cached because there is no c_code_cache_version() function associated to at least one of those Ops.
    Value:  False

cmodule__remove_gxx_opt (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x00000288F8FB92B0>>)
    Doc:  If True, will remove the -O* parameter passed to g++.This is useful to debug in gdb modules compiled by Aesara.The parameter -g is passed by default to g++
    Value:  False

cmodule__compilation_warning (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x00000288F8FB93D0>>)
    Doc:  If True, will print compilation warnings.
    Value:  False

cmodule__preload_cache (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x00000288F8FB9370>>)
    Doc:  If set to True, will preload the C module cache at import time
    Value:  False

cmodule__age_thresh_use (<class 'int'>)
    Doc:  In seconds. The time after which Aesara won't reuse a compile c module.
    Value:  2073600

cmodule__debug (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x00000288F8FB9490>>)
    Doc:  If True, define a DEBUG macro (if not exists) for any compiled C code.
    Value:  False

compile__wait (<class 'int'>)
    Doc:  Time to wait before retrying to acquire the compile lock.
    Value:  5

compile__timeout (<class 'int'>)
    Doc:  In seconds, time that a process will wait before deciding to
    override an existing lock. An override only happens when the existing
    lock is held by the same owner *and* has not been 'refreshed' by this
    owner for more than this period. Refreshes are done every half timeout
    period for running processes.
    Value:  120

ctc__root (<class 'str'>)
    Doc:  Directory which contains the root of Baidu CTC library. It is assumed         that the compiled library is either inside the build, lib or lib64         subdirectory, and the header inside the include directory.
    Value:

tensor__cmp_sloppy (<class 'int'>)
    Doc:  Relax aesara.tensor.math._allclose (0) not at all, (1) a bit, (2) more
    Value:  0

tensor__local_elemwise_fusion (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x00000288F8FB9640>>)
    Doc:  Enable or not in fast_run mode(fast_run optimization) the elemwise fusion optimization
    Value:  True

lib__amblibm (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x00000288F8FB9700>>)
    Doc:  Use amd's amdlibm numerical library
    Value:  False

tensor__insert_inplace_optimizer_validate_nb (<class 'int'>)
    Doc:  -1: auto, if graph have less then 500 nodes 1, else 10
    Value:  -1

traceback__limit (<class 'int'>)
    Doc:  The number of stack to trace. -1 mean all.
    Value:  8

traceback__compile_limit (<class 'int'>)
    Doc:  The number of stack to trace to keep during compilation. -1 mean all. If greater then 0, will also make us save Aesara internal stack trace.
    Value:  0

experimental__unpickle_gpu_on_cpu (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x00000288F8FB9880>>)
    Doc:  Allow unpickling of pickled GpuArrays as numpy.ndarrays.This is useful, if you want to open a GpuArray without having cuda installed.If you have cuda installed, this will force unpickling tobe done on the cpu to numpy.ndarray.Please be aware that this may get you access to the data,however, trying to unpicke gpu functions will not succeed.This flag is experimental and may be removed any time, whengpu<>cpu transparency is solved.
    Value:  False

experimental__local_alloc_elemwise (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x00000288F8FB98B0>>)
    Doc:  DEPRECATED: If True, enable the experimental optimization local_alloc_elemwise. Generates error if not True. Use optimizer_excluding=local_alloc_elemwise to disable.
    Value:  True

experimental__local_alloc_elemwise_assert (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x00000288F8FB9940>>)
    Doc:  When the local_alloc_elemwise is applied, add an assert to highlight shape errors.
    Value:  True

warn__ignore_bug_before ({'0.9', '1.0.5', '1.0.4', '0.8', '0.4.1', '0.8.2', '0.6', '1.0.3', '0.7', 'None', '1.0', '0.5', '0.3', '1.0.1', '0.8.1', '0.10', '1.0.2', 'all', '0.4'})
    Doc:  If 'None', we warn about all Aesara bugs found by default. If 'all', we don't warn about Aesara bugs found by default. If a version, we print only the warnings relative to Aesara bugs found after that version. Warning for specific bugs can be configured with specific [warn] flags.
    Value:  0.9

exception_verbosity ({'low', 'high'})
    Doc:  If 'low', the text of exceptions will generally refer to apply nodes with short names such as Elemwise{add_no_inplace}. If 'high', some exceptions will also refer to apply nodes with long descriptions  like:
        A. Elemwise{add_no_inplace}
                B. log_likelihood_v_given_h
                C. log_likelihood_h
    Value:  low

print_test_value (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x00000288F8FB9A30>>)
    Doc:  If 'True', the __eval__ of an Aesara variable will return its test_value when this is available. This has the practical conseguence that, e.g., in debugging `my_var` will print the same as `my_var.tag.test_value` when a test value is defined.
    Value:  False

compute_test_value ({'warn', 'raise', 'ignore', 'pdb', 'off'})
    Doc:  If 'True', Aesara will run each op at graph build time, using Constants, SharedVariables and the tag 'test_value' as inputs to the function. This helps the user track down problems in the graph before it gets optimized.
    Value:  off

compute_test_value_opt ({'warn', 'raise', 'ignore', 'pdb', 'off'})
    Doc:  For debugging Aesara optimization only. Same as compute_test_value, but is used during Aesara optimization
    Value:  off

check_input (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x00000288F8FB9A90>>)
    Doc:  Specify if types should check their input in their C code. It can be used to speed up compilation, reduce overhead (particularly for scalars) and reduce the number of generated C files.
    Value:  True

NanGuardMode__nan_is_error (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x00000288F8FB9AC0>>)
    Doc:  Default value for nan_is_error
    Value:  True

NanGuardMode__inf_is_error (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x00000288F8FB9B20>>)
    Doc:  Default value for inf_is_error
    Value:  True

NanGuardMode__big_is_error (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x00000288F8FB9BB0>>)
    Doc:  Default value for big_is_error
    Value:  True

NanGuardMode__action ({'raise', 'pdb', 'warn'})
    Doc:  What NanGuardMode does when it finds a problem
    Value:  raise

DebugMode__patience (<class 'int'>)
    Doc:  Optimize graph this many times to detect inconsistency
    Value:  10

DebugMode__check_c (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x00000288F8FB9C40>>)
    Doc:  Run C implementations where possible
    Value:  True

DebugMode__check_py (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x00000288F8FB9CD0>>)
    Doc:  Run Python implementations where possible
    Value:  True

DebugMode__check_finite (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x00000288F8FB9CA0>>)
    Doc:  True -> complain about NaN/Inf results
    Value:  True

DebugMode__check_strides (<class 'int'>)
    Doc:  Check that Python- and C-produced ndarrays have same strides. On difference: (0) - ignore, (1) warn, or (2) raise error
    Value:  0

DebugMode__warn_input_not_reused (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x00000288F8FB9D30>>)
    Doc:  Generate a warning when destroy_map or view_map says that an op works inplace, but the op did not reuse the input for its output.
    Value:  True

DebugMode__check_preallocated_output (<class 'str'>)
    Doc:  Test thunks with pre-allocated memory as output storage. This is a list of strings separated by "":"". Valid values are: ""initial"" (initial storage in storage map, happens with Scan),""previous"" (previously-returned memory), ""c_contiguous"", ""f_contiguous"", ""strided"" (positive and negative strides), ""wrong_size"" (larger and smaller dimensions), and ""ALL"" (all of the above).
    Value:

DebugMode__check_preallocated_output_ndim (<class 'int'>)
    Doc:  When testing with ""strided"" preallocated output memory, test all combinations of strides over that number of (inner-most) dimensions. You may want to reduce that number to reduce memory or time usage, but it is advised to keep a minimum of 2.
    Value:  4

profiling__time_thunks (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x00000288F8FB9DC0>>)
    Doc:  Time individual thunks when profiling
    Value:  True

profiling__n_apply (<class 'int'>)
    Doc:  Number of Apply instances to print by default
    Value:  20

profiling__n_ops (<class 'int'>)
    Doc:  Number of Ops to print by default
    Value:  20

profiling__output_line_width (<class 'int'>)
    Doc:  Max line width for the profiling output
    Value:  512

profiling__min_memory_size (<class 'int'>)
    Doc:  For the memory profile, do not print Apply nodes if the size
                 of their outputs (in bytes) is lower than this threshold
    Value:  1024

profiling__min_peak_memory (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x00000288F8FB9F10>>)
    Doc:  The min peak memory usage of the order
    Value:  False

profiling__destination (<class 'str'>)
    Doc:  File destination of the profiling output
    Value:  stderr

profiling__debugprint (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x00000288F8FB9F70>>)

    Doc:  Do a debugprint of the profiled functions
    Value:  False

profiling__ignore_first_call (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x00000288F8FB9FA0>>)
    Doc:  Do we ignore the first call of an Aesara function.
    Value:  False

on_shape_error ({'warn', 'raise'})
    Doc:  warn: print a warning and use the default value. raise: raise an error
    Value:  warn

openmp (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x00000288F8FC0070>>)
    Doc:  Allow (or not) parallel computation on the CPU with OpenMP. This is the default value used when creating an Op that supports OpenMP parallelization. It is preferable to define it via the Aesara configuration file ~/.aesararc or with the environment variable AESARA_FLAGS. Parallelization is only done for some operations that implement it, and even for operations that implement parallelism, each operation is free to respect this flag or not. You can control the number of threads used with the environment variable OMP_NUM_THREADS. If it is set to 1, we disable openmp in Aesara by default.
    Value:  False

openmp_elemwise_minsize (<class 'int'>)
    Doc:  If OpenMP is enabled, this is the minimum size of vectors for which the openmp parallelization is enabled in element wise ops.
    Value:  200000

optimizer_excluding (<class 'str'>)
    Doc:  When using the default mode, we will remove optimizer with these tags. Separate tags with ':'.
    Value:

optimizer_including (<class 'str'>)
    Doc:  When using the default mode, we will add optimizer with these tags. Separate tags with ':'.
    Value:

optimizer_requiring (<class 'str'>)
    Doc:  When using the default mode, we will require optimizer with these tags. Separate tags with ':'.
    Value:

optdb__position_cutoff (<class 'float'>)
    Doc:  Where to stop eariler during optimization. It represent the position of the optimizer where to stop.
    Value:  inf

optdb__max_use_ratio (<class 'float'>)
    Doc:  A ratio that prevent infinite loop in EquilibriumOptimizer.
    Value:  8.0

cycle_detection ({'regular', 'fast'})
    Doc:  If cycle_detection is set to regular, most inplaces are allowed,but it is slower. If cycle_detection is set to faster, less inplacesare allowed, but it makes the compilation faster.The interaction of which one give the lower peak memory usage iscomplicated and not predictable, so if you are close to the peakmemory usage, triyng both could give you a small gain.
    Value:  regular

check_stack_trace ({'warn', 'raise', 'off', 'log'})
    Doc:  A flag for checking the stack trace during the optimization process. default (off): does not check the stack trace of any optimization log: inserts a dummy stack trace that identifies the optimizationthat inserted the variable that had an empty stack trace.warn: prints a warning if a stack trace is missing and also a dummystack trace is inserted that indicates which optimization insertedthe variable that had an empty stack trace.raise: raises an exception if a stack trace is missing
    Value:  off

metaopt__verbose (<class 'int'>)
    Doc:  0 for silent, 1 for only warnings, 2 for full output withtimings and selected implementation
    Value:  0

metaopt__optimizer_excluding (<class 'str'>)
    Doc:  exclude optimizers with these tags. Separate tags with ':'.
    Value:

metaopt__optimizer_including (<class 'str'>)
    Doc:  include optimizers with these tags. Separate tags with ':'.
    Value:

profile (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x00000288F8FC0310>>)
    Doc:  If VM should collect profile information
    Value:  False

profile_optimizer (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x00000288F8FC0370>>)
    Doc:  If VM should collect optimizer profile information
    Value:  False

profile_memory (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x00000288F8FC03A0>>)
    Doc:  If VM should collect memory profile information and print it
    Value:  False

<aesara.configparser.ConfigParam object at 0x00000288F8FC03D0>
    Doc:  Useful only for the vm linkers. When lazy is None, auto detect if lazy evaluation is needed and use the appropriate version. If lazy is True/False, force the version used between Loop/LoopGC and Stack.
    Value:  None

unittests__rseed (<class 'str'>)
    Doc:  Seed to use for randomized unit tests. Special value 'random' means using a seed of None.
    Value:  666

warn__round (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x00000288F8FC0490>>)
    Doc:  Warn when using `tensor.round` with the default mode. Round changed its default from `half_away_from_zero` to `half_to_even` to have the same default as NumPy.
    Value:  False

numba__vectorize_target ({'cuda', 'parallel', 'cpu'})
    Doc:  Default target for numba.vectorize.
    Value:  cpu

numba__fastmath (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x00000288F8FC0580>>)
    Doc:  If True, use Numba's fastmath mode.
    Value:  True

numba__cache (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x00000288F8FC0610>>)
    Doc:  If True, use Numba's file based caching.
    Value:  True

compiledir_format (<class 'str'>)
    Doc:  Format string for platform-dependent compiled module subdirectory
(relative to base_compiledir). Available keys: aesara_version, device,
gxx_version, hostname, numpy_version, platform, processor,
python_bitwidth, python_int_bitwidth, python_version, short_platform.
Defaults to compiledir_%(short_platform)s-%(processor)s-%(python_versi
on)s-%(python_bitwidth)s.
    Value:  compiledir_%(short_platform)s-%(processor)s-%(python_version)s-%(python_bitwidth)s

<aesara.configparser.ConfigParam object at 0x00000288F8FC0A30>
    Doc:  platform-independent root directory for compiled modules
    Value:  C:\\Users\\XXX\\AppData\\Local\\Aesara

<aesara.configparser.ConfigParam object at 0x00000288F8FC0A00>
    Doc:  platform-dependent cache directory for compiled modules
    Value:  C:\\Users\\XXX\\AppData\\Local\\Aesara\\compiledir_Windows-10-10.0.19043-SP0-Intel64_Family_6_Model_165_Stepping_2_GenuineIntel-3.9.12-64

<aesara.configparser.ConfigParam object at 0x00000288F8FC0A90>
    Doc:  Directory to cache pre-compiled kernels for the gpuarray backend.
    Value:  C:\\Users\\XXX\\AppData\\Local\\Aesara\\compiledir_Windows-10-10.0.19043-SP0-Intel64_Family_6_Model_165_Stepping_2_GenuineIntel-3.9.12-64\\gpuarray_kernels

blas__ldflags (<class 'str'>)
    Doc:  lib[s] to include for [Fortran] level-3 blas implementation
    Value:

blas__check_openmp (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x00000288F931F070>>)
    Doc:  Check for openmp library conflict.
WARNING: Setting this to False leaves you open to wrong results in blas-related operations.
    Value:  True

scan__allow_gc (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x00000288924FBFD0>>)
    Doc:  Allow/disallow gc inside of Scan (default: False)
    Value:  False

scan__allow_output_prealloc (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x00000288924FBF10>>)
    Doc:  Allow/disallow memory preallocation for outputs inside of scan (default: True)
    Value:  True
```
* Python version: 3.9.12
* Operating system: Windows 10
* How did you install Aesara: pip
",,,2022-04-19 17:14:08,2022-04-19 21:10:15,"rlouf mentioned 2022-04-19 20:08:09,rlouf subscribed 2022-04-19 20:08:09,brandonwillard transferred 2022-04-19 21:09:57,aesara-devs locked 2022-04-19 21:10:15,brandonwillard converted_to_discussion 2022-04-19 21:10:15",rlouf aesara-devs brandonwillard hectormz,3
732,923,Numba `Scan` fails when sit-sot sequences aren't full length,ricardoV94,"```python
import aesara
import aesara.tensor as at
import numpy as np

k = at.iscalar(""k"")
A = at.vector(""A"")

result, _ = aesara.scan(fn=lambda prior_result, A: prior_result * A,
                              outputs_info=at.ones_like(A),
                              non_sequences=A,
                              n_steps=k)

final_result = result[-1]

power = aesara.function(inputs=[A, k], outputs=final_result, mode=""NUMBA"")

print(power(range(10), 2))
print(power(range(10), 4))
```
```
[0. 1. 2. 3. 4. 5. 6. 7. 8. 9.]
[0. 1. 2. 3. 4. 5. 6. 7. 8. 9.]
```",bug help wanted important Numba Scan,brandonwillard,2022-04-22 17:48:16,2022-10-04 23:09:31,"ricardoV94 labeled 2022-04-22 17:48:16,ricardoV94 labeled 2022-04-22 17:48:16,ricardoV94 labeled 2022-04-22 17:48:16,brandonwillard labeled 2022-04-22 17:51:18,brandonwillard labeled 2022-04-22 17:52:20,brandonwillard renamed 2022-05-16 23:16:58,brandonwillard renamed 2022-05-16 23:17:58,brandonwillard assigned 2022-09-21 18:04:57,brandonwillard closed 2022-10-04 23:09:31",ricardoV94 kc611 brandonwillard,8
737,931,Add code of conduct,dgerlanc,"Add a code of conduct for Aesara. There is no current code of conduct. 

The [Django Project code of conduct](https://www.djangoproject.com/conduct/) seems reasonable.
",documentation,dgerlanc,2022-04-25 23:30:53,2022-04-29 17:29:05,"dgerlanc assigned 2022-04-25 23:30:59,dgerlanc labeled 2022-04-25 23:35:05,brandonwillard closed 2022-04-29 17:29:05",dgerlanc brandonwillard,0
739,934,Create Mac ARM Dev Environment,dgerlanc,"As per #909, `environment.yml` does not work on ARM Macs because of `MKL`.

We can create another environment file, `environment-arm.yml`, with an alternate `BLAS` specification. This file may also be used on `Linux-ARM` systems.

I'd propose adding the following two lines:

```yaml
  - nomkl
  - openblas
```
* Operating system: ARM Mac
* How did you install Aesara: conda
",good first issue help wanted MacOS Conda,,2022-04-26 17:03:54,2022-11-26 23:09:20,"dgerlanc labeled 2022-04-26 17:03:54,dgerlanc labeled 2022-04-26 17:03:54,brandonwillard labeled 2022-04-27 16:50:01,brandonwillard labeled 2022-04-27 16:50:05,ricardoV94 mentioned 2022-05-07 16:48:31,ricardoV94 subscribed 2022-05-07 16:48:31,larryshamalama mentioned 2022-05-07 21:11:57,larryshamalama subscribed 2022-05-07 21:11:57,ricardoV94 mentioned 2022-05-07 21:11:57,ricardoV94 subscribed 2022-05-07 21:11:57,dgerlanc referenced 2022-05-11 19:12:28,dgerlanc referenced 2022-05-13 15:10:08,dgerlanc referenced 2022-05-24 18:01:14,dgerlanc referenced 2022-05-31 17:37:54,dgerlanc referenced 2022-11-02 16:31:39,dgerlanc referenced 2022-11-21 16:29:11,dgerlanc referenced 2022-11-21 17:18:15,dgerlanc referenced 2022-11-21 18:10:24,dgerlanc referenced 2022-11-26 21:46:30,dgerlanc closed 2022-11-26 23:09:20,dgerlanc referenced 2022-11-26 23:09:21",dgerlanc ricardoV94 brandonwillard larryshamalama,4
740,936,Use `SeedSequence` in `RandomStream`,ricardoV94,"Numpy docs advise using [SeedSequence](https://numpy.org/doc/stable/reference/random/bit_generators/generated/numpy.random.SeedSequence.html#numpy.random.SeedSequence) for spawning independent `bit_generators`: https://numpy.org/doc/stable/reference/random/parallel.html

Their method should be quite more robust to collisions than our naive `random.integers(2**30)` strategy.

https://github.com/aesara-devs/aesara/blob/c19ac79965eb4b3b11f432fd46efc620aa28f72f/aesara/tensor/random/utils.py#L253

```python
import math

unique_seeds = 2**30
for n_seeds in (10, 100, 1_000, 10_000, 100_000):
    # birthday paradox probability: https://en.wikipedia.org/wiki/Birthday_problem
    p_collision = 1-math.perm(unique_seeds, n_seeds) / unique_seeds**n_seeds
    print(f""{n_seeds=}, {p_collision=}"")

```
```
n_seeds=10, p_collision=4.1909515080540416e-08
n_seeds=100, p_collision=4.610036260510597e-06
n_seeds=1000, p_collision=0.0004650875835883195
n_seeds=10000, p_collision=0.04549425469529611
n_seeds=100000, p_collision=0.9905023499278603
```


",enhancement help wanted important random variables,,2022-04-28 07:20:41,2022-04-29 15:26:44,"ricardoV94 renamed 2022-04-28 07:38:08,ricardoV94 renamed 2022-04-28 07:38:19,brandonwillard labeled 2022-04-28 07:46:19,brandonwillard labeled 2022-04-28 07:46:19,brandonwillard labeled 2022-04-28 07:46:19,brandonwillard labeled 2022-04-28 07:46:19,brandonwillard renamed 2022-04-29 01:20:14,brandonwillard closed 2022-04-29 15:26:45",ricardoV94 brandonwillard,1
743,944,Aesara fails to set the correct default blas ldflags to link with MKL on windows,lucianopaz,"## Description of your problem or feature request

MKL libraries on windows are supplied as dynamic link libraries (e.g. `mkl_core.1.dll`) and don't have the lib prefix. When aesara tries to set the default blas ldflags [here](https://github.com/aesara-devs/aesara/blob/f7a506ff3a599a8101502b7ff4d7b0c642366a22/aesara/link/c/cmodule.py#L2635) it does a few things:

1. It tries to import `mkl`, which should be available if `mkl-services` was installed via conda or mamba.
2. Sets up a gcc styled library directory flag `-L` [see here](https://github.com/aesara-devs/aesara/blob/f7a506ff3a599a8101502b7ff4d7b0c642366a22/aesara/link/c/cmodule.py#L2763)
3. It sets the default libraries to try to link: `-lmkl_core -lmkl_rt -lmkl_intel_thread`
4. It then tries to compile and run a small [test script that links to mkl](https://github.com/aesara-devs/aesara/blob/f7a506ff3a599a8101502b7ff4d7b0c642366a22/aesara/link/c/cmodule.py#L1961-L1993)


**Please provide the full traceback of any errors.**
The test script fails to compile and run [here](https://github.com/aesara-devs/aesara/blob/f7a506ff3a599a8101502b7ff4d7b0c642366a22/aesara/link/c/cmodule.py#L1985-L1987). You can run it manually using this snippet:

```python
import textwrap
from aesara.link.c.cmodule import std_lib_dirs, GCC_compiler
import os
import sys


flags = [
    '-L""C:\\\\Users\\\\chulo\\\\Anaconda3\\\\Library\\\\bin""',
    '-lmkl_core',
    '-lmkl_intel_thread',
    '-lmkl_rt',
]


test_code = textwrap.dedent(
    """"""\\
    extern ""C"" double ddot_(int*, double*, int*, double*, int*);
    int main(int argc, char** argv)
    {
        int Nx = 5;
        int Sx = 1;
        double x[5] = {0, 1, 2, 3, 4};
        double r = ddot_(&Nx, x, &Sx, x, &Sx);
        if ((r - 30.) > 1e-6 || (r - 30.) < -1e-6)
        {
            return -1;
        }
        return 0;
    }
    """"""
)
cflags = list(flags)
# to support path that includes spaces, we need to wrap it with double quotes on Windows
path_wrapper = '""' if os.name == ""nt"" else """"
cflags.extend([f""-L{path_wrapper}{d}{path_wrapper}"" for d in std_lib_dirs()])

res = GCC_compiler.try_compile_tmp(
    test_code, tmp_prefix=""try_blas_"", flags=cflags, try_run=True, output=True
)
```

This results in
```python
>>> print(res[-1].decode(""utf-8""))

C:/msys64/mingw64/bin/../lib/gcc/x86_64-w64-mingw32/11.3.0/../../../../x86_64-w64-mingw32/bin/ld.exe: cannot find -lmkl_core: No such file or directory
C:/msys64/mingw64/bin/../lib/gcc/x86_64-w64-mingw32/11.3.0/../../../../x86_64-w64-mingw32/bin/ld.exe: cannot find -lmkl_intel_thread: No such file or directory
C:/msys64/mingw64/bin/../lib/gcc/x86_64-w64-mingw32/11.3.0/../../../../x86_64-w64-mingw32/bin/ld.exe: cannot find -lmkl_rt: No such file or directory
collect2.exe: error: ld returned 1 exit status
```


**Please provide any additional information below.**

Since the mkl .dll libraries don't have the lib prefix, we should provide their absolute paths to be able to link them when compiling. This means that setting the flags to:

```python
import sys

flags = [
    os.path.join(sys.prefix, ""Library"", ""bin"", ""mkl_core.1.dll""),
    os.path.join(sys.prefix, ""Library"", ""bin"", ""mkl_intel_thread.1.dll""),
    os.path.join(sys.prefix, ""Library"", ""bin"", ""mkl_rt.1.dll""),
]
```

makes the compilation succeed.

## Versions and main components

* Aesara version: 2.6.2
* Aesara config (`python -c ""import aesara; print(aesara.config)""`)
* Python version: 3.9.7
* Operating system: Windows 10
* How did you install Aesara: (conda/pip) conda
",bug help wanted Windows performance concern,,2022-05-04 12:36:19,2022-05-07 16:20:58,"brandonwillard labeled 2022-05-04 19:05:07,brandonwillard labeled 2022-05-04 19:05:07,brandonwillard labeled 2022-05-04 19:05:11,brandonwillard labeled 2022-05-04 19:05:19,brandonwillard closed 2022-05-07 16:20:58",lucianopaz brandonwillard,0
744,945,Add `Scan` input/output labels to the debug output,brandonwillard,"When printing a `Scan` node using `aesara.dprint`, it would be helpful to have labels next to each outer/inner-input/output describing exactly which types of inputs/outputs they are.

Here's an example from #921:
```python
import aesara
import aesara.tensor as at


k = at.constant([0], dtype=""int64"")

scan_out, _ = aesara.scan(
    fn=lambda x: k[0],
    outputs_info=[at.zeros((), dtype=""int64"")],
    n_steps=4,
)

aesara.dprint(scan_out)
# Subtensor{int64::} [id A]
#  |for{cpu,scan_fn} [id B] (outer_out_sit_sot-0)
#  | |TensorConstant{4} [id C] (n_steps)
#  | |IncSubtensor{Set;:int64:} [id D] (outer_in_sit_sot-0)
#  |   |AllocEmpty{dtype='int64'} [id E]
#  |   | |Elemwise{add,no_inplace} [id F]
#  |   |   |TensorConstant{4} [id C]
#  |   |   |Subtensor{int64} [id G]
#  |   |     |Shape [id H]
#  |   |     | |Rebroadcast{(0, False)} [id I]
#  |   |     |   |InplaceDimShuffle{x} [id J]
#  |   |     |     |Alloc [id K]
#  |   |     |       |TensorConstant{0} [id L]
#  |   |     |ScalarConstant{0} [id M]
#  |   |Rebroadcast{(0, False)} [id I]
#  |   |ScalarFromTensor [id N]
#  |     |Subtensor{int64} [id G]
#  |ScalarConstant{1} [id O]
#
# Inner graphs:
#
# for{cpu,scan_fn} [id B] (outer_out_sit_sot-0)
#  >Subtensor{int64} [id P] (inner_out_sit_sot-0)
#  > |TensorConstant{(1,) of 0} [id Q]
#  > |ScalarConstant{0} [id R]
```",enhancement Scan tooling,,2022-05-04 21:46:07,2022-05-07 00:37:29,"brandonwillard labeled 2022-05-04 21:46:07,brandonwillard labeled 2022-05-04 21:46:07,brandonwillard labeled 2022-05-04 21:46:07,brandonwillard closed 2022-05-07 00:37:30",brandonwillard,0
752,954,Gracefully skip nightly releases when credentials aren't present,hectormz,"## Description of your problem or feature request

My `main` branch of my fork of `aesara` runs a nightly build via GitHub Actions, and fails each night because I don't have the secrets/credentials. I get a notification of the failed Action every night, and assume this happens for others too.

Can we change the GitHub Action to disable this? I think two options would be to either skip the build if the secret credentials are not found, or to check if it is run on the main `aesara-devs` copy. ",enhancement help wanted CI,,2022-05-10 01:53:20,2022-06-07 16:40:25,"brandonwillard renamed 2022-05-10 21:07:22,brandonwillard labeled 2022-05-10 21:08:05,brandonwillard labeled 2022-05-10 21:08:05,brandonwillard labeled 2022-05-10 21:08:05,brandonwillard connected 2022-06-07 16:40:05,brandonwillard closed 2022-06-07 16:40:26",brandonwillard hectormz,3
753,955,MergeOptimization tries to merge nodes with different static shapes,ricardoV94,"Still need to investigate this one a bit further, showed up in https://github.com/pymc-devs/pymc/pull/5757

```python
import aesara
import aesara.tensor as at
import numpy as np

w = aesara.shared(np.random.rand(20))
u = aesara.shared(np.random.rand(20))

z0 = at.arange(0, 20).astype(""float64"")
z0_ = z0.dimshuffle(""x"", 0)
hwz = at.tanh(z0_.dot(w))
z1 = z0_ + at.outer(hwz, u)
z1 = z1.flatten()
aesara.config.on_opt_error = ""raise""
at.jacobian(z1, z0).eval()
```

<details>
 <summary>Traceback</summary>

```
<<!! BUG IN FGRAPH.REPLACE OR A LISTENER !!>> <class 'TypeError'> The type of the replacement (TensorType(float64, (1, None))) must be compatible with the type of the original Variable (TensorType(float64, (1, 20))). MergeOptimizer
ERROR (aesara.graph.opt): SeqOptimizer apply MergeOptimizer
ERROR (aesara.graph.opt): Traceback:
ERROR (aesara.graph.opt): Traceback (most recent call last):
  File ""/home/ricardo/Documents/Projects/pymc3-venv/lib/python3.8/site-packages/aesara/graph/opt.py"", line 280, in apply
    sub_prof = optimizer.optimize(fgraph)
  File ""/home/ricardo/Documents/Projects/pymc3-venv/lib/python3.8/site-packages/aesara/graph/opt.py"", line 103, in optimize
    ret = self.apply(fgraph, *args, **kwargs)
  File ""/home/ricardo/Documents/Projects/pymc3-venv/lib/python3.8/site-packages/aesara/graph/opt.py"", line 746, in apply
    fgraph.replace_all_validate(pairs, reason=""MergeOptimizer"")
  File ""/home/ricardo/Documents/Projects/pymc3-venv/lib/python3.8/site-packages/aesara/graph/features.py"", line 553, in replace_all_validate
    fgraph.replace(r, new_r, reason=reason, verbose=False, **kwargs)
  File ""/home/ricardo/Documents/Projects/pymc3-venv/lib/python3.8/site-packages/aesara/graph/fg.py"", line 511, in replace
    self.change_node_input(
  File ""/home/ricardo/Documents/Projects/pymc3-venv/lib/python3.8/site-packages/aesara/graph/fg.py"", line 428, in change_node_input
    raise TypeError(
TypeError: The type of the replacement (TensorType(float64, (1, None))) must be compatible with the type of the original Variable (TensorType(float64, (1, 20))).
Traceback (most recent call last):
  File ""/home/ricardo/Documents/Projects/pymc3-venv/lib/python3.8/site-packages/aesara/link/vm.py"", line 1112, in make_all
    node.op.make_thunk(node, storage_map, compute_map, [], impl=impl)
  File ""/home/ricardo/Documents/Projects/pymc3-venv/lib/python3.8/site-packages/aesara/scan/op.py"", line 1411, in make_thunk
    isinstance(out, TensorVariable) for out in self.fn.maker.fgraph.outputs
  File ""/home/ricardo/Documents/Projects/pymc3-venv/lib/python3.8/site-packages/aesara/scan/op.py"", line 1349, in fn
    self._fn = function(
  File ""/home/ricardo/Documents/Projects/pymc3-venv/lib/python3.8/site-packages/aesara/compile/function/__init__.py"", line 317, in function
    fn = pfunc(
  File ""/home/ricardo/Documents/Projects/pymc3-venv/lib/python3.8/site-packages/aesara/compile/function/pfunc.py"", line 363, in pfunc
    return orig_function(
  File ""/home/ricardo/Documents/Projects/pymc3-venv/lib/python3.8/site-packages/aesara/compile/function/types.py"", line 1725, in orig_function
    m = Maker(
  File ""/home/ricardo/Documents/Projects/pymc3-venv/lib/python3.8/site-packages/aesara/compile/function/types.py"", line 1464, in __init__
    optimizer_profile = optimizer(fgraph)
  File ""/home/ricardo/Documents/Projects/pymc3-venv/lib/python3.8/site-packages/aesara/graph/opt.py"", line 112, in __call__
    return self.optimize(fgraph)
  File ""/home/ricardo/Documents/Projects/pymc3-venv/lib/python3.8/site-packages/aesara/graph/opt.py"", line 103, in optimize
    ret = self.apply(fgraph, *args, **kwargs)
  File ""/home/ricardo/Documents/Projects/pymc3-venv/lib/python3.8/site-packages/aesara/graph/opt.py"", line 291, in apply
    self.failure_callback(e, self, optimizer)
  File ""/home/ricardo/Documents/Projects/pymc3-venv/lib/python3.8/site-packages/aesara/graph/opt.py"", line 226, in warn
    raise exc
  File ""/home/ricardo/Documents/Projects/pymc3-venv/lib/python3.8/site-packages/aesara/graph/opt.py"", line 280, in apply
    sub_prof = optimizer.optimize(fgraph)
  File ""/home/ricardo/Documents/Projects/pymc3-venv/lib/python3.8/site-packages/aesara/graph/opt.py"", line 103, in optimize
    ret = self.apply(fgraph, *args, **kwargs)
  File ""/home/ricardo/Documents/Projects/pymc3-venv/lib/python3.8/site-packages/aesara/graph/opt.py"", line 746, in apply
    fgraph.replace_all_validate(pairs, reason=""MergeOptimizer"")
  File ""/home/ricardo/Documents/Projects/pymc3-venv/lib/python3.8/site-packages/aesara/graph/features.py"", line 553, in replace_all_validate
    fgraph.replace(r, new_r, reason=reason, verbose=False, **kwargs)
  File ""/home/ricardo/Documents/Projects/pymc3-venv/lib/python3.8/site-packages/aesara/graph/fg.py"", line 511, in replace
    self.change_node_input(
  File ""/home/ricardo/Documents/Projects/pymc3-venv/lib/python3.8/site-packages/aesara/graph/fg.py"", line 428, in change_node_input
    raise TypeError(
TypeError: The type of the replacement (TensorType(float64, (1, None))) must be compatible with the type of the original Variable (TensorType(float64, (1, 20))).
During handling of the above exception, another exception occurred:
Traceback (most recent call last):
  File ""/home/ricardo/Documents/Projects/pymc3-venv/lib/python3.8/site-packages/IPython/core/interactiveshell.py"", line 3418, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File ""<ipython-input-37-615f4005dd2e>"", line 15, in <module>
    at.jacobian(z1, z0).eval()
  File ""/home/ricardo/Documents/Projects/pymc3-venv/lib/python3.8/site-packages/aesara/graph/basic.py"", line 567, in eval
    self._fn_cache[inputs] = function(inputs, self)
  File ""/home/ricardo/Documents/Projects/pymc3-venv/lib/python3.8/site-packages/aesara/compile/function/__init__.py"", line 317, in function
    fn = pfunc(
  File ""/home/ricardo/Documents/Projects/pymc3-venv/lib/python3.8/site-packages/aesara/compile/function/pfunc.py"", line 363, in pfunc
    return orig_function(
  File ""/home/ricardo/Documents/Projects/pymc3-venv/lib/python3.8/site-packages/aesara/compile/function/types.py"", line 1736, in orig_function
    fn = m.create(defaults)
  File ""/home/ricardo/Documents/Projects/pymc3-venv/lib/python3.8/site-packages/aesara/compile/function/types.py"", line 1631, in create
    _fn, _i, _o = self.linker.make_thunk(
  File ""/home/ricardo/Documents/Projects/pymc3-venv/lib/python3.8/site-packages/aesara/link/basic.py"", line 254, in make_thunk
    return self.make_all(
  File ""/home/ricardo/Documents/Projects/pymc3-venv/lib/python3.8/site-packages/aesara/link/vm.py"", line 1121, in make_all
    raise_with_op(fgraph, node)
  File ""/home/ricardo/Documents/Projects/pymc3-venv/lib/python3.8/site-packages/aesara/link/utils.py"", line 538, in raise_with_op
    raise exc_value.with_traceback(exc_trace)
  File ""/home/ricardo/Documents/Projects/pymc3-venv/lib/python3.8/site-packages/aesara/link/vm.py"", line 1112, in make_all
    node.op.make_thunk(node, storage_map, compute_map, [], impl=impl)
  File ""/home/ricardo/Documents/Projects/pymc3-venv/lib/python3.8/site-packages/aesara/scan/op.py"", line 1411, in make_thunk
    isinstance(out, TensorVariable) for out in self.fn.maker.fgraph.outputs
  File ""/home/ricardo/Documents/Projects/pymc3-venv/lib/python3.8/site-packages/aesara/scan/op.py"", line 1349, in fn
    self._fn = function(
  File ""/home/ricardo/Documents/Projects/pymc3-venv/lib/python3.8/site-packages/aesara/compile/function/__init__.py"", line 317, in function
    fn = pfunc(
  File ""/home/ricardo/Documents/Projects/pymc3-venv/lib/python3.8/site-packages/aesara/compile/function/pfunc.py"", line 363, in pfunc
    return orig_function(
  File ""/home/ricardo/Documents/Projects/pymc3-venv/lib/python3.8/site-packages/aesara/compile/function/types.py"", line 1725, in orig_function
    m = Maker(
  File ""/home/ricardo/Documents/Projects/pymc3-venv/lib/python3.8/site-packages/aesara/compile/function/types.py"", line 1464, in __init__
    optimizer_profile = optimizer(fgraph)
  File ""/home/ricardo/Documents/Projects/pymc3-venv/lib/python3.8/site-packages/aesara/graph/opt.py"", line 112, in __call__
    return self.optimize(fgraph)
  File ""/home/ricardo/Documents/Projects/pymc3-venv/lib/python3.8/site-packages/aesara/graph/opt.py"", line 103, in optimize
    ret = self.apply(fgraph, *args, **kwargs)
  File ""/home/ricardo/Documents/Projects/pymc3-venv/lib/python3.8/site-packages/aesara/graph/opt.py"", line 291, in apply
    self.failure_callback(e, self, optimizer)
  File ""/home/ricardo/Documents/Projects/pymc3-venv/lib/python3.8/site-packages/aesara/graph/opt.py"", line 226, in warn
    raise exc
  File ""/home/ricardo/Documents/Projects/pymc3-venv/lib/python3.8/site-packages/aesara/graph/opt.py"", line 280, in apply
    sub_prof = optimizer.optimize(fgraph)
  File ""/home/ricardo/Documents/Projects/pymc3-venv/lib/python3.8/site-packages/aesara/graph/opt.py"", line 103, in optimize
    ret = self.apply(fgraph, *args, **kwargs)
  File ""/home/ricardo/Documents/Projects/pymc3-venv/lib/python3.8/site-packages/aesara/graph/opt.py"", line 746, in apply
    fgraph.replace_all_validate(pairs, reason=""MergeOptimizer"")
  File ""/home/ricardo/Documents/Projects/pymc3-venv/lib/python3.8/site-packages/aesara/graph/features.py"", line 553, in replace_all_validate
    fgraph.replace(r, new_r, reason=reason, verbose=False, **kwargs)
  File ""/home/ricardo/Documents/Projects/pymc3-venv/lib/python3.8/site-packages/aesara/graph/fg.py"", line 511, in replace
    self.change_node_input(
  File ""/home/ricardo/Documents/Projects/pymc3-venv/lib/python3.8/site-packages/aesara/graph/fg.py"", line 428, in change_node_input
    raise TypeError(
TypeError: The type of the replacement (TensorType(float64, (1, None))) must be compatible with the type of the original Variable (TensorType(float64, (1, 20))).
Apply node that caused the error: for{cpu,scan_fn}(TensorConstant{20}, TensorConstant{[ 0  1  2 .. 17 18 19]}, TensorConstant{20}, Reshape{1}.0, InplaceDimShuffle{x,0}.0, Elemwise{Composite{(i0 - sqr(i1))}}[(0, 1)].0, InplaceDimShuffle{0,x}.0)
Toposort index: 8
Inputs types: [TensorType(int64, ()), TensorType(int64, (20,)), TensorType(int64, ()), TensorType(float64, (None,)), TensorType(float64, (1, None)), TensorType(float64, (1,)), TensorType(float64, (None, 1))]
HINT: Use a linker other than the C linker to print the inputs' shapes and strides.
HINT: Re-running with most Aesara optimizations disabled could provide a back-trace showing when this node was created. This can be done by setting the Aesara flag 'optimizer=fast_compile'. If that does not work, Aesara optimizations can be disabled with 'optimizer=None'.
HINT: Use the Aesara flag `exception_verbosity=high` for a debug print-out and storage map footprint of this Apply node.
```

</details>",bug important graph rewriting shape inference,ricardoV94,2022-05-11 09:05:40,2022-07-07 22:09:19,"ricardoV94 renamed 2022-05-11 09:11:53,ricardoV94 renamed 2022-05-11 09:21:19,ricardoV94 labeled 2022-05-11 09:37:24,ricardoV94 labeled 2022-05-11 09:37:24,ricardoV94 labeled 2022-05-11 09:37:24,brandonwillard labeled 2022-05-11 19:43:53,ricardoV94 assigned 2022-05-12 07:11:21,brandonwillard closed 2022-07-07 22:09:19",ricardoV94 brandonwillard,2
758,964,Copied function does not respect original single output format,ricardoV94,"```python
import aesara
import aesara.tensor as at

f1 = aesara.function([], at.constant(1.0))
f2 = f1.copy()
print(f1(), f2())  # 1.0 [array(1., dtype=float32)]
```",bug help wanted,,2022-05-18 14:26:51,2022-05-20 04:29:12,"ricardoV94 renamed 2022-05-18 14:27:05,ricardoV94 labeled 2022-05-19 18:12:35,brandonwillard labeled 2022-05-20 02:52:04,brandonwillard closed 2022-05-20 04:29:12",ricardoV94 brandonwillard,0
759,965,"As a site visitor, I miss an explanation of how Aesara is different",pkese,"I wish there was a sentence somewhere near the top of README explaining why use Aesara instead of something else.

Something akin to *""unlike Tensorflow and PyTorch, Aesara lets you XXXXX, and compared to JAX, Aesara does YYYYY.""*

And if it is based on Theano (I'm just guessing), then maybe explain what's was so special about Theano, as to be worth keeping it around.

This would be of great help for anyone trying to understand and get familiar with this project.

Oh, and as an end-user, I'm interested more in how my experience will be different with Aesara, rather than if it is implemented in Python or C++.",question,,2022-05-19 13:40:18,2022-05-19 15:28:57,"brandonwillard labeled 2022-05-19 15:28:40,aesara-devs locked 2022-05-19 15:28:57,brandonwillard converted_to_discussion 2022-05-19 15:28:57",aesara-devs brandonwillard pkese,1
760,967,"Add ""What is Aesara"" description to readme",twiecki,"### Discussed in https://github.com/aesara-devs/aesara/discussions/879

Text from @brandonwillard:

Aesara is a fork of Theano, and Theano was commonly referred to as a ""deep learning"" (DL) library, but Aesara is **not** a DL library.  

Designations like ""deep learning library"" reflect the priorities/goals of a library; specifically, that the library serves the purposes of DL and its computational needs.  Aesara is not explicitly intended to serve the purpose of constructing and evaluating DL models, but that doesn't mean it can't serve that purpose well.

As far as designations or labels are concerned, instead of describing our project's priorities/goals based on an area of study or application (e.g. DL, machine learning, statistical modeling, etc.), we prefer to focus on the _functionality_ that Aesara is expected to provide, and that's primarily **symbolic tensor computations**.

The designation ""tensor library"" is more apt, but, unlike most other tensor libraries (e.g. TensorFlow, PyTorch, etc.), Aesara is more focused on what one might call the _symbolic_ functionality.  

As a library, Aesara focuses on and advocates the extension of its core offerings, which are as follows: 
- a framework for flexible graph-based representations of computations,
    - E.g. the construction of custom `Type`s, `Variable`s, `Op`s, and lower-level graph elements
- implementations of basic tensor objects and operations,
    - E.g. `Type`, `Variable`, and `Op` implementations that mirror ""tensor""-based NumPy and SciPy offerings, and their gradients
- graph analysis and rewriting, 
    - E.g. the general manipulation of graphs for the purposes of ""optimization"", automation, etc.
- and code transpilation.
    - E.g. the conversion of graphs into performant code via other target languages/representations

Most tensor libraries perform these operations to some extent, but many do not expose the underlying operations for use at any level other than internal library development.  Furthermore, when they do, many libraries cross a large language barrier that unnecessarily hampers rapid development (e.g. moving from Python to C++ and back).  

For most tensor libraries, a NumPy-like interface to compiled tensor computations is the primary/only offering of the library.  Aesara takes the opposite approach and views _all_ the aforementioned operations as part of the core offerings of the library, but it also stitches them together so that the library can be used like other tensor libraries.

There are some concrete reasons for taking this approach, and one is the representation and construction of efficient domain-specific symbolic computations.  If you follow the history of this project, you can see that it grew out of work on PyMC, and PyMC is a library for domain-specific (i.e. probabilistic modeling) computations.  Likewise, the other `aesara-devs` projects demonstrate the use of Aesara graphs as an intermediate representation (IR) for a domain-specific language/interface (e.g. [`aeppl`](https://github.com/aesara-devs/aeppl) provides a graph representation for a PPL) and advanced automations based on IR (e.g. [`aemcmc`](https://github.com/aesara-devs/aemcmc) as a means of constructing custom samplers from IR, `aeppl` as a means of automatically deriving log-probabilities for basic tensor operations represented in IR).  
This topic is a little more advanced and doesn't really have parallels in other tensor libraries, but it's one of the things that Aesara uniquely facilitates.

The PyMC/probabilistic programming connection is similar to the DL connection Theano had, but&mdash;unlike Theano&mdash;we don't want Aesara to be conflated with one of its domains of application&mdash;like probabilistic modeling.  Those primary domains of application will always have _some_ influence on the development of Aesara, but that's also why we need to avoid labels/designations like ""deep learning library"" and focus on the functionality, so that we don't unnecessarily compromise Aesara's general applicability, relative simplicity, and/or prevent useful input/collaboration from other domains.
",documentation good first issue,,2022-05-19 21:57:33,2022-06-05 05:27:05,"twiecki labeled 2022-05-19 21:57:33,twiecki labeled 2022-05-19 21:57:33,brandonwillard mentioned 2022-05-19 21:57:33,brandonwillard subscribed 2022-05-19 21:57:33,ricardoV94 closed 2022-06-05 05:27:05",Mount-Blanc ricardoV94 twiecki brandonwillard,1
764,971,Bug in formatting of generated JAX code files.,jhrcook,"## Description of your problem or feature request

I am trying to sample from a hierarchical model using the Numpyro JAX backend, but get a funny error for some models. To begin, I have sampled successfully from other models, so the JAX backend is working fine. Unfortunately, because of the problem discussed [here](https://discourse.pymc.io/t/pymc-4-0-0b6-module-jax-ops-has-no-attribute-index-update/9267), I have had to downgrade my version of `jax` to v0.2.28 (and consequently `jaxlib` to v0.3.0). I would hope that updating `jax` would fix this problem, but it’s not an option yet.

Here is the error message I receive when trying to sample using Numpyro:

```
Traceback (most recent call last):

  File /usr/local/Caskroom/miniconda/base/envs/bluishred/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3397 in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)

  Input In [77] in <cell line: 1>
    m9_trace = model_fitting_pipeline(

  Input In [8] in model_fitting_pipeline
    trace = sample_model(

  File ~/Developer/haigis-lab/bluishred/bluishred/model_fitting.py:104 in sample_model
    trace = _sample_model(m, sample_kwargs, mcmc_backend)

  File ~/Developer/haigis-lab/bluishred/bluishred/model_fitting.py:37 in _sample_model
    trace = sample_numpyro_nuts(**sample_kwargs)

  File /usr/local/Caskroom/miniconda/base/envs/bluishred/lib/python3.10/site-packages/pymc/sampling_jax.py:483 in sample_numpyro_nuts
    logp_fn = get_jaxified_logp(model, negative_logp=False)

  File /usr/local/Caskroom/miniconda/base/envs/bluishred/lib/python3.10/site-packages/pymc/sampling_jax.py:106 in get_jaxified_logp
    logp_fn = get_jaxified_graph(inputs=model.value_vars, outputs=[model_logpt])

  File /usr/local/Caskroom/miniconda/base/envs/bluishred/lib/python3.10/site-packages/pymc/sampling_jax.py:99 in get_jaxified_graph
    return jax_funcify(fgraph)

  File /usr/local/Caskroom/miniconda/base/envs/bluishred/lib/python3.10/functools.py:889 in wrapper
    return dispatch(args[0].__class__)(*args, **kw)

  File /usr/local/Caskroom/miniconda/base/envs/bluishred/lib/python3.10/site-packages/aesara/link/jax/dispatch.py:669 in jax_funcify_FunctionGraph
    return fgraph_to_python(

  File /usr/local/Caskroom/miniconda/base/envs/bluishred/lib/python3.10/site-packages/aesara/link/utils.py:791 in fgraph_to_python
    fgraph_def = compile_function_src(

  File /usr/local/Caskroom/miniconda/base/envs/bluishred/lib/python3.10/site-packages/aesara/link/utils.py:609 in compile_function_src
    mod_code = compile(src, filename, mode=""exec"")

  File /var/folders/r4/qpcdgl_14hbd412snp1jnv300000gn/T/tmpilbd79jk:14
    9 9]})
    ^
IndentationError: unexpected indent
```

And here is the head of the file `/var/folders/r4/qpcdgl_14hbd412snp1jnv300000gn/T/tmpilbd79jk`:

```python
def jax_funcified_fgraph(mu_mu_a, sigma_mu_a_log_, mu_a, sigma_a_log_, delta_a, sigma_b_log_, b, sigma_c_log_, c, sigma_d_log_, delta_d, sigma_k_log_, k, alpha_log_):
    # AdvancedSubtensor1(k, TensorConstant{[0 0 0 0 0..9 9 9 9 9]})
    auto_1195180 = subtensor(k, auto_1144647)
    # Elemwise{exp,no_inplace}(sigma_d_log__)
    auto_1192542 = exp(sigma_d_log_)
    # AdvancedSubtensor1(c, TensorConstant{[0 0 0 0 0..9 9 9 9 9]})
    auto_1195178 = subtensor1(c, auto_1144647)
    # AdvancedSubtensor(b, TensorConstant{[0 0 0 0 0..9 9 9 9 9]}, TensorConstant{[0 0 0 0 0..1 1 1 1 1]})
    auto_1192551 = subtensor2(b, auto_1144647, auto_1144655)
    # Elemwise{exp,no_inplace}(sigma_a_log__)
    auto_1192553 = exp(sigma_a_log_)
    # AdvancedSubtensor1(mu_a, TensorConstant{[0 0 0 0 1.. 9 9
     9 9]})
    auto_1195177 = subtensor3(mu_a, auto_1144172)
    # Elemwise{exp,no_inplace}(alpha_log__)
    auto_1192534 = exp(alpha_log_)
    # Elemwise{exp,no_inplace}(sigma_k_log__)
```

I have tried to reproduce the error with a minimal, self-contained, reproducible example, but am unable to replicate the error. Still, it was recommended to me to open an Issue to make the devs aware of the problem. Because of this limitation, I am willing to provide as much additional information as requested.

## Versions and main components

* Aesara version: 2.6.2
* Aesara config (`python -c ""import aesara; print(aesara.config)""`)
* PyMC version: 4.0.0b6 
* jax version: 0.2.28
* jaxlib version:  0.3.0
* Python version: 3.10.4
* Operating system: macOS Monterey v12.3.1
* How did you install Aesara: `conda`
",JAX MWE needed,,2022-05-29 18:04:22,2022-06-03 08:27:35,"ricardoV94 renamed 2022-06-01 07:08:32,ricardoV94 labeled 2022-06-01 07:09:04,ricardoV94 labeled 2022-06-01 07:09:04,brandonwillard unlabeled 2022-06-03 06:29:45,brandonwillard labeled 2022-06-03 06:29:45,brandonwillard connected 2022-06-03 07:08:03,brandonwillard closed 2022-06-03 08:27:35,jhrcook mentioned 2022-06-03 09:49:42,jhrcook subscribed 2022-06-03 09:49:42,jhrcook mentioned 2022-06-03 18:43:25,jhrcook subscribed 2022-06-03 18:43:25",ricardoV94 jhrcook brandonwillard,3
770,979,`Elemwise.infer_shape` incorrectly relies on its outputs' broadcast patterns,brandonwillard,"### Discussed in https://github.com/aesara-devs/aesara/discussions/978

<div type='discussions-op-text'>

<sup>Originally posted by **jessegrabowski** June  5, 2022</sup>
Hello,

I am trying to hunt down the cause of a shape issue in my code and I am really stumped. Here's a minimal example that reproduces the error:

```python
import numpy as np
import aesara.tensor as at
from aesara.tensor.nlinalg import matrix_dot

def step_func(y, P, Z, H):
    nan_mask = at.isnan(y)
    W = at.set_subtensor(at.eye(y.shape[0])[nan_mask, nan_mask], 0.0)
    Z_masked = W.dot(Z)
    H_masked = W.dot(H)
    
    F = matrix_dot(Z_masked, P, Z_masked.T) + H_masked
    F_inv = at.linalg.solve(F, at.eye(F.shape[0]))
    
    K = matrix_dot(P, Z_masked.T, F_inv)
    z = matrix_dot(K, H_masked, K.T)
    
    return z

data = np.arange(10).astype(float)[:, None]

k_obs = 1
k_states = 2
k_stochastic = 2

Z = at.zeros((k_obs, k_states))
H = at.zeros((k_obs, k_obs))
P = at.zeros((k_stochastic, k_stochastic))

Z = at.set_subtensor(Z[0, 0], 1.0)
H = at.set_subtensor(H[0, 0], 0.3)
P = at.set_subtensor(P[[0, 1], [0, 1]], 0.7)

z = step_func(data[0], P, Z, H)

```
<summary>Long error traceback </summary>
<details><p>

```python
---------------------------------------------------------------------------
AssertionError                            Traceback (most recent call last)
Input In [201], in <cell line: 1>()
----> 1 z.eval()

File ~\\miniconda3\\envs\\pymc_dev\\lib\\site-packages\\aesara\\graph\\basic.py:566, in Variable.eval(self, inputs_to_values)
    564 inputs = tuple(sorted(inputs_to_values.keys(), key=id))
    565 if inputs not in self._fn_cache:
--> 566     self._fn_cache[inputs] = function(inputs, self)
    567 args = [inputs_to_values[param] for param in inputs]
    569 rval = self._fn_cache[inputs](*args)

File ~\\miniconda3\\envs\\pymc_dev\\lib\\site-packages\\aesara\\compile\\function\\__init__.py:337, in function(inputs, outputs, mode, updates, givens, no_default_updates, accept_inplace, name, rebuild_strict, allow_input_downcast, profile, on_unused_input)
    331     fn = orig_function(
    332         inputs, outputs, mode=mode, accept_inplace=accept_inplace, name=name
    333     )
    334 else:
    335     # note: pfunc will also call orig_function -- orig_function is
    336     #      a choke point that all compilation must pass through
--> 337     fn = pfunc(
    338         params=inputs,
    339         outputs=outputs,
    340         mode=mode,
    341         updates=updates,
    342         givens=givens,
    343         no_default_updates=no_default_updates,
    344         accept_inplace=accept_inplace,
    345         name=name,
    346         rebuild_strict=rebuild_strict,
    347         allow_input_downcast=allow_input_downcast,
    348         on_unused_input=on_unused_input,
    349         profile=profile,
    350         output_keys=output_keys,
    351     )
    352 return fn

File ~\\miniconda3\\envs\\pymc_dev\\lib\\site-packages\\aesara\\compile\\function\\pfunc.py:363, in pfunc(params, outputs, mode, updates, givens, no_default_updates, accept_inplace, name, rebuild_strict, allow_input_downcast, profile, on_unused_input, output_keys)
    350     profile = ProfileStats(message=profile)
    352 inputs, cloned_outputs = construct_pfunc_ins_and_outs(
    353     params,
    354     outputs,
   (...)
    360     allow_input_downcast,
    361 )
--> 363 return orig_function(
    364     inputs,
    365     cloned_outputs,
    366     mode,
    367     accept_inplace=accept_inplace,
    368     name=name,
    369     profile=profile,
    370     on_unused_input=on_unused_input,
    371     output_keys=output_keys,
    372 )

File ~\\miniconda3\\envs\\pymc_dev\\lib\\site-packages\\aesara\\compile\\function\\types.py:1732, in orig_function(inputs, outputs, mode, accept_inplace, name, profile, on_unused_input, output_keys)
   1730 try:
   1731     Maker = getattr(mode, ""function_maker"", FunctionMaker)
-> 1732     m = Maker(
   1733         inputs,
   1734         outputs,
   1735         mode,
   1736         accept_inplace=accept_inplace,
   1737         profile=profile,
   1738         on_unused_input=on_unused_input,
   1739         output_keys=output_keys,
   1740         name=name,
   1741     )
   1742     with config.change_flags(compute_test_value=""off""):
   1743         fn = m.create(defaults)

File ~\\miniconda3\\envs\\pymc_dev\\lib\\site-packages\\aesara\\compile\\function\\types.py:1471, in FunctionMaker.__init__(self, inputs, outputs, mode, accept_inplace, function_builder, profile, on_unused_input, fgraph, output_keys, name)
   1465 opt_time = None
   1467 with config.change_flags(
   1468     compute_test_value=config.compute_test_value_opt,
   1469     traceback__limit=config.traceback__compile_limit,
   1470 ):
-> 1471     optimizer_profile = optimizer(fgraph)
   1473     end_optimizer = time.time()
   1474     opt_time = end_optimizer - start_optimizer

File ~\\miniconda3\\envs\\pymc_dev\\lib\\site-packages\\aesara\\graph\\opt.py:112, in GlobalOptimizer.__call__(self, fgraph)
    106 def __call__(self, fgraph):
    107     """"""Optimize a `FunctionGraph`.
    108 
    109     This is the same as ``self.optimize(fgraph)``.
    110 
    111     """"""
--> 112     return self.optimize(fgraph)

File ~\\miniconda3\\envs\\pymc_dev\\lib\\site-packages\\aesara\\graph\\opt.py:103, in GlobalOptimizer.optimize(self, fgraph, *args, **kwargs)
     94 """"""
     95 
     96 This is meant as a shortcut for the following::
   (...)
    100 
    101 """"""
    102 self.add_requirements(fgraph)
--> 103 ret = self.apply(fgraph, *args, **kwargs)
    104 return ret

File ~\\miniconda3\\envs\\pymc_dev\\lib\\site-packages\\aesara\\graph\\opt.py:280, in SeqOptimizer.apply(self, fgraph)
    278 nb_nodes_before = len(fgraph.apply_nodes)
    279 t0 = time.time()
--> 280 sub_prof = optimizer.optimize(fgraph)
    281 l.append(float(time.time() - t0))
    282 sub_profs.append(sub_prof)

File ~\\miniconda3\\envs\\pymc_dev\\lib\\site-packages\\aesara\\graph\\opt.py:103, in GlobalOptimizer.optimize(self, fgraph, *args, **kwargs)
     94 """"""
     95 
     96 This is meant as a shortcut for the following::
   (...)
    100 
    101 """"""
    102 self.add_requirements(fgraph)
--> 103 ret = self.apply(fgraph, *args, **kwargs)
    104 return ret

File ~\\miniconda3\\envs\\pymc_dev\\lib\\site-packages\\aesara\\graph\\opt.py:280, in SeqOptimizer.apply(self, fgraph)
    278 nb_nodes_before = len(fgraph.apply_nodes)
    279 t0 = time.time()
--> 280 sub_prof = optimizer.optimize(fgraph)
    281 l.append(float(time.time() - t0))
    282 sub_profs.append(sub_prof)

File ~\\miniconda3\\envs\\pymc_dev\\lib\\site-packages\\aesara\\graph\\opt.py:103, in GlobalOptimizer.optimize(self, fgraph, *args, **kwargs)
     94 """"""
     95 
     96 This is meant as a shortcut for the following::
   (...)
    100 
    101 """"""
    102 self.add_requirements(fgraph)
--> 103 ret = self.apply(fgraph, *args, **kwargs)
    104 return ret

File ~\\miniconda3\\envs\\pymc_dev\\lib\\site-packages\\aesara\\graph\\opt.py:2329, in EquilibriumOptimizer.apply(self, fgraph, start_from)
   2327 nb = change_tracker.nb_imported
   2328 t_opt = time.time()
-> 2329 lopt_change = self.process_node(fgraph, node, lopt)
   2330 time_opts[lopt] += time.time() - t_opt
   2331 if not lopt_change:

File ~\\miniconda3\\envs\\pymc_dev\\lib\\site-packages\\aesara\\graph\\opt.py:1850, in NavigatorOptimizer.process_node(self, fgraph, node, lopt)
   1848 lopt = lopt or self.local_opt
   1849 try:
-> 1850     replacements = lopt.transform(fgraph, node)
   1851 except Exception as e:
   1852     if self.failure_callback is not None:

File ~\\miniconda3\\envs\\pymc_dev\\lib\\site-packages\\aesara\\graph\\opt.py:1055, in FromFunctionLocalOptimizer.transform(self, fgraph, node)
   1050     if not (
   1051         node.op in self._tracks or isinstance(node.op, self._tracked_types)
   1052     ):
   1053         return False
-> 1055 return self.fn(fgraph, node)

File ~\\miniconda3\\envs\\pymc_dev\\lib\\site-packages\\aesara\\tensor\\blas.py:1796, in local_dot22_to_ger_or_gemv(fgraph, node)
   1793 elif not xb[0] and not xb[1] and yb[1]:
   1794     # x is matrix, y is vector, try gemv
   1795     yv = y.dimshuffle(0)
-> 1796     zeros = at.AllocEmpty(x.dtype)(x.shape[0])
   1797     rval = gemv_no_inplace(zeros, one, x, yv, zero)
   1798     new_out = [rval.dimshuffle(0, ""x"")]

File ~\\miniconda3\\envs\\pymc_dev\\lib\\site-packages\\aesara\\graph\\op.py:294, in Op.__call__(self, *inputs, **kwargs)
    252 r""""""Construct an `Apply` node using :meth:`Op.make_node` and return its outputs.
    253 
    254 This method is just a wrapper around :meth:`Op.make_node`.
   (...)
    291 
    292 """"""
    293 return_list = kwargs.pop(""return_list"", False)
--> 294 node = self.make_node(*inputs, **kwargs)
    296 if config.compute_test_value != ""off"":
    297     compute_test_value(node)

File ~\\miniconda3\\envs\\pymc_dev\\lib\\site-packages\\aesara\\tensor\\basic.py:4176, in AllocEmpty.make_node(self, *_shape)
   4175 def make_node(self, *_shape):
-> 4176     _shape, bcast = infer_broadcastable(_shape)
   4177     otype = TensorType(dtype=self.dtype, shape=bcast)
   4178     output = otype()

File ~\\miniconda3\\envs\\pymc_dev\\lib\\site-packages\\aesara\\tensor\\basic.py:1447, in infer_broadcastable(shape)
   1443     raise TypeError(f""Shapes must be scalar integers; got {s_as_str}"")
   1445 sh = [check_type(as_tensor_variable(s, ndim=0)) for s in shape]
-> 1447 shape_fg = FunctionGraph(
   1448     outputs=sh,
   1449     features=[ShapeFeature()],
   1450     clone=True,
   1451 )
   1452 folded_shape = optimize_graph(shape_fg, custom_opt=topo_constant_folding).outputs
   1454 bcast = tuple(getattr(s, ""data"", s) == 1 for s in folded_shape)

File ~\\miniconda3\\envs\\pymc_dev\\lib\\site-packages\\aesara\\graph\\fg.py:155, in FunctionGraph.__init__(self, inputs, outputs, features, clone, update_mapping, memo, copy_inputs, copy_orphans)
    152     self.add_input(in_var, check=False)
    154 for output in outputs:
--> 155     self.import_var(output, reason=""init"")
    156 for i, output in enumerate(outputs):
    157     self.clients[output].append((""output"", i))

File ~\\miniconda3\\envs\\pymc_dev\\lib\\site-packages\\aesara\\graph\\fg.py:296, in FunctionGraph.import_var(self, var, reason, import_missing)
    294 # Imports the owners of the variables
    295 if var.owner and var.owner not in self.apply_nodes:
--> 296     self.import_node(var.owner, reason=reason, import_missing=import_missing)
    297 elif (
    298     var.owner is None
    299     and not isinstance(var, Constant)
    300     and var not in self.inputs
    301 ):
    302     from aesara.graph.null_type import NullType

File ~\\miniconda3\\envs\\pymc_dev\\lib\\site-packages\\aesara\\graph\\fg.py:377, in FunctionGraph.import_node(self, apply_node, check, reason, import_missing)
    375         self.variables.add(input)
    376     self.add_client(input, (node, i))
--> 377 self.execute_callbacks(""on_import"", node, reason)

File ~\\miniconda3\\envs\\pymc_dev\\lib\\site-packages\\aesara\\graph\\fg.py:579, in FunctionGraph.execute_callbacks(self, name, *args, **kwargs)
    577         continue
    578     tf0 = time.time()
--> 579     fn(self, *args, **kwargs)
    580     self.execute_callbacks_times[feature] += time.time() - tf0
    581 self.execute_callbacks_time += time.time() - t0

File ~\\miniconda3\\envs\\pymc_dev\\lib\\site-packages\\aesara\\tensor\\basic_opt.py:1313, in ShapeFeature.on_import(self, fgraph, node, reason)
   1310         o_shapes[sh_idx] = tuple(new_shape)
   1312 for r, s in zip(node.outputs, o_shapes):
-> 1313     self.set_shape(r, s)

File ~\\miniconda3\\envs\\pymc_dev\\lib\\site-packages\\aesara\\tensor\\basic_opt.py:1086, in ShapeFeature.set_shape(self, r, s, override)
   1084         shape_vars.append(constant(r.type.shape[i], dtype=""int64""))
   1085     else:
-> 1086         shape_vars.append(self.unpack(s[i], r))
   1087 assert all(
   1088     not hasattr(r.type, ""broadcastable"") or not r.type.broadcastable[i] or
   1089     # The two following comparison are a speed optimization
   (...)
   1093     for i in range(r.type.ndim)
   1094 )
   1095 self.shape_of[r] = tuple(shape_vars)

File ~\\miniconda3\\envs\\pymc_dev\\lib\\site-packages\\aesara\\tensor\\basic_opt.py:986, in ShapeFeature.unpack(self, s_i, var)
    977 """"""Return a symbolic integer scalar for the shape element s_i.
    978 
    979 The s_i argument was produced by the infer_shape() of an Op subclass.
   (...)
    983 
    984 """"""
    985 # unpack the s_i that the Op returned
--> 986 assert s_i is not None
    987 if s_i == 1:
    988     # don't make the optimizer merge a zillion ones together
    989     # by always returning the same object to represent 1
    990     return self.lscalar_one
```
</p></details>

The error seems to be a confluence of several things, because I found several ways to make it go away. None of them, however, give me a general solution to my problem, so here I am.

The first is that if I just set up everything with symbolic objects rather than using `at.zeros`, it works fine. For example:

```python
data = at.matrix()
P = at.matrix()
Z = at.matrix()
H = at.matrix()

z = step_func(data[0], P, Z, H)
z.eval({data:np.arange(10).astype(float)[:, None],
        P: np.eye(2),
        Z: np.array([[1.0, 0.0]]),
        H: np.array([[1.0]])})
```

Does not throw an error. As an aside, I have run into a lot of shape issues trying to use `at.zeros` and `at.zeros_like` in general. Should these be avoided? I am using them here because of that's how I've written my API. These tend to be sparse matrices, and I wanted users to be able to assign only the relevant elements when they set up their problem.

Anyway, given the zeros/set_subtensor setup, the proximate cause of the error is the final matrix_dot inside the function. Removing this, I can get back all the intermediate computations and confirm that shapes conform. In particular, confirm that H_masked isn't being cast to a scalar. This is important because the ultimate cause seems to have something to do with the matrix H being 1 x 1. Changing the input variables so that H is 2 x 2 fixes the problem, for example:

```python
data = np.arange(10).astype(float)[:, None]
data = data.repeat(2, axis=1)

k_obs = 2
k_states = 2
k_stochastic = 2

Z = at.zeros((k_obs, k_states))
H = at.zeros((k_obs, k_obs))
P = at.zeros((k_stochastic, k_stochastic))

Z = at.set_subtensor(Z[[0, 1], [0, 1]], 1.0)
H = at.set_subtensor(H[[0, 1], [0, 1]], 0.3)
P = at.set_subtensor(P[[0, 1], [0, 1]], 0.7)
z = step_func(data[0], P, Z, H)
z.eval()
````

Does not throw an error. H being 1x1 is an important special case to my problem (one observed time series), so I really want to figure this out.

In addition, removing the first four lines that zero out columns of H and Z associated with missing data also fixes the problem. That is, this `step_func` works fine:

```python
def step_func(y, P, Z, H):
    F = matrix_dot(Z, P, Z.T) + H
    F_inv = at.linalg.solve(F, at.eye(F.shape[0]))
    
    K = matrix_dot(P, Z.T, F_inv)
    z = matrix_dot(K, H, K.T)
    
    return z
```

But then I lose the ability to interpolate missing data.

My favorite fix, however, is changing `F = matrix_dot(Z_masked, P, Z_masked.T) + H_masked` to `F = matrix_dot(Z_masked, P, Z_masked.T)`. I have no idea why that particular addition (pun intended) would cause a shape error in a matrix multiplication down-stream.

I hope I'm missing something obvious as usual. Also as usual, your help and time are greatly appreciated.

</div>",bug shape inference,,2022-06-05 18:52:10,2022-06-08 12:25:50,"brandonwillard labeled 2022-06-05 18:52:10,brandonwillard labeled 2022-06-05 18:52:10,brandonwillard mentioned 2022-06-06 18:45:30,brandonwillard subscribed 2022-06-06 18:45:30,jessegrabowski mentioned 2022-06-06 18:46:28,jessegrabowski subscribed 2022-06-06 18:46:28,brandonwillard renamed 2022-06-06 19:29:00,brandonwillard closed 2022-06-08 12:25:50",jessegrabowski brandonwillard,3
773,982,Update of random variables not working in JAX backend,ricardoV94,"```python
import aesara
import aesara.tensor as at
import numpy as np

rng = aesara.shared(np.random.default_rng())
x = at.random.normal(rng=rng)
f = aesara.function([], x, updates={rng: x.owner.outputs[0]}, mode=""JAX"")
assert f() != f()  # Fails
```",bug JAX backend compatibility,,2022-06-07 08:05:09,2022-09-30 13:33:16,"ricardoV94 labeled 2022-06-07 08:05:09,ricardoV94 labeled 2022-06-07 08:05:09,ricardoV94 labeled 2022-09-20 08:43:47,ricardoV94 closed 2022-09-30 13:33:16",ricardoV94,2
775,984,Gemm fails with simple broadcasting case,ricardoV94,"```python
import aesara
import aesara.tensor as at
import numpy as np

b_v = np.ones((1, 100))
w_v = np.ones((6, 100))
x_v = np.ones((10, 6))

x = at.matrix(""x"")
b = at.matrix(""b"")
w = at.matrix(""w"")

out = at.dot(x, w) + b

# It works fine if mode=""FAST_COMPILE"", which avoids introducing the GEMM Op
f = aesara.function([x, b, w], out)  

aesara.dprint(f, print_type=True)
```
```
Gemm{no_inplace} [id A] <TensorType(float64, (None, None))> ''   0
 |b [id B] <TensorType(float64, (None, None))>
 |TensorConstant{1.0} [id C] <TensorType(float64, ())>
 |x [id D] <TensorType(float64, (None, None))>
 |w [id E] <TensorType(float64, (None, None))>
 |TensorConstant{1.0} [id C] <TensorType(float64, ())>
```

```python
f(x_v, b_v, w_v)
```
```python
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
File ~/miniconda3/envs/aesara/lib/python3.10/site-packages/aesara/compile/function/types.py:964, in Function.__call__(self, *args, **kwargs)
    962 try:
    963     outputs = (
--> 964         self.fn()
    965         if output_subset is None
    966         else self.fn(output_subset=output_subset)
    967     )
    968 except Exception:

ValueError: Shape mismatch: x has 10 rows but z has 1 rows

During handling of the above exception, another exception occurred:

ValueError                                Traceback (most recent call last)
Input In [6], in <cell line: 19>()
     15 f = aesara.function([x, b, w], out)
     17 aesara.dprint(f, print_type=True)
---> 19 f(x_v, b_v, w_v)

File ~/miniconda3/envs/aesara/lib/python3.10/site-packages/aesara/compile/function/types.py:977, in Function.__call__(self, *args, **kwargs)
    975     if hasattr(self.fn, ""thunks""):
    976         thunk = self.fn.thunks[self.fn.position_of_error]
--> 977     raise_with_op(
    978         self.maker.fgraph,
    979         node=self.fn.nodes[self.fn.position_of_error],
    980         thunk=thunk,
    981         storage_map=getattr(self.fn, ""storage_map"", None),
    982     )
    983 else:
    984     # old-style linkers raise their own exceptions
    985     raise

File ~/miniconda3/envs/aesara/lib/python3.10/site-packages/aesara/link/utils.py:538, in raise_with_op(fgraph, node, thunk, exc_info, storage_map)
    533     warnings.warn(
    534         f""{exc_type} error does not allow us to add an extra error message""
    535     )
    536     # Some exception need extra parameter in inputs. So forget the
    537     # extra long error message in that case.
--> 538 raise exc_value.with_traceback(exc_trace)

File ~/miniconda3/envs/aesara/lib/python3.10/site-packages/aesara/compile/function/types.py:964, in Function.__call__(self, *args, **kwargs)
    961 t0_fn = time.time()
    962 try:
    963     outputs = (
--> 964         self.fn()
    965         if output_subset is None
    966         else self.fn(output_subset=output_subset)
    967     )
    968 except Exception:
    969     restore_defaults()

ValueError: Shape mismatch: x has 10 rows but z has 1 rows
Apply node that caused the error: Gemm{no_inplace}(b, TensorConstant{1.0}, x, w, TensorConstant{1.0})
Toposort index: 0
Inputs types: [TensorType(float64, (None, None)), TensorType(float64, ()), TensorType(float64, (None, None)), TensorType(float64, (None, None)), TensorType(float64, ())]
Inputs shapes: [(1, 100), (), (10, 6), (6, 100), ()]
Inputs strides: [(800, 8), (), (48, 8), (800, 8), ()]
Inputs values: ['not shown', array(1.), 'not shown', 'not shown', array(1.)]
Outputs clients: [['output']]

HINT: Re-running with most Aesara optimizations disabled could provide a back-trace showing when this node was created. This can be done by setting the Aesara flag 'optimizer=fast_compile'. If that does not work, Aesara optimizations can be disabled with 'optimizer=None'.
HINT: Use the Aesara flag `exception_verbosity=high` for a debug print-out and storage map footprint of this Apply node.
```
",bug important,ricardoV94,2022-06-07 13:50:05,2022-06-13 17:35:45,"ricardoV94 labeled 2022-06-07 13:50:05,brandonwillard labeled 2022-06-07 22:38:08,ricardoV94 assigned 2022-06-08 10:58:50,ricardoV94 closed 2022-06-13 17:35:45",ricardoV94 brandonwillard,8
776,985,The handling of static shapes in `IfElse` may be stricter than absolutely necessary,dfm,"The Aesara `ifelse` is stricter about shapes than Theano was. For example, the following works in Theano:

```python
import theano
import theano.tensor as tt
from theano.ifelse import ifelse

x = tt.scalar()
res = ifelse(tt.gt(x, 0.5), tt.zeros(3), tt.zeros(4))
theano.function([x], res)(2.0)
```

This returns `array([0., 0., 0.])` as expected.

But the equivalent Aesara code crashes with either

- `TypeError: IfElse requires compatible types for true and false return values`, or
- `Dimension 0 in Rebroadcast's input was supposed to be 1 (got 3 instead)` if one of the branches is broadcastable.

```python
import aesara
import aesara.tensor as at
from aesara.ifelse import ifelse

x = at.scalar()
res = ifelse(at.gt(x, 0.5), at.zeros(3), at.zeros(4))
aesara.function([x], res)(2.0)
```

To get the `Rebroadcast` error, change `at.zeros(4)` to `tt.zeros(1)`, for example.

This is not a deal breaker for me since I'm already supporting a JAX interface that has the same limitation, but @brandonwillard suggests that it may be possible to handle shapes more flexibly so I wanted to open this here!

## Versions and main components

* Aesara version: 2.7.1
* Aesara config (`python -c ""import aesara; print(aesara.config)""`): On colab
* Python version: 3.7
* Operating system: Colab
* How did you install Aesara: (conda/pip) pip
",bug help wanted shape inference,,2022-06-07 14:33:59,2022-08-04 14:37:05,"brandonwillard mentioned 2022-06-07 14:34:00,brandonwillard subscribed 2022-06-07 14:34:00,brandonwillard labeled 2022-06-07 15:52:50,brandonwillard labeled 2022-06-07 15:52:50,brandonwillard labeled 2022-06-07 15:52:58,brandonwillard closed 2022-08-04 14:37:06",dfm brandonwillard,4
778,987,Incorrect result for AdvancedIncSubtensor in numba backend,aseyboldt,"The numba implementation of `AdvancedIncSubtensor1` (and probably also `AdvancedIncSubtensor`) returns incorrect results if indices are used more than once:
```python
import aesara
import numba
import aesara.tensor as at
import numpy as np

x = at.dvector(""x"")
y = x[np.zeros(2, dtype=int)]
z = y.sum()

func = aesara.function([x], [z, at.grad(z, x)])
func2 = aesara.function([x], [z, at.grad(z, x)], mode=aesara.compile.NUMBA)

data = np.random.default_rng(42).normal(size=1)

func(data)
# Output is the correct [array(0.60943416), array([2.])]

func2(data)
# Output is [0.6094341595088627, array([1.])]
```
This is because it uses code like this:
```python
data = np.zeros(2)
data[[0, 0]] += 1
data
```
but due to buffering this only increments data[0] once instead of twice.

If we add a different implementation for the op it works fine:

```python
@aesara.link.numba.dispatch.numba_funcify.register(aesara.tensor.subtensor.AdvancedIncSubtensor1)
def numba_funcify_IncSubtensor(op, node, **kwargs):

    def incsubtensor_fn(z, vals, idxs):
        z = z.copy()
        for idx, val in zip(idxs, vals):
            z[idx] += val
        return z

    return aesara.link.numba.dispatch.basic.numba_njit(incsubtensor_fn)
```",duplicate,,2022-06-08 18:13:16,2022-06-09 17:24:14,"brandonwillard labeled 2022-06-09 17:23:29,brandonwillard closed 2022-06-09 17:24:15,brandonwillard closed 2022-08-04 18:11:12",ricardoV94 aseyboldt brandonwillard,1
779,990,Add JAX implementation for log1mexp,twiecki,It seems like there is no JAX implementation for the `log1mexp` Op. Should be easy to add.,good first issue JAX,,2022-06-12 16:57:46,2022-06-15 16:22:35,"twiecki labeled 2022-06-12 16:57:46,twiecki labeled 2022-06-12 16:57:46,brandonwillard connected 2022-06-14 11:05:20,ricardoV94 closed 2022-06-15 16:22:35",ricardoV94 twiecki brandonwillard,0
780,992,Error w/ deepcopy of pymc v4 model,hectormz,"## Generating a deep copy of pymc v4 model results in aesara error

**Please provide a minimal, self-contained, and reproducible example.**
```python
from copy import deepcopy

import numpy as np
import pymc as pm

RANDOM_SEED = 8927
rng = np.random.default_rng(RANDOM_SEED)

# True parameter values
alpha, sigma = 1, 1
beta = [1, 2.5]

# Size of dataset
size = 100

# Predictor variable
X1 = np.random.randn(size)
X2 = np.random.randn(size) * 0.2

# Simulate outcome variable
Y = alpha + beta[0] * X1 + beta[1] * X2 + rng.normal(size=size) * sigma

basic_model = pm.Model()

with basic_model:

    # Priors for unknown model parameters
    alpha = pm.Normal(""alpha"", mu=0, sigma=10)
    beta = pm.Normal(""beta"", mu=0, sigma=10, shape=2)
    sigma = pm.HalfNormal(""sigma"", sigma=1)

    # Expected value of outcome
    mu = alpha + beta[0] * X1 + beta[1] * X2

    # Likelihood (sampling distribution) of observations
    Y_obs = pm.Normal(""Y_obs"", mu=mu, sigma=sigma, observed=Y)

model_copy = deepcopy(basic_model)
```

**Please provide the full traceback of any errors.**
```python
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
Input In [9], in <cell line: 41>()
     38     # Likelihood (sampling distribution) of observations
     39     Y_obs = pm.Normal(""Y_obs"", mu=mu, sigma=sigma, observed=Y)
---> 41 model_copy = deepcopy(basic_model)

File ~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\copy.py:172, in deepcopy(x, memo, _nil)
    170                 y = x
    171             else:
--> 172                 y = _reconstruct(x, memo, *rv)
    174 # If is its own copy, don't memoize.
    175 if y is not x:

File ~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\copy.py:270, in _reconstruct(x, memo, func, args, state, listiter, dictiter, deepcopy)
    268 if state is not None:
    269     if deep:
--> 270         state = deepcopy(state, memo)
    271     if hasattr(y, '__setstate__'):
    272         y.__setstate__(state)

File ~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\copy.py:146, in deepcopy(x, memo, _nil)
    144 copier = _deepcopy_dispatch.get(cls)
    145 if copier is not None:
--> 146     y = copier(x, memo)
    147 else:
    148     if issubclass(cls, type):

File ~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\copy.py:230, in _deepcopy_dict(x, memo, deepcopy)
    228 memo[id(x)] = y
    229 for key, value in x.items():
--> 230     y[deepcopy(key, memo)] = deepcopy(value, memo)
    231 return y

File ~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\copy.py:146, in deepcopy(x, memo, _nil)
    144 copier = _deepcopy_dispatch.get(cls)
    145 if copier is not None:
--> 146     y = copier(x, memo)
    147 else:
    148     if issubclass(cls, type):

File ~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\copy.py:230, in _deepcopy_dict(x, memo, deepcopy)
    228 memo[id(x)] = y
    229 for key, value in x.items():
--> 230     y[deepcopy(key, memo)] = deepcopy(value, memo)
    231 return y

File ~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\copy.py:172, in deepcopy(x, memo, _nil)
    170                 y = x
    171             else:
--> 172                 y = _reconstruct(x, memo, *rv)
    174 # If is its own copy, don't memoize.
    175 if y is not x:

File ~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\copy.py:270, in _reconstruct(x, memo, func, args, state, listiter, dictiter, deepcopy)
    268 if state is not None:
    269     if deep:
--> 270         state = deepcopy(state, memo)
    271     if hasattr(y, '__setstate__'):
    272         y.__setstate__(state)

    [... skipping similar frames: _deepcopy_dict at line 230 (1 times), deepcopy at line 146 (1 times)]

File ~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\copy.py:172, in deepcopy(x, memo, _nil)
    170                 y = x
    171             else:
--> 172                 y = _reconstruct(x, memo, *rv)
    174 # If is its own copy, don't memoize.
    175 if y is not x:

File ~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\copy.py:270, in _reconstruct(x, memo, func, args, state, listiter, dictiter, deepcopy)
    268 if state is not None:
    269     if deep:
--> 270         state = deepcopy(state, memo)
    271     if hasattr(y, '__setstate__'):
    272         y.__setstate__(state)

    [... skipping similar frames: deepcopy at line 146 (1 times)]

File ~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\copy.py:230, in _deepcopy_dict(x, memo, deepcopy)
    228 memo[id(x)] = y
    229 for key, value in x.items():
--> 230     y[deepcopy(key, memo)] = deepcopy(value, memo)
    231 return y

File ~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\copy.py:146, in deepcopy(x, memo, _nil)
    144 copier = _deepcopy_dispatch.get(cls)
    145 if copier is not None:
--> 146     y = copier(x, memo)
    147 else:
    148     if issubclass(cls, type):

File ~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\copy.py:205, in _deepcopy_list(x, memo, deepcopy)
    203 append = y.append
    204 for a in x:
--> 205     append(deepcopy(a, memo))
    206 return y

File ~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\copy.py:172, in deepcopy(x, memo, _nil)
    170                 y = x
    171             else:
--> 172                 y = _reconstruct(x, memo, *rv)
    174 # If is its own copy, don't memoize.
    175 if y is not x:

File ~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\copy.py:270, in _reconstruct(x, memo, func, args, state, listiter, dictiter, deepcopy)
    268 if state is not None:
    269     if deep:
--> 270         state = deepcopy(state, memo)
    271     if hasattr(y, '__setstate__'):
    272         y.__setstate__(state)

File ~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\copy.py:146, in deepcopy(x, memo, _nil)
    144 copier = _deepcopy_dispatch.get(cls)
    145 if copier is not None:
--> 146     y = copier(x, memo)
    147 else:
    148     if issubclass(cls, type):

File ~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\copy.py:230, in _deepcopy_dict(x, memo, deepcopy)
    228 memo[id(x)] = y
    229 for key, value in x.items():
--> 230     y[deepcopy(key, memo)] = deepcopy(value, memo)
    231 return y

File ~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\copy.py:153, in deepcopy(x, memo, _nil)
    151 copier = getattr(x, ""__deepcopy__"", None)
    152 if copier is not None:
--> 153     y = copier(memo)
    154 else:
    155     reductor = dispatch_table.get(cls)

File ~\\.virtualenvs\\pymc4-venv\\lib\\site-packages\\aesara\\link\\basic.py:142, in Container.__deepcopy__(self, memo)
    132 r = type(self)(
    133     deepcopy(self.type, memo=memo),
    134     deepcopy(self.storage, memo=memo),
   (...)
    138     name=deepcopy(self.name, memo=memo),
    139 )
    140 # Work around NumPy deepcopy of ndarray with 0 dimension that
    141 # don't return an ndarray.
--> 142 if r.storage[0] is not None and not self.type.is_valid_value(r.storage[0]):
    143     assert not data_was_in_memo
    144     assert self.type.is_valid_value(self.storage[0])

TypeError: is_valid_value() missing 1 required positional argument: 'strict'
```

This used to work on pymc3 and theano-pymc


## Versions and main components

* Aesara version: 2.66
* Aesara config (`python -c ""import aesara; print(aesara.config)""`)
* Python version: 3.9
* Operating system:  Windows
* How did you install Aesara: pip
* Pymc version: 4.0
",bug MWE needed,,2022-06-13 17:14:35,2022-06-14 21:01:16,"brandonwillard labeled 2022-06-14 09:13:17,brandonwillard labeled 2022-06-14 09:13:17,brandonwillard mentioned 2022-06-14 15:21:57,brandonwillard subscribed 2022-06-14 15:21:57,brandonwillard mentioned 2022-06-14 15:47:00,brandonwillard subscribed 2022-06-14 15:47:00,brandonwillard unlabeled 2022-06-14 17:58:26,brandonwillard labeled 2022-06-14 17:58:26,brandonwillard closed 2022-06-14 21:01:16,brandonwillard mentioned 2022-06-14 21:11:07,brandonwillard subscribed 2022-06-14 21:11:07",brandonwillard hectormz,5
785,999,Bad input argument using simple function,adrianhelmlingcornell,"## Description of your problem or feature request

I get the following error when trying to pretty-print my first aesara functions:

**Please provide a minimal, self-contained, and reproducible example.**
```python
import aesara
import aesara.tensor as at
import numpy as np
from scipy.special import gamma

c1 = 1
c2 = gamma(5)  # Euler Gamma function
z = at.dscalar('z')

f1 = aesara.function([z], c1 * z)
f2 = aesara.function([z], c2 * z)

print(aesara.pp(f1(z)), aesara.pp(f2(z)))
```

**Please provide the full traceback of any errors.**
```python
  File ""PATH_TO_SCRIPT"", line 30, in <module>
    print(aesara.pp(varphi_t_func(z)), aesara.pp(varphi_r_func(z)))
  File ""/home/adrian/anaconda3/envs/py39/lib/python3.10/site-packages/aesara/compile/function/types.py"", line 863, in __call__
    s.storage[0] = s.type.filter(
  File ""/home/adrian/anaconda3/envs/py39/lib/python3.10/site-packages/aesara/tensor/type.py"", line 134, in filter
    raise TypeError(
TypeError: Bad input argument with name ""z"" to aesara function with name ""PATH_TO_SCRIPT:27"" at index 0 (0-based).  
Backtrace when that variable is created:

trace 0
  File ""PATH_TO_SCRIPT"", line 22, in <module>
    z = at.dscalar('z')
trace 1
  File ""PATH_TO_SCRIPT"", line 24, in <module>
    f1 = c1 * z  # actual combination of numeric variables
Expected an array-like object, but found a Variable: maybe you are trying to call a function on a (possibly shared) variable instead of a numeric array?
```

**Please provide any additional information below.**


## Versions and main components

* Aesara version: 2.7.2
* Aesara config: See here: [aesara_config.txt](https://github.com/aesara-devs/aesara/files/8915874/aesara_config.txt)
* Python version: 3.10.5
* Operating system: Ubuntu 18l.04
* How did you install Aesara: conda
",,,2022-06-16 05:15:59,2022-06-16 05:34:27,"aesara-devs locked 2022-06-16 05:34:27,ricardoV94 converted_to_discussion 2022-06-16 05:34:27",ricardoV94 aesara-devs adrianhelmlingcornell,1
786,1001,Remove JAX omnistaging warning,ricardoV94,"JAX version 0.2.12 was released in April 2021

```
UserWarning: JAX omnistaging couldn't be disabled: Disabling of omnistaging is no longer supported in JAX version 0.2.12 and higher: see https://github.com/google/jax/blob/main/design_notes/omnistaging.md.
  warnings.warn(f""JAX omnistaging couldn't be disabled: {e}
```",JAX request discussion,,2022-06-16 09:52:19,2022-09-15 17:01:50,"ricardoV94 labeled 2022-06-16 09:52:19,ricardoV94 labeled 2022-06-16 09:52:19,brandonwillard closed 2022-09-15 17:01:50",rlouf ricardoV94 brandonwillard,2
788,1005,Numpy warnings should be filtered more precisely,maresb,"Numpy emits some false-positive warnings which we should filter. In #980 we filter messages which were previously being missed. But while examining the code, we noticed that the current filtering may be too aggressive in some places.

[Here](https://github.com/aesara-devs/aesara/blob/d6858fe23af29cb28379723175895c9d02fdca51/aesara/link/c/cmodule.py#L2722) we do

```python
numpy.distutils.system_info.system_info.verbosity = 0
```

but we don't set it back to the original value. This could lead to side-effects.

Also, the previous line

```python
with warnings.catch_warnings(record=True):
```

fails to capture certain warnings (hence the need for #980), and it's not clear to me which messages, if any, it is actually filtering out.

It would be nice to

1. understand/document/test the function of these two lines
2. ensure that these lines don't filter out true-positive warnings, or cause side-effects in Aesara or downstream.",enhancement question C-backend Windows,,2022-06-19 10:23:10,2022-08-19 19:32:56,"brandonwillard labeled 2022-06-20 19:15:08,brandonwillard labeled 2022-06-20 19:15:08,brandonwillard labeled 2022-06-20 19:15:14,brandonwillard connected 2022-06-20 19:15:33,brandonwillard labeled 2022-08-19 17:51:47,maresb closed 2022-08-19 19:32:56",maresb brandonwillard,2
789,1006,Aesara is not threadsafe,ferrine,"## Aesara is not threadsafe

**Please provide a minimal, self-contained, and reproducible example.**
```python
import threading
import aesara
import numpy as np
import aesara.tensor as at
import aesara.compile.compiledir
aesara.compile.compiledir.cleanup()
np.random.seed(32)
def run_aesara(i, out):
    s = np.random.randint(10, 50)
    a = at.random.normal(size=(s, s))
    fn = aesara.function([],a)
    for _ in range(10):
        fn()
    out[i] = True
    
T = 32
out = [None] * T

threads = [threading.Thread(target=run_aesara, args=(i, out)) for i in range(T)]
[t.start() for t in threads]
[t.join() for t in threads]
assert all(out)
```

**Please provide the full traceback of any errors.**
```python
ERROR (aesara.graph.opt): Optimization failure due to: constant_folding
ERROR (aesara.graph.opt): node: Subtensor{int64}(TensorConstant{(2,) of 36}, ScalarConstant{1})
ERROR (aesara.graph.opt): TRACEBACK:
ERROR (aesara.graph.opt): Traceback (most recent call last):
  File ""/home/ferres/.miniconda3/envs/pymc/lib/python3.9/site-packages/aesara/graph/opt.py"", line 1850, in process_node
    replacements = lopt.transform(fgraph, node)
  File ""/home/ferres/.miniconda3/envs/pymc/lib/python3.9/site-packages/aesara/graph/opt.py"", line 1055, in transform
    return self.fn(fgraph, node)
  File ""/home/ferres/.miniconda3/envs/pymc/lib/python3.9/site-packages/aesara/tensor/basic_opt.py"", line 2943, in constant_folding
    thunk = node.op.make_thunk(node, storage_map, compute_map, no_recycling=[])
  File ""/home/ferres/.miniconda3/envs/pymc/lib/python3.9/site-packages/aesara/link/c/op.py"", line 131, in make_thunk
    return self.make_c_thunk(node, storage_map, compute_map, no_recycling)
  File ""/home/ferres/.miniconda3/envs/pymc/lib/python3.9/site-packages/aesara/link/c/op.py"", line 96, in make_c_thunk
    outputs = cl.make_thunk(
  File ""/home/ferres/.miniconda3/envs/pymc/lib/python3.9/site-packages/aesara/link/c/basic.py"", line 1192, in make_thunk
    cthunk, module, in_storage, out_storage, error_storage = self.__compile__(
  File ""/home/ferres/.miniconda3/envs/pymc/lib/python3.9/site-packages/aesara/link/c/basic.py"", line 1127, in __compile__
    thunk, module = self.cthunk_factory(
  File ""/home/ferres/.miniconda3/envs/pymc/lib/python3.9/site-packages/aesara/link/c/basic.py"", line 1623, in cthunk_factory
    module = get_module_cache().module_from_key(key=key, lnk=self)
  File ""/home/ferres/.miniconda3/envs/pymc/lib/python3.9/site-packages/aesara/link/c/cmodule.py"", line 1183, in module_from_key
    module = self._get_from_hash(module_hash, key)
  File ""/home/ferres/.miniconda3/envs/pymc/lib/python3.9/site-packages/aesara/link/c/cmodule.py"", line 1086, in _get_from_hash
    key_data.add_key(key, save_pkl=bool(key[0]))
  File ""/home/ferres/.miniconda3/envs/pymc/lib/python3.9/site-packages/aesara/link/c/cmodule.py"", line 526, in add_key
    assert key not in self.keys
AssertionError
```

**Please provide any additional information below.**


## Versions and main components

* Aesara version: '2.6.6'
",,,2022-06-21 14:40:44,2022-06-21 16:09:57,"aesara-devs locked 2022-06-21 16:09:56,brandonwillard converted_to_discussion 2022-06-21 16:09:57",aesara-devs brandonwillard fonnesbeck ricardoV94 ferrine,3
792,1014,Target JAX's intermediate representation with the JAX linker,rlouf,"The JAX linker currently target the library's numpy-like high level API. For instance, the `Dot` Op is translated using `jax.numpy.dot`:

```python
@jax_funcify.register(Dot)
def jax_funcify_Dot(op, **kwargs):
    def dot(x, y):
        return jnp.dot(x, y)

    return dot
```

However JAX is a symbolic library (albeit a limited one) and has its own intermediate representation. When the user calls a function written with `jax.numpy` primitive for the first time, JAX traces the function and converts it to a [Jaxpr](https://jax.readthedocs.io/en/latest/jaxpr.html) that is then processed by XLA. Therefore, when one transpiles their Aeasara code to JAX and runs the resulting code is traced. This is completely unnecessary since all the information needed to build JAX's intermediate representation is already contained in the Aesara graph.

We could therefore, in theory, translate Aesara's Ops directly to JAX's intermediate representation. We would not only improve runtime performance (*gain to be estimated*), but also have more freedom for the transpilation since we won't be limited to JAX's high level API.

## Proof of concept

Before opening a PR, I will try in the comments of this issue to translate the following Aesara graph:

``` python
import aesara
import aesara.tensor as at

a = at.vector()
b = at.vector()

c = a + b

aesara.dprint(c)
# : Elemwise{add,no_inplace} [id A]
# :  |<TensorType(float64, (None,))> [id B]
# :  |<TensorType(float64, (None,))> [id C]
```

To its JAX equivalent:

```python
import jax.numpy as np
from jax import lax
from jax import make_jaxpr

def add_fn(a, b):
    return lax.add(a, b)

print(make_jaxpr(add_fn)(np.array([1., 1.]), np.array([1., 1.])))
# { lambda ; a:f32[2] b:f32[2]. let c:f32[2] = add a b in (c,) }
```

This example is simple, but raises the question of how types and shapes are handled in JAX's IR. In particular, I am currently not sure that JAX can handle arrays of unknown (but fixed) length. If it cannot we can imagine a ""delayed transpilation"" where Aesara would generate JAX's IR when the function is called with arguments.",enhancement JAX important refactor,rlouf,2022-06-24 15:06:48,2022-09-14 21:02:07,"rlouf assigned 2022-06-24 15:07:29,rlouf labeled 2022-06-24 15:07:55,brandonwillard labeled 2022-06-24 16:35:16,brandonwillard labeled 2022-06-24 16:35:16,brandonwillard labeled 2022-06-24 16:35:16,aesara-devs locked 2022-09-14 21:02:07,rlouf converted_to_discussion 2022-09-14 21:02:07",ricardoV94 rlouf aesara-devs brandonwillard,7
794,1017,`ifelse` does not work with arbitrary data structures,rlouf,"## What I expect

I expect `ifelse` to work when I pass two arbitrarily nested python data structures that have identical structures, and whose leaves are `TensorVariables` of identical shape and dtype.

This would greatly facilitate the use of `aesara` for ""complex"" projects such as `aehmc`.

## What I observe

### It works with a tuple of `TensorVariable`

An `IfElse` Op can be created passing a tuple or a list of `TensorVariable`s for each branch of the condition:

```python
import aesara.tensor as at
from aesara.ifelse import ifelse

cond = at.as_tensor(0, dtype=bool)

q_left = at.vector()
energy_left = at.scalar()
energy_grad_left = at.vector()
state_left = (q_left, energy_left, energy_grad_left)

q_right = at.vector()
energy_right = at.scalar()
energy_grad_right = at.vector()
state_right = (q_right, energy_right, energy_grad_right)

new_state = ifelse(cond, state_left, state_right)
```

### It does not work with nested tuples

However, it does not work with nested structures. Here is a simplified version of something I have encountered [in aehmc](https://github.com/aesara-devs/aehmc/blob/ede3f376d07c1f1858b8b55fea4a937207913535/aehmc/trajectory.py#L480):

```python
weight_left = at.scalar()
left = (state_left, weight_left)

weight_right = at.scalar()
right = (state_right, weight_right)

new = ifelse(cond, left, right)
```

Which returns a `TypeError`.

### It does not work with dictionaries

In the same way, it will not work with dictionaries as input, the following also returns a `TypeError`:

```python
import aesara.tensor as at
from aesara.tensor.var import TensorVariable
from aesara.ifelse import ifelse

state_left = {""q"": at.vector(), ""energy"": at.scalar(), ""energy_grad"": at.vector()}
state_right = {""q"": at.vector(), ""energy"": at.scalar(), ""energy_grad"": at.vector()}

cond = at.as_tensor(0, dtype=bool)
state = ifelse(cond, state_left, state_right)
```

### It does not work with namedtuples

`IfElse` returns a tuple when `NamedTuple`s are passed an input, causing errors downstream.

```python
from typing import NamedTuple

import aesara.tensor as at
from aesara.tensor.var import TensorVariable
from aesara.ifelse import ifelse


class State(NamedTuple):
    q: TensorVariable
    energy: TensorVariable
    energy_grad: TensorVariable

state_left = State(at.vector(), at.scalar(), at.vector())
state_right = State(at.vector(), at.scalar(), at.vector())

cond = at.as_tensor(0, dtype=bool)
state = ifelse(cond, state_left, state_right)
state.q
```",question,,2022-06-27 10:39:18,2022-06-29 05:35:50,"rlouf closed 2022-06-27 10:47:21,rlouf renamed 2022-06-27 14:25:29,rlouf renamed 2022-06-27 14:25:46,rlouf renamed 2022-06-27 15:00:14,rlouf renamed 2022-06-27 15:01:24,rlouf reopened 2022-06-27 15:01:41,rlouf renamed 2022-06-27 15:01:52,brandonwillard labeled 2022-06-29 05:28:35,aesara-devs locked 2022-06-29 05:35:50,brandonwillard converted_to_discussion 2022-06-29 05:35:50",rlouf aesara-devs brandonwillard,1
797,1021,Test value warning when `compute_test_value` is off,mattearllongshot,"## Description of your problem or feature request

```python
import logging

import aesara
import aesara.tensor as at
import numpy as np

a = at.vector()
b = aesara.shared(np.zeros((0,)))

f = aesara.function([a], a + (a * b))

b.set_value(np.array([1,]))
print(f(np.array([3,])))
```

When running I get this warning:

```
WARNING (aesara.tensor.basic_opt): Cannot construct a scalar test value from a test value with no size: <TensorType(float64, (None,))>
```

However, computing test values is disabled:

```bash
$ python -c 'import aesara; print(aesara.config)' | grep -A3 '^compute_test_value\\>'
compute_test_value ({'ignore', 'warn', 'off', 'raise', 'pdb'})
    Doc:  If 'True', Aesara will run each op at graph build time, using Constants, SharedVariables and the tag 'test_value' as inputs to the function. This helps the user track down problems in the graph before it gets optimized.
    Value:  off
```

## Versions and main components

* Aesara version: 2.5.1
* Aesara config (`python -c ""import aesara; print(aesara.config)""`) 
[aesara_config.txt](https://github.com/aesara-devs/aesara/files/9019091/aesara_config.txt)

* Python version: Python 3.8.13
* Operating system: Ubuntu 18.04.6 LTS
* How did you install Aesara: conda
",bug help wanted graph rewriting,,2022-06-30 10:52:39,2022-07-04 00:58:55,"brandonwillard labeled 2022-06-30 23:35:56,brandonwillard labeled 2022-06-30 23:35:56,brandonwillard labeled 2022-06-30 23:36:09,brandonwillard closed 2022-07-04 00:58:55",brandonwillard mattearllongshot,3
798,1022,Missing numba implementation of expit,aseyboldt,"## Description of your problem or feature request

In numba mode, we can't compile functions that contain sigmoid operations:

```python
import aesara.tensor as at
from scipy import special
import numba

x = at.dvector(""x"")
y = at.sigmoid(x)
func = aesara.function([x], y, mode=""NUMBA"")  # Fails in numba compilation

# Because this isn't implemented
@numba.njit
def foo(x):
    return special.expit(x)

foo(0.1)
```

## Versions and main components

* Aesara version d09e222b06 (current main)",help wanted Numba,,2022-06-30 14:30:05,2022-08-20 15:49:09,"brandonwillard labeled 2022-06-30 20:38:38,brandonwillard labeled 2022-06-30 20:38:38,aseyboldt mentioned 2022-08-20 12:09:20,aseyboldt subscribed 2022-08-20 12:09:20,aseyboldt closed 2022-08-20 15:49:09",ricardoV94 aseyboldt brandonwillard,2
799,1023,Segfault in numba mode on M1,twiecki,"## Running below code causes the kernel to die on M1, works fine on Intel.

**Please provide a minimal, self-contained, and reproducible example.**
```python
import aesara.tensor as at
import numba

var = at.dscalar(""a"")
logp = -var ** 2

joined = at.dvector(""joined_variables"")

grads = at.grad(logp, [var])
grad = at.concatenate([grad.ravel() for grad in grads])

func = aesara.function(
    (joined,), (logp, grad), givens=[(var, joined[:].reshape(()))], mode=aesara.compile.NUMBA
)
func(np.zeros(1))
```

**Please provide the full traceback of any errors.**
```python
Segfault.
```

**Please provide any additional information below.**


## Versions and main components

* Aesara version: 2.7.2
* Aesara config (`python -c ""import aesara; print(aesara.config)""`)

<details>

```
floatX ({'float16', 'float32', 'float64'}) 
    Doc:  Default floating-point precision for python casts.

Note: float16 support is experimental, use at your own risk.
    Value:  float64

warn_float64 ({'warn', 'ignore', 'pdb', 'raise'}) 
    Doc:  Do an action when a tensor variable with float64 dtype is created.
    Value:  ignore

pickle_test_value (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x11a93ad40>>) 
    Doc:  Dump test values while pickling model. If True, test values will be dumped with model.
    Value:  True

cast_policy ({'numpy+floatX', 'custom'}) 
    Doc:  Rules for implicit type casting
    Value:  custom

deterministic ({'default', 'more'}) 
    Doc:  If `more`, sometimes we will select some implementation that are more deterministic, but slower.  Also see the dnn.conv.algo* flags to cover more cases.
    Value:  default

device (cpu)
    Doc:  Default device for computations. only cpu is supported for now
    Value:  cpu

force_device (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x11cd086d0>>) 
    Doc:  Raise an error if we can't use the specified device
    Value:  False

conv__assert_shape (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x11cd08730>>) 
    Doc:  If True, AbstractConv* ops will verify that user-provided shapes match the runtime shapes (debugging option, may slow down compilation)
    Value:  False

print_global_stats (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x11cd08760>>) 
    Doc:  Print some global statistics (time spent) at the end
    Value:  False

assert_no_cpu_op ({'warn', 'ignore', 'pdb', 'raise'}) 
    Doc:  Raise an error/warning if there is a CPU op in the computational graph.
    Value:  ignore

unpickle_function (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x11cd0a080>>) 
    Doc:  Replace unpickled Aesara functions with None. This is useful to unpickle old graphs that pickled them when it shouldn't
    Value:  True

<aesara.configparser.ConfigParam object at 0x11cd0a0b0>
    Doc:  Default compilation mode
    Value:  Mode

cxx (<class 'str'>) 
    Doc:  The C++ compiler to use. Currently only g++ is supported, but supporting additional compilers should not be too difficult. If it is empty, no C++ code is compiled.
    Value:  /usr/bin/clang++

linker ({'c|py', 'vm', 'c|py_nogc', 'vm_nogc', 'cvm_nogc', 'c', 'cvm', 'py'}) 
    Doc:  Default linker used if the aesara flags mode is Mode
    Value:  cvm

allow_gc (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x11cd0a020>>) 
    Doc:  Do we default to delete intermediate results during Aesara function calls? Doing so lowers the memory requirement, but asks that we reallocate memory at the next function call. This is implemented for the default linker, but may not work for all linkers.
    Value:  True

optimizer ({'o1', 'None', 'fast_compile', 'o3', 'unsafe', 'o4', 'merge', 'o2', 'fast_run'}) 
    Doc:  Default optimizer. If not None, will use this optimizer with the Mode
    Value:  o4

optimizer_verbose (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x11cd0a1a0>>) 
    Doc:  If True, we print all optimization being applied
    Value:  False

on_opt_error ({'warn', 'ignore', 'pdb', 'raise'}) 
    Doc:  What to do when an optimization crashes: warn and skip it, raise the exception, or fall into the pdb debugger.
    Value:  warn

nocleanup (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x11cd0a2c0>>) 
    Doc:  Suppress the deletion of code files that did not compile cleanly
    Value:  False

on_unused_input ({'raise', 'ignore', 'warn'}) 
    Doc:  What to do if a variable in the 'inputs' list of  aesara.function() is not used in the graph.
    Value:  raise

gcc__cxxflags (<class 'str'>) 
    Doc:  Extra compiler flags for gcc
    Value:   -Wno-c++11-narrowing -fno-exceptions -fno-unwind-tables -fno-asynchronous-unwind-tables

cmodule__warn_no_version (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x11cd0a200>>) 
    Doc:  If True, will print a warning when compiling one or more Op with C code that can't be cached because there is no c_code_cache_version() function associated to at least one of those Ops.
    Value:  False

cmodule__remove_gxx_opt (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x11cd0a320>>) 
    Doc:  If True, will remove the -O* parameter passed to g++.This is useful to debug in gdb modules compiled by Aesara.The parameter -g is passed by default to g++
    Value:  False

cmodule__compilation_warning (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x11cd0a3b0>>) 
    Doc:  If True, will print compilation warnings.
    Value:  False

cmodule__preload_cache (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x11cd0a3e0>>) 
    Doc:  If set to True, will preload the C module cache at import time
    Value:  False

cmodule__age_thresh_use (<class 'int'>) 
    Doc:  In seconds. The time after which Aesara won't reuse a compile c module.
    Value:  2073600

cmodule__debug (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x11cd0a440>>) 
    Doc:  If True, define a DEBUG macro (if not exists) for any compiled C code.
    Value:  False

compile__wait (<class 'int'>) 
    Doc:  Time to wait before retrying to acquire the compile lock.
    Value:  5

compile__timeout (<class 'int'>) 
    Doc:  In seconds, time that a process will wait before deciding to
    override an existing lock. An override only happens when the existing
    lock is held by the same owner *and* has not been 'refreshed' by this
    owner for more than this period. Refreshes are done every half timeout
    period for running processes.
    Value:  120

ctc__root (<class 'str'>) 
    Doc:  Directory which contains the root of Baidu CTC library. It is assumed         that the compiled library is either inside the build, lib or lib64         subdirectory, and the header inside the include directory.
    Value:  

tensor__cmp_sloppy (<class 'int'>) 
    Doc:  Relax aesara.tensor.math._allclose (0) not at all, (1) a bit, (2) more
    Value:  0

tensor__local_elemwise_fusion (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x11cd0a5f0>>) 
    Doc:  Enable or not in fast_run mode(fast_run optimization) the elemwise fusion optimization
    Value:  True

lib__amblibm (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x11cd0a680>>) 
    Doc:  Use amd's amdlibm numerical library
    Value:  False

tensor__insert_inplace_optimizer_validate_nb (<class 'int'>) 
    Doc:  -1: auto, if graph have less then 500 nodes 1, else 10
    Value:  -1

traceback__limit (<class 'int'>) 
    Doc:  The number of stack to trace. -1 mean all.
    Value:  8

traceback__compile_limit (<class 'int'>) 
    Doc:  The number of stack to trace to keep during compilation. -1 mean all. If greater then 0, will also make us save Aesara internal stack trace.
    Value:  0

experimental__local_alloc_elemwise (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x11cd0a800>>) 
    Doc:  DEPRECATED: If True, enable the experimental optimization local_alloc_elemwise. Generates error if not True. Use optimizer_excluding=local_alloc_elemwise to disable.
    Value:  True

experimental__local_alloc_elemwise_assert (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x11cd0a830>>) 
    Doc:  When the local_alloc_elemwise is applied, add an assert to highlight shape errors.
    Value:  True

warn__ignore_bug_before ({'0.8.1', '0.3', '0.5', '1.0.1', '1.0', '1.0.5', '0.8.2', 'all', '1.0.4', '0.8', '0.4', '1.0.3', '1.0.2', '0.6', '0.7', '0.4.1', 'None', '0.10', '0.9'}) 
    Doc:  If 'None', we warn about all Aesara bugs found by default. If 'all', we don't warn about Aesara bugs found by default. If a version, we print only the warnings relative to Aesara bugs found after that version. Warning for specific bugs can be configured with specific [warn] flags.
    Value:  0.9

exception_verbosity ({'high', 'low'}) 
    Doc:  If 'low', the text of exceptions will generally refer to apply nodes with short names such as Elemwise{add_no_inplace}. If 'high', some exceptions will also refer to apply nodes with long descriptions  like:
        A. Elemwise{add_no_inplace}
                B. log_likelihood_v_given_h
                C. log_likelihood_h
    Value:  low

print_test_value (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x11cd0a980>>) 
    Doc:  If 'True', the __eval__ of an Aesara variable will return its test_value when this is available. This has the practical conseguence that, e.g., in debugging `my_var` will print the same as `my_var.tag.test_value` when a test value is defined.
    Value:  False

compute_test_value ({'raise', 'off', 'warn', 'ignore', 'pdb'}) 
    Doc:  If 'True', Aesara will run each op at graph build time, using Constants, SharedVariables and the tag 'test_value' as inputs to the function. This helps the user track down problems in the graph before it gets optimized.
    Value:  off

compute_test_value_opt ({'raise', 'off', 'warn', 'ignore', 'pdb'}) 
    Doc:  For debugging Aesara optimization only. Same as compute_test_value, but is used during Aesara optimization
    Value:  off

check_input (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x11cd0aa10>>) 
    Doc:  Specify if types should check their input in their C code. It can be used to speed up compilation, reduce overhead (particularly for scalars) and reduce the number of generated C files.
    Value:  True

NanGuardMode__nan_is_error (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x11cd0aa40>>) 
    Doc:  Default value for nan_is_error
    Value:  True

NanGuardMode__inf_is_error (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x11cd0aa70>>) 
    Doc:  Default value for inf_is_error
    Value:  True

NanGuardMode__big_is_error (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x11cd0ab00>>) 
    Doc:  Default value for big_is_error
    Value:  True

NanGuardMode__action ({'raise', 'warn', 'pdb'}) 
    Doc:  What NanGuardMode does when it finds a problem
    Value:  raise

DebugMode__patience (<class 'int'>) 
    Doc:  Optimize graph this many times to detect inconsistency
    Value:  10

DebugMode__check_c (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x11cd0ab90>>) 
    Doc:  Run C implementations where possible
    Value:  True

DebugMode__check_py (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x11cd0ac20>>) 
    Doc:  Run Python implementations where possible
    Value:  True

DebugMode__check_finite (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x11cd0ac50>>) 
    Doc:  True -> complain about NaN/Inf results
    Value:  True

DebugMode__check_strides (<class 'int'>) 
    Doc:  Check that Python- and C-produced ndarrays have same strides. On difference: (0) - ignore, (1) warn, or (2) raise error
    Value:  0

DebugMode__warn_input_not_reused (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x11cd0acb0>>) 
    Doc:  Generate a warning when destroy_map or view_map says that an op works inplace, but the op did not reuse the input for its output.
    Value:  True

DebugMode__check_preallocated_output (<class 'str'>) 
    Doc:  Test thunks with pre-allocated memory as output storage. This is a list of strings separated by "":"". Valid values are: ""initial"" (initial storage in storage map, happens with Scan),""previous"" (previously-returned memory), ""c_contiguous"", ""f_contiguous"", ""strided"" (positive and negative strides), ""wrong_size"" (larger and smaller dimensions), and ""ALL"" (all of the above).
    Value:  

DebugMode__check_preallocated_output_ndim (<class 'int'>) 
    Doc:  When testing with ""strided"" preallocated output memory, test all combinations of strides over that number of (inner-most) dimensions. You may want to reduce that number to reduce memory or time usage, but it is advised to keep a minimum of 2.
    Value:  4

profiling__time_thunks (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x11cd0ad40>>) 
    Doc:  Time individual thunks when profiling
    Value:  True

profiling__n_apply (<class 'int'>) 
    Doc:  Number of Apply instances to print by default
    Value:  20

profiling__n_ops (<class 'int'>) 
    Doc:  Number of Ops to print by default
    Value:  20

profiling__output_line_width (<class 'int'>) 
    Doc:  Max line width for the profiling output
    Value:  512

profiling__min_memory_size (<class 'int'>) 
    Doc:  For the memory profile, do not print Apply nodes if the size
                 of their outputs (in bytes) is lower than this threshold
    Value:  1024

profiling__min_peak_memory (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x11cd0ae90>>) 
    Doc:  The min peak memory usage of the order
    Value:  False

profiling__destination (<class 'str'>) 
    Doc:  File destination of the profiling output
    Value:  stderr

profiling__debugprint (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x11cd0aef0>>) 
    Doc:  Do a debugprint of the profiled functions
    Value:  False

profiling__ignore_first_call (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x11cd0af20>>) 
    Doc:  Do we ignore the first call of an Aesara function.
    Value:  False

on_shape_error ({'warn', 'raise'}) 
    Doc:  warn: print a warning and use the default value. raise: raise an error
    Value:  warn

openmp (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x11cd0af80>>) 
    Doc:  Allow (or not) parallel computation on the CPU with OpenMP. This is the default value used when creating an Op that supports OpenMP parallelization. It is preferable to define it via the Aesara configuration file ~/.aesararc or with the environment variable AESARA_FLAGS. Parallelization is only done for some operations that implement it, and even for operations that implement parallelism, each operation is free to respect this flag or not. You can control the number of threads used with the environment variable OMP_NUM_THREADS. If it is set to 1, we disable openmp in Aesara by default.
    Value:  False

openmp_elemwise_minsize (<class 'int'>) 
    Doc:  If OpenMP is enabled, this is the minimum size of vectors for which the openmp parallelization is enabled in element wise ops.
    Value:  200000

optimizer_excluding (<class 'str'>) 
    Doc:  When using the default mode, we will remove optimizer with these tags. Separate tags with ':'.
    Value:  

optimizer_including (<class 'str'>) 
    Doc:  When using the default mode, we will add optimizer with these tags. Separate tags with ':'.
    Value:  

optimizer_requiring (<class 'str'>) 
    Doc:  When using the default mode, we will require optimizer with these tags. Separate tags with ':'.
    Value:  

optdb__position_cutoff (<class 'float'>) 
    Doc:  Where to stop eariler during optimization. It represent the position of the optimizer where to stop.
    Value:  inf

optdb__max_use_ratio (<class 'float'>) 
    Doc:  A ratio that prevent infinite loop in EquilibriumOptimizer.
    Value:  8.0

cycle_detection ({'regular', 'fast'}) 
    Doc:  If cycle_detection is set to regular, most inplaces are allowed,but it is slower. If cycle_detection is set to faster, less inplacesare allowed, but it makes the compilation faster.The interaction of which one give the lower peak memory usage iscomplicated and not predictable, so if you are close to the peakmemory usage, triyng both could give you a small gain.
    Value:  regular

check_stack_trace ({'log', 'raise', 'warn', 'off'}) 
    Doc:  A flag for checking the stack trace during the optimization process. default (off): does not check the stack trace of any optimization log: inserts a dummy stack trace that identifies the optimizationthat inserted the variable that had an empty stack trace.warn: prints a warning if a stack trace is missing and also a dummystack trace is inserted that indicates which optimization insertedthe variable that had an empty stack trace.raise: raises an exception if a stack trace is missing
    Value:  off

metaopt__verbose (<class 'int'>) 
    Doc:  0 for silent, 1 for only warnings, 2 for full output withtimings and selected implementation
    Value:  0

metaopt__optimizer_excluding (<class 'str'>) 
    Doc:  exclude optimizers with these tags. Separate tags with ':'.
    Value:  

metaopt__optimizer_including (<class 'str'>) 
    Doc:  include optimizers with these tags. Separate tags with ':'.
    Value:  

profile (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x11cd0b280>>) 
    Doc:  If VM should collect profile information
    Value:  False

profile_optimizer (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x11cd0b2b0>>) 
    Doc:  If VM should collect optimizer profile information
    Value:  False

profile_memory (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x11cd0b2e0>>) 
    Doc:  If VM should collect memory profile information and print it
    Value:  False

<aesara.configparser.ConfigParam object at 0x11cd0b310>
    Doc:  Useful only for the VM Linkers. When lazy is None, auto detect if lazy evaluation is needed and use the appropriate version. If the C loop isn't being used and lazy is True, use the Stack VM; otherwise, use the Loop VM.
    Value:  None

unittests__rseed (<class 'str'>) 
    Doc:  Seed to use for randomized unit tests. Special value 'random' means using a seed of None.
    Value:  666

warn__round (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x11cd0b3d0>>) 
    Doc:  Warn when using `tensor.round` with the default mode. Round changed its default from `half_away_from_zero` to `half_to_even` to have the same default as NumPy.
    Value:  False

numba__vectorize_target ({'cuda', 'cpu', 'parallel'}) 
    Doc:  Default target for numba.vectorize.
    Value:  cpu

numba__fastmath (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x11cd0b490>>) 
    Doc:  If True, use Numba's fastmath mode.
    Value:  True

numba__cache (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x11cd0b520>>) 
    Doc:  If True, use Numba's file based caching.
    Value:  True

compiledir_format (<class 'str'>) 
    Doc:  Format string for platform-dependent compiled module subdirectory
(relative to base_compiledir). Available keys: aesara_version, device,
gxx_version, hostname, numpy_version, platform, processor,
python_bitwidth, python_int_bitwidth, python_version, short_platform.
Defaults to compiledir_%(short_platform)s-%(processor)s-
%(python_version)s-%(python_bitwidth)s.
    Value:  compiledir_%(short_platform)s-%(processor)s-%(python_version)s-%(python_bitwidth)s

<aesara.configparser.ConfigParam object at 0x11cd0b5e0>
    Doc:  platform-independent root directory for compiled modules
    Value:  /Users/twiecki/.aesara

<aesara.configparser.ConfigParam object at 0x11cd0b580>
    Doc:  platform-dependent cache directory for compiled modules
    Value:  /Users/twiecki/.aesara/compiledir_macOS-12.3.1-arm64-i386-64bit-i386-3.10.4-64

blas__ldflags (<class 'str'>) 
    Doc:  lib[s] to include for [Fortran] level-3 blas implementation
    Value:  -L/Users/twiecki/miniforge3/envs/pymc4/lib -lopenblas -lopenblas -Wl,-rpath,/Users/twiecki/miniforge3/envs/pymc4/lib

blas__check_openmp (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x11fc5c970>>) 
    Doc:  Check for openmp library conflict.
WARNING: Setting this to False leaves you open to wrong results in blas-related operations.
    Value:  True

scan__allow_gc (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x164670fa0>>) 
    Doc:  Allow/disallow gc inside of Scan (default: False)
    Value:  False

scan__allow_output_prealloc (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x16462b040>>) 
    Doc:  Allow/disallow memory preallocation for outputs inside of scan (default: True)
    Value:  True
```

</details>

* Python version: 3.9
* Operating system: OSX
* How did you install Aesara: (conda/pip): conda
",bug MacOS Numba,,2022-06-30 20:42:26,2022-07-01 18:21:48,"twiecki labeled 2022-06-30 20:42:26,twiecki labeled 2022-06-30 20:42:26,twiecki labeled 2022-06-30 20:42:26,twiecki mentioned 2022-06-30 21:49:39,twiecki subscribed 2022-06-30 21:49:39,brandonwillard mentioned 2022-07-01 06:26:04,brandonwillard subscribed 2022-07-01 06:26:04,aseyboldt mentioned 2022-07-01 06:26:04,aseyboldt subscribed 2022-07-01 06:26:04,aseyboldt mentioned 2022-07-01 14:39:46,aseyboldt subscribed 2022-07-01 14:39:46,twiecki referenced 2022-07-01 16:01:45,twiecki referenced 2022-07-01 16:36:54,brandonwillard closed 2022-07-01 18:21:49,brandonwillard referenced 2022-07-01 18:21:50",aseyboldt twiecki brandonwillard,6
806,1037,Warning: Using NumPy C-API based implementation for BLAS functions,Ander-MZ,"## Description of your problem or feature request

When importing **aesara**, I get a warning that NumPy API will be used for BLAS functions. I have google around a lot and everything points to a linking issue or some flags, but honestly I understand basically nothing about flags and such.

Could someone point me as to how can I correctly configure my env to use the library? I include a file with my env's details.

**Please provide a minimal, self-contained, and reproducible example.**
```python
import aesara
```

**Please provide the full traceback of any errors.**
```python
Python 3.9.13 | packaged by conda-forge | (main, May 27 2022, 17:00:52) 
[Clang 13.0.1 ] on darwin
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import aesara
WARNING (aesara.tensor.blas): Using NumPy C-API based implementation for BLAS functions.
```

**Please provide any additional information below.**

I checked that my env's BLAS library is MKL:

The command ```readlink /opt/anaconda3/envs/pyMC/lib/libblas.3.dylib``` outputs: libmkl_rt.dylib

## Versions and main components

* Aesara version: 2.7.3
* Aesara config: See attached file
* Python version: 3.9.13
* Operating system: MacOS 11.15.2
* How did you install Aesara: conda

[aesara_config.txt](https://github.com/aesara-devs/aesara/files/9040877/aesara_config.txt)

[myenv.txt](https://github.com/aesara-devs/aesara/files/9040920/myenv.txt)
",,,2022-07-04 16:11:05,2022-07-06 22:39:55,"aesara-devs locked 2022-07-06 22:39:54,brandonwillard converted_to_discussion 2022-07-06 22:39:55",Ander-MZ aesara-devs brandonwillard,0
809,1043,RandomVariables raise ValueError when size would broadcast parameter,ricardoV94,"We have a common check in multivariate distributions (+ categorical), that raises when a non-size is not strictly equal to the batched dimensions of the parameters, saying objects cannot be broadcast to a single shape.

https://github.com/aesara-devs/aesara/blob/510a9618454293fc00b3cfa78b337c1f2d6d2af3/aesara/tensor/random/basic.py#L377-L383

I don't know why we are imposing this limitation. Even if there is a good reason, the message is wrong, because the batched shapes may very well be broadcastable to size:

```python
# Raises ValueError: shape mismatch: objects cannot be broadcast to a single shape
# However batch_shape=(1,) and size=(5,) are clearly broadcastable
at.random.dirichlet([[0.2, 0.3, 0.5]], size=5).eval()
```

The message for the CategoricalRV is more accurate:

https://github.com/aesara-devs/aesara/blob/510a9618454293fc00b3cfa78b337c1f2d6d2af3/aesara/tensor/random/basic.py#L651-L652

However I still don't get why we would not broadcast the parameter to match size, when this is would be valid. Otherwise, the following should also be invalid, no?

```python
# Just fine
at.random.normal([0], size=5).eval()
```",question random variables,,2022-07-09 14:04:58,2022-07-09 22:33:26,"brandonwillard labeled 2022-07-09 22:27:07,brandonwillard labeled 2022-07-09 22:27:07,aesara-devs locked 2022-07-09 22:33:26,brandonwillard converted_to_discussion 2022-07-09 22:33:26",ricardoV94 aesara-devs brandonwillard,0
811,1046,Request: Add Gaussian Hypergeometric Function,ColtAllen,"## Description of your problem or feature request

[I'm currently rewriting the Lifetimes library to run on PyMC](https://github.com/ColtAllen/btyd), but the likelihood function of one of the models requires a [Gaussian Hypergeometric function](https://en.wikipedia.org/wiki/Hypergeometric_function) be written in Aesara. This is a special case of the generalized hypergeometric function and a separate matter from the hypergeometric distribution, which Aesara already implements in `HyperGeometricRV`.

[The SciPy implementation is written in cython](https://github.com/scipy/scipy/blob/main/scipy/special/_hyp2f1.pxd) and I'm not quite sure how to go about brute-forcing this in `aesara.tensor`, but given enough assistance I may be able to whip something up for a future PR.

",enhancement help wanted SciPy compatibility Op implementation,,2022-07-11 14:09:25,2023-01-30 21:49:52,"brandonwillard labeled 2022-07-11 20:31:46,brandonwillard labeled 2022-07-11 20:31:46,brandonwillard labeled 2022-07-11 20:31:46,brandonwillard labeled 2022-07-11 20:31:46,ColtAllen mentioned 2022-09-12 23:25:48,ColtAllen subscribed 2022-09-12 23:25:48,brandonwillard mentioned 2022-11-09 18:37:31,brandonwillard subscribed 2022-11-09 18:37:31,brandonwillard mentioned 2022-11-09 19:11:11,brandonwillard subscribed 2022-11-09 19:11:11,brandonwillard connected 2022-11-10 20:36:10,brandonwillard closed 2023-01-30 21:49:52",ricardoV94 brandonwillard ColtAllen,21
812,1047,"PyPI JSON API returns `""requires_dist"":null`",keesterbrugge,"## Description of your problem or feature request

I use a self-hosted pypi-mirror that adds the dependencies of added modules by looking at the `requires_dist` field returned by the pypi json api. [`pymc` nicely returns it's dependencies](https://pypi.org/pypi/pymc/json)  defined in `setup.py`but [`aesara` and `aeppl` do not](https://pypi.org/pypi/aesara/json). 

Could you please add this, it would make the install and adoption process a lot easier especially in companies with private pip repo's. 

## Versions and main components

* Aesara version: 2.7.5
* Aesara config (`python -c ""import aesara; print(aesara.config)""`)
* Python version: 3.8
* Operating system: macOS Monterey v 12.4
* How did you install Aesara: pip
",enhancement CI,,2022-07-11 16:19:14,2022-07-11 23:20:24,"brandonwillard labeled 2022-07-11 20:23:24,brandonwillard labeled 2022-07-11 20:23:24,brandonwillard connected 2022-07-11 23:20:17,brandonwillard closed 2022-07-11 23:20:24,brandonwillard renamed 2022-07-11 23:21:24,brandonwillard renamed 2022-07-11 23:21:50",keesterbrugge brandonwillard,0
816,1051,Consider using `pybind11` in C-backend ,ferrine,"The idea is to simplify our c++ backend using
https://github.com/tbenthompson/cppimport
and 
https://github.com/pybind/pybind11",,,2022-07-13 12:40:18,2022-07-13 14:34:44,"brandonwillard renamed 2022-07-13 14:34:17,brandonwillard renamed 2022-07-13 14:34:32,aesara-devs locked 2022-07-13 14:34:43,brandonwillard converted_to_discussion 2022-07-13 14:34:44",ferrine aesara-devs brandonwillard,1
819,1055,RandomVariables raise ValueError when size would broadcast parameter,ricardoV94,"### Discussed in https://github.com/aesara-devs/aesara/discussions/1044

<div type='discussions-op-text'>

<sup>Originally posted by **ricardoV94** July  9, 2022</sup>
We have a common check in multivariate distributions (+ categorical), that raises when a non-size is not strictly equal to the batched dimensions of the parameters, saying objects cannot be broadcast to a single shape.

https://github.com/aesara-devs/aesara/blob/510a9618454293fc00b3cfa78b337c1f2d6d2af3/aesara/tensor/random/basic.py#L377-L383

I don't know why we are imposing this limitation. Even if there is a good reason, the message is wrong, because the batched shapes may very well be broadcastable to size:

```python
# Raises ValueError: shape mismatch: objects cannot be broadcast to a single shape
# However batch_shape=(1,) and size=(5,) are clearly broadcastable
at.random.dirichlet([[0.2, 0.3, 0.5]], size=5).eval()
```

The message for the CategoricalRV is more accurate:

https://github.com/aesara-devs/aesara/blob/510a9618454293fc00b3cfa78b337c1f2d6d2af3/aesara/tensor/random/basic.py#L651-L652

However I still don't get why we would not broadcast the parameter to match size, when this is would be valid. Otherwise, the following should also be invalid, no?

```python
# Just fine
at.random.normal([0], size=5).eval()
```</div>",question random variables,,2022-07-17 14:52:13,2022-07-17 22:07:25,"ricardoV94 labeled 2022-07-17 14:52:13,ricardoV94 labeled 2022-07-17 14:52:13,aesara-devs locked 2022-07-17 22:07:25,brandonwillard converted_to_discussion 2022-07-17 22:07:25",ricardoV94 aesara-devs brandonwillard,1
821,1058,Erroneous error when using `size` with broadcastable independent dimensions in `RandomVariable`,brandonwillard,"### Discussed in https://github.com/aesara-devs/aesara/discussions/1044

<div type='discussions-op-text'>

<sup>Originally posted by **ricardoV94** July  9, 2022</sup>
We have a common check in multivariate distributions (+ categorical), that raises when a non-size is not strictly equal to the batched dimensions of the parameters, saying objects cannot be broadcast to a single shape.

https://github.com/aesara-devs/aesara/blob/510a9618454293fc00b3cfa78b337c1f2d6d2af3/aesara/tensor/random/basic.py#L377-L383

```python
# Raises ValueError: shape mismatch: objects cannot be broadcast to a single shape
# However batch_shape=(1,) and size=(5,) are clearly broadcastable
at.random.dirichlet([[0.2, 0.3, 0.5]], size=5).eval()
```
</div>

To clarify: 

This looks like a bug in the snippet of code referenced above, and not a general issue with the way `size` is interpreted, because—for instance—`at.random.dirichlet([[0.2, 0.3, 0.5]], size=(5,)).shape.eval()` yields `[5, 3]` and `at.random.dirichlet([[0.2, 0.3, 0.5], [0.2, 0.3, 0.5]], size=(2,)).eval()` ""broadcasts"" without an issue, so it seems to be a problem specific to _broadcastable independent dimensions_ and the code above.",bug random variables Op implementation,ricardoV94,2022-07-17 23:29:13,2022-07-17 23:49:30,"brandonwillard labeled 2022-07-17 23:29:13,brandonwillard labeled 2022-07-17 23:29:13,brandonwillard labeled 2022-07-17 23:29:13,ricardoV94 assigned 2022-07-17 23:29:21,brandonwillard connected 2022-07-17 23:30:26,brandonwillard closed 2022-07-17 23:49:30",ricardoV94 brandonwillard,0
825,1063,Inconsistency in numba mode when passing scalar to function,twiecki,"In numba mode:
```python
import aesara
aesara.config.mode = ""NUMBA""
import aesara.tensor as at

x = at.scalar(name=""x"")
f = aesara.function([x], 2*x)
print(repr(f(10)))  # prints 20.0
```

In c mode:

```python
import aesara
import aesara.tensor as at

x = at.scalar(name=""x"")
f = aesara.function([x], 2*x)
print(repr(f(10))) # prints array(20.)
```

This came up in https://github.com/pymc-devs/pymc/issues/5937. Found by [bherwerth](https://github.com/bherwerth).",bug backend compatibility Numba,,2022-07-19 22:26:08,2022-07-23 01:27:45,"twiecki labeled 2022-07-19 22:26:08,twiecki labeled 2022-07-19 22:26:08,twiecki labeled 2022-07-19 22:26:08,brandonwillard closed 2022-07-23 01:27:45",bherwerth twiecki brandonwillard,4
828,1067,blas/mkl issue on Google colab,twiecki,"Here's an example google colab that installs aesara 2.7.7 and then tries to import: https://colab.research.google.com/drive/1bPq3mcIlcw6mJQE8Gf8bcEdVhJdJSUpB#scrollTo=36fiA6TJjLJC

Error:
```
---------------------------------------------------------------------------
NoSectionError                            Traceback (most recent call last)
[/usr/local/lib/python3.7/dist-packages/aesara/configparser.py](https://localhost:8080/#) in fetch_val_for_key(self, key, delete_key)
    236             try:
--> 237                 return self._aesara_cfg.get(section, option)
    238             except InterpolationError:

12 frames
NoSectionError: No section: 'blas'

During handling of the above exception, another exception occurred:

KeyError                                  Traceback (most recent call last)
KeyError: 'blas__ldflags'

During handling of the above exception, another exception occurred:

ModuleNotFoundError                       Traceback (most recent call last)
ModuleNotFoundError: No module named 'mkl'

During handling of the above exception, another exception occurred:

RuntimeError                              Traceback (most recent call last)
[/usr/local/lib/python3.7/dist-packages/aesara/link/c/cmodule.py](https://localhost:8080/#) in check_mkl_openmp()
   2691 you set this flag and don't set the appropriate environment or make
   2692 sure you have the right version you *will* get wrong results.
-> 2693 """"""
   2694         )
   2695 

RuntimeError: 
Could not import 'mkl'.  If you are using conda, update the numpy
packages to the latest build otherwise, set MKL_THREADING_LAYER=GNU in
your environment for MKL 2018.

If you have MKL 2017 install and are not in a conda environment you
can set the Aesara flag blas__check_openmp to False.  Be warned that if
you set this flag and don't set the appropriate environment or make
sure you have the right version you *will* get wrong results.
```

Setting `%env MKL_THREADING_LAYER=GNU` allows importing, but it's not clear why aesara can't figure that out itself, or what the problem is exactly.

Output of check_blas.py:
```

        Some results that you can compare against. They were 10 executions
        of gemm in float64 with matrices of shape 2000x2000 (M=N=K=2000).
        All memory layout was in C order.

        CPU tested: Xeon E5345(2.33Ghz, 8M L2 cache, 1333Mhz FSB),
                    Xeon E5430(2.66Ghz, 12M L2 cache, 1333Mhz FSB),
                    Xeon E5450(3Ghz, 12M L2 cache, 1333Mhz FSB),
                    Xeon X5560(2.8Ghz, 12M L2 cache, hyper-threads?)
                    Core 2 E8500, Core i7 930(2.8Ghz, hyper-threads enabled),
                    Core i7 950(3.07GHz, hyper-threads enabled)
                    Xeon X5550(2.67GHz, 8M l2 cache?, hyper-threads enabled)


        Libraries tested:
            * numpy with ATLAS from distribution (FC9) package (1 thread)
            * manually compiled numpy and ATLAS with 2 threads
            * goto 1.26 with 1, 2, 4 and 8 threads
            * goto2 1.13 compiled with multiple threads enabled

                          Xeon   Xeon   Xeon  Core2 i7    i7     Xeon   Xeon
        lib/nb threads    E5345  E5430  E5450 E8500 930   950    X5560  X5550

        numpy 1.3.0 blas                                                775.92s
        numpy_FC9_atlas/1 39.2s  35.0s  30.7s 29.6s 21.5s 19.60s
        goto/1            18.7s  16.1s  14.2s 13.7s 16.1s 14.67s
        numpy_MAN_atlas/2 12.0s  11.6s  10.2s  9.2s  9.0s
        goto/2             9.5s   8.1s   7.1s  7.3s  8.1s  7.4s
        goto/4             4.9s   4.4s   3.7s  -     4.1s  3.8s
        goto/8             2.7s   2.4s   2.0s  -     4.1s  3.8s
        openblas/1                                        14.04s
        openblas/2                                         7.16s
        openblas/4                                         3.71s
        openblas/8                                         3.70s
        mkl 11.0.083/1            7.97s
        mkl 10.2.2.025/1                                         13.7s
        mkl 10.2.2.025/2                                          7.6s
        mkl 10.2.2.025/4                                          4.0s
        mkl 10.2.2.025/8                                          2.0s
        goto2 1.13/1                                                     14.37s
        goto2 1.13/2                                                      7.26s
        goto2 1.13/4                                                      3.70s
        goto2 1.13/8                                                      1.94s
        goto2 1.13/16                                                     3.16s

        Test time in float32. There were 10 executions of gemm in
        float32 with matrices of shape 5000x5000 (M=N=K=5000)
        All memory layout was in C order.


        cuda version      8.0    7.5    7.0
        gpu
        M40               0.45s  0.47s
        k80               0.92s  0.96s
        K6000/NOECC       0.71s         0.69s
        P6000/NOECC       0.25s

        Titan X (Pascal)  0.28s
        GTX Titan X       0.45s  0.45s  0.47s
        GTX Titan Black   0.66s  0.64s  0.64s
        GTX 1080          0.35s
        GTX 980 Ti               0.41s
        GTX 970                  0.66s
        GTX 680                         1.57s
        GTX 750 Ti               2.01s  2.01s
        GTX 750                  2.46s  2.37s
        GTX 660                  2.32s  2.32s
        GTX 580                  2.42s
        GTX 480                  2.87s
        TX1                             7.6s (float32 storage and computation)
        GT 610                          33.5s
        
Some Aesara flags:
    blas__ldflags= -L/usr/local/lib -lmkl_rt -lpthread -lm -lm
    compiledir= /root/.aesara/compiledir_Linux-5.4.188+-x86_64-with-Ubuntu-18.04-bionic-x86_64-3.7.13-64
    floatX= float64
    device= cpu
Some OS information:
    sys.platform= linux
    sys.version= 3.7.13 (default, Apr 24 2022, 01:04:09) 
[GCC 7.5.0]
    sys.prefix= /usr
Some environment variables:
    MKL_NUM_THREADS= None
    OMP_NUM_THREADS= None
    GOTO_NUM_THREADS= None

Numpy config: (used when the Aesara flag ""blas__ldflags"" is empty)
blas_mkl_info:
  NOT AVAILABLE
blis_info:
  NOT AVAILABLE
openblas_info:
    libraries = ['openblas', 'openblas']
    library_dirs = ['/usr/local/lib']
    language = c
    define_macros = [('HAVE_CBLAS', None)]
    runtime_library_dirs = ['/usr/local/lib']
blas_opt_info:
    libraries = ['openblas', 'openblas']
    library_dirs = ['/usr/local/lib']
    language = c
    define_macros = [('HAVE_CBLAS', None)]
    runtime_library_dirs = ['/usr/local/lib']
lapack_mkl_info:
  NOT AVAILABLE
openblas_lapack_info:
    libraries = ['openblas', 'openblas']
    library_dirs = ['/usr/local/lib']
    language = c
    define_macros = [('HAVE_CBLAS', None)]
    runtime_library_dirs = ['/usr/local/lib']
lapack_opt_info:
    libraries = ['openblas', 'openblas']
    library_dirs = ['/usr/local/lib']
    language = c
    define_macros = [('HAVE_CBLAS', None)]
    runtime_library_dirs = ['/usr/local/lib']
Supported SIMD extensions in this NumPy install:
    baseline = SSE,SSE2,SSE3
    found = SSSE3,SSE41,POPCNT,SSE42,AVX,F16C,FMA3,AVX2
    not found = AVX512F,AVX512CD,AVX512_KNL,AVX512_KNM,AVX512_SKX,AVX512_CLX,AVX512_CNL,AVX512_ICL
Numpy dot module: numpy
Numpy location: /usr/local/lib/python3.7/dist-packages/numpy/__init__.py
Numpy version: 1.21.6

We executed 10 calls to gemm with a and b matrices of shapes (5000, 5000) and (5000, 5000).

Total execution time: 74.73s on CPU (with direct Aesara binding to blas).

Try to run this script a few times. Experience shows that the first time is not as fast as following calls. The difference is not big, but consistent.
```

CC @canyon289 @junpenglao @ColCarroll ",,,2022-07-20 21:43:59,2022-07-21 01:12:11,"ColCarroll mentioned 2022-07-20 21:43:59,ColCarroll subscribed 2022-07-20 21:43:59,canyon289 mentioned 2022-07-20 21:43:59,canyon289 subscribed 2022-07-20 21:43:59,junpenglao mentioned 2022-07-20 21:43:59,junpenglao subscribed 2022-07-20 21:43:59,aesara-devs locked 2022-07-21 01:12:11,brandonwillard converted_to_discussion 2022-07-21 01:12:11",junpenglao aesara-devs brandonwillard ColCarroll canyon289 twiecki,1
831,1072,logsumexp().eval() overflows,peterkomar-aws,"## Evaluating `aesara.tensor.logsumexp` on large numbers overflows

When evaluating the result of `logsumexp`, I observe float overflow. Not surprisingly, `scipy.special.logsumexp` avoids this problem. (See code below.)

I am not an expert, but this could be because the implementation of `logsumexp` (https://github.com/aesara-devs/aesara/blob/main/aesara/tensor/math.py#L2826-L2851) explicitly calls `exp`, `sum` and `log` sequentially, as opposed to implementing the log-sum-exp trick (https://en.wikipedia.org/wiki/LogSumExp#log-sum-exp_trick_for_log-domain_calculations).

**Minimal, self-contained, and reproducible example.**
```python
import scipy
import aesara

from scipy.special import logsumexp as logsumexp_scipy
from aesara.tensor import logsumexp as logsumexp_aesara

print(f""scipy version: {scipy.__version__}"")
print(f""aesara version: {aesara.__version__}"")

small_numbers = [1, 2, 3, 4]
large_numbers = [1000, 2000, 3000, 4000]
small_and_large_numebrs = [1, 2, 3000, 4000]

inputs = (small_numbers, large_numbers, small_and_large_numebrs)

for example_id, input_ in enumerate(inputs):
    print('-'*79)
    print(f""Example {example_id}"")
    print(f""input: {input_}"")
    print(f""scipy.special.logsumexp(input): {logsumexp_scipy(input_)}"")
    print(f""aesara.tensor.logsumexp(input).eval(): {logsumexp_aesara(input_).eval()}"")
```

**Std output**
```python
scipy version: 1.8.0
aesara version: 2.7.7
-------------------------------------------------------------------------------
Example 0
input: [1, 2, 3, 4]
scipy.special.logsumexp(input): 4.440189698561196
aesara.tensor.logsumexp(input).eval(): 4.440189698561196
-------------------------------------------------------------------------------
Example 1
input: [1000, 2000, 3000, 4000]
scipy.special.logsumexp(input): 4000.0
aesara.tensor.logsumexp(input).eval(): inf
-------------------------------------------------------------------------------
Example 2
input: [1, 2, 3000, 4000]
scipy.special.logsumexp(input): 4000.0
aesara.tensor.logsumexp(input).eval(): inf
```


## Versions and main components

* Aesara version: 2.7.7
* Aesara config (`python -c ""import aesara; print(aesara.config)""`)
```
floatX ({'float32', 'float64', 'float16'}) 
    Doc:  Default floating-point precision for python casts.

Note: float16 support is experimental, use at your own risk.
    Value:  float64

warn_float64 ({'ignore', 'warn', 'raise', 'pdb'}) 
    Doc:  Do an action when a tensor variable with float64 dtype is created.
    Value:  ignore

pickle_test_value (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x106b3fb80>>) 
    Doc:  Dump test values while pickling model. If True, test values will be dumped with model.
    Value:  True

cast_policy ({'custom', 'numpy+floatX'}) 
    Doc:  Rules for implicit type casting
    Value:  custom

deterministic ({'more', 'default'}) 
    Doc:  If `more`, sometimes we will select some implementation that are more deterministic, but slower.  Also see the dnn.conv.algo* flags to cover more cases.
    Value:  default

device (cpu)
    Doc:  Default device for computations. only cpu is supported for now
    Value:  cpu

force_device (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x106b3f250>>) 
    Doc:  Raise an error if we can't use the specified device
    Value:  False

conv__assert_shape (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x106b3f880>>) 
    Doc:  If True, AbstractConv* ops will verify that user-provided shapes match the runtime shapes (debugging option, may slow down compilation)
    Value:  False

print_global_stats (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x106b3f280>>) 
    Doc:  Print some global statistics (time spent) at the end
    Value:  False

assert_no_cpu_op ({'ignore', 'warn', 'raise', 'pdb'}) 
    Doc:  Raise an error/warning if there is a CPU op in the computational graph.
    Value:  ignore

unpickle_function (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x106b3fa00>>) 
    Doc:  Replace unpickled Aesara functions with None. This is useful to unpickle old graphs that pickled them when it shouldn't
    Value:  True

<aesara.configparser.ConfigParam object at 0x106b3fa30>
    Doc:  Default compilation mode
    Value:  Mode

cxx (<class 'str'>) 
    Doc:  The C++ compiler to use. Currently only g++ is supported, but supporting additional compilers should not be too difficult. If it is empty, no C++ code is compiled.
    Value:  /usr/bin/clang++

linker ({'vm', 'vm_nogc', 'c|py', 'cvm_nogc', 'py', 'cvm', 'c|py_nogc', 'c'}) 
    Doc:  Default linker used if the aesara flags mode is Mode
    Value:  cvm

allow_gc (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x106b3f970>>) 
    Doc:  Do we default to delete intermediate results during Aesara function calls? Doing so lowers the memory requirement, but asks that we reallocate memory at the next function call. This is implemented for the default linker, but may not work for all linkers.
    Value:  True

optimizer ({'o2', 'fast_compile', 'merge', 'o3', 'fast_run', 'None', 'unsafe', 'o4', 'o1'}) 
    Doc:  Default optimizer. If not None, will use this optimizer with the Mode
    Value:  o4

optimizer_verbose (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x106b3fcd0>>) 
    Doc:  If True, we print all optimization being applied
    Value:  False

on_opt_error ({'ignore', 'warn', 'raise', 'pdb'}) 
    Doc:  What to do when an optimization crashes: warn and skip it, raise the exception, or fall into the pdb debugger.
    Value:  warn

nocleanup (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x106b3f850>>) 
    Doc:  Suppress the deletion of code files that did not compile cleanly
    Value:  False

on_unused_input ({'ignore', 'warn', 'raise'}) 
    Doc:  What to do if a variable in the 'inputs' list of  aesara.function() is not used in the graph.
    Value:  raise

gcc__cxxflags (<class 'str'>) 
    Doc:  Extra compiler flags for gcc
    Value:  

cmodule__warn_no_version (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x106b3fb50>>) 
    Doc:  If True, will print a warning when compiling one or more Op with C code that can't be cached because there is no c_code_cache_version() function associated to at least one of those Ops.
    Value:  False

cmodule__remove_gxx_opt (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x106b3fd30>>) 
    Doc:  If True, will remove the -O* parameter passed to g++.This is useful to debug in gdb modules compiled by Aesara.The parameter -g is passed by default to g++
    Value:  False

cmodule__compilation_warning (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x106b3fdc0>>) 
    Doc:  If True, will print compilation warnings.
    Value:  False

cmodule__preload_cache (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x106b3fdf0>>) 
    Doc:  If set to True, will preload the C module cache at import time
    Value:  False

cmodule__age_thresh_use (<class 'int'>) 
    Doc:  In seconds. The time after which Aesara won't reuse a compile c module.
    Value:  2073600

cmodule__debug (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x106b3fe50>>) 
    Doc:  If True, define a DEBUG macro (if not exists) for any compiled C code.
    Value:  False

compile__wait (<class 'int'>) 
    Doc:  Time to wait before retrying to acquire the compile lock.
    Value:  5

compile__timeout (<class 'int'>) 
    Doc:  In seconds, time that a process will wait before deciding to
    override an existing lock. An override only happens when the existing
    lock is held by the same owner *and* has not been 'refreshed' by this
    owner for more than this period. Refreshes are done every half timeout
    period for running processes.
    Value:  120

ctc__root (<class 'str'>) 
    Doc:  Directory which contains the root of Baidu CTC library. It is assumed         that the compiled library is either inside the build, lib or lib64         subdirectory, and the header inside the include directory.
    Value:  

tensor__cmp_sloppy (<class 'int'>) 
    Doc:  Relax aesara.tensor.math._allclose (0) not at all, (1) a bit, (2) more
    Value:  0

tensor__local_elemwise_fusion (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x106b3ffa0>>) 
    Doc:  Enable or not in fast_run mode(fast_run optimization) the elemwise fusion optimization
    Value:  True

lib__amblibm (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x106b800d0>>) 
    Doc:  Use amd's amdlibm numerical library
    Value:  False

tensor__insert_inplace_optimizer_validate_nb (<class 'int'>) 
    Doc:  -1: auto, if graph have less then 500 nodes 1, else 10
    Value:  -1

traceback__limit (<class 'int'>) 
    Doc:  The number of stack to trace. -1 mean all.
    Value:  8

traceback__compile_limit (<class 'int'>) 
    Doc:  The number of stack to trace to keep during compilation. -1 mean all. If greater then 0, will also make us save Aesara internal stack trace.
    Value:  0

experimental__local_alloc_elemwise (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x106b80250>>) 
    Doc:  DEPRECATED: If True, enable the experimental optimization local_alloc_elemwise. Generates error if not True. Use optimizer_excluding=local_alloc_elemwise to disable.
    Value:  True

experimental__local_alloc_elemwise_assert (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x106b80280>>) 
    Doc:  When the local_alloc_elemwise is applied, add an assert to highlight shape errors.
    Value:  True

warn__ignore_bug_before ({'0.6', '1.0', '1.0.2', '0.7', '0.8.1', '1.0.3', '0.4', '0.10', '0.8', '0.3', '1.0.5', 'None', '1.0.4', '0.8.2', '0.4.1', '0.9', '1.0.1', 'all', '0.5'}) 
    Doc:  If 'None', we warn about all Aesara bugs found by default. If 'all', we don't warn about Aesara bugs found by default. If a version, we print only the warnings relative to Aesara bugs found after that version. Warning for specific bugs can be configured with specific [warn] flags.
    Value:  0.9

exception_verbosity ({'high', 'low'}) 
    Doc:  If 'low', the text of exceptions will generally refer to apply nodes with short names such as Elemwise{add_no_inplace}. If 'high', some exceptions will also refer to apply nodes with long descriptions  like:
        A. Elemwise{add_no_inplace}
                B. log_likelihood_v_given_h
                C. log_likelihood_h
    Value:  low

print_test_value (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x106b803d0>>) 
    Doc:  If 'True', the __eval__ of an Aesara variable will return its test_value when this is available. This has the practical conseguence that, e.g., in debugging `my_var` will print the same as `my_var.tag.test_value` when a test value is defined.
    Value:  False

compute_test_value ({'ignore', 'pdb', 'off', 'raise', 'warn'}) 
    Doc:  If 'True', Aesara will run each op at graph build time, using Constants, SharedVariables and the tag 'test_value' as inputs to the function. This helps the user track down problems in the graph before it gets optimized.
    Value:  off

compute_test_value_opt ({'ignore', 'pdb', 'off', 'raise', 'warn'}) 
    Doc:  For debugging Aesara optimization only. Same as compute_test_value, but is used during Aesara optimization
    Value:  off

check_input (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x106b80460>>) 
    Doc:  Specify if types should check their input in their C code. It can be used to speed up compilation, reduce overhead (particularly for scalars) and reduce the number of generated C files.
    Value:  True

NanGuardMode__nan_is_error (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x106b80490>>) 
    Doc:  Default value for nan_is_error
    Value:  True

NanGuardMode__inf_is_error (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x106b804c0>>) 
    Doc:  Default value for inf_is_error
    Value:  True

NanGuardMode__big_is_error (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x106b80550>>) 
    Doc:  Default value for big_is_error
    Value:  True

NanGuardMode__action ({'warn', 'raise', 'pdb'}) 
    Doc:  What NanGuardMode does when it finds a problem
    Value:  raise

DebugMode__patience (<class 'int'>) 
    Doc:  Optimize graph this many times to detect inconsistency
    Value:  10

DebugMode__check_c (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x106b805e0>>) 
    Doc:  Run C implementations where possible
    Value:  True

DebugMode__check_py (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x106b80670>>) 
    Doc:  Run Python implementations where possible
    Value:  True

DebugMode__check_finite (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x106b806a0>>) 
    Doc:  True -> complain about NaN/Inf results
    Value:  True

DebugMode__check_strides (<class 'int'>) 
    Doc:  Check that Python- and C-produced ndarrays have same strides. On difference: (0) - ignore, (1) warn, or (2) raise error
    Value:  0

DebugMode__warn_input_not_reused (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x106b80700>>) 
    Doc:  Generate a warning when destroy_map or view_map says that an op works inplace, but the op did not reuse the input for its output.
    Value:  True

DebugMode__check_preallocated_output (<class 'str'>) 
    Doc:  Test thunks with pre-allocated memory as output storage. This is a list of strings separated by "":"". Valid values are: ""initial"" (initial storage in storage map, happens with Scan),""previous"" (previously-returned memory), ""c_contiguous"", ""f_contiguous"", ""strided"" (positive and negative strides), ""wrong_size"" (larger and smaller dimensions), and ""ALL"" (all of the above).
    Value:  

DebugMode__check_preallocated_output_ndim (<class 'int'>) 
    Doc:  When testing with ""strided"" preallocated output memory, test all combinations of strides over that number of (inner-most) dimensions. You may want to reduce that number to reduce memory or time usage, but it is advised to keep a minimum of 2.
    Value:  4

profiling__time_thunks (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x106b80790>>) 
    Doc:  Time individual thunks when profiling
    Value:  True

profiling__n_apply (<class 'int'>) 
    Doc:  Number of Apply instances to print by default
    Value:  20

profiling__n_ops (<class 'int'>) 
    Doc:  Number of Ops to print by default
    Value:  20

profiling__output_line_width (<class 'int'>) 
    Doc:  Max line width for the profiling output
    Value:  512

profiling__min_memory_size (<class 'int'>) 
    Doc:  For the memory profile, do not print Apply nodes if the size
                 of their outputs (in bytes) is lower than this threshold
    Value:  1024

profiling__min_peak_memory (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x106b808e0>>) 
    Doc:  The min peak memory usage of the order
    Value:  False

profiling__destination (<class 'str'>) 
    Doc:  File destination of the profiling output
    Value:  stderr

profiling__debugprint (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x106b80940>>) 
    Doc:  Do a debugprint of the profiled functions
    Value:  False

profiling__ignore_first_call (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x106b80970>>) 
    Doc:  Do we ignore the first call of an Aesara function.
    Value:  False

on_shape_error ({'warn', 'raise'}) 
    Doc:  warn: print a warning and use the default value. raise: raise an error
    Value:  warn

openmp (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x106b809d0>>) 
    Doc:  Allow (or not) parallel computation on the CPU with OpenMP. This is the default value used when creating an Op that supports OpenMP parallelization. It is preferable to define it via the Aesara configuration file ~/.aesararc or with the environment variable AESARA_FLAGS. Parallelization is only done for some operations that implement it, and even for operations that implement parallelism, each operation is free to respect this flag or not. You can control the number of threads used with the environment variable OMP_NUM_THREADS. If it is set to 1, we disable openmp in Aesara by default.
    Value:  False

openmp_elemwise_minsize (<class 'int'>) 
    Doc:  If OpenMP is enabled, this is the minimum size of vectors for which the openmp parallelization is enabled in element wise ops.
    Value:  200000

optimizer_excluding (<class 'str'>) 
    Doc:  When using the default mode, we will remove optimizer with these tags. Separate tags with ':'.
    Value:  

optimizer_including (<class 'str'>) 
    Doc:  When using the default mode, we will add optimizer with these tags. Separate tags with ':'.
    Value:  

optimizer_requiring (<class 'str'>) 
    Doc:  When using the default mode, we will require optimizer with these tags. Separate tags with ':'.
    Value:  

optdb__position_cutoff (<class 'float'>) 
    Doc:  Where to stop eariler during optimization. It represent the position of the optimizer where to stop.
    Value:  inf

optdb__max_use_ratio (<class 'float'>) 
    Doc:  A ratio that prevent infinite loop in EquilibriumOptimizer.
    Value:  8.0

cycle_detection ({'regular', 'fast'}) 
    Doc:  If cycle_detection is set to regular, most inplaces are allowed,but it is slower. If cycle_detection is set to faster, less inplacesare allowed, but it makes the compilation faster.The interaction of which one give the lower peak memory usage iscomplicated and not predictable, so if you are close to the peakmemory usage, triyng both could give you a small gain.
    Value:  regular

check_stack_trace ({'warn', 'raise', 'off', 'log'}) 
    Doc:  A flag for checking the stack trace during the optimization process. default (off): does not check the stack trace of any optimization log: inserts a dummy stack trace that identifies the optimizationthat inserted the variable that had an empty stack trace.warn: prints a warning if a stack trace is missing and also a dummystack trace is inserted that indicates which optimization insertedthe variable that had an empty stack trace.raise: raises an exception if a stack trace is missing
    Value:  off

metaopt__verbose (<class 'int'>) 
    Doc:  0 for silent, 1 for only warnings, 2 for full output withtimings and selected implementation
    Value:  0

metaopt__optimizer_excluding (<class 'str'>) 
    Doc:  exclude optimizers with these tags. Separate tags with ':'.
    Value:  

metaopt__optimizer_including (<class 'str'>) 
    Doc:  include optimizers with these tags. Separate tags with ':'.
    Value:  

profile (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x106b80cd0>>) 
    Doc:  If VM should collect profile information
    Value:  False

profile_optimizer (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x106b80d00>>) 
    Doc:  If VM should collect optimizer profile information
    Value:  False

profile_memory (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x106b80d30>>) 
    Doc:  If VM should collect memory profile information and print it
    Value:  False

<aesara.configparser.ConfigParam object at 0x106b80d60>
    Doc:  Useful only for the VM Linkers. When lazy is None, auto detect if lazy evaluation is needed and use the appropriate version. If the C loop isn't being used and lazy is True, use the Stack VM; otherwise, use the Loop VM.
    Value:  None

unittests__rseed (<class 'str'>) 
    Doc:  Seed to use for randomized unit tests. Special value 'random' means using a seed of None.
    Value:  666

warn__round (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x106b80e20>>) 
    Doc:  Warn when using `tensor.round` with the default mode. Round changed its default from `half_away_from_zero` to `half_to_even` to have the same default as NumPy.
    Value:  False

numba__vectorize_target ({'parallel', 'cpu', 'cuda'}) 
    Doc:  Default target for numba.vectorize.
    Value:  cpu

numba__fastmath (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x106b80ee0>>) 
    Doc:  If True, use Numba's fastmath mode.
    Value:  True

numba__cache (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x106b80f70>>) 
    Doc:  If True, use Numba's file based caching.
    Value:  True

compiledir_format (<class 'str'>) 
    Doc:  Format string for platform-dependent compiled module subdirectory
(relative to base_compiledir). Available keys: aesara_version, device,
gxx_version, hostname, numpy_version, platform, processor,
python_bitwidth, python_int_bitwidth, python_version, short_platform.
Defaults to compiledir_%(short_platform)s-%(processor)s-
%(python_version)s-%(python_bitwidth)s.
    Value:  compiledir_%(short_platform)s-%(processor)s-%(python_version)s-%(python_bitwidth)s

<aesara.configparser.ConfigParam object at 0x106b81030>
    Doc:  platform-independent root directory for compiled modules
    Value:  /Users/pkomar/.aesara

<aesara.configparser.ConfigParam object at 0x106b80fd0>
    Doc:  platform-dependent cache directory for compiled modules
    Value:  /Users/pkomar/.aesara/compiledir_macOS-12.4-x86_64-i386-64bit-i386-3.10.2-64

blas__ldflags (<class 'str'>) 
    Doc:  lib[s] to include for [Fortran] level-3 blas implementation
    Value:  -Wl,-framework -Wl,Accelerate

blas__check_openmp (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x109185600>>) 
    Doc:  Check for openmp library conflict.
WARNING: Setting this to False leaves you open to wrong results in blas-related operations.
    Value:  True

scan__allow_gc (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x12720da50>>) 
    Doc:  Allow/disallow gc inside of Scan (default: False)
    Value:  False

scan__allow_output_prealloc (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x1271c3bb0>>) 
    Doc:  Allow/disallow memory preallocation for outputs inside of scan (default: True)
    Value:  True
```
* Python version: Python 3.10.2
* Operating system: macOS Monterey, Version 12.4
* How did you install Aesara: (conda/pip) pip (while installing pymc)
",,,2022-07-22 15:26:59,2022-07-22 16:39:36,"peterkomar-aws closed 2022-07-22 16:39:36,ricardoV94 mentioned 2022-07-22 16:39:36,ricardoV94 subscribed 2022-07-22 16:39:36",ricardoV94 peterkomar-aws brandonwillard,6
834,1079,JAX `FillDiagonal` implementation is missing,ferrine,"The implementation is missing 
https://github.com/aesara-devs/aesara/blob/8763981ca4263e153c68e6be39c03a272c027b60/aesara/link/jax/dispatch.py#L958


**Please provide any additional information below.**
An implementation that seems to work here is this
https://github.com/google/jax/issues/2680#issuecomment-804269672

## Versions and main components

* Aesara version: 8763981ca4263e153c68e6be39c03a272c027b60
",enhancement JAX,,2022-07-28 18:35:28,2022-07-31 20:41:00,"brandonwillard labeled 2022-07-28 23:20:46,brandonwillard labeled 2022-07-28 23:20:51,brandonwillard renamed 2022-07-28 23:27:05,brandonwillard closed 2022-07-31 20:41:01",ferrine brandonwillard,0
842,1094,Incorrect broadcasting logic in `local_elemwise_alloc`,brandonwillard,"Some broken broadcasting-related logic is causing `local_elemwise_alloc` to produce graphs that fail to perform simple multiplications:
```python
import numpy as np
import aesara
import aesara.tensor as at
from aesara.compile.mode import get_default_mode


# We're interested in evaluating expressions like the following:
# np.ones((3, 2)) * np.ones((1, 2))

y = at.matrix(""y"")
x = at.matrix(""x"")

z = at.ones(y.shape) * x

with aesara.config.change_flags(optimizer_verbose=True):
    z_fn = aesara.function([x, y], z)
# rewriting: rewrite local_shape_to_shape_i replaces Shape.0 of Shape(y) with MakeVector{dtype='int64'}.0 of MakeVector{dtype='int64'}(Shape_i{0}.0, Shape_i{1}.0)
# rewriting: rewrite local_subtensor_make_vector replaces Subtensor{int64}.0 of Subtensor{int64}(MakeVector{dtype='int64'}.0, ScalarConstant{0}) with Shape_i{0}.0 of Shape_i{0}(y)
# rewriting: rewrite local_subtensor_make_vector replaces Subtensor{int64}.0 of Subtensor{int64}(MakeVector{dtype='int64'}.0, ScalarConstant{1}) with Shape_i{1}.0 of Shape_i{1}(y)
# rewriting: rewrite local_elemwise_alloc replaces Elemwise{mul,no_inplace}.0 of Elemwise{mul,no_inplace}(Alloc.0, x) with Elemwise{mul,no_inplace}.0 of Elemwise{mul,no_inplace}(InplaceDimShuffle{x,x}.0, Assert{msg=Aesara Assert failed!}.0)
# rewriting: rewrite constant_folding replaces InplaceDimShuffle{x,x}.0 of InplaceDimShuffle{x,x}(TensorConstant{1.0}) with TensorConstant{(1, 1) of 1.0} of None
# rewriting: rewrite local_mul_specialize replaces Elemwise{mul,no_inplace}.0 of Elemwise{mul,no_inplace}(TensorConstant{(1, 1) of 1.0}, Assert{msg=Aesara Assert failed!}.0) with Assert{msg=Aesara Assert failed!}.0 of Assert{msg=Aesara Assert failed!}(x, Elemwise{eq,no_inplace}.0, Elemwise{eq,no_inplace}.0)

aesara.dprint(z_fn)
# DeepCopyOp [id A] 7
#  |Assert{msg=Aesara Assert failed!} [id B] 6
#    |x [id C]
#    |Elemwise{eq,no_inplace} [id D] 5
#    | |Shape_i{0} [id E] 4
#    | | |y [id F]
#    | |Shape_i{0} [id G] 3
#    |   |x [id C]
#    |Elemwise{eq,no_inplace} [id H] 2
#      |Shape_i{1} [id I] 1
#      | |y [id F]
#      |Shape_i{1} [id J] 0
#        |x [id C]

z_fn(np.ones((1, 2)), np.ones((3, 2)))
# AssertionError: Aesara Assert failed!

mode = get_default_mode().excluding(""local_elemwise_alloc"")
z_fn_2 = aesara.function([x, y], z, mode=mode)

aesara.dprint(z_fn_2)
# Elemwise{Mul}[(0, 0)] [id A] 3
#  |Alloc [id B] 2
#  | |TensorConstant{1.0} [id C]
#  | |Shape_i{0} [id D] 1
#  | | |y [id E]
#  | |Shape_i{1} [id F] 0
#  |   |y [id E]
#  |x [id G]

z_fn(np.ones((1, 2)), np.ones((3, 2)))
# array([[1., 1.],
#        [1., 1.],
#        [1., 1.]])
```
I believe this is at least somewhat related to the problematic use of static shape/broadcast information discussed in https://github.com/aesara-devs/aesara/issues/1089.",bug help wanted important graph rewriting,,2022-08-03 23:16:50,2022-08-13 23:37:55,"brandonwillard labeled 2022-08-03 23:16:50,brandonwillard labeled 2022-08-03 23:16:50,brandonwillard labeled 2022-08-03 23:16:50,brandonwillard labeled 2022-08-03 23:16:50,brandonwillard closed 2022-08-13 23:37:55",brandonwillard,0
844,1097,Cache miss occurs due to use of hash() in key,mattearllongshot,"## Description of your problem or feature request

In certain circumstances we end up having incorrect cache misses leading to extra storage / CPU usage.

To reproduce, save the following script as `bug.py`:

```python
import aesara
import aesara.tensor as at

params = at.vector()
probabilities = params + 1
aesara.function([params], [probabilities])
```

Run the following commands:

```bash
$ rm -r ~/.aesara/compiledir_Linux-4.15--generic-x86_64-with-glibc2.10-x86_64-3.8.13-64/
$ python bug.py
$ ls ~/.aesara/compiledir_Linux-4.15--generic-x86_64-with-glibc2.10-x86_64-3.8.13-64/tmp*
/home/matthew/.aesara/compiledir_Linux-4.15--generic-x86_64-with-glibc2.10-x86_64-3.8.13-64/tmpmwdm85mq:
__init__.py  key.pkl  mba10987274f369529454d4a60996746c3927712a0b1b3928446a3c275151e2ee.so  mod.cpp  __pycache__

/home/matthew/.aesara/compiledir_Linux-4.15--generic-x86_64-with-glibc2.10-x86_64-3.8.13-64/tmpyzxlvysm:
__init__.py  key.pkl  m5d69910cf9a4555423d5e68cc8357d194094f92b3878920c98b2a24e728523c5.so  mod.cpp  __pycache__
$ python bug.py
$ ls ~/.aesara/compiledir_Linux-4.15--generic-x86_64-with-glibc2.10-x86_64-3.8.13-64/tmp*
/home/matthew/.aesara/compiledir_Linux-4.15--generic-x86_64-with-glibc2.10-x86_64-3.8.13-64/tmpmwdm85mq:
__init__.py  key.pkl  mba10987274f369529454d4a60996746c3927712a0b1b3928446a3c275151e2ee.so  mod.cpp  __pycache__

/home/matthew/.aesara/compiledir_Linux-4.15--generic-x86_64-with-glibc2.10-x86_64-3.8.13-64/tmpoi084cuv:
__init__.py  key.pkl  mba10987274f369529454d4a60996746c3927712a0b1b3928446a3c275151e2ee.so  mod.cpp  __pycache__

/home/matthew/.aesara/compiledir_Linux-4.15--generic-x86_64-with-glibc2.10-x86_64-3.8.13-64/tmpyzxlvysm:
__init__.py  key.pkl  m5d69910cf9a4555423d5e68cc8357d194094f92b3878920c98b2a24e728523c5.so  mod.cpp  __pycache__
```

On the first run, there are two cache dirs, but on the second there are three, with an identical module.  The key files are different but the mod.cpp is the same.  Subsequent runs add additional duplicate directories.

Running with `PYTHONHASHSEED=1234` stops the duplicates being created:

```bash
$ rm -r ~/.aesara/compiledir_Linux-4.15--generic-x86_64-with-glibc2.10-x86_64-3.8.13-64/
$ PYTHONHASHSEED=1234 python bug.py
$ ls ~/.aesara/compiledir_Linux-4.15--generic-x86_64-with-glibc2.10-x86_64-3.8.13-64/tmp*
/home/matthew/.aesara/compiledir_Linux-4.15--generic-x86_64-with-glibc2.10-x86_64-3.8.13-64/tmp7bj_c23_:
__init__.py  key.pkl  m5d69910cf9a4555423d5e68cc8357d194094f92b3878920c98b2a24e728523c5.so  mod.cpp  __pycache__

/home/matthew/.aesara/compiledir_Linux-4.15--generic-x86_64-with-glibc2.10-x86_64-3.8.13-64/tmpmrtrbg48:
__init__.py  key.pkl  mba10987274f369529454d4a60996746c3927712a0b1b3928446a3c275151e2ee.so  mod.cpp  __pycache__
$ PYTHONHASHSEED=1234 python bug.py
$ ls ~/.aesara/compiledir_Linux-4.15--generic-x86_64-with-glibc2.10-x86_64-3.8.13-64/tmp*
/home/matthew/.aesara/compiledir_Linux-4.15--generic-x86_64-with-glibc2.10-x86_64-3.8.13-64/tmp7bj_c23_:
__init__.py  key.pkl  m5d69910cf9a4555423d5e68cc8357d194094f92b3878920c98b2a24e728523c5.so  mod.cpp  __pycache__

/home/matthew/.aesara/compiledir_Linux-4.15--generic-x86_64-with-glibc2.10-x86_64-3.8.13-64/tmpmrtrbg48:
__init__.py  key.pkl  mba10987274f369529454d4a60996746c3927712a0b1b3928446a3c275151e2ee.so  mod.cpp  __pycache__
```

I believe this is due to parts of the key being computed using python's `hash` which by default is seeded differently for each new process, for example [ExternalCOp.c_code_cache_version](https://github.com/aesara-devs/aesara/blob/main/aesara/link/c/op.py#L451).


## Versions and main components

* Aesara version:2.7.9
* Aesara config (`python -c ""import aesara; print(aesara.config)""`)  [aesara_config.txt](https://github.com/aesara-devs/aesara/files/9269795/aesara_config.txt)
* Python version: Python 3.8.13
* Operating system: Ubuntu 18.04.6 LTS
* How did you install Aesara: conda
",bug help wanted important C-backend,,2022-08-05 15:10:39,2022-08-12 00:40:07,"brandonwillard labeled 2022-08-06 06:13:35,brandonwillard labeled 2022-08-06 06:13:35,brandonwillard labeled 2022-08-06 06:13:35,brandonwillard labeled 2022-08-06 06:13:35,brandonwillard connected 2022-08-11 18:23:40,brandonwillard closed 2022-08-12 00:40:07",rlouf brandonwillard mattearllongshot,3
845,1098,Remove `CholeskyGrad` Op,purna135,"I'm wondering if we should remove the [CholeskyGrad](https://github.com/aesara-devs/aesara/blob/main/aesara/tensor/slinalg.py#L126-L190) Op entirely because we have a working [L_Op](https://github.com/aesara-devs/aesara/blob/main/aesara/tensor/slinalg.py#L70) inside the `Cholesky` Op and are no longer using `CholeskyGrad` Op.

Thanks to @ricardoV94 for sharing this reference (https://github.com/aesara-devs/aesara/commit/1c8f8d6ace92683359ef74401e57a7b09a209f7e), which shows that `CholeskyGrad` was used in the original implementation of `Cholesky` but was later removed.",help wanted refactor,,2022-08-05 15:15:38,2022-11-24 11:25:21,"ricardoV94 mentioned 2022-08-05 15:15:38,ricardoV94 subscribed 2022-08-05 15:15:38,brandonwillard labeled 2022-08-06 17:26:03,brandonwillard labeled 2022-08-06 17:26:03,brandonwillard milestoned 2022-11-17 16:23:14,rlouf closed 2022-11-24 11:25:22",purna135 rlouf sudarsan2k5 brandonwillard ricardoV94,3
848,1101,"Missing `c_code_cache_version` in `grad(sum(a), a)` graph",mattearllongshot,"## Description of your problem or feature request

With this graph we do a compilation for each process that runs (storing it for the duration of the process), rather than fully utilizing the cache.  This can take a while if launching many processes.

Write the following to `bug.py`:
```python
import aesara
import aesara.tensor as at

a = at.vector()
aesara.function([a], [at.grad(at.sum(a), a)])

import time
print('sleeping')
time.sleep(1000)
```

Run the following:
```bash
$ rm -r ~/.aesara/compiledir_Linux-4.15--generic-x86_64-with-glibc2.10-x86_64-3.8.13-64/
$ AESARA_FLAGS=""cmodule__warn_no_version=True"" python bug.py
WARNING (aesara.link.c.cmodule): not all the following op(s) implement c_code_cache_version(). This makes them recompiled for each process.[<aesara.tensor.basic.Alloc object at 0x7efdc902fe20>]
sleeping
^Z
[2]+  Stopped                 AESARA_FLAGS=""cmodule__warn_no_version=True"" python bug.py
$ ls ~/.aesara/compiledir_Linux-4.15--generic-x86_64-with-glibc2.10-x86_64-3.8.13-64/tmp*
/home/matthew/.aesara/compiledir_Linux-4.15--generic-x86_64-with-glibc2.10-x86_64-3.8.13-64/tmpagqncksm:
__init__.py  key.pkl  mba10987274f369529454d4a60996746c3927712a0b1b3928446a3c275151e2ee.so  mod.cpp  __pycache__

/home/matthew/.aesara/compiledir_Linux-4.15--generic-x86_64-with-glibc2.10-x86_64-3.8.13-64/tmpb0w3p1wz:
__init__.py  key.pkl  m6c1526e289fbc101e04dd872f32684efbca30d864c16c82ff681a372c78477f7.so  mod.cpp  __pycache__

/home/matthew/.aesara/compiledir_Linux-4.15--generic-x86_64-with-glibc2.10-x86_64-3.8.13-64/tmpimzptk_5:
__init__.py  mb6a3fac88303d6c4edebfbc631c7a259b4e25534130e9774504486e712d37215.so  mod.cpp  __pycache__
$
```

The dir with `mb6a3fac88303d6c4edebfbc631c7a259b4e25534130e9774504486e712d37215.so` in it is missing a key.pkl file, and is recompiled for each new proc that runs.  The warning message suggests that it is a case of adding a missing `c_code_cache_version` method for part of the graph.  Is this possible?

## Versions and main components

* Aesara version:2.7.9
* Aesara config (`python -c ""import aesara; print(aesara.config)""`)  [aesara_config.txt](https://github.com/aesara-devs/aesara/files/9269795/aesara_config.txt)
* Python version: Python 3.8.13
* Operating system: Ubuntu 18.04.6 LTS
* How did you install Aesara: conda",bug help wanted important C-backend,,2022-08-06 09:14:06,2022-08-12 00:40:07,"brandonwillard labeled 2022-08-06 17:22:19,brandonwillard labeled 2022-08-06 17:22:19,brandonwillard labeled 2022-08-06 17:22:19,brandonwillard labeled 2022-08-06 17:22:19,brandonwillard renamed 2022-08-06 17:26:34,brandonwillard closed 2022-08-12 00:40:07",brandonwillard mattearllongshot,2
854,1114,DefaultRandomState is not updated in .eval,ferrine,"## Description of your problem
Not sure if it is the desired behaviour or just a bug

```python
r = at.random.uniform(size=10)
print(r.eval())
print(r.eval())
```
<img width=""592"" alt=""image"" src=""https://user-images.githubusercontent.com/11705326/184608564-84dd5181-156d-4fa7-adbf-8393d6c08b1d.png"">

I noticed that when I use non-default rng, this works as I expect it to
<img width=""652"" alt=""image"" src=""https://user-images.githubusercontent.com/11705326/184608748-d1d06529-8185-4c0c-a184-271c38154037.png"">
",,,2022-08-15 09:13:28,2022-08-15 09:45:09,ferrine closed 2022-08-15 09:45:09,ferrine ricardoV94,1
855,1116,Fuse `CAReduce`s and `Elemwise`s,brandonwillard,"We aren't currently converting `CAReduce`s of `Elemwise`s into single `CAReduce`s:

```python
import aesara
import aesara.tensor as at


x = at.vector(""x"")
y = at.exp(x).sum()

y_fn = aesara.function([x], y)

aesara.dprint(y_fn)
# Sum{acc_dtype=float64} [id A] 1
#  |Elemwise{exp,no_inplace} [id B] 0
#    |x [id C]
```

As the example shows, we first compute the `Elemwise` then immediately reduce it. Instead, we could fuse the `CAReduce` and `Elemwise` scalar `Op`s (i.e. create a `Composite` scalar `Op` corresponding to `add(exp(x), exp(y))`) and use that in a single `CAReduce` node that avoids the need for intermediate storage.

The rewrites required to do this would incorporate most of the same logic from `aesara.tensor.basic_opt.FusionOptimizer`, but, instead of producing another `Elemwise`, it would result in a new `CAReduce`.
",enhancement help wanted important graph rewriting performance concern,,2022-08-15 17:05:11,2022-11-22 15:57:40,"brandonwillard labeled 2022-08-15 17:05:11,brandonwillard labeled 2022-08-15 17:05:11,brandonwillard labeled 2022-08-15 17:05:11,brandonwillard labeled 2022-08-15 17:05:11,brandonwillard labeled 2022-08-15 17:05:11,aseyboldt mentioned 2022-08-15 17:06:19,aseyboldt subscribed 2022-08-15 17:06:19,brandonwillard connected 2022-11-04 22:38:55,brandonwillard closed 2022-11-22 15:57:40",aseyboldt brandonwillard,1
856,1118,Simplify `Sum`s of `MakeVector`s and `Join`s,brandonwillard,"It looks like we're unnecessarily creating arrays for useless `MakeVector`s and `Join`s that are only ever `Sum`ed.

For example:
```python
import aesara
import aesara.tensor as at


a = at.scalar(""a"")
b = at.scalar(""b"")
z = at.as_tensor([a, b]).sum()

aesara.dprint(z)
# Sum{acc_dtype=float64} [id A]
#  |MakeVector{dtype='float64'} [id B]
#    |a [id C]
#    |b [id D]

z_fn = aesara.function([a, b], z)

aesara.dprint(z_fn)
# Sum{acc_dtype=float64} [id A] 1
#  |MakeVector{dtype='float64'} [id B] 0
#    |a [id C]
#    |b [id D]


X = at.matrix(""X"")
Y = at.matrix(""X"")
z = at.as_tensor([X, Y]).sum()

aesara.dprint(z)
# Sum{acc_dtype=float64} [id A]
#  |Join [id B]
#    |TensorConstant{0} [id C]
#    |InplaceDimShuffle{x,0,1} [id D]
#    | |X [id E]
#    |InplaceDimShuffle{x,0,1} [id F]
#      |X [id G]

z_fn = aesara.function([X, Y], z)

aesara.dprint(z_fn)
# Sum{acc_dtype=float64} [id A] 3
#  |Join [id B] 2
#    |TensorConstant{0} [id C]
#    |InplaceDimShuffle{x,0,1} [id D] 1
#    | |X [id E]
#    |InplaceDimShuffle{x,0,1} [id F] 0
#      |X [id G]
```
We should be able to remove the `MakeVector`s and `Join`s and simply sum their arguments.",duplicate help wanted important graph rewriting performance concern,,2022-08-16 23:05:36,2022-08-17 14:54:35,"brandonwillard labeled 2022-08-16 23:05:36,brandonwillard labeled 2022-08-16 23:05:36,brandonwillard labeled 2022-08-16 23:05:36,brandonwillard labeled 2022-08-16 23:05:36,brandonwillard closed 2022-08-17 14:54:35,brandonwillard labeled 2022-08-17 14:54:42",ricardoV94 brandonwillard,3
857,1123,Provide a C implementation for `TensorFromScalar`,brandonwillard,"As noted in #1119, `TensorFromScalar` does not have a C implementation, and it could really use one.",enhancement important C-backend Op implementation,,2022-08-17 21:44:34,2022-08-20 17:45:12,"brandonwillard labeled 2022-08-17 21:44:34,brandonwillard labeled 2022-08-17 21:44:34,brandonwillard labeled 2022-08-17 21:44:34,brandonwillard labeled 2022-08-17 21:44:34,brandonwillard closed 2022-08-20 17:45:12",brandonwillard,0
859,1125,Large number of test failures with numpy 1.23.2,mgorny,"## Description of your problem or feature request

When running aesara's test suite against numpy 1.23.2, the following tests fail:

<details><summary>List of test failures (213)</summary>

```
FAILED tests/link/c/test_op.py::test_ExternalCOp_c_code_cache_version - ValueError: too many values to unpack (expected 3)
FAILED tests/tensor/test_blas.py::TestBlasStrides::test_dot22 - ValueError: some matrix has no unit stride
FAILED tests/sparse/sandbox/test_sp.py::TestSP::test_multilayer_conv - ValueError: Inexact indices into sparse matrices are not allowed
FAILED tests/tensor/test_blas.py::TestBlasStrides::test_dot22scalar - ValueError: some matrix has no unit stride
FAILED tests/sparse/sandbox/test_sp.py::TestSP::test_maxpool - ValueError: Inexact indices into sparse matrices are not allowed
FAILED tests/tensor/test_blas.py::TestBlasStrides::test_gemm - ValueError: some matrix has no unit stride
FAILED tests/tensor/test_blas_c.py::TestBlasStrides::test_dot22 - ValueError: some matrix has no unit stride
FAILED tests/tensor/test_blas_c.py::TestBlasStrides::test_dot22scalar - ValueError: some matrix has no unit stride
FAILED tests/tensor/test_blas_c.py::TestBlasStrides::test_gemm - ValueError: some matrix has no unit stride
FAILED tests/tensor/test_blas_c.py::TestBlasStridesC::test_dot22 - ValueError: some matrix has no unit stride
FAILED tests/tensor/test_blas_c.py::TestBlasStridesC::test_dot22scalar - ValueError: some matrix has no unit stride
FAILED tests/tensor/test_blas_c.py::TestBlasStridesC::test_gemm - ValueError: some matrix has no unit stride
FAILED tests/tensor/test_blas_scipy.py::TestBlasStrides::test_dot22 - ValueError: some matrix has no unit stride
FAILED tests/tensor/test_blas_scipy.py::TestBlasStrides::test_dot22scalar - ValueError: some matrix has no unit stride
FAILED tests/tensor/test_blas_scipy.py::TestBlasStrides::test_gemm - ValueError: some matrix has no unit stride
FAILED tests/tensor/test_blas_scipy.py::TestBlasStridesScipy::test_dot22 - ValueError: some matrix has no unit stride
FAILED tests/tensor/test_blas_scipy.py::TestBlasStridesScipy::test_dot22scalar - ValueError: some matrix has no unit stride
FAILED tests/tensor/test_blas_scipy.py::TestBlasStridesScipy::test_gemm - ValueError: some matrix has no unit stride
FAILED tests/tensor/test_math.py::TestDenseDot::test_good - ValueError: ('some matrix has no unit stride\\nApply node that caused the...
FAILED tests/tensor/test_blas.py::TestGemm::test_shape_0 - ValueError: some matrix has no unit stride
FAILED tests/tensor/test_sort.py::TestTopK::test_topk_1d[16--10-float64-False] - IndexError: only integers, slices (`:`), ellipsis (...
FAILED tests/tensor/test_sort.py::TestTopK::test_topk_1d[16--10-float16-False] - IndexError: only integers, slices (`:`), ellipsis (...
FAILED tests/tensor/test_sort.py::TestTopK::test_topk_1d[16--10-int16-False] - IndexError: only integers, slices (`:`), ellipsis (`....
FAILED tests/tensor/test_sort.py::TestTopK::test_topk_1d[16--10-int8-False] - IndexError: only integers, slices (`:`), ellipsis (`.....
FAILED tests/tensor/test_sort.py::TestTopK::test_topk_1d[16-n//2-float64-False] - IndexError: only integers, slices (`:`), ellipsis ...
FAILED tests/tensor/test_sort.py::TestTopK::test_topk_1d[16-n//2-float16-False] - IndexError: only integers, slices (`:`), ellipsis ...
FAILED tests/tensor/test_sort.py::TestTopK::test_topk_1d[16-n//2-int16-False] - IndexError: only integers, slices (`:`), ellipsis (`...
FAILED tests/tensor/test_sort.py::TestTopK::test_topk_1d[16-n//2-int8-False] - IndexError: only integers, slices (`:`), ellipsis (`....
FAILED tests/tensor/test_sort.py::TestTopK::test_topk_1d[16-n-1-float64-False] - IndexError: only integers, slices (`:`), ellipsis (...
FAILED tests/tensor/test_sort.py::TestTopK::test_topk_1d[16-n-1-float16-False] - IndexError: only integers, slices (`:`), ellipsis (...
FAILED tests/tensor/test_sort.py::TestTopK::test_topk_1d[16-n-1-int16-False] - IndexError: only integers, slices (`:`), ellipsis (`....
FAILED tests/tensor/test_sort.py::TestTopK::test_topk_1d[16-n-1-int8-False] - IndexError: only integers, slices (`:`), ellipsis (`.....
FAILED tests/tensor/test_sort.py::TestTopK::test_topk_1d[16-1-n-float64-False] - IndexError: only integers, slices (`:`), ellipsis (...
FAILED tests/tensor/test_sort.py::TestTopK::test_topk_1d[16-1-n-float16-False] - IndexError: only integers, slices (`:`), ellipsis (...
FAILED tests/tensor/test_sort.py::TestTopK::test_topk_1d[16-1-n-int16-False] - IndexError: only integers, slices (`:`), ellipsis (`....
FAILED tests/tensor/test_sort.py::TestTopK::test_topk_1d[16-1-n-int8-False] - IndexError: only integers, slices (`:`), ellipsis (`.....
FAILED tests/tensor/test_sort.py::TestTopK::test_topk_1d[61--10-float64-False] - IndexError: only integers, slices (`:`), ellipsis (...
FAILED tests/tensor/test_sort.py::TestTopK::test_topk_1d[61--10-float16-False] - IndexError: only integers, slices (`:`), ellipsis (...
FAILED tests/tensor/test_sort.py::TestTopK::test_topk_1d[61--10-int16-False] - IndexError: only integers, slices (`:`), ellipsis (`....
FAILED tests/tensor/test_sort.py::TestTopK::test_topk_1d[61--10-int8-False] - IndexError: only integers, slices (`:`), ellipsis (`.....
FAILED tests/tensor/test_sort.py::TestTopK::test_topk_1d[61-n//2-float64-False] - IndexError: only integers, slices (`:`), ellipsis ...
FAILED tests/tensor/test_sort.py::TestTopK::test_topk_1d[61-n//2-float16-False] - IndexError: only integers, slices (`:`), ellipsis ...
FAILED tests/tensor/test_sort.py::TestTopK::test_topk_1d[61-n//2-int16-False] - IndexError: only integers, slices (`:`), ellipsis (`...
FAILED tests/tensor/test_sort.py::TestTopK::test_topk_1d[61-n//2-int8-False] - IndexError: only integers, slices (`:`), ellipsis (`....
FAILED tests/tensor/test_sort.py::TestTopK::test_topk_1d[61-n-1-float64-False] - IndexError: only integers, slices (`:`), ellipsis (...
FAILED tests/tensor/test_sort.py::TestTopK::test_topk_1d[61-n-1-float16-False] - IndexError: only integers, slices (`:`), ellipsis (...
FAILED tests/tensor/test_sort.py::TestTopK::test_topk_1d[61-n-1-int16-False] - IndexError: only integers, slices (`:`), ellipsis (`....
FAILED tests/tensor/test_sort.py::TestTopK::test_topk_1d[61-n-1-int8-False] - IndexError: only integers, slices (`:`), ellipsis (`.....
FAILED tests/tensor/test_sort.py::TestTopK::test_argtopk_1d[16--10-float32-False-int32] - IndexError: only integers, slices (`:`), e...
FAILED tests/tensor/test_sort.py::TestTopK::test_topk_1d[61-1-n-float64-False] - IndexError: only integers, slices (`:`), ellipsis (...
FAILED tests/tensor/test_sort.py::TestTopK::test_argtopk_1d[16--10-float32-False-int64] - IndexError: only integers, slices (`:`), e...
FAILED tests/tensor/test_sort.py::TestTopK::test_topk_1d[61-1-n-float16-False] - IndexError: only integers, slices (`:`), ellipsis (...
FAILED tests/tensor/test_sort.py::TestTopK::test_argtopk_1d[16--10-int32-False-int32] - IndexError: only integers, slices (`:`), ell...
FAILED tests/tensor/test_sort.py::TestTopK::test_topk_1d[61-1-n-int16-False] - IndexError: only integers, slices (`:`), ellipsis (`....
FAILED tests/tensor/test_sort.py::TestTopK::test_argtopk_1d[16--10-int32-False-int64] - IndexError: only integers, slices (`:`), ell...
FAILED tests/tensor/test_sort.py::TestTopK::test_topk_1d[61-1-n-int8-False] - IndexError: only integers, slices (`:`), ellipsis (`.....
FAILED tests/tensor/test_sort.py::TestTopK::test_argtopk_1d[16-n//2-float32-False-int32] - IndexError: only integers, slices (`:`), ...
FAILED tests/tensor/test_sort.py::TestTopK::test_topk_1d[257--10-float64-False] - IndexError: only integers, slices (`:`), ellipsis ...
FAILED tests/tensor/test_sort.py::TestTopK::test_argtopk_1d[16-n//2-float32-False-int64] - IndexError: only integers, slices (`:`), ...
FAILED tests/tensor/test_sort.py::TestTopK::test_argtopk_1d[16-n//2-int32-False-int32] - IndexError: only integers, slices (`:`), el...
FAILED tests/tensor/test_sort.py::TestTopK::test_topk_1d[257--10-float16-False] - IndexError: only integers, slices (`:`), ellipsis ...
FAILED tests/tensor/test_sort.py::TestTopK::test_argtopk_1d[16-n//2-int32-False-int64] - IndexError: only integers, slices (`:`), el...
FAILED tests/tensor/test_sort.py::TestTopK::test_topk_1d[257--10-int16-False] - IndexError: only integers, slices (`:`), ellipsis (`...
FAILED tests/tensor/test_sort.py::TestTopK::test_argtopk_1d[16-n-1-float32-False-int32] - IndexError: only integers, slices (`:`), e...
FAILED tests/tensor/test_sort.py::TestTopK::test_topk_1d[257--10-int8-False] - IndexError: only integers, slices (`:`), ellipsis (`....
FAILED tests/tensor/test_sort.py::TestTopK::test_argtopk_1d[16-n-1-float32-False-int64] - IndexError: only integers, slices (`:`), e...
FAILED tests/tensor/test_sort.py::TestTopK::test_topk_1d[257-n//2-float64-False] - IndexError: only integers, slices (`:`), ellipsis...
FAILED tests/tensor/test_sort.py::TestTopK::test_argtopk_1d[16-n-1-int32-False-int32] - IndexError: only integers, slices (`:`), ell...
FAILED tests/tensor/test_sort.py::TestTopK::test_argtopk_1d[16-n-1-int32-False-int64] - IndexError: only integers, slices (`:`), ell...
FAILED tests/tensor/test_sort.py::TestTopK::test_topk_1d[257-n//2-float16-False] - IndexError: only integers, slices (`:`), ellipsis...
FAILED tests/tensor/test_sort.py::TestTopK::test_topk_1d[257-n//2-int16-False] - IndexError: only integers, slices (`:`), ellipsis (...
FAILED tests/tensor/test_sort.py::TestTopK::test_combined_1d[61-10-int32-False-int32] - IndexError: only integers, slices (`:`), ell...
FAILED tests/tensor/test_sort.py::TestTopK::test_argtopk_1d[61--10-float32-False-int32] - IndexError: only integers, slices (`:`), e...
FAILED tests/tensor/test_sort.py::TestTopK::test_combined_1d[61-10-int32-False-int64] - IndexError: only integers, slices (`:`), ell...
FAILED tests/tensor/test_sort.py::TestTopK::test_topk_1d[257-n//2-int8-False] - IndexError: only integers, slices (`:`), ellipsis (`...
FAILED tests/tensor/test_sort.py::TestTopK::test_argtopk_1d[61--10-float32-False-int64] - IndexError: only integers, slices (`:`), e...
FAILED tests/tensor/test_sort.py::TestTopK::test_combined_1d[61-n//2-float32-False-int32] - IndexError: only integers, slices (`:`),...
FAILED tests/tensor/test_sort.py::TestTopK::test_topk_1d[257-n-1-float64-False] - IndexError: only integers, slices (`:`), ellipsis ...
FAILED tests/tensor/test_sort.py::TestTopK::test_argtopk_1d[61--10-int32-False-int32] - IndexError: only integers, slices (`:`), ell...
FAILED tests/tensor/test_sort.py::TestTopK::test_combined_1d[61-n//2-float32-False-int64] - IndexError: only integers, slices (`:`),...
FAILED tests/tensor/test_sort.py::TestTopK::test_argtopk_1d[61--10-int32-False-int64] - IndexError: only integers, slices (`:`), ell...
FAILED tests/tensor/test_sort.py::TestTopK::test_topk_1d[257-n-1-float16-False] - IndexError: only integers, slices (`:`), ellipsis ...
FAILED tests/tensor/test_sort.py::TestTopK::test_argtopk_nd[shp4-(1+n)//2-float32-False-int32] - IndexError: only integers, slices (...
FAILED tests/tensor/test_sort.py::TestTopK::test_combined_1d[61-n//2-int32-False-int32] - IndexError: only integers, slices (`:`), e...
FAILED tests/tensor/test_sort.py::TestTopK::test_argtopk_1d[61-n//2-float32-False-int32] - IndexError: only integers, slices (`:`), ...
FAILED tests/tensor/test_sort.py::TestTopK::test_topk_1d[257-n-1-int16-False] - IndexError: only integers, slices (`:`), ellipsis (`...
FAILED tests/tensor/test_sort.py::TestTopK::test_combined_1d[61-n//2-int32-False-int64] - IndexError: only integers, slices (`:`), e...
FAILED tests/tensor/test_sort.py::TestTopK::test_argtopk_nd[shp5-(1+n)//2-float32-False-int64] - IndexError: only integers, slices (...
FAILED tests/tensor/test_sort.py::TestTopK::test_topk_1d[257-n-1-int8-False] - IndexError: only integers, slices (`:`), ellipsis (`....
FAILED tests/tensor/test_sort.py::TestTopK::test_argtopk_1d[61-n//2-float32-False-int64] - IndexError: only integers, slices (`:`), ...
FAILED tests/tensor/test_sort.py::TestTopK::test_combined_1d[61-n-1-float32-False-int32] - IndexError: only integers, slices (`:`), ...
FAILED tests/tensor/test_sort.py::TestTopK::test_argtopk_nd[shp6-(1+n)//2-int32-False-int32] - IndexError: only integers, slices (`:...
FAILED tests/tensor/test_sort.py::TestTopK::test_argtopk_1d[61-n//2-int32-False-int32] - IndexError: only integers, slices (`:`), el...
FAILED tests/tensor/test_sort.py::TestTopK::test_combined_1d[61-n-1-float32-False-int64] - IndexError: only integers, slices (`:`), ...
FAILED tests/tensor/test_sort.py::TestTopK::test_topk_1d[257-1-n-float64-False] - IndexError: only integers, slices (`:`), ellipsis ...
FAILED tests/tensor/test_sort.py::TestTopK::test_argtopk_nd[shp7-(1+n)//2-int32-False-int64] - IndexError: only integers, slices (`:...
FAILED tests/tensor/test_sort.py::TestTopK::test_argtopk_1d[61-n//2-int32-False-int64] - IndexError: only integers, slices (`:`), el...
FAILED tests/tensor/test_sort.py::TestTopK::test_combined_1d[61-n-1-int32-False-int32] - IndexError: only integers, slices (`:`), el...
FAILED tests/tensor/test_sort.py::TestTopK::test_argtopk_1d[61-n-1-float32-False-int32] - IndexError: only integers, slices (`:`), e...
FAILED tests/tensor/test_sort.py::TestTopK::test_combined_1d[61-n-1-int32-False-int64] - IndexError: only integers, slices (`:`), el...
FAILED tests/tensor/test_sort.py::TestTopK::test_topk_1d[257-1-n-float16-False] - IndexError: only integers, slices (`:`), ellipsis ...
FAILED tests/tensor/test_sort.py::TestTopK::test_argtopk_nd[shp12-1-n-float32-False-int32] - IndexError: only integers, slices (`:`)...
FAILED tests/tensor/test_sort.py::TestTopK::test_combined_1d[61-1-n-float32-False-int32] - IndexError: only integers, slices (`:`), ...
FAILED tests/tensor/test_sort.py::TestTopK::test_argtopk_1d[61-n-1-float32-False-int64] - IndexError: only integers, slices (`:`), e...
FAILED tests/tensor/test_sort.py::TestTopK::test_topk_1d[257-1-n-int16-False] - IndexError: only integers, slices (`:`), ellipsis (`...
FAILED tests/tensor/test_sort.py::TestTopK::test_argtopk_nd[shp13-1-n-float32-False-int64] - IndexError: only integers, slices (`:`)...
FAILED tests/tensor/test_sort.py::TestTopK::test_argtopk_1d[61-n-1-int32-False-int32] - IndexError: only integers, slices (`:`), ell...
FAILED tests/tensor/test_sort.py::TestTopK::test_topk_1d[257-1-n-int8-False] - IndexError: only integers, slices (`:`), ellipsis (`....
FAILED tests/tensor/test_sort.py::TestTopK::test_combined_1d[61-1-n-float32-False-int64] - IndexError: only integers, slices (`:`), ...
FAILED tests/tensor/test_sort.py::TestTopK::test_argtopk_1d[61-n-1-int32-False-int64] - IndexError: only integers, slices (`:`), ell...
FAILED tests/tensor/test_sort.py::TestTopK::test_topk_1d[2049-1337-float64-False] - IndexError: only integers, slices (`:`), ellipsi...
FAILED tests/tensor/test_sort.py::TestTopK::test_combined_1d[61-1-n-int32-False-int32] - IndexError: only integers, slices (`:`), el...
FAILED tests/tensor/test_sort.py::TestTopK::test_argtopk_nd[shp14-1-n-int32-False-int32] - IndexError: only integers, slices (`:`), ...
FAILED tests/tensor/test_sort.py::TestTopK::test_combined_1d[61-1-n-int32-False-int64] - IndexError: only integers, slices (`:`), el...
FAILED tests/tensor/test_sort.py::TestTopK::test_argtopk_nd[shp15-1-n-int32-False-int64] - IndexError: only integers, slices (`:`), ...
FAILED tests/tensor/test_sort.py::TestTopK::test_argtopk_1d[257--10-float32-False-int32] - IndexError: only integers, slices (`:`), ...
FAILED tests/tensor/test_sort.py::TestTopK::test_combined_1d[16-10-float32-False-int32] - IndexError: only integers, slices (`:`), e...
FAILED tests/tensor/test_sort.py::TestTopK::test_combined_1d[16-10-float32-False-int64] - IndexError: only integers, slices (`:`), e...
FAILED tests/tensor/test_sort.py::TestTopK::test_combined_1d[257-10-float32-False-int32] - IndexError: only integers, slices (`:`), ...
FAILED tests/tensor/test_sort.py::TestTopK::test_argtopk_1d[257--10-float32-False-int64] - IndexError: only integers, slices (`:`), ...
FAILED tests/tensor/test_sort.py::TestTopK::test_combined_1d[16-10-int32-False-int32] - IndexError: only integers, slices (`:`), ell...
FAILED tests/tensor/test_sort.py::TestTopK::test_combined_1d[257-10-float32-False-int64] - IndexError: only integers, slices (`:`), ...
FAILED tests/tensor/test_sort.py::TestTopK::test_argtopk_1d[257--10-int32-False-int32] - IndexError: only integers, slices (`:`), el...
FAILED tests/tensor/test_sort.py::TestTopK::test_argtopk_1d[257--10-int32-False-int64] - IndexError: only integers, slices (`:`), el...
FAILED tests/tensor/test_sort.py::TestTopK::test_combined_1d[257-10-int32-False-int32] - IndexError: only integers, slices (`:`), el...
FAILED tests/tensor/test_sort.py::TestTopK::test_combined_1d[16-10-int32-False-int64] - IndexError: only integers, slices (`:`), ell...
FAILED tests/tensor/test_sort.py::TestTopK::test_argtopk_nd[shp20-(1+n)//2-float32-False-int32] - IndexError: only integers, slices ...
FAILED tests/tensor/test_sort.py::TestTopK::test_combined_1d[16-n//2-float32-False-int32] - IndexError: only integers, slices (`:`),...
FAILED tests/tensor/test_sort.py::TestTopK::test_combined_1d[257-10-int32-False-int64] - IndexError: only integers, slices (`:`), el...
FAILED tests/tensor/test_sort.py::TestTopK::test_argtopk_1d[257-n//2-float32-False-int32] - IndexError: only integers, slices (`:`),...
FAILED tests/tensor/test_sort.py::TestTopK::test_combined_1d[16-n//2-float32-False-int64] - IndexError: only integers, slices (`:`),...
FAILED tests/tensor/test_sort.py::TestTopK::test_combined_1d[16-n//2-int32-False-int32] - IndexError: only integers, slices (`:`), e...
FAILED tests/tensor/test_sort.py::TestTopK::test_combined_1d[257-n//2-float32-False-int32] - IndexError: only integers, slices (`:`)...
FAILED tests/tensor/test_sort.py::TestTopK::test_argtopk_1d[257-n//2-float32-False-int64] - IndexError: only integers, slices (`:`),...
FAILED tests/tensor/test_sort.py::TestTopK::test_argtopk_nd[shp21-(1+n)//2-float32-False-int64] - IndexError: only integers, slices ...
FAILED tests/tensor/test_sort.py::TestTopK::test_combined_1d[257-n//2-float32-False-int64] - IndexError: only integers, slices (`:`)...
FAILED tests/tensor/test_sort.py::TestTopK::test_argtopk_1d[257-n//2-int32-False-int32] - IndexError: only integers, slices (`:`), e...
FAILED tests/tensor/test_sort.py::TestTopK::test_combined_1d[16-n//2-int32-False-int64] - IndexError: only integers, slices (`:`), e...
FAILED tests/tensor/test_sort.py::TestTopK::test_combined_1d[257-n//2-int32-False-int32] - IndexError: only integers, slices (`:`), ...
FAILED tests/tensor/test_sort.py::TestTopK::test_argtopk_1d[257-n//2-int32-False-int64] - IndexError: only integers, slices (`:`), e...
FAILED tests/tensor/test_sort.py::TestTopK::test_combined_1d[16-n-1-float32-False-int32] - IndexError: only integers, slices (`:`), ...
FAILED tests/tensor/test_sort.py::TestTopK::test_argtopk_nd[shp22-(1+n)//2-int32-False-int32] - IndexError: only integers, slices (`...
FAILED tests/tensor/test_sort.py::TestTopK::test_combined_1d[16-n-1-float32-False-int64] - IndexError: only integers, slices (`:`), ...
FAILED tests/tensor/test_sort.py::TestTopK::test_combined_1d[257-n//2-int32-False-int64] - IndexError: only integers, slices (`:`), ...
FAILED tests/tensor/test_sort.py::TestTopK::test_argtopk_1d[257-n-1-float32-False-int32] - IndexError: only integers, slices (`:`), ...
FAILED tests/tensor/test_sort.py::TestTopK::test_combined_1d[16-n-1-int32-False-int32] - IndexError: only integers, slices (`:`), el...
FAILED tests/tensor/test_sort.py::TestTopK::test_argtopk_nd[shp23-(1+n)//2-int32-False-int64] - IndexError: only integers, slices (`...
FAILED tests/tensor/test_sort.py::TestTopK::test_combined_1d[257-n-1-float32-False-int32] - IndexError: only integers, slices (`:`),...
FAILED tests/tensor/test_sort.py::TestTopK::test_argtopk_1d[257-n-1-float32-False-int64] - IndexError: only integers, slices (`:`), ...
FAILED tests/tensor/test_sort.py::TestTopK::test_combined_1d[16-n-1-int32-False-int64] - IndexError: only integers, slices (`:`), el...
FAILED tests/tensor/test_sort.py::TestTopK::test_combined_1d[257-n-1-float32-False-int64] - IndexError: only integers, slices (`:`),...
FAILED tests/tensor/test_sort.py::TestTopK::test_argtopk_1d[257-n-1-int32-False-int32] - IndexError: only integers, slices (`:`), el...
FAILED tests/tensor/test_sort.py::TestTopK::test_combined_1d[16-1-n-float32-False-int32] - IndexError: only integers, slices (`:`), ...
FAILED tests/tensor/test_sort.py::TestTopK::test_combined_1d[257-n-1-int32-False-int32] - IndexError: only integers, slices (`:`), e...
FAILED tests/tensor/test_sort.py::TestTopK::test_argtopk_1d[257-n-1-int32-False-int64] - IndexError: only integers, slices (`:`), el...
FAILED tests/tensor/test_sort.py::TestTopK::test_combined_1d[16-1-n-float32-False-int64] - IndexError: only integers, slices (`:`), ...
FAILED tests/tensor/test_sort.py::TestTopK::test_combined_1d[257-n-1-int32-False-int64] - IndexError: only integers, slices (`:`), e...
FAILED tests/tensor/test_sort.py::TestTopK::test_combined_1d[16-1-n-int32-False-int32] - IndexError: only integers, slices (`:`), el...
FAILED tests/tensor/test_sort.py::TestTopK::test_argtopk_1d[2049-1337-float32-False-int32] - IndexError: only integers, slices (`:`)...
FAILED tests/tensor/test_sort.py::TestTopK::test_combined_1d[16-1-n-int32-False-int64] - IndexError: only integers, slices (`:`), el...
FAILED tests/tensor/test_sort.py::TestTopK::test_argtopk_nd[shp28-1-n-float32-False-int32] - IndexError: only integers, slices (`:`)...
FAILED tests/tensor/test_sort.py::TestTopK::test_combined_1d[257-1-n-float32-False-int32] - IndexError: only integers, slices (`:`),...
FAILED tests/tensor/test_sort.py::TestTopK::test_grad[False-1-n-shp0] - IndexError: only integers, slices (`:`), ellipsis (`...`), n...
FAILED tests/tensor/test_sort.py::TestTopK::test_combined_1d[257-1-n-float32-False-int64] - IndexError: only integers, slices (`:`),...
FAILED tests/tensor/test_sort.py::TestTopK::test_combined_1d[61-10-float32-False-int32] - IndexError: only integers, slices (`:`), e...
FAILED tests/tensor/test_sort.py::TestTopK::test_grad[False-1-n-shp1] - IndexError: only integers, slices (`:`), ellipsis (`...`), n...
FAILED tests/tensor/test_sort.py::TestTopK::test_combined_1d[257-1-n-int32-False-int32] - IndexError: only integers, slices (`:`), e...
FAILED tests/tensor/test_sort.py::TestTopK::test_combined_1d[61-10-float32-False-int64] - IndexError: only integers, slices (`:`), e...
FAILED tests/tensor/test_sort.py::TestTopK::test_argtopk_nd[shp29-1-n-float32-False-int64] - IndexError: only integers, slices (`:`)...
FAILED tests/tensor/test_sort.py::TestTopK::test_combined_1d[257-1-n-int32-False-int64] - IndexError: only integers, slices (`:`), e...
FAILED tests/tensor/test_sort.py::TestTopK::test_grad[False-1-n-shp2] - IndexError: only integers, slices (`:`), ellipsis (`...`), n...
FAILED tests/tensor/test_sort.py::TestTopK::test_combined_1d[2049-1337-float32-False-int32] - IndexError: only integers, slices (`:`...
FAILED tests/tensor/test_sort.py::TestTopK::test_argtopk_nd[shp30-1-n-int32-False-int32] - IndexError: only integers, slices (`:`), ...
FAILED tests/tensor/test_sort.py::TestTopK::test_argtopk_1d_collision[18-n//2-int32-False] - IndexError: only integers, slices (`:`)...
FAILED tests/tensor/test_sort.py::TestTopK::test_argtopk_nd[shp31-1-n-int32-False-int64] - IndexError: only integers, slices (`:`), ...
FAILED tests/tensor/test_sort.py::TestTopK::test_argtopk_1d_collision[18-n//2-float32-False] - IndexError: only integers, slices (`:...
FAILED tests/tensor/test_sort.py::TestTopK::test_argtopk_1d_collision[62-n//2-int32-False] - IndexError: only integers, slices (`:`)...
FAILED tests/tensor/test_sort.py::TestTopK::test_argtopk_nd[shp36-(1+n)//2-float32-False-int32] - IndexError: only integers, slices ...
FAILED tests/tensor/test_sort.py::TestTopK::test_argtopk_1d_collision[62-n//2-float32-False] - IndexError: only integers, slices (`:...
FAILED tests/tensor/test_sort.py::TestTopK::test_argtopk_nd[shp37-(1+n)//2-float32-False-int64] - IndexError: only integers, slices ...
FAILED tests/tensor/test_sort.py::TestTopK::test_argtopk_1d_collision[258-n//2-int32-False] - IndexError: only integers, slices (`:`...
FAILED tests/tensor/test_sort.py::TestTopK::test_argtopk_nd[shp38-(1+n)//2-int32-False-int32] - IndexError: only integers, slices (`...
FAILED tests/tensor/test_sort.py::TestTopK::test_argtopk_1d_collision[258-n//2-float32-False] - IndexError: only integers, slices (`...
FAILED tests/tensor/test_sort.py::TestTopK::test_argtopk_nd[shp39-(1+n)//2-int32-False-int64] - IndexError: only integers, slices (`...
FAILED tests/tensor/test_sort.py::TestTopK::test_argtopk_1d_collision[2048-1337-float32-False] - IndexError: only integers, slices (...
FAILED tests/tensor/test_sort.py::TestTopK::test_argtopk_nd[shp44-1-n-float32-False-int32] - IndexError: only integers, slices (`:`)...
FAILED tests/tensor/test_sort.py::TestTopK::test_argtopk_nd[shp45-1-n-float32-False-int64] - IndexError: only integers, slices (`:`)...
FAILED tests/tensor/test_sort.py::TestTopK::test_argtopk_nd[shp46-1-n-int32-False-int32] - IndexError: only integers, slices (`:`), ...
FAILED tests/tensor/test_sort.py::TestTopK::test_argtopk_nd[shp47-1-n-int32-False-int64] - IndexError: only integers, slices (`:`), ...
FAILED tests/tensor/test_sort.py::TestTopK::test_grad[False-1-n-shp3] - IndexError: only integers, slices (`:`), ellipsis (`...`), n...
FAILED tests/tensor/test_subtensor.py::TestSubtensor::test_boolean - IndexError: only integers, slices (`:`), ellipsis (`...`), nump...
FAILED tests/tensor/test_sort.py::TestTopKInferShape::test_combined_infer_shape[(1+n)//2-shp0] - IndexError: only integers, slices (...
FAILED tests/tensor/test_sort.py::TestTopKInferShape::test_combined_infer_shape[(1+n)//2-shp1] - IndexError: only integers, slices (...
FAILED tests/tensor/test_sort.py::TestTopKInferShape::test_combined_infer_shape[(1+n)//2-shp2] - IndexError: only integers, slices (...
FAILED tests/tensor/test_sort.py::TestTopKInferShape::test_combined_infer_shape[(1+n)//2-shp3] - IndexError: only integers, slices (...
FAILED tests/tensor/test_sort.py::TestTopKInferShape::test_combined_infer_shape[(1+n)//2-shp4] - IndexError: only integers, slices (...
FAILED tests/tensor/test_sort.py::TestTopKInferShape::test_combined_infer_shape[n-1-shp0] - IndexError: only integers, slices (`:`),...
FAILED tests/tensor/test_sort.py::TestTopKInferShape::test_combined_infer_shape[n-1-shp1] - IndexError: only integers, slices (`:`),...
FAILED tests/tensor/test_sort.py::TestTopKInferShape::test_combined_infer_shape[n-1-shp2] - IndexError: only integers, slices (`:`),...
FAILED tests/tensor/test_sort.py::TestTopKInferShape::test_combined_infer_shape[n-1-shp3] - IndexError: only integers, slices (`:`),...
FAILED tests/tensor/test_sort.py::TestTopKInferShape::test_combined_infer_shape[n-1-shp4] - IndexError: only integers, slices (`:`),...
FAILED tests/tensor/test_blas.py::test_dot22 - ValueError: some matrix has no unit stride
FAILED tests/tensor/test_subtensor.py::TestSubtensor::test_list_slice - IndexError: only integers, slices (`:`), ellipsis (`...`), n...
FAILED tests/tensor/test_sort.py::TestTopK::test_grad[False-(1+n)//2-shp0] - IndexError: only integers, slices (`:`), ellipsis (`......
FAILED tests/tensor/test_sort.py::TestTopK::test_grad[False-(1+n)//2-shp1] - IndexError: only integers, slices (`:`), ellipsis (`......
FAILED tests/tensor/test_sort.py::TestTopK::test_grad[False-(1+n)//2-shp2] - IndexError: only integers, slices (`:`), ellipsis (`......
FAILED tests/tensor/test_sort.py::TestTopK::test_grad[False-(1+n)//2-shp3] - IndexError: only integers, slices (`:`), ellipsis (`......
FAILED tests/tensor/test_sort.py::TestTopK::test_grad[False-n-1-shp0] - IndexError: only integers, slices (`:`), ellipsis (`...`), n...
FAILED tests/tensor/test_sort.py::TestTopK::test_grad[False-n-1-shp1] - IndexError: only integers, slices (`:`), ellipsis (`...`), n...
FAILED tests/tensor/test_sort.py::TestTopK::test_grad[False-n-1-shp2] - IndexError: only integers, slices (`:`), ellipsis (`...`), n...
FAILED tests/tensor/test_sort.py::TestTopK::test_grad[False-n-1-shp3] - IndexError: only integers, slices (`:`), ellipsis (`...`), n...
FAILED tests/tensor/signal/test_pool.py::TestDownsampleFactorMax::test_DownsampleFactorMaxStride - IndexError: only integers, slices...
FAILED tests/tensor/signal/test_pool.py::TestDownsampleFactorMax::test_DownsampleFactorMaxPaddingStride - IndexError: only integers,...
```
</details>

**Please provide the full traceback of any errors.**
Due to sheer size of the tracebacks, I'm attaching the complete test log as .gz compressed file (1.6M): 
[dev-python:aesara-2.7.10:20220818-062432.log.gz](https://github.com/aesara-devs/aesara/files/9371691/dev-python.aesara-2.7.10.20220818-062432.log.gz)

## Versions and main components

* Aesara version: 2.7.10
* Aesara config (`python -c ""import aesara; print(aesara.config)""`): [aesara-config.txt](https://github.com/aesara-devs/aesara/files/9371706/aesara-config.txt)
* Python version: 3.8.13
* Operating system: Gentoo Linux
* How did you install Aesara: (conda/pip) distribution package
",,,2022-08-18 08:09:51,2022-10-07 04:25:15,mgorny closed 2022-10-07 04:25:15,brandonwillard thesamesam mgorny,6
861,1128,Add a C implementation for `BroadcastTo` ,brandonwillard,"We could use a C implementation for `BroadcastTo` now that it's in regular use.

_Originally posted by @mattearllongshot in https://github.com/aesara-devs/aesara/issues/1124#issuecomment-1220690266_",enhancement help wanted C-backend performance concern Op implementation,zoj613,2022-08-19 16:16:01,2022-08-26 21:32:10,"mattearllongshot mentioned 2022-08-19 16:16:01,mattearllongshot subscribed 2022-08-19 16:16:01,brandonwillard labeled 2022-08-19 16:16:38,brandonwillard labeled 2022-08-19 16:16:38,brandonwillard labeled 2022-08-19 16:16:38,brandonwillard labeled 2022-08-19 16:16:38,brandonwillard labeled 2022-08-19 16:16:38,zoj613 assigned 2022-08-19 20:11:44,zoj613 mentioned 2022-08-19 23:31:17,zoj613 subscribed 2022-08-19 23:31:17,brandonwillard closed 2022-08-26 21:32:10",zoj613 mattearllongshot brandonwillard,1
862,1129,rel-2.7.10 breaks aesara-base conda-forge package,maresb,"## Description of your problem or feature request

**Please provide the full traceback of any errors.**

See https://github.com/conda-forge/aesara-feedstock/pull/103/checks

```python
import: 'aesara'
WARNING (aesara.configdefaults): g++ not detected!  Aesara will be unable to compile C-implementations and will default to Python. Performance may be severely degraded. To remove this warning, set Aesara flags cxx to an empty string.
/home/conda/feedstock_root/build_artifacts/aesara-suite_1660947771805/_test_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_pla/lib/python3.10/site-packages/aesara/link/c/cmodule.py:2077: UserWarning: `aesara.config.cxx` is not an identifiable `g++` compiler. Aesara will disable compiler optimizations specific to `g++`. At worst, this could cause slow downs.
Those parameters can be added manually via the `cxxflags` setting.
  warnings.warn(
Traceback (most recent call last):
  File ""/home/conda/feedstock_root/build_artifacts/aesara-suite_1660947771805/_test_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_pla/lib/python3.10/site-packages/aesara/link/c/cutils.py"", line 103, in <module>
    from cutils_ext.cutils_ext import *  # noqa
ModuleNotFoundError: No module named 'cutils_ext.cutils_ext'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/home/conda/feedstock_root/build_artifacts/aesara-suite_1660947771805/_test_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_pla/lib/python3.10/site-packages/aesara/link/c/cutils.py"", line 113, in <module>
    from cutils_ext.cutils_ext import *  # noqa
ModuleNotFoundError: No module named 'cutils_ext.cutils_ext'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/home/conda/feedstock_root/build_artifacts/aesara-suite_1660947771805/test_tmp/run_test.py"", line 2, in <module>
    import aesara
  File ""/home/conda/feedstock_root/build_artifacts/aesara-suite_1660947771805/_test_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_pla/lib/python3.10/site-packages/aesara/__init__.py"", line 126, in <module>
    from aesara import scalar, tensor
  File ""/home/conda/feedstock_root/build_artifacts/aesara-suite_1660947771805/_test_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_pla/lib/python3.10/site-packages/aesara/scalar/__init__.py"", line 1, in <module>
    from .basic import *
  File ""/home/conda/feedstock_root/build_artifacts/aesara-suite_1660947771805/_test_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_pla/lib/python3.10/site-packages/aesara/scalar/basic.py"", line 25, in <module>
    from aesara import printing
  File ""/home/conda/feedstock_root/build_artifacts/aesara-suite_1660947771805/_test_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_pla/lib/python3.10/site-packages/aesara/printing.py"", line 18, in <module>
    from aesara.compile import Function, SharedVariable
  File ""/home/conda/feedstock_root/build_artifacts/aesara-suite_1660947771805/_test_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_pla/lib/python3.10/site-packages/aesara/compile/__init__.py"", line 19, in <module>
    from aesara.compile.mode import (
  File ""/home/conda/feedstock_root/build_artifacts/aesara-suite_1660947771805/_test_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_pla/lib/python3.10/site-packages/aesara/compile/mode.py"", line 28, in <module>
    from aesara.link.c.basic import CLinker, OpWiseCLinker
  File ""/home/conda/feedstock_root/build_artifacts/aesara-suite_1660947771805/_test_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_pla/lib/python3.10/site-packages/aesara/link/c/basic.py"", line 24, in <module>
    from aesara.link.c import cutils
  File ""/home/conda/feedstock_root/build_artifacts/aesara-suite_1660947771805/_test_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_pla/lib/python3.10/site-packages/aesara/link/c/cutils.py"", line 116, in <module>
    compile_cutils()
  File ""/home/conda/feedstock_root/build_artifacts/aesara-suite_1660947771805/_test_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_pla/lib/python3.10/site-packages/aesara/link/c/cutils.py"", line 80, in compile_cutils
    cmodule.GCC_compiler.compile_str(""cutils_ext"", code, location=loc, preargs=args)
  File ""/home/conda/feedstock_root/build_artifacts/aesara-suite_1660947771805/_test_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_pla/lib/python3.10/site-packages/aesara/link/c/cmodule.py"", line 2514, in compile_str
    raise MissingGXX(""g++ not available! We can't compile c code."")
aesara.link.c.exceptions.MissingGXX: g++ not available! We can't compile c code.
Tests failed for aesara-base-2.7.10-py310hff52083_0.tar.bz2 - moving package to /home/conda/feedstock_root/build_artifacts/broken

```

**Please provide any additional information below.**

I suspect the issue is caused by the removal of lazy import logic for `cutils` in https://github.com/aesara-devs/aesara/commit/9b3e9d8dda09d7c3947f9658cf326f1e30430bae.

Hopefully I can look more in-depth tomorrow. @brandonwillard, is there any particular reason why the import logic was changed? Was it to improve type inference? If so, perhaps we could reenable the lazy import logic and find a better way to achieve type hints?",bug,brandonwillard,2022-08-19 23:04:24,2022-08-20 23:23:39,"brandonwillard mentioned 2022-08-19 23:04:25,brandonwillard subscribed 2022-08-19 23:04:25,brandonwillard labeled 2022-08-20 00:00:34,brandonwillard assigned 2022-08-20 00:00:35,brandonwillard closed 2022-08-20 23:23:40",maresb brandonwillard,1
871,1141,Parameterization of WeibullRV missing scale?,Armavica,"## Description of your problem or feature request

I am trying to upgrade `PyMC` to `aesara=2.8.2`, and some of the tests failing are related to `WeibullRV`.

I think that it might have to do with commit fcd1c4c59ec13a2411d5f121f89d476e30902834 which imposes the prototype of `WeibullRV.__call__(self, shape, size, **kwargs)`. However, the [Weibull distribution](https://en.wikipedia.org/wiki/Weibull_distribution) is usually described by two parameters: its scale and its shape. The scale is missing from the function prototype, which might explain the errors on `PyMC`'s side.

cc @rlouf because I think that you are the author of this commit

**Please provide a minimal, self-contained, and reproducible example.**
```python
import pymc as pm # (branch Armavica:aesara-rewrite)
with pm.Model() as model:
    pm.Weibull(""x"", alpha=1.0, beta=1.0, shape=(3, 4))
```

**Please provide the full traceback of any errors.**
```python
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
Input In [3], in <cell line: 1>()
      1 with pm.Model() as model:
----> 2     pm.Weibull(""x"", alpha=1.0, beta=1.0, size=(3, 4))

File ~/src/pymc/pymc/distributions/distribution.py:263, in Distribution.__new__(cls, name, rng, dims, initval, observed, total_size, transform, *args, **kwargs)
    259     raise TypeError(f""Name needs to be a string but got: {name}"")
    261 # Create the RV and process dims and observed to determine
    262 # a shape by which the created RV may need to be resized.
--> 263 rv_out, dims, observed, resize_shape = _make_rv_and_resize_shape(
    264     cls=cls, dims=dims, model=model, observed=observed, args=args, **kwargs
    265 )
    267 if resize_shape:
    268     # A batch size was specified through `dims`, or implied by `observed`.
    269     rv_out = change_rv_size(rv=rv_out, new_size=resize_shape, expand=True)

File ~/src/pymc/pymc/distributions/distribution.py:165, in _make_rv_and_resize_shape(cls, dims, model, observed, args, **kwargs)
    162 """"""Creates the RV and processes dims or observed to determine a resize shape.""""""
    163 # Create the RV without dims information, because that's not something tracked at the Aesara level.
    164 # If necessary we'll later replicate to a different size implied by already known dims.
--> 165 rv_out = cls.dist(*args, **kwargs)
    166 ndim_actual = rv_out.ndim
    167 resize_shape = None

File ~/src/pymc/pymc/distributions/continuous.py:2585, in Weibull.dist(cls, alpha, beta, *args, **kwargs)
   2582 alpha = at.as_tensor_variable(floatX(alpha))
   2583 beta = at.as_tensor_variable(floatX(beta))
-> 2585 return super().dist([alpha, beta], *args, **kwargs)

File ~/src/pymc/pymc/distributions/distribution.py:351, in Distribution.dist(cls, dist_params, shape, **kwargs)
    346 create_size, ndim_expected, ndim_batch, ndim_supp = find_size(
    347     shape=shape, size=size, ndim_supp=cls.rv_op.ndim_supp
    348 )
    349 # Create the RV with a `size` right away.
    350 # This is not necessarily the final result.
--> 351 rv_out = cls.rv_op(*dist_params, size=create_size, **kwargs)
    353 # Replicate dimensions may be prepended via a shape with Ellipsis as the last element:
    354 if shape is not None and Ellipsis in shape:

TypeError: WeibullRV.__call__() got multiple values for argument 'size'
```

**Please provide any additional information below.**


## Versions and main components

* Aesara version: 2.8.2
* Aesara config (`python -c ""import aesara; print(aesara.config)""`)
* Python version: 3.10.5
* Operating system: linux
* How did you install Aesara: conda
",,,2022-08-23 18:37:01,2022-08-24 02:33:38,"rlouf mentioned 2022-08-23 18:42:23,rlouf subscribed 2022-08-23 18:42:23,Armavica closed 2022-08-24 02:33:38",rlouf Armavica,5
874,1146,`ChoiceRV`'s behavior also differs from NumPy's `choice`,rlouf,"`ChoiceRV`'s behavior differs from NumPy's `choice` in that it does not accept input tensors that have more than one dimension while NumPy does. The following raises an error in Aesara:

```python
import aesara.tensor as at
import numpy as np

arr = np.vstack([[1,2], [3,4]])
srng = at.random.RandomStream(0)
c_rv = srng.choice(at.as_tensor(arr))
print(c_rv.eval())
```

while this does not:

```python
import numpy as np

arr = np.vstack([[1,2], [3, 4]])
rng = np.random.default_rng()
print(rng.choice(arr))
# [3,, 4]
```

I could not find an explanation for this anywhere in the codebase, although it seems that this was intentional as the tests explicitly test for the above example raising an exception. If you can think of a specific reason for this behavior I will document it, otherwise I will change it to follow NumPy's.

_Originally posted by @rlouf in https://github.com/aesara-devs/aesara/issues/1105#issuecomment-1218334644_",bug documentation question random variables,,2022-08-24 19:56:22,2022-09-02 15:03:46,"rlouf mentioned 2022-08-24 19:56:22,rlouf subscribed 2022-08-24 19:56:22,brandonwillard labeled 2022-08-24 19:57:07,brandonwillard labeled 2022-08-24 19:57:07,brandonwillard labeled 2022-08-24 19:57:07,brandonwillard labeled 2022-08-25 02:55:32,brandonwillard closed 2022-09-02 15:03:47",rlouf brandonwillard,2
875,1147,Consider converting `*Subtensor*` indices to unsigned integers,brandonwillard,"We could apply an unsigned int dtype conversion to the index inputs of `*Subtensor*` index arguments in an attempt to get some small performance improvements&mdash;as mentioned in https://github.com/aesara-devs/aesara/pull/1081#issuecomment-1206589448.  

One bit of reasoning for this in the general C indexing case&mdash;and not the Python/NumPy wraparound discussed in the linked PR/comment above&mdash;is that some architectures might produce faster code by removing unnecessary instructions when an unsigned integer is used for indexing (e.g. omitting a [""sign extension""](https://stackoverflow.com/questions/55604029/should-i-always-use-size-t-when-indexing-arrays/55604267#55604267)).

Since a dtype conversion at run-time could be costly, we might need to restrict such a rewrite to only constant index inputs.  Anyone implementing this might want to first check that, though.

Also, this dtype conversion isn't universally helpful.  On my machine, only Numba sees a small benefit from it, so we would need to determine whether or not it's even worth implementing in Aesara, instead of as [a rewrite in Numba itself](https://numba.readthedocs.io/en/stable/developer/rewrites.html) (i.e. not in our transpiled Numba code).  Furthermore, we should first check with the Numba community and determine why this isn't already an optimization on their (or LLVM's) end.

In the meantime, if one wants this potential improvement, they can manually set their index variables to unsigned integers&mdash;especially when/if they're constants.",help wanted question graph rewriting Numba,,2022-08-25 19:56:50,2022-08-28 18:03:57,"brandonwillard labeled 2022-08-25 19:56:50,brandonwillard labeled 2022-08-25 19:56:50,brandonwillard labeled 2022-08-25 19:56:50,brandonwillard labeled 2022-08-25 19:56:50,ricardoV94 mentioned 2022-08-27 16:26:33,ricardoV94 subscribed 2022-08-27 16:26:33,brandonwillard closed 2022-08-28 18:03:57",ricardoV94 aseyboldt brandonwillard,9
880,1156,`clone_replace`'s `share_inputs` doesn't consider `RandomVariables` as inputs,lucianopaz,"## Description of your problem or feature request

I have a biggish hierarchical model and I want to test an intervention where I force some of the random variables to be exactly equal to 0. There are many ways in which I can do this, but I tried to use `clone_replace` to replace the intervened random variables with zeros of the correct shape and dtype. I chose not to do this by setting `givens` when I compiled the function, because I actually want my function to output both conditions: with and without interventions.

The problem I faced was that `clone_replace` made copies of the random variables that weren't being changed (the random generators and other constant inputs were not cloned because those were the graph inputs), and then when I compiled a function to draw samples, the intervention model resampled the cloned variables from their prior. Here's a minimal example:

**Please provide a minimal, self-contained, and reproducible example.**
```python
a = at.random.normal(loc=3, scale=0.01, name=""a"", size=2)
b = at.random.normal(loc=1, scale=0.01, name=""b"", size=(2, 2))
c = at.random.normal(loc=100, scale=0.01, name=""c"", size=(2, 2, 2))
d = pm.Normal.dist(mu=(a + b + c).flatten(), sigma=0.01)
d.name = ""d""
d_clone = aesara.graph.basic.clone_replace(
    [d], replace={c: at.zeros(c.shape, dtype=c.dtype)}, share_inputs=True
)
f = aesara.function([a, b], d_clone, on_unused_input=""ignore"")
f(a=np.zeros(2), b=np.zeros((2, 2)))
```

Which prints:

```python
[array([3.98356155, 3.9887164 , 3.99668762, 4.01093951, 3.98949882, 0129146 , 3.9909093 , 4.04306206])]
```

instead of being centered around zero.

If instead of using `clone_replace` I use `clone_get_equiv`, I can put together something that works like I want it to:

```python
a = at.random.normal(loc=3, scale=0.01, name=""a"", size=2)
b = at.random.normal(loc=1, scale=0.01, name=""b"", size=(2, 2))
c = at.random.normal(loc=100, scale=0.01, name=""c"", size=(2, 2, 2))
d = pm.Normal.dist(mu=(a + b + c).flatten(), sigma=0.01)
d.name = ""d""
clone_map = aesara.graph.basic.clone_get_equiv([], [d],)
fg = aesara.graph.fg.FunctionGraph(None, [clone_map[d]], clone=False)
fg.replace_all(
    [
        (clone_map[a], a),
        (clone_map[b], b),
        (clone_map[c], at.zeros(clone_map[c].shape, dtype=clone_map[c].dtype)),
    ],
    import_missing=True,
)
aesara.function([a, b], clone_map[d])(a=np.zeros(2), b=np.zeros((2, 2)))
```

Which prints out

```python
array([ 0.00141859, -0.02443352, -0.00727199,  0.00841104, -0.00885378, -0.00093795,  0.00688666, -0.00521271])
```

I understand now why variables `a` and `b` are not shared after calling `clone_replace` but I think that it would be useful to either:

- Have `clone_replace` return the mapping between original and cloned nodes
- Be able to pass an `inputs` list to `clone_replace` that indicates which nodes should be considered inputs to be shared.


## Versions and main components

* Aesara version: 2.6.6
* Aesara config (`python -c ""import aesara; print(aesara.config)""`)
* Python version: 3.9
* Operating system: Ubuntu 18.04
* How did you install Aesara: (conda/pip) conda
",question graph rewriting random variables,,2022-08-30 13:43:17,2022-08-30 16:33:12,"brandonwillard labeled 2022-08-30 15:39:06,brandonwillard labeled 2022-08-30 15:39:06,brandonwillard labeled 2022-08-30 15:39:06,brandonwillard renamed 2022-08-30 15:39:31,aesara-devs locked 2022-08-30 16:33:12,brandonwillard converted_to_discussion 2022-08-30 16:33:12",rlouf lucianopaz aesara-devs brandonwillard,3
881,1158,Selecting a submodule from a list according to a tracer,icysapphire,"I have a pool of convolutional layers and I would like to select which layer to use depending on the value of a tracer. The following is the sketch of my custom module:
```python
def setup(self):
    self.pool_conv = [nn.Conv(features=3,
                                  kernel_size=(7, 7),
                                  kernel_init=get_init_fn(self.pool_init),
                                  bias_init=get_init_fn(""zeros""),
                                  strides=(2, 2),
                                  padding=((3, 3), (3, 3)),
                                  use_bias=True) for _ in range(self.size)]


@nn.compact
def __call__(self, x, prompt_mask=None, task_id=-1, query=None, stop_gradients=False):
    ....
    initializing = self.is_mutable_collection('params')
    if initializing:
        for j in range(self.size):
            logging.info('making sure that conv%s is intiialized', j)
            tmp = self.pool_conv[j](x[0])

    @partial(jit, static_argnums=0)
    def get_conv_from_pool(conv_idx, sample):
        return self.pool_conv[conv_idx](sample)

    conv_out = get_conv_from_pool(conv_idx=jax.device_get(conv_idx).item(), sample=x[i])
    ...
```

**Please provide the full traceback of any errors.**
```python
  File ""/clp/models/module.py"", line 264, in __call__
    conv_out = get_conv_from_pool(conv_idx=jax.device_get(conv_idx).item(), sample=x[i])
  File ""/miniconda3/envs/fo4/lib/python3.9/site-packages/jax/_src/device_array.py"", line 226, in item
    return int(self)
jax._src.errors.ConcretizationTypeError: Abstract tracer value encountered where concrete value is expected: Traced<ShapedArray(int32[])>with<DynamicJaxprTrace(level=0/1)>
The problem arose with the `int` function. If trying to convert the data type of a value, try using `x.astype(int)` or `jnp.array(x, int)` instead.

```

I am a newbie and I am trying to figure out a possible implementation considering JAX compilation model.. can you help me? It seems that static args do not help here..",,,2022-08-31 14:42:31,2022-08-31 19:14:48,"aesara-devs locked 2022-08-31 19:14:48,rlouf converted_to_discussion 2022-08-31 19:14:48",rlouf icysapphire aesara-devs,0
883,1161,[issue] Can not import in python 3.9.12 with aesara 2.8.2. ,panwanke,"
When I import aesara in python, I get this error. It seems to related with mkl library, but I have installed numpy+mkl. Should I install oneAPI?

---------------------------------------------------------------------------
NoSectionError                            Traceback (most recent call last)
File d:\\Miniconda3\\envs\\hddm39\\lib\\site-packages\\aesara\\configparser.py:237, in AesaraConfigParser.fetch_val_for_key(self, key, delete_key)
    236 try:
--> 237     return self._aesara_cfg.get(section, option)
    238 except InterpolationError:

File d:\\Miniconda3\\envs\\hddm39\\lib\\configparser.py:781, in RawConfigParser.get(self, section, option, raw, vars, fallback)
    780 try:
--> 781     d = self._unify_values(section, vars)
    782 except NoSectionError:

File d:\\Miniconda3\\envs\\hddm39\\lib\\configparser.py:1152, in RawConfigParser._unify_values(self, section, vars)
   1151     if section != self.default_section:
-> 1152         raise NoSectionError(section) from None
...
   2459     ]
   2460     if windows_styled_libs:
   2461         selected_lib = sorted(windows_styled_libs, key=sort_key)[-1]

FileNotFoundError: [WinError 3] The system cannot find the specified path: 'C:/Program Files (x86)/Intel/oneAPI/mkl/latest/lib/intel64'",,,2022-09-01 02:24:25,2022-09-01 04:06:41,"aesara-devs locked 2022-09-01 04:06:41,brandonwillard converted_to_discussion 2022-09-01 04:06:41",panwanke aesara-devs brandonwillard,0
885,1164,Type hint of `specify_shape` does not allow for `None` in shape,ricardoV94,https://github.com/aesara-devs/aesara/blob/7f8af9bc28755d93dca3afff2534a8a5f5ecbd80/aesara/tensor/shape.py#L531-L540,bug good first issue typing,,2022-09-01 13:36:48,2022-11-27 13:49:26,"ricardoV94 labeled 2022-09-01 13:36:48,ricardoV94 labeled 2022-09-01 13:36:48,brandonwillard labeled 2022-09-01 15:30:01,rlouf closed 2022-11-27 13:49:26",rlouf ricardoV94 sudarsan2k5 brandonwillard,1
887,1167,"Compile `Mode`s still uses ""optimize"" instead of ""rewrite""",rlouf,"For example:

https://github.com/aesara-devs/aesara/blob/e40c827462ff2956010794ac94a38e70ae3a3131/aesara/compile/mode.py#L304",help wanted refactor,,2022-09-05 07:05:15,2022-09-06 21:29:30,"rlouf labeled 2022-09-05 07:05:15,rlouf labeled 2022-09-05 07:05:15,rlouf renamed 2022-09-05 07:05:33,rlouf closed 2022-09-06 21:29:30",rlouf brandonwillard,2
890,1176,Numba `Scan` fails when multiple `None` values are passed in `outputs_info`,jessegrabowski,"Referencing a error raised in #1174, when an aesara `Scan` outputs_info includes more than one None (more than one output that behaves as a map), the placeholder variable name for this output appears to be re-used, resulting in a syntax error in the generated numba code. 

MRP:

```python
k = at.iscalar('k')
A = at.dvector('A')

def power_step(prior_result, x):
    return prior_result * x, prior_result * x * x, prior_result * x * x * x

result, _ = aesara.scan(power_step,
                        non_sequences=[A],
                        outputs_info=[at.ones_like(A), None, None],
                        n_steps=k)

numba_power = aesara.function([k, A], result, mode='NUMBA')
```

<details><summary>SyntaxError: duplicate argument 'auto_25426' in function definition</summary>

<p>

```python

Traceback (most recent call last):

  File ~/opt/anaconda3/envs/econ/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3398 in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)

  Input In [16] in <cell line: 1>
    numba_power = aesara.function([k, A], result, mode='NUMBA')

  File ~/opt/anaconda3/envs/econ/lib/python3.10/site-packages/aesara/compile/function/__init__.py:317 in function
    fn = pfunc(

  File ~/opt/anaconda3/envs/econ/lib/python3.10/site-packages/aesara/compile/function/pfunc.py:374 in pfunc
    return orig_function(

  File ~/opt/anaconda3/envs/econ/lib/python3.10/site-packages/aesara/compile/function/types.py:1763 in orig_function
    fn = m.create(defaults)

  File ~/opt/anaconda3/envs/econ/lib/python3.10/site-packages/aesara/compile/function/types.py:1656 in create
    _fn, _i, _o = self.linker.make_thunk(

  File ~/opt/anaconda3/envs/econ/lib/python3.10/site-packages/aesara/link/basic.py:254 in make_thunk
    return self.make_all(

  File ~/opt/anaconda3/envs/econ/lib/python3.10/site-packages/aesara/link/basic.py:698 in make_all
    thunks, nodes, jit_fn = self.create_jitable_thunk(

  File ~/opt/anaconda3/envs/econ/lib/python3.10/site-packages/aesara/link/basic.py:642 in create_jitable_thunk
    converted_fgraph = self.fgraph_convert(

  File ~/opt/anaconda3/envs/econ/lib/python3.10/site-packages/aesara/link/numba/linker.py:10 in fgraph_convert
    return numba_funcify(fgraph, **kwargs)

  File ~/opt/anaconda3/envs/econ/lib/python3.10/functools.py:889 in wrapper
    return dispatch(args[0].__class__)(*args, **kw)

  File ~/opt/anaconda3/envs/econ/lib/python3.10/site-packages/aesara/link/numba/dispatch/basic.py:381 in numba_funcify_FunctionGraph
    return fgraph_to_python(

  File ~/opt/anaconda3/envs/econ/lib/python3.10/site-packages/aesara/link/utils.py:741 in fgraph_to_python
    compiled_func = op_conversion_fn(

  File ~/opt/anaconda3/envs/econ/lib/python3.10/functools.py:889 in wrapper
    return dispatch(args[0].__class__)(*args, **kw)

  File ~/opt/anaconda3/envs/econ/lib/python3.10/site-packages/aesara/link/numba/dispatch/scan.py:153 in numba_funcify_Scan
    scalar_op_fn = compile_function_src(

  File ~/opt/anaconda3/envs/econ/lib/python3.10/site-packages/aesara/link/utils.py:605 in compile_function_src
    mod_code = compile(src, filename, mode=""exec"")

  File /var/folders/wy/ph4j9vrx23v000gc9flt78y40000gn/T/tmpq2whnvfo:2
    def scan(n_steps, auto_29768, auto_25426, auto_25426, auto_25427):
                                              ^
SyntaxError: duplicate argument 'auto_25426' in function definition

```

</p>

</details>


## Versions and main components

* Aesara version: 2.7.5
<details><summary>Aesara config (`python -c ""import aesara; print(aesara.config)""`)</summary>

<p>

```
floatX ({'float64', 'float32', 'float16'}) 
    Doc:  Default floating-point precision for python casts.

Note: float16 support is experimental, use at your own risk.
    Value:  float64

warn_float64 ({'raise', 'ignore', 'pdb', 'warn'}) 
    Doc:  Do an action when a tensor variable with float64 dtype is created.
    Value:  ignore

pickle_test_value (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x10fea35b0>>) 
    Doc:  Dump test values while pickling model. If True, test values will be dumped with model.
    Value:  True

cast_policy ({'custom', 'numpy+floatX'}) 
    Doc:  Rules for implicit type casting
    Value:  custom

deterministic ({'more', 'default'}) 
    Doc:  If `more`, sometimes we will select some implementation that are more deterministic, but slower.  Also see the dnn.conv.algo* flags to cover more cases.
    Value:  default

device (cpu)
    Doc:  Default device for computations. only cpu is supported for now
    Value:  cpu

force_device (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x115ea41f0>>) 
    Doc:  Raise an error if we can't use the specified device
    Value:  False

conv__assert_shape (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x115ea4250>>) 
    Doc:  If True, AbstractConv* ops will verify that user-provided shapes match the runtime shapes (debugging option, may slow down compilation)
    Value:  False

print_global_stats (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x115ea4280>>) 
    Doc:  Print some global statistics (time spent) at the end
    Value:  False

assert_no_cpu_op ({'raise', 'ignore', 'pdb', 'warn'}) 
    Doc:  Raise an error/warning if there is a CPU op in the computational graph.
    Value:  ignore

unpickle_function (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x115ea5ba0>>) 
    Doc:  Replace unpickled Aesara functions with None. This is useful to unpickle old graphs that pickled them when it shouldn't
    Value:  True

<aesara.configparser.ConfigParam object at 0x115ea5bd0>
    Doc:  Default compilation mode
    Value:  Mode

cxx (<class 'str'>) 
    Doc:  The C++ compiler to use. Currently only g++ is supported, but supporting additional compilers should not be too difficult. If it is empty, no C++ code is compiled.
    Value:  /Users/jessegrabowski/opt/anaconda3/envs/econ/bin/clang++

linker ({'cvm_nogc', 'py', 'c|py_nogc', 'c|py', 'cvm', 'c', 'vm', 'vm_nogc'}) 
    Doc:  Default linker used if the aesara flags mode is Mode
    Value:  cvm

allow_gc (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x115ea5b40>>) 
    Doc:  Do we default to delete intermediate results during Aesara function calls? Doing so lowers the memory requirement, but asks that we reallocate memory at the next function call. This is implemented for the default linker, but may not work for all linkers.
    Value:  True

optimizer ({'fast_compile', 'o2', 'o3', 'fast_run', 'None', 'unsafe', 'o1', 'o4', 'merge'}) 
    Doc:  Default optimizer. If not None, will use this optimizer with the Mode
    Value:  o4

optimizer_verbose (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x115ea5d50>>) 
    Doc:  If True, we print all optimization being applied
    Value:  False

on_opt_error ({'raise', 'ignore', 'pdb', 'warn'}) 
    Doc:  What to do when an optimization crashes: warn and skip it, raise the exception, or fall into the pdb debugger.
    Value:  warn

nocleanup (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x115ea5de0>>) 
    Doc:  Suppress the deletion of code files that did not compile cleanly
    Value:  False

on_unused_input ({'raise', 'ignore', 'warn'}) 
    Doc:  What to do if a variable in the 'inputs' list of  aesara.function() is not used in the graph.
    Value:  raise

gcc__cxxflags (<class 'str'>) 
    Doc:  Extra compiler flags for gcc
    Value:   -Wno-c++11-narrowing -fno-exceptions -fno-unwind-tables -fno-asynchronous-unwind-tables

cmodule__warn_no_version (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x115ea5c90>>) 
    Doc:  If True, will print a warning when compiling one or more Op with C code that can't be cached because there is no c_code_cache_version() function associated to at least one of those Ops.
    Value:  False

cmodule__remove_gxx_opt (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x115ea5e40>>) 
    Doc:  If True, will remove the -O* parameter passed to g++.This is useful to debug in gdb modules compiled by Aesara.The parameter -g is passed by default to g++
    Value:  False

cmodule__compilation_warning (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x115ea5ed0>>) 
    Doc:  If True, will print compilation warnings.
    Value:  False

cmodule__preload_cache (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x115ea5f00>>) 
    Doc:  If set to True, will preload the C module cache at import time
    Value:  False

cmodule__age_thresh_use (<class 'int'>) 
    Doc:  In seconds. The time after which Aesara won't reuse a compile c module.
    Value:  2073600

cmodule__debug (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x115ea5f60>>) 
    Doc:  If True, define a DEBUG macro (if not exists) for any compiled C code.
    Value:  False

compile__wait (<class 'int'>) 
    Doc:  Time to wait before retrying to acquire the compile lock.
    Value:  5

compile__timeout (<class 'int'>) 
    Doc:  In seconds, time that a process will wait before deciding to
    override an existing lock. An override only happens when the existing
    lock is held by the same owner *and* has not been 'refreshed' by this
    owner for more than this period. Refreshes are done every half timeout
    period for running processes.
    Value:  120

ctc__root (<class 'str'>) 
    Doc:  Directory which contains the root of Baidu CTC library. It is assumed         that the compiled library is either inside the build, lib or lib64         subdirectory, and the header inside the include directory.
    Value:  

tensor__cmp_sloppy (<class 'int'>) 
    Doc:  Relax aesara.tensor.math._allclose (0) not at all, (1) a bit, (2) more
    Value:  0

tensor__local_elemwise_fusion (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x115ea6110>>) 
    Doc:  Enable or not in fast_run mode(fast_run optimization) the elemwise fusion optimization
    Value:  True

lib__amblibm (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x115ea61a0>>) 
    Doc:  Use amd's amdlibm numerical library
    Value:  False

tensor__insert_inplace_optimizer_validate_nb (<class 'int'>) 
    Doc:  -1: auto, if graph have less then 500 nodes 1, else 10
    Value:  -1

traceback__limit (<class 'int'>) 
    Doc:  The number of stack to trace. -1 mean all.
    Value:  8

traceback__compile_limit (<class 'int'>) 
    Doc:  The number of stack to trace to keep during compilation. -1 mean all. If greater then 0, will also make us save Aesara internal stack trace.
    Value:  0

experimental__local_alloc_elemwise (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x115ea6320>>) 
    Doc:  DEPRECATED: If True, enable the experimental optimization local_alloc_elemwise. Generates error if not True. Use optimizer_excluding=local_alloc_elemwise to disable.
    Value:  True

experimental__local_alloc_elemwise_assert (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x115ea6350>>) 
    Doc:  When the local_alloc_elemwise is applied, add an assert to highlight shape errors.
    Value:  True

warn__ignore_bug_before ({'1.0.2', '0.4', '1.0.4', '1.0', '0.9', '0.8.1', '0.10', 'None', '0.8.2', '0.5', '0.3', '0.8', '0.7', '1.0.5', '1.0.3', '0.4.1', 'all', '1.0.1', '0.6'}) 
    Doc:  If 'None', we warn about all Aesara bugs found by default. If 'all', we don't warn about Aesara bugs found by default. If a version, we print only the warnings relative to Aesara bugs found after that version. Warning for specific bugs can be configured with specific [warn] flags.
    Value:  0.9

exception_verbosity ({'low', 'high'}) 
    Doc:  If 'low', the text of exceptions will generally refer to apply nodes with short names such as Elemwise{add_no_inplace}. If 'high', some exceptions will also refer to apply nodes with long descriptions  like:
        A. Elemwise{add_no_inplace}
                B. log_likelihood_v_given_h
                C. log_likelihood_h
    Value:  low

print_test_value (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x115ea64a0>>) 
    Doc:  If 'True', the __eval__ of an Aesara variable will return its test_value when this is available. This has the practical conseguence that, e.g., in debugging `my_var` will print the same as `my_var.tag.test_value` when a test value is defined.
    Value:  False

compute_test_value ({'raise', 'ignore', 'pdb', 'off', 'warn'}) 
    Doc:  If 'True', Aesara will run each op at graph build time, using Constants, SharedVariables and the tag 'test_value' as inputs to the function. This helps the user track down problems in the graph before it gets optimized.
    Value:  off

compute_test_value_opt ({'raise', 'ignore', 'pdb', 'off', 'warn'}) 
    Doc:  For debugging Aesara optimization only. Same as compute_test_value, but is used during Aesara optimization
    Value:  off

check_input (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x115ea6530>>) 
    Doc:  Specify if types should check their input in their C code. It can be used to speed up compilation, reduce overhead (particularly for scalars) and reduce the number of generated C files.
    Value:  True

NanGuardMode__nan_is_error (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x115ea6560>>) 
    Doc:  Default value for nan_is_error
    Value:  True

NanGuardMode__inf_is_error (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x115ea6590>>) 
    Doc:  Default value for inf_is_error
    Value:  True

NanGuardMode__big_is_error (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x115ea6620>>) 
    Doc:  Default value for big_is_error
    Value:  True

NanGuardMode__action ({'raise', 'pdb', 'warn'}) 
    Doc:  What NanGuardMode does when it finds a problem
    Value:  raise

DebugMode__patience (<class 'int'>) 
    Doc:  Optimize graph this many times to detect inconsistency
    Value:  10

DebugMode__check_c (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x115ea66b0>>) 
    Doc:  Run C implementations where possible
    Value:  True

DebugMode__check_py (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x115ea6740>>) 
    Doc:  Run Python implementations where possible
    Value:  True

DebugMode__check_finite (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x115ea6770>>) 
    Doc:  True -> complain about NaN/Inf results
    Value:  True

DebugMode__check_strides (<class 'int'>) 
    Doc:  Check that Python- and C-produced ndarrays have same strides. On difference: (0) - ignore, (1) warn, or (2) raise error
    Value:  0

DebugMode__warn_input_not_reused (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x115ea67d0>>) 
    Doc:  Generate a warning when destroy_map or view_map says that an op works inplace, but the op did not reuse the input for its output.
    Value:  True

DebugMode__check_preallocated_output (<class 'str'>) 
    Doc:  Test thunks with pre-allocated memory as output storage. This is a list of strings separated by "":"". Valid values are: ""initial"" (initial storage in storage map, happens with Scan),""previous"" (previously-returned memory), ""c_contiguous"", ""f_contiguous"", ""strided"" (positive and negative strides), ""wrong_size"" (larger and smaller dimensions), and ""ALL"" (all of the above).
    Value:  

DebugMode__check_preallocated_output_ndim (<class 'int'>) 
    Doc:  When testing with ""strided"" preallocated output memory, test all combinations of strides over that number of (inner-most) dimensions. You may want to reduce that number to reduce memory or time usage, but it is advised to keep a minimum of 2.
    Value:  4

profiling__time_thunks (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x115ea6860>>) 
    Doc:  Time individual thunks when profiling
    Value:  True

profiling__n_apply (<class 'int'>) 
    Doc:  Number of Apply instances to print by default
    Value:  20

profiling__n_ops (<class 'int'>) 
    Doc:  Number of Ops to print by default
    Value:  20

profiling__output_line_width (<class 'int'>) 
    Doc:  Max line width for the profiling output
    Value:  512

profiling__min_memory_size (<class 'int'>) 
    Doc:  For the memory profile, do not print Apply nodes if the size
                 of their outputs (in bytes) is lower than this threshold
    Value:  1024

profiling__min_peak_memory (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x115ea69b0>>) 
    Doc:  The min peak memory usage of the order
    Value:  False

profiling__destination (<class 'str'>) 
    Doc:  File destination of the profiling output
    Value:  stderr

profiling__debugprint (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x115ea6a10>>) 
    Doc:  Do a debugprint of the profiled functions
    Value:  False

profiling__ignore_first_call (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x115ea6a40>>) 
    Doc:  Do we ignore the first call of an Aesara function.
    Value:  False

on_shape_error ({'raise', 'warn'}) 
    Doc:  warn: print a warning and use the default value. raise: raise an error
    Value:  warn

openmp (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x115ea6aa0>>) 
    Doc:  Allow (or not) parallel computation on the CPU with OpenMP. This is the default value used when creating an Op that supports OpenMP parallelization. It is preferable to define it via the Aesara configuration file ~/.aesararc or with the environment variable AESARA_FLAGS. Parallelization is only done for some operations that implement it, and even for operations that implement parallelism, each operation is free to respect this flag or not. You can control the number of threads used with the environment variable OMP_NUM_THREADS. If it is set to 1, we disable openmp in Aesara by default.
    Value:  False

openmp_elemwise_minsize (<class 'int'>) 
    Doc:  If OpenMP is enabled, this is the minimum size of vectors for which the openmp parallelization is enabled in element wise ops.
    Value:  200000

optimizer_excluding (<class 'str'>) 
    Doc:  When using the default mode, we will remove optimizer with these tags. Separate tags with ':'.
    Value:  

optimizer_including (<class 'str'>) 
    Doc:  When using the default mode, we will add optimizer with these tags. Separate tags with ':'.
    Value:  

optimizer_requiring (<class 'str'>) 
    Doc:  When using the default mode, we will require optimizer with these tags. Separate tags with ':'.
    Value:  

optdb__position_cutoff (<class 'float'>) 
    Doc:  Where to stop eariler during optimization. It represent the position of the optimizer where to stop.
    Value:  inf

optdb__max_use_ratio (<class 'float'>) 
    Doc:  A ratio that prevent infinite loop in EquilibriumOptimizer.
    Value:  8.0

cycle_detection ({'fast', 'regular'}) 
    Doc:  If cycle_detection is set to regular, most inplaces are allowed,but it is slower. If cycle_detection is set to faster, less inplacesare allowed, but it makes the compilation faster.The interaction of which one give the lower peak memory usage iscomplicated and not predictable, so if you are close to the peakmemory usage, triyng both could give you a small gain.
    Value:  regular

check_stack_trace ({'off', 'raise', 'log', 'warn'}) 
    Doc:  A flag for checking the stack trace during the optimization process. default (off): does not check the stack trace of any optimization log: inserts a dummy stack trace that identifies the optimizationthat inserted the variable that had an empty stack trace.warn: prints a warning if a stack trace is missing and also a dummystack trace is inserted that indicates which optimization insertedthe variable that had an empty stack trace.raise: raises an exception if a stack trace is missing
    Value:  off

metaopt__verbose (<class 'int'>) 
    Doc:  0 for silent, 1 for only warnings, 2 for full output withtimings and selected implementation
    Value:  0

metaopt__optimizer_excluding (<class 'str'>) 
    Doc:  exclude optimizers with these tags. Separate tags with ':'.
    Value:  

metaopt__optimizer_including (<class 'str'>) 
    Doc:  include optimizers with these tags. Separate tags with ':'.
    Value:  

profile (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x115ea6da0>>) 
    Doc:  If VM should collect profile information
    Value:  False

profile_optimizer (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x115ea6dd0>>) 
    Doc:  If VM should collect optimizer profile information
    Value:  False

profile_memory (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x115ea6e00>>) 
    Doc:  If VM should collect memory profile information and print it
    Value:  False

<aesara.configparser.ConfigParam object at 0x115ea6e30>
    Doc:  Useful only for the VM Linkers. When lazy is None, auto detect if lazy evaluation is needed and use the appropriate version. If the C loop isn't being used and lazy is True, use the Stack VM; otherwise, use the Loop VM.
    Value:  None

unittests__rseed (<class 'str'>) 
    Doc:  Seed to use for randomized unit tests. Special value 'random' means using a seed of None.
    Value:  666

warn__round (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x115ea6ef0>>) 
    Doc:  Warn when using `tensor.round` with the default mode. Round changed its default from `half_away_from_zero` to `half_to_even` to have the same default as NumPy.
    Value:  False

numba__vectorize_target ({'cuda', 'cpu', 'parallel'}) 
    Doc:  Default target for numba.vectorize.
    Value:  cpu

numba__fastmath (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x115ea6fb0>>) 
    Doc:  If True, use Numba's fastmath mode.
    Value:  True

numba__cache (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x115ea7040>>) 
    Doc:  If True, use Numba's file based caching.
    Value:  True

compiledir_format (<class 'str'>) 
    Doc:  Format string for platform-dependent compiled module subdirectory
(relative to base_compiledir). Available keys: aesara_version, device,
gxx_version, hostname, numpy_version, platform, processor,
python_bitwidth, python_int_bitwidth, python_version, short_platform.
Defaults to compiledir_%(short_platform)s-%(processor)s-
%(python_version)s-%(python_bitwidth)s.
    Value:  compiledir_%(short_platform)s-%(processor)s-%(python_version)s-%(python_bitwidth)s

<aesara.configparser.ConfigParam object at 0x115ea70d0>
    Doc:  platform-independent root directory for compiled modules
    Value:  /Users/jessegrabowski/.aesara

<aesara.configparser.ConfigParam object at 0x115ea7010>
    Doc:  platform-dependent cache directory for compiled modules
    Value:  /Users/jessegrabowski/.aesara/compiledir_macOS-10.15.7-x86_64-i386-64bit-i386-3.10.6-64

blas__ldflags (<class 'str'>) 
    Doc:  lib[s] to include for [Fortran] level-3 blas implementation
    Value:  -L/Users/jessegrabowski/opt/anaconda3/envs/econ/lib -lmkl_core -lmkl_intel_thread -lmkl_rt -Wl,-rpath,/Users/jessegrabowski/opt/anaconda3/envs/econ/lib

blas__check_openmp (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x11611d900>>) 
    Doc:  Check for openmp library conflict.
WARNING: Setting this to False leaves you open to wrong results in blas-related operations.
    Value:  True

scan__allow_gc (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x14c746740>>) 
    Doc:  Allow/disallow gc inside of Scan (default: False)
    Value:  False

scan__allow_output_prealloc (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x14c718910>>) 
    Doc:  Allow/disallow memory preallocation for outputs inside of scan (default: True)
    Value:  True

```

</p>

</details>

* Python version: 3.10.6
* Operating system: Mac OS Catalina 10.15.7
* How did you install Aesara: conda
",bug help wanted Numba Scan,,2022-09-13 14:59:37,2022-09-21 03:24:16,"brandonwillard labeled 2022-09-13 15:33:35,brandonwillard labeled 2022-09-13 15:33:35,brandonwillard labeled 2022-09-13 15:33:35,brandonwillard labeled 2022-09-13 15:33:35,brandonwillard renamed 2022-09-13 15:33:49,rlouf connected 2022-09-20 13:23:52,brandonwillard closed 2022-09-21 03:24:16",rlouf jessegrabowski brandonwillard,3
891,1180,Make `CheckAndRaise.c_code` work with `ScalarType`s,brandonwillard,"`CheckAndRaise.c_code` doesn't support `ScalarType` inputs, which results in the use of the slower `CheckAndRaise.perform`.

For example:
```python
import aesara
from aesara.raise_op import Assert
from aesara.compile.mode import Mode

from aesara.scalar.basic import float64


a = float64()
z = Assert()(a, a > 0)

aesara.dprint(z, print_type=True)
# Assert{msg=Aesara Assert failed!} [id A] <float64>
#  |<float64> [id B] <float64>
#  |TensorFromScalar [id C] <TensorType(bool, ())>
#    |GT [id D] <bool>
#      |<float64> [id B] <float64>
#      |ScalarConstant{0} [id E] <int8>

z_fn = aesara.function([a], z, mode=Mode(linker=""c""), profile=True)
# NotImplementedError: CheckAndRaise c_code not implemented for input type float64
```

N.B. `DeepCopyOp` also isn't implemented for `ScalarType`s, so that graph will not compile with `linker=""c""`; however, it will with `linker=""cvm""` after `CheckAndRaise.c_code` is fixed.",C-backend performance concern,,2022-09-13 22:52:14,2022-09-16 16:47:35,"brandonwillard labeled 2022-09-13 22:52:14,brandonwillard labeled 2022-09-13 22:52:14,brandonwillard closed 2022-09-16 16:47:36",ricardoV94 brandonwillard,2
893,1182,`c_code` directories missing from `package_data`,eganster,"It seems like some `c_code` directories (namely `aesara/scalar/c_code`, `aesara/scan/c_code` and `aesara/tensor/c_code`) are missing when installing version 2.8.3:

Minimum example:
```python
import aesara.tensor
import numpy as np

aesara.tensor.gt(np.linspace(0, 1, 10), 0.5)
```

Traceback:
```python
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/USER/virtualenvs/test/lib/python3.10/site-packages/aesara/graph/op.py"", line 297, in __call__
    node = self.make_node(*inputs, **kwargs)
  File ""/home/USER/virtualenvs/test/lib/python3.10/site-packages/aesara/tensor/elemwise.py"", line 481, in make_node
    out_dtypes, out_shapes, inputs = self.get_output_info(DimShuffle, *inputs)
  File ""/home/USER/virtualenvs/test/lib/python3.10/site-packages/aesara/tensor/elemwise.py"", line 408, in get_output_info
    dim_shuffle(
  File ""/home/USER/virtualenvs/test/lib/python3.10/site-packages/aesara/tensor/elemwise.py"", line 130, in __init__
    super().__init__([self.c_func_file], self.c_func_name)
  File ""/home/USER/virtualenvs/test/lib/python3.10/site-packages/aesara/link/c/op.py"", line 334, in __init__
    self.load_c_code(self.func_files)
  File ""/home/USER/virtualenvs/test/lib/python3.10/site-packages/aesara/link/c/op.py"", line 355, in load_c_code
    with open(func_file) as f:
FileNotFoundError: [Errno 2] No such file or directory: '/home/USER/virtualenvs/test/lib/python3.10/site-packages/aesara/tensor/c_code/dimshuffle.c'
```

I think this bug was introduced in #1135: The ""old""
```python
        package_data={
            """": [
                ""*.txt"",
                ...
            ],
```

should be
```python
[options.package_data]
* =
    *.txt
     ....
```
as otherwise only the `aesara` directory is searched for the listed files/directories, but not any subdirectories such that  `aesara/scalar/c_code`, `aesara/scan/c_code` and `aesara/tensor/c_code` are not picked up (see also the [setuptools docu on package_data](https://setuptools.pypa.io/en/latest/userguide/datafiles.html#package-data)).

## Versions and main components

* Aesara version: 2.8.3
* Aesara config: Fresh install, no user specific settings
* Python version: 3.10
* Operating system: Linux
* How did you install Aesara: pip
",bug important,,2022-09-14 15:47:31,2022-09-14 19:56:04,"brandonwillard labeled 2022-09-14 18:24:22,brandonwillard labeled 2022-09-14 18:24:22,brandonwillard renamed 2022-09-14 18:28:24,brandonwillard closed 2022-09-14 19:56:05",maresb eganster brandonwillard,2
902,1195,Bug in `JITLinker` when first output of inner `FunctionGraph` is an input variable,ricardoV94,"An error is raised when a function with `mode in (""NUMBA"", ""JAX"")` returns a `SharedVariable` that has updates.

```python
import aesara
import aesara.tensor as at

x = aesara.shared(0, name=""x"")
f = aesara.function([], [x], updates={x: x+1}, mode=""NUMBA"")
```

```python
File ~/miniconda3/envs/aesara/lib/python3.10/site-packages/aesara/compile/function/__init__.py:317, in function(inputs, outputs, mode, updates, givens, no_default_updates, accept_inplace, name, rebuild_strict, allow_input_downcast, profile, on_unused_input)
    311     fn = orig_function(
    312         inputs, outputs, mode=mode, accept_inplace=accept_inplace, name=name
    313     )
    314 else:
    315     # note: pfunc will also call orig_function -- orig_function is
    316     #      a choke point that all compilation must pass through
--> 317     fn = pfunc(
    318         params=inputs,
    319         outputs=outputs,
    320         mode=mode,
    321         updates=updates,
    322         givens=givens,
    323         no_default_updates=no_default_updates,
    324         accept_inplace=accept_inplace,
    325         name=name,
    326         rebuild_strict=rebuild_strict,
    327         allow_input_downcast=allow_input_downcast,
    328         on_unused_input=on_unused_input,
    329         profile=profile,
    330         output_keys=output_keys,
    331     )
    332 return fn

File ~/miniconda3/envs/aesara/lib/python3.10/site-packages/aesara/compile/function/pfunc.py:363, in pfunc(params, outputs, mode, updates, givens, no_default_updates, accept_inplace, name, rebuild_strict, allow_input_downcast, profile, on_unused_input, output_keys)
    350     profile = ProfileStats(message=profile)
    352 inputs, cloned_outputs = construct_pfunc_ins_and_outs(
    353     params,
    354     outputs,
   (...)
    360     allow_input_downcast,
    361 )
--> 363 return orig_function(
    364     inputs,
    365     cloned_outputs,
    366     mode,
    367     accept_inplace=accept_inplace,
    368     name=name,
    369     profile=profile,
    370     on_unused_input=on_unused_input,
    371     output_keys=output_keys,
    372 )

File ~/miniconda3/envs/aesara/lib/python3.10/site-packages/aesara/compile/function/types.py:1738, in orig_function(inputs, outputs, mode, accept_inplace, name, profile, on_unused_input, output_keys)
   1727     m = Maker(
   1728         inputs,
   1729         outputs,
   (...)
   1735         name=name,
   1736     )
   1737     with config.change_flags(compute_test_value=""off""):
-> 1738         fn = m.create(defaults)
   1739 finally:
   1740     t2 = time.time()

File ~/miniconda3/envs/aesara/lib/python3.10/site-packages/aesara/compile/function/types.py:1633, in FunctionMaker.create(self, input_storage, trustme, storage_map)
   1630 start_import_time = aesara.link.c.cmodule.import_time
   1632 with config.change_flags(traceback__limit=config.traceback__compile_limit):
-> 1633     _fn, _i, _o = self.linker.make_thunk(
   1634         input_storage=input_storage_lists, storage_map=storage_map
   1635     )
   1637 end_linker = time.time()
   1639 linker_time = end_linker - start_linker

File ~/miniconda3/envs/aesara/lib/python3.10/site-packages/aesara/link/basic.py:254, in LocalLinker.make_thunk(self, input_storage, output_storage, storage_map, **kwargs)
    247 def make_thunk(
    248     self,
    249     input_storage: Optional[""InputStorageType""] = None,
   (...)
    252     **kwargs,
    253 ) -> Tuple[""BasicThunkType"", ""InputStorageType"", ""OutputStorageType""]:
--> 254     return self.make_all(
    255         input_storage=input_storage,
    256         output_storage=output_storage,
    257         storage_map=storage_map,
    258     )[:3]

File ~/miniconda3/envs/aesara/lib/python3.10/site-packages/aesara/link/basic.py:702, in JITLinker.make_all(self, input_storage, output_storage, storage_map)
    696     compute_map[k] = [k.owner is None]
    698 thunks, nodes, jit_fn = self.create_jitable_thunk(
    699     compute_map, nodes, input_storage, output_storage, storage_map
    700 )
--> 702 computed, last_user = gc_helper(nodes)
    704 if self.allow_gc:
    705     post_thunk_old_storage = []

File ~/miniconda3/envs/aesara/lib/python3.10/site-packages/aesara/link/utils.py:263, in gc_helper(node_list)
    261 computed = set()
    262 for node in node_list:
--> 263     for input in node.inputs:
    264         last_user[input] = node
    265     for output in node.outputs:

AttributeError: 'NoneType' object has no attribute 'inputs'
```",bug JAX backend compatibility Numba,,2022-09-20 08:20:35,2022-09-23 04:28:30,"ricardoV94 labeled 2022-09-20 08:20:35,ricardoV94 labeled 2022-09-20 08:20:35,ricardoV94 labeled 2022-09-20 08:20:35,ricardoV94 labeled 2022-09-20 08:20:35,ricardoV94 renamed 2022-09-20 08:21:06,brandonwillard closed 2022-09-21 21:52:10,brandonwillard labeled 2022-09-21 21:52:19,ricardoV94 reopened 2022-09-22 05:55:38,ricardoV94 unlabeled 2022-09-22 07:03:35,ricardoV94 renamed 2022-09-22 09:45:29,brandonwillard closed 2022-09-23 04:28:31",ricardoV94 brandonwillard,3
903,1196,Numba backend ignores state of `RandomStateSharedVariable`,ricardoV94,"I am not sure if this was a conscious decision or just a limitation

```python
import numpy as np
import aesara
import aesara.tensor as at

rng = aesara.shared(np.random.RandomState(123), name=""rng"")
x = at.random.normal(rng=rng)
f = aesara.function([], x, mode=""NUMBA"")
assert f() == f()  # AssertionError
```

It also ignores manually defined updates. Here is a contrived example:
```python
import numpy as np
import aesara
import aesara.tensor as at

rng = aesara.shared(np.random.RandomState(123), name=""rng"")
next_rng = aesara.shared(np.random.RandomState(123), name=""next_rng"")
x = at.random.normal(rng=rng)
f = aesara.function([], x, updates={rng: next_rng}, mode=""NUMBA"")
assert f() == f()  # AssertionError
```
",duplicate question backend compatibility Numba random variables,,2022-09-20 08:35:47,2022-09-21 21:29:27,"ricardoV94 labeled 2022-09-20 08:36:32,ricardoV94 labeled 2022-09-20 08:36:34,ricardoV94 labeled 2022-09-20 08:36:37,ricardoV94 labeled 2022-09-20 08:36:42,ricardoV94 renamed 2022-09-20 08:58:01,ricardoV94 renamed 2022-09-20 08:58:24,brandonwillard closed 2022-09-21 21:29:27,brandonwillard closed 2022-09-21 21:45:13,brandonwillard labeled 2022-09-21 21:45:24",ricardoV94 brandonwillard,3
906,1200,Gradient failing due to wrong output type of `Split`,ricardoV94,"I couldn't quite figure out the source of the problem yet:

```python
import aesara.tensor as at

value = at.vector()
bvalue = at.concatenate([value, at.sum(value)[None]], axis=-1)
logp = bvalue + [1, 2, 3]
at.grad(logp.sum(), wrt=value)
```

```python
Traceback (most recent call last):
  File ""/home/ricardo/Documents/Projects/aesara/venv/lib/python3.8/site-packages/IPython/core/interactiveshell.py"", line 3441, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File ""<ipython-input-11-9aab63d9fe6b>"", line 5, in <module>
    at.grad(logp.sum(), wrt=value)
  File ""/home/ricardo/Documents/Projects/aesara/aesara/gradient.py"", line 623, in grad
    _rval: Sequence[Variable] = _populate_grad_dict(
  File ""/home/ricardo/Documents/Projects/aesara/aesara/gradient.py"", line 1434, in _populate_grad_dict
    rval = [access_grad_cache(elem) for elem in wrt]
  File ""/home/ricardo/Documents/Projects/aesara/aesara/gradient.py"", line 1434, in <listcomp>
    rval = [access_grad_cache(elem) for elem in wrt]
  File ""/home/ricardo/Documents/Projects/aesara/aesara/gradient.py"", line 1387, in access_grad_cache
    term = access_term_cache(node)[idx]
  File ""/home/ricardo/Documents/Projects/aesara/aesara/gradient.py"", line 1058, in access_term_cache
    output_grads = [access_grad_cache(var) for var in node.outputs]
  File ""/home/ricardo/Documents/Projects/aesara/aesara/gradient.py"", line 1058, in <listcomp>
    output_grads = [access_grad_cache(var) for var in node.outputs]
  File ""/home/ricardo/Documents/Projects/aesara/aesara/gradient.py"", line 1387, in access_grad_cache
    term = access_term_cache(node)[idx]
  File ""/home/ricardo/Documents/Projects/aesara/aesara/gradient.py"", line 1213, in access_term_cache
    input_grads = node.op.L_op(inputs, node.outputs, new_output_grads)
  File ""/home/ricardo/Documents/Projects/aesara/aesara/graph/op.py"", line 392, in L_op
    return self.grad(inputs, output_grads)
  File ""/home/ricardo/Documents/Projects/aesara/aesara/tensor/elemwise.py"", line 272, in grad
    DimShuffle(gz.type.broadcastable, grad_order)(
  File ""/home/ricardo/Documents/Projects/aesara/aesara/tensor/elemwise.py"", line 165, in __init__
    raise ValueError(
ValueError: Cannot drop a non-broadcastable dimension: (False,), []
```",bug,,2022-09-20 15:46:12,2022-09-20 21:12:49,"ricardoV94 labeled 2022-09-20 15:54:21,ricardoV94 labeled 2022-09-20 15:54:25,ricardoV94 renamed 2022-09-20 16:25:16,ricardoV94 unlabeled 2022-09-20 20:09:29,brandonwillard closed 2022-09-20 21:12:49",ricardoV94 brandonwillard,1
910,1206,Mypy apparently not ignoring hidden files in CI?,brandonwillard,It looks like Mypy is failing during `pre-commit` on `main` (see [here](https://github.com/aesara-devs/aesara/actions/runs/3110384162/jobs/5041583792#step:4:86)) because it's checking a hidden cache file now (i.e. `/home/runner/.cache/pre-commit/repog49g9cxw/py_env-python3.10/lib/python3.10/site-packages/numpy/__init__.pyi`).,bug help wanted important CI,,2022-09-23 05:22:26,2022-09-23 23:57:32,"brandonwillard labeled 2022-09-23 05:22:26,brandonwillard labeled 2022-09-23 05:22:26,brandonwillard labeled 2022-09-23 05:22:26,brandonwillard labeled 2022-09-23 05:22:26,brandonwillard closed 2022-09-23 20:55:48,brandonwillard reopened 2022-09-23 22:04:28,brandonwillard closed 2022-09-23 23:57:32",brandonwillard,1
916,1216,`ScanMerge` renames variables,ricardoV94,"First reported in https://github.com/pymc-devs/pymc/issues/6148

```python
import aesara
import aesara.tensor as at

delta = at.scalar(""delta"")

def step(prev, d):
    return prev + d

m1 = at.scalar(""m1"")
res, _ = aesara.scan(step, outputs_info=[m1], n_steps=3, non_sequences=[delta])
m2 = res[-1]
res, _ = aesara.scan(step, outputs_info=[m2], n_steps=3, non_sequences=[delta])
m3 = res[-1]

grad_m2 = aesara.grad(m2, wrt=delta)
grad_m3 = aesara.grad(m3, wrt=delta)

print(delta.name)  # delta
f = aesara.function([m1, delta], (grad_m2, grad_m3))
print(delta.name)  # delta1
```",graph rewriting Scan unexpected behavior,,2022-09-27 17:17:15,2022-09-29 22:00:28,"ricardoV94 renamed 2022-09-27 17:17:26,ricardoV94 labeled 2022-09-27 19:45:36,ricardoV94 labeled 2022-09-27 19:45:36,ricardoV94 labeled 2022-09-27 19:45:36,brandonwillard unlabeled 2022-09-28 20:38:40,brandonwillard labeled 2022-09-28 20:40:55,brandonwillard unlabeled 2022-09-28 20:47:12,brandonwillard renamed 2022-09-28 20:49:09,brandonwillard labeled 2022-09-28 21:08:06,brandonwillard closed 2022-09-29 22:00:29",ricardoV94 brandonwillard,2
919,1220,Broken `GEMM` C implementation when using NumPy >= 1.23,brandonwillard,"It looks like https://github.com/numpy/numpy/pull/21477 (i.e. NumPy >= 1.23) is causing issues in the C implementation of `GEMM`.  

Here's some example test output illustrating the difference:
```python
# Using NumPy 1.23.3
tests/tensor/test_blas.py::TestGemm::test_shape_0
x.shape=(0, 5), x.strides=(0, 0)
y.shape=(5, 4), y.strides=(16, 4)
z.shape=(0, 4), z.strides=(0, 0)
...
```
```python
# Using NumPy 1.22.3
tests/tensor/test_blas.py::TestGemm::test_shape_0
x.shape=(0, 5), x.strides=(20, 4)
y.shape=(5, 4), y.strides=(16, 4)
z.shape=(0, 4), z.strides=(16, 4)
```",bug help wanted important C-backend NumPy compatibility,,2022-09-28 23:27:04,2022-09-29 19:11:51,"brandonwillard labeled 2022-09-28 23:27:04,brandonwillard labeled 2022-09-28 23:27:04,brandonwillard labeled 2022-09-28 23:27:04,brandonwillard labeled 2022-09-28 23:27:04,brandonwillard labeled 2022-09-28 23:27:04,brandonwillard referenced 2022-09-29 04:42:00,brandonwillard connected 2022-09-29 04:44:09,brandonwillard closed 2022-09-29 19:11:51,brandonwillard referenced 2022-09-29 19:11:51",brandonwillard,0
920,1221,Mypy cannot determine type of `op_debug_information`,brandonwillard,"MyPy run by `pre-commit` will occasionally err with the following:
```
aesara/scan/op.py:3443: error: Cannot determine type of ""op_debug_information""  [has-type]
Found 1 error in 1 file (checked 1 source file)
```

This issue appears to be due to differences in command-line module ordering (see https://github.com/pre-commit/pre-commit/issues/1580#issuecomment-679413936, https://github.com/pypa/pip/issues/9502#issue-792721417, and https://github.com/python/mypy/issues/9954).

Let's see if we can remove this ordering disparity, because it's awfully annoying.",testing important CI typing,,2022-09-29 17:46:39,2022-12-09 03:52:11,"brandonwillard labeled 2022-09-29 17:46:39,brandonwillard labeled 2022-09-29 17:46:39,brandonwillard labeled 2022-09-29 17:46:39,brandonwillard labeled 2022-09-29 17:46:39,markusschmaus mentioned 2022-09-29 17:49:14,markusschmaus subscribed 2022-09-29 17:49:14,LegrandNico mentioned 2022-09-29 17:49:14,LegrandNico subscribed 2022-09-29 17:49:14,brandonwillard renamed 2022-10-05 16:38:05,brandonwillard closed 2022-12-09 03:52:11",rlouf markusschmaus LegrandNico brandonwillard ricardoV94,5
921,1223,Add `at.convolve` Op,rlouf,Which would be the equivalent of `np.convolve`: https://numpy.org/doc/stable/reference/generated/numpy.convolve.html,enhancement good first issue help wanted NumPy compatibility Op implementation,,2022-09-30 14:16:40,2023-04-23 19:30:30,"rlouf labeled 2022-09-30 14:17:21,rlouf labeled 2022-09-30 14:17:22,rlouf labeled 2022-09-30 14:17:22,rlouf labeled 2022-09-30 14:17:22,rlouf labeled 2022-09-30 14:17:22,brandonwillard closed 2023-04-23 19:30:31",rlouf brandonwillard,0
923,1228,Failure to link with MLK_RT under Windows,maresb,"## Description of your problem or feature request

There is a [report on Discourse](https://discourse.pymc.io/t/failure-to-link-with-mkl-rt-under-windows/10502) about an issue from a Windows user regarding a linking problem. @lucianopaz believes the issue is related to #947:

> [@opherdonchin](https://discourse.pymc.io/u/opherdonchin) , if it’s anything like the issue that I mentioned, you won’t be able to fix it using ldflags. Those will get a -l prepend and won’t link properly. The [PR](https://github.com/aesara-devs/aesara/pull/947) where I had fixed the old issue did some things to link to the full path of the dynamic link library, but it had to do so in aesara itself. I think that you’ll have to open an issue in aesara to get this fixed. I can help out with this after Monday

## Versions and main components

* Aesara version: 2.8.2
* Aesara config (`python -c ""import aesara; print(aesara.config)""`)
* Python version: 3.10.6
* Operating system: Windows 10
* How did you install Aesara: mamba
",C-backend MWE needed Windows,,2022-10-01 16:08:22,2022-10-05 15:53:43,"lucianopaz mentioned 2022-10-01 16:08:22,lucianopaz subscribed 2022-10-01 16:08:23,maresb mentioned 2022-10-01 17:37:58,maresb subscribed 2022-10-01 17:37:58,brandonwillard labeled 2022-10-01 18:18:27,brandonwillard labeled 2022-10-01 18:18:27,brandonwillard labeled 2022-10-01 18:18:27,brandonwillard mentioned 2022-10-01 18:25:42,brandonwillard subscribed 2022-10-01 18:25:42,opherdonchin mentioned 2022-10-01 18:25:42,opherdonchin subscribed 2022-10-01 18:25:42,brandonwillard mentioned 2022-10-01 18:55:10,brandonwillard subscribed 2022-10-01 18:55:10,opherdonchin mentioned 2022-10-01 18:55:10,opherdonchin subscribed 2022-10-01 18:55:10,brandonwillard mentioned 2022-10-01 21:08:06,brandonwillard subscribed 2022-10-01 21:08:06,maresb mentioned 2022-10-01 21:08:06,maresb subscribed 2022-10-01 21:08:06,lucianopaz mentioned 2022-10-05 06:57:01,lucianopaz subscribed 2022-10-05 06:57:01,opherdonchin mentioned 2022-10-05 07:57:04,opherdonchin subscribed 2022-10-05 07:57:04,lucianopaz mentioned 2022-10-05 14:26:44,lucianopaz subscribed 2022-10-05 14:26:44,brandonwillard mentioned 2022-10-05 14:47:56,brandonwillard subscribed 2022-10-05 14:47:56,aesara-devs locked 2022-10-05 15:53:43,brandonwillard converted_to_discussion 2022-10-05 15:53:43",lucianopaz aesara-devs brandonwillard maresb opherdonchin,9
924,1233,Failure cases for `Scan` with Numba backend when `outputs_info` mixes `None` and `TensorVariable`s,jessegrabowski,"I have found two modes of failure in this case. I apologize if it's bad form to mix them in a single issue, but they seem quite related so I'm presenting them together. Here's an MRE with some test cases:

```python
x = at.dscalar('x')

def threes():
    return at.constant(3.0)

def count(x):
    return x + 1

def mixed_returns_1(x):
    return x + 1, at.constant(3.0)

def mixed_returns_2(x):
    return x + 1, x

count_result, count_updates = aesara.scan(count,
                                          outputs_info=[x],
                                          n_steps=10)

three_result, three_updates = aesara.scan(threes,
                                          outputs_info=[None],
                                          n_steps=10)

mixed_result1, mixed_updates1 = aesara.scan(mixed_returns_1,
                                          outputs_info=[x, None],
                                          n_steps=10)

mixed_result2, mixed_updates2 = aesara.scan(mixed_returns_2,
                                          outputs_info=[x, None],
                                          n_steps=10)


f_count = aesara.function([x], count_result, mode='NUMBA', updates=count_updates)
f_three = aesara.function([], three_result, mode='NUMBA', updates=three_updates)
f_mixed1 = aesara.function([x], mixed_result1, mode='NUMBA', updates=mixed_updates1)
f_mixed2 = aesara.function([x], mixed_result2, mode='NUMBA', updates=mixed_updates2)
```

The first two cases work as expected:
```
f_count(3)
>>> array([ 3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.])

f_three()
>>> array([3., 3., 3., 3., 3., 3., 3., 3., 3., 3.], dtype=float32)
```

The third case fails because (I think?) a variable is not automatically created for `at.constant(3)`.

```
f_mixed1(3)
```
<details><summary>NameError: name 'auto_15128' is not defined</summary>

<p>

```
---------------------------------------------------------------------------
TypingError                               Traceback (most recent call last)
File ~\\miniconda3\\envs\\econ\\lib\\site-packages\\aesara\\link\\utils.py:202, in streamline.<locals>.streamline_default_f()
    199 for thunk, node, old_storage in zip(
    200     thunks, order, post_thunk_old_storage
    201 ):
--> 202     thunk()
    203     for old_s in old_storage:

File ~\\miniconda3\\envs\\econ\\lib\\site-packages\\aesara\\link\\basic.py:668, in JITLinker.create_jitable_thunk.<locals>.thunk(fgraph, fgraph_jit, thunk_inputs, thunk_outputs)
    662 def thunk(
    663     fgraph=self.fgraph,
    664     fgraph_jit=fgraph_jit,
    665     thunk_inputs=thunk_inputs,
    666     thunk_outputs=thunk_outputs,
    667 ):
--> 668     outputs = fgraph_jit(*[x[0] for x in thunk_inputs])
    670     for o_var, o_storage, o_val in zip(fgraph.outputs, thunk_outputs, outputs):

File ~\\miniconda3\\envs\\econ\\lib\\site-packages\\numba\\core\\dispatcher.py:468, in _DispatcherBase._compile_for_args(self, *args, **kws)
    466         e.patch_message(msg)
--> 468     error_rewrite(e, 'typing')
    469 except errors.UnsupportedError as e:
    470     # Something unsupported is present in the user code, add help info

File ~\\miniconda3\\envs\\econ\\lib\\site-packages\\numba\\core\\dispatcher.py:409, in _DispatcherBase._compile_for_args.<locals>.error_rewrite(e, issue_type)
    408 else:
--> 409     raise e.with_traceback(None)

TypingError: Failed in nopython mode pipeline (step: nopython frontend)
Failed in nopython mode pipeline (step: nopython frontend)
Failed in nopython mode pipeline (step: nopython frontend)
NameError: name 'auto_15128' is not defined
During: resolving callee type: type(CPUDispatcher(<function numba_funcified_fgraph at 0x0000029591A25820>))
During: typing of call at C:\\Users\\Jesse\\AppData\\Local\\Temp\\tmp4096101l (8)

During: resolving callee type: type(CPUDispatcher(<function numba_funcified_fgraph at 0x0000029591A25820>))
During: typing of call at C:\\Users\\Jesse\\AppData\\Local\\Temp\\tmp4096101l (8)


File ""..\\..\\AppData\\Local\\Temp\\tmp4096101l"", line 8:
def scan(n_steps, auto_17049_0, auto_15051_1):
    <source elided>
        inner_args = (auto_17049_0[i], )
        (auto_17049_0[i+1], auto_15051_1[i]) = numba_at_inner_func(*inner_args)
        ^

During: resolving callee type: type(CPUDispatcher(<function scan at 0x0000029591A25670>))
During: typing of call at C:\\Users\\Jesse\\AppData\\Local\\Temp\\tmpmiu4neto (12)

During: resolving callee type: type(CPUDispatcher(<function scan at 0x0000029591A25670>))
During: typing of call at C:\\Users\\Jesse\\AppData\\Local\\Temp\\tmpmiu4neto (12)


File ""..\\..\\AppData\\Local\\Temp\\tmpmiu4neto"", line 12:
def numba_funcified_fgraph(x):
    <source elided>
    # forall_inplace,cpu,scan_fn}(TensorConstant{10}, IncSubtensor{InplaceSet;:int64:}.0, TensorConstant{10})
    auto_17051, auto_17052 = scan(auto_15051, auto_17049, auto_15051)
    ^


During handling of the above exception, another exception occurred:

TypingError                               Traceback (most recent call last)
Cell In [35], line 1
----> 1 f_mixed1(3)

File ~\\miniconda3\\envs\\econ\\lib\\site-packages\\aesara\\compile\\function\\types.py:971, in Function.__call__(self, *args, **kwargs)
    968 t0_fn = time.time()
    969 try:
    970     outputs = (
--> 971         self.vm()
    972         if output_subset is None
    973         else self.vm(output_subset=output_subset)
    974     )
    975 except Exception:
    976     restore_defaults()

File ~\\miniconda3\\envs\\econ\\lib\\site-packages\\aesara\\link\\utils.py:206, in streamline.<locals>.streamline_default_f()
    204             old_s[0] = None
    205 except Exception:
--> 206     raise_with_op(fgraph, node, thunk)

File ~\\miniconda3\\envs\\econ\\lib\\site-packages\\aesara\\link\\utils.py:534, in raise_with_op(fgraph, node, thunk, exc_info, storage_map)
    529     warnings.warn(
    530         f""{exc_type} error does not allow us to add an extra error message""
    531     )
    532     # Some exception need extra parameter in inputs. So forget the
    533     # extra long error message in that case.
--> 534 raise exc_value.with_traceback(exc_trace)

File ~\\miniconda3\\envs\\econ\\lib\\site-packages\\aesara\\link\\utils.py:202, in streamline.<locals>.streamline_default_f()
    198 try:
    199     for thunk, node, old_storage in zip(
    200         thunks, order, post_thunk_old_storage
    201     ):
--> 202         thunk()
    203         for old_s in old_storage:
    204             old_s[0] = None

File ~\\miniconda3\\envs\\econ\\lib\\site-packages\\aesara\\link\\basic.py:668, in JITLinker.create_jitable_thunk.<locals>.thunk(fgraph, fgraph_jit, thunk_inputs, thunk_outputs)
    662 def thunk(
    663     fgraph=self.fgraph,
    664     fgraph_jit=fgraph_jit,
    665     thunk_inputs=thunk_inputs,
    666     thunk_outputs=thunk_outputs,
    667 ):
--> 668     outputs = fgraph_jit(*[x[0] for x in thunk_inputs])
    670     for o_var, o_storage, o_val in zip(fgraph.outputs, thunk_outputs, outputs):
    671         compute_map[o_var][0] = True

File ~\\miniconda3\\envs\\econ\\lib\\site-packages\\numba\\core\\dispatcher.py:468, in _DispatcherBase._compile_for_args(self, *args, **kws)
    464         msg = (f""{str(e).rstrip()} \\n\\nThis error may have been caused ""
    465                f""by the following argument(s):\\n{args_str}\\n"")
    466         e.patch_message(msg)
--> 468     error_rewrite(e, 'typing')
    469 except errors.UnsupportedError as e:
    470     # Something unsupported is present in the user code, add help info
    471     error_rewrite(e, 'unsupported_error')

File ~\\miniconda3\\envs\\econ\\lib\\site-packages\\numba\\core\\dispatcher.py:409, in _DispatcherBase._compile_for_args.<locals>.error_rewrite(e, issue_type)
    407     raise e
    408 else:
--> 409     raise e.with_traceback(None)

TypingError: Failed in nopython mode pipeline (step: nopython frontend)
Failed in nopython mode pipeline (step: nopython frontend)
Failed in nopython mode pipeline (step: nopython frontend)
NameError: name 'auto_15128' is not defined
During: resolving callee type: type(CPUDispatcher(<function numba_funcified_fgraph at 0x0000029591A25820>))
During: typing of call at C:\\Users\\Jesse\\AppData\\Local\\Temp\\tmp4096101l (8)

During: resolving callee type: type(CPUDispatcher(<function numba_funcified_fgraph at 0x0000029591A25820>))
During: typing of call at C:\\Users\\Jesse\\AppData\\Local\\Temp\\tmp4096101l (8)


File ""..\\..\\AppData\\Local\\Temp\\tmp4096101l"", line 8:
def scan(n_steps, auto_17049_0, auto_15051_1):
    <source elided>
        inner_args = (auto_17049_0[i], )
        (auto_17049_0[i+1], auto_15051_1[i]) = numba_at_inner_func(*inner_args)
        ^

During: resolving callee type: type(CPUDispatcher(<function scan at 0x0000029591A25670>))
During: typing of call at C:\\Users\\Jesse\\AppData\\Local\\Temp\\tmpmiu4neto (12)

During: resolving callee type: type(CPUDispatcher(<function scan at 0x0000029591A25670>))
During: typing of call at C:\\Users\\Jesse\\AppData\\Local\\Temp\\tmpmiu4neto (12)


File ""..\\..\\AppData\\Local\\Temp\\tmpmiu4neto"", line 12:
def numba_funcified_fgraph(x):
    <source elided>
    # forall_inplace,cpu,scan_fn}(TensorConstant{10}, IncSubtensor{InplaceSet;:int64:}.0, TensorConstant{10})
    auto_17051, auto_17052 = scan(auto_15051, auto_17049, auto_15051)
    ^

Apply node that caused the error: forall_inplace,cpu,scan_fn}(TensorConstant{10}, IncSubtensor{InplaceSet;:int64:}.0, TensorConstant{10})
Toposort index: 4
Inputs types: [TensorType(int8, ()), TensorType(float64, (None,)), TensorType(int8, ())]
Inputs shapes: [()]
Inputs strides: [()]
Inputs values: [array(3.)]
Outputs clients: [['output'], ['output']]

HINT: Re-running with most Aesara optimizations disabled could provide a back-trace showing when this node was created. This can be done by setting the Aesara flag 'optimizer=fast_compile'. If that does not work, Aesara optimizations can be disabled with 'optimizer=None'.
HINT: Use the Aesara flag `exception_verbosity=high` for a debug print-out and storage map footprint of this Apply node.

```

</p>

</details>

The final case fails due to a failed item assignment. It seems that the return variable associated with the `None` is an array inside a list? 

```
f_mixed2(3)
```

<details><summary>No implementation of function Function(<built-in function setitem>) found for signature: >>> setitem(list(array(float64, 0d, C))<iv=None>, int32, float64)</summary>

<p>

```
---------------------------------------------------------------------------
TypingError                               Traceback (most recent call last)
File ~\\miniconda3\\envs\\econ\\lib\\site-packages\\aesara\\link\\utils.py:202, in streamline.<locals>.streamline_default_f()
    199 for thunk, node, old_storage in zip(
    200     thunks, order, post_thunk_old_storage
    201 ):
--> 202     thunk()
    203     for old_s in old_storage:

File ~\\miniconda3\\envs\\econ\\lib\\site-packages\\aesara\\link\\basic.py:668, in JITLinker.create_jitable_thunk.<locals>.thunk(fgraph, fgraph_jit, thunk_inputs, thunk_outputs)
    662 def thunk(
    663     fgraph=self.fgraph,
    664     fgraph_jit=fgraph_jit,
    665     thunk_inputs=thunk_inputs,
    666     thunk_outputs=thunk_outputs,
    667 ):
--> 668     outputs = fgraph_jit(*[x[0] for x in thunk_inputs])
    670     for o_var, o_storage, o_val in zip(fgraph.outputs, thunk_outputs, outputs):

File ~\\miniconda3\\envs\\econ\\lib\\site-packages\\numba\\core\\dispatcher.py:468, in _DispatcherBase._compile_for_args(self, *args, **kws)
    466         e.patch_message(msg)
--> 468     error_rewrite(e, 'typing')
    469 except errors.UnsupportedError as e:
    470     # Something unsupported is present in the user code, add help info

File ~\\miniconda3\\envs\\econ\\lib\\site-packages\\numba\\core\\dispatcher.py:409, in _DispatcherBase._compile_for_args.<locals>.error_rewrite(e, issue_type)
    408 else:
--> 409     raise e.with_traceback(None)

TypingError: Failed in nopython mode pipeline (step: nopython frontend)
Failed in nopython mode pipeline (step: nopython frontend)
No implementation of function Function(<built-in function setitem>) found for signature:
 
 >>> setitem(list(array(float64, 0d, C))<iv=None>, int32, float64)
 
There are 16 candidate implementations:
    - Of which 14 did not match due to:
    Overload of function 'setitem': File: <numerous>: Line N/A.
      With argument(s): '(list(array(float64, 0d, C))<iv=None>, int32, float64)':
     No match.
    - Of which 2 did not match due to:
    Overload in function 'SetItemSequence.generic': File: numba\\core\\typing\\collections.py: Line 56.
      With argument(s): '(list(array(float64, 0d, C))<iv=None>, int32, float64)':
     Rejected as the implementation raised a specific error:
       TypingError: invalid setitem with value of float64 to element of array(float64, 0d, C)
  raised from C:\\Users\\Jesse\\miniconda3\\envs\\econ\\lib\\site-packages\\numba\\core\\typing\\collections.py:65

During: typing of setitem at C:\\Users\\Jesse\\AppData\\Local\\Temp\\tmpjararahe (8)

File ""..\\..\\AppData\\Local\\Temp\\tmpjararahe"", line 8:
def scan(n_steps, auto_17955_0, auto_15146_1):
    <source elided>
        inner_args = (auto_17955_0[i], )
        (auto_17955_0[i+1], auto_15146_1[i]) = numba_at_inner_func(*inner_args)
        ^

During: resolving callee type: type(CPUDispatcher(<function scan at 0x0000029591450F70>))
During: typing of call at C:\\Users\\Jesse\\AppData\\Local\\Temp\\tmp4zfz_tx0 (12)

During: resolving callee type: type(CPUDispatcher(<function scan at 0x0000029591450F70>))
During: typing of call at C:\\Users\\Jesse\\AppData\\Local\\Temp\\tmp4zfz_tx0 (12)


File ""..\\..\\AppData\\Local\\Temp\\tmp4zfz_tx0"", line 12:
def numba_funcified_fgraph(x):
    <source elided>
    # forall_inplace,cpu,scan_fn}(TensorConstant{10}, IncSubtensor{InplaceSet;:int64:}.0, TensorConstant{10})
    auto_17957, auto_17958 = scan(auto_15146, auto_17955, auto_15146)
    ^


During handling of the above exception, another exception occurred:

TypingError                               Traceback (most recent call last)
Cell In [33], line 1
----> 1 f_mixed2(3)

File ~\\miniconda3\\envs\\econ\\lib\\site-packages\\aesara\\compile\\function\\types.py:971, in Function.__call__(self, *args, **kwargs)
    968 t0_fn = time.time()
    969 try:
    970     outputs = (
--> 971         self.vm()
    972         if output_subset is None
    973         else self.vm(output_subset=output_subset)
    974     )
    975 except Exception:
    976     restore_defaults()

File ~\\miniconda3\\envs\\econ\\lib\\site-packages\\aesara\\link\\utils.py:206, in streamline.<locals>.streamline_default_f()
    204             old_s[0] = None
    205 except Exception:
--> 206     raise_with_op(fgraph, node, thunk)

File ~\\miniconda3\\envs\\econ\\lib\\site-packages\\aesara\\link\\utils.py:534, in raise_with_op(fgraph, node, thunk, exc_info, storage_map)
    529     warnings.warn(
    530         f""{exc_type} error does not allow us to add an extra error message""
    531     )
    532     # Some exception need extra parameter in inputs. So forget the
    533     # extra long error message in that case.
--> 534 raise exc_value.with_traceback(exc_trace)

File ~\\miniconda3\\envs\\econ\\lib\\site-packages\\aesara\\link\\utils.py:202, in streamline.<locals>.streamline_default_f()
    198 try:
    199     for thunk, node, old_storage in zip(
    200         thunks, order, post_thunk_old_storage
    201     ):
--> 202         thunk()
    203         for old_s in old_storage:
    204             old_s[0] = None

File ~\\miniconda3\\envs\\econ\\lib\\site-packages\\aesara\\link\\basic.py:668, in JITLinker.create_jitable_thunk.<locals>.thunk(fgraph, fgraph_jit, thunk_inputs, thunk_outputs)
    662 def thunk(
    663     fgraph=self.fgraph,
    664     fgraph_jit=fgraph_jit,
    665     thunk_inputs=thunk_inputs,
    666     thunk_outputs=thunk_outputs,
    667 ):
--> 668     outputs = fgraph_jit(*[x[0] for x in thunk_inputs])
    670     for o_var, o_storage, o_val in zip(fgraph.outputs, thunk_outputs, outputs):
    671         compute_map[o_var][0] = True

File ~\\miniconda3\\envs\\econ\\lib\\site-packages\\numba\\core\\dispatcher.py:468, in _DispatcherBase._compile_for_args(self, *args, **kws)
    464         msg = (f""{str(e).rstrip()} \\n\\nThis error may have been caused ""
    465                f""by the following argument(s):\\n{args_str}\\n"")
    466         e.patch_message(msg)
--> 468     error_rewrite(e, 'typing')
    469 except errors.UnsupportedError as e:
    470     # Something unsupported is present in the user code, add help info
    471     error_rewrite(e, 'unsupported_error')

File ~\\miniconda3\\envs\\econ\\lib\\site-packages\\numba\\core\\dispatcher.py:409, in _DispatcherBase._compile_for_args.<locals>.error_rewrite(e, issue_type)
    407     raise e
    408 else:
--> 409     raise e.with_traceback(None)

TypingError: Failed in nopython mode pipeline (step: nopython frontend)
Failed in nopython mode pipeline (step: nopython frontend)
No implementation of function Function(<built-in function setitem>) found for signature:
 
 >>> setitem(list(array(float64, 0d, C))<iv=None>, int32, float64)
 
There are 16 candidate implementations:
    - Of which 14 did not match due to:
    Overload of function 'setitem': File: <numerous>: Line N/A.
      With argument(s): '(list(array(float64, 0d, C))<iv=None>, int32, float64)':
     No match.
    - Of which 2 did not match due to:
    Overload in function 'SetItemSequence.generic': File: numba\\core\\typing\\collections.py: Line 56.
      With argument(s): '(list(array(float64, 0d, C))<iv=None>, int32, float64)':
     Rejected as the implementation raised a specific error:
       TypingError: invalid setitem with value of float64 to element of array(float64, 0d, C)
  raised from C:\\Users\\Jesse\\miniconda3\\envs\\econ\\lib\\site-packages\\numba\\core\\typing\\collections.py:65

During: typing of setitem at C:\\Users\\Jesse\\AppData\\Local\\Temp\\tmpjararahe (8)

File ""..\\..\\AppData\\Local\\Temp\\tmpjararahe"", line 8:
def scan(n_steps, auto_17955_0, auto_15146_1):
    <source elided>
        inner_args = (auto_17955_0[i], )
        (auto_17955_0[i+1], auto_15146_1[i]) = numba_at_inner_func(*inner_args)
        ^

During: resolving callee type: type(CPUDispatcher(<function scan at 0x0000029591450F70>))
During: typing of call at C:\\Users\\Jesse\\AppData\\Local\\Temp\\tmp4zfz_tx0 (12)

During: resolving callee type: type(CPUDispatcher(<function scan at 0x0000029591450F70>))
During: typing of call at C:\\Users\\Jesse\\AppData\\Local\\Temp\\tmp4zfz_tx0 (12)


File ""..\\..\\AppData\\Local\\Temp\\tmp4zfz_tx0"", line 12:
def numba_funcified_fgraph(x):
    <source elided>
    # forall_inplace,cpu,scan_fn}(TensorConstant{10}, IncSubtensor{InplaceSet;:int64:}.0, TensorConstant{10})
    auto_17957, auto_17958 = scan(auto_15146, auto_17955, auto_15146)
    ^

Apply node that caused the error: forall_inplace,cpu,scan_fn}(TensorConstant{10}, IncSubtensor{InplaceSet;:int64:}.0, TensorConstant{10})
Toposort index: 4
Inputs types: [TensorType(int8, ()), TensorType(float64, (None,)), TensorType(int8, ())]
Inputs shapes: [()]
Inputs strides: [()]
Inputs values: [array(3.)]
Outputs clients: [['output'], ['output']]

HINT: Re-running with most Aesara optimizations disabled could provide a back-trace showing when this node was created. This can be done by setting the Aesara flag 'optimizer=fast_compile'. If that does not work, Aesara optimizations can be disabled with 'optimizer=None'.
HINT: Use the Aesara flag `exception_verbosity=high` for a debug print-out and storage map footprint of this Apply node.

```

</p>

</details>

I have this list-wrapping hypothesis because I can re-create nearly the exact error message with the following code snippet:

```python
@njit
def set_item(l, i, x):
    l[i] = x
    return l

set_item([np.zeros(())], 0, 3)
```

Note that `aesara.gradient.jacobian` fails out-of-the-box with the final case (invalid indexing) when you try to compile it to numba.
",bug important Numba Scan,,2022-10-04 21:38:45,2022-10-05 01:05:54,"jessegrabowski renamed 2022-10-04 21:38:58,brandonwillard labeled 2022-10-04 22:02:55,brandonwillard labeled 2022-10-04 22:02:55,brandonwillard labeled 2022-10-04 22:02:55,brandonwillard labeled 2022-10-04 22:02:55,brandonwillard labeled 2022-10-04 22:03:00,brandonwillard unlabeled 2022-10-04 23:08:34,brandonwillard connected 2022-10-04 23:12:24,brandonwillard closed 2022-10-05 01:05:54",jessegrabowski brandonwillard,2
926,1236,Can't import aesara on windows : FileNotFoundError ,lt-brs,"## Can't import aesara on windows : FileNotFoundError 

When I try to import aesara, the following error is raised. I'm working on Windows 10, 64bits.
Most surprisingly, the error occur in a jupyter-notebook but not in VS Code.

**Please provide a minimal, self-contained, and reproducible example.**
```python
import aesara
```

**Please provide the full traceback of any errors.**
```python
---------------------------------------------------------------------------
NoSectionError                            Traceback (most recent call last)
File ~\\anaconda3\\envs\\prism_prod\\lib\\site-packages\\aesara\\configparser.py:237, in AesaraConfigParser.fetch_val_for_key(self, key, delete_key)
    236 try:
--> 237     return self._aesara_cfg.get(section, option)
    238 except InterpolationError:
File ~\\anaconda3\\envs\\prism_prod\\lib\\configparser.py:781, in RawConfigParser.get(self, section, option, raw, vars, fallback)
    780 try:
--> 781     d = self._unify_values(section, vars)
    782 except NoSectionError:
File ~\\anaconda3\\envs\\prism_prod\\lib\\configparser.py:1152, in RawConfigParser._unify_values(self, section, vars)
   1151     if section != self.default_section:
-> 1152         raise NoSectionError(section) from None
   1153 # Update with the entry specific variables
NoSectionError: No section: 'blas'
During handling of the above exception, another exception occurred:
KeyError                                  Traceback (most recent call last)
File ~\\anaconda3\\envs\\prism_prod\\lib\\site-packages\\aesara\\configparser.py:353, in ConfigParam.__get__(self, cls, type_, delete_key)
    352 try:
--> 353     val_str = cls.fetch_val_for_key(self.name, delete_key=delete_key)
    354     self.is_default = False
File ~\\anaconda3\\envs\\prism_prod\\lib\\site-packages\\aesara\\configparser.py:241, in AesaraConfigParser.fetch_val_for_key(self, key, delete_key)
    240 except (NoOptionError, NoSectionError):
--> 241     raise KeyError(key)
KeyError: 'blas__ldflags'
During handling of the above exception, another exception occurred:
FileNotFoundError                         Traceback (most recent call last)
Cell In [2], line 1
----> 1 import aesara
File ~\\anaconda3\\envs\\prism_prod\\lib\\site-packages\\aesara\\__init__.py:120
    116     return as_tensor_variable(x, **kwargs)
    119 # isort: off
--> 120 from aesara import scalar, tensor
    121 from aesara.compile import (
    122     In,
    123     Mode,
   (...)
    129     shared,
    130 )
    131 from aesara.compile.function import function, function_dump
File ~\\anaconda3\\envs\\prism_prod\\lib\\site-packages\\aesara\\tensor\\__init__.py:105
    103 # adds shared-variable constructors
    104 from aesara.tensor import sharedvar  # noqa
--> 105 from aesara.tensor import (  # noqa
    106     blas,
    107     blas_c,
    108     blas_scipy,
    109     nnet,
    110     xlogx,
    111 )
    112 import aesara.tensor.rewriting
    115 # isort: off
File ~\\anaconda3\\envs\\prism_prod\\lib\\site-packages\\aesara\\tensor\\blas.py:162
    160 from aesara.scalar import bool as bool_t
    161 from aesara.tensor import basic as at
--> 162 from aesara.tensor.blas_headers import blas_header_text, blas_header_version
    163 from aesara.tensor.elemwise import DimShuffle, Elemwise
    164 from aesara.tensor.exceptions import NotScalarConstantError
File ~\\anaconda3\\envs\\prism_prod\\lib\\site-packages\\aesara\\tensor\\blas_headers.py:1015
    997             header += textwrap.dedent(
    998                 """"""\\
    999                     static float sdot_(int* Nx, float* x, int* Sx, float* y, int* Sy)
   (...)
   1009                     """"""
   1010             )
   1012     return header + blas_code
-> 1015 if not config.blas__ldflags:
   1016     _logger.warning(""Using NumPy C-API based implementation for BLAS functions."")
   1019 def mkl_threads_text():
File ~\\anaconda3\\envs\\prism_prod\\lib\\site-packages\\aesara\\configparser.py:357, in ConfigParam.__get__(self, cls, type_, delete_key)
    355 except KeyError:
    356     if callable(self.default):
--> 357         val_str = self.default()
    358     else:
    359         val_str = self.default
File ~\\anaconda3\\envs\\prism_prod\\lib\\site-packages\\aesara\\link\\c\\cmodule.py:2863, in default_blas_ldflags()
   2861 if any(""mkl"" in fl for fl in ret):
   2862     ret.extend([""-lm"", ""-lm""])
-> 2863 res = try_blas_flag(ret)
   2864 if res:
   2865     if ""mkl"" in res:
File ~\\anaconda3\\envs\\prism_prod\\lib\\site-packages\\aesara\\link\\c\\cmodule.py:1995, in try_blas_flag(flags)
   1992 path_wrapper = '""' if os.name == ""nt"" else """"
   1993 cflags.extend([f""-L{path_wrapper}{d}{path_wrapper}"" for d in std_lib_dirs()])
-> 1995 res = GCC_compiler.try_compile_tmp(
   1996     test_code, tmp_prefix=""try_blas_"", flags=cflags, try_run=True
   1997 )
   1998 # res[0]: shows successful compilation
   1999 # res[1]: shows successful execution
   2000 if res and res[0] and res[1]:
File ~\\anaconda3\\envs\\prism_prod\\lib\\site-packages\\aesara\\link\\c\\cmodule.py:2396, in GCC_compiler.try_compile_tmp(cls, src_code, tmp_prefix, flags, try_run, output, comp_args)
   2383 @classmethod
   2384 def try_compile_tmp(
   2385     cls,
   (...)
   2391     comp_args=True,
   2392 ):
   2393     return cls._try_compile_tmp(
   2394         src_code,
   2395         tmp_prefix,
-> 2396         cls.patch_ldflags(flags),
   2397         try_run,
   2398         output,
   2399         config.cxx,
   2400         comp_args,
   2401     )
File ~\\anaconda3\\envs\\prism_prod\\lib\\site-packages\\aesara\\link\\c\\cmodule.py:2434, in GCC_compiler.patch_ldflags(flag_list)
   2432 if not libs:
   2433     return flag_list
-> 2434 libs = GCC_compiler.linking_patch(lib_dirs, libs)
   2435 for flag_idx, lib in zip(flag_idxs, libs):
   2436     flag_list[flag_idx] = lib
File ~\\anaconda3\\envs\\prism_prod\\lib\\site-packages\\aesara\\link\\c\\cmodule.py:2455, in GCC_compiler.linking_patch(lib_dirs, libs)
   2451 for lib_dir in lib_dirs:
   2452     lib_dir = lib_dir.strip('""')
   2453     windows_styled_libs = [
   2454         fname
-> 2455         for fname in os.listdir(lib_dir)
   2456         if not (os.path.isdir(os.path.join(lib_dir, fname)))
   2457         and fname.split(""."")[0] == lib
   2458         and fname.split(""."")[-1] in [""dll"", ""lib""]
   2459     ]
   2460     if windows_styled_libs:
   2461         selected_lib = sorted(windows_styled_libs, key=sort_key)[-1]
FileNotFoundError: [WinError 3] Le chemin d’accès spécifié est introuvable: 'D:\\\\a\\\\1\\\\s\\\\numpy\\\\build\\\\openblas_info'
```


## Versions and main components

* Aesara version: 2.8.2
* Aesara config (`python -c ""import aesara; print(aesara.config)""`) : can't import aesara
* Python version: 3.9.11
* Operating system: Windows 64 bits
* How did you install Aesara: (conda/pip) pip (via poetry)
",,,2022-10-05 09:59:50,2022-10-05 15:51:09,"aesara-devs locked 2022-10-05 15:51:09,brandonwillard converted_to_discussion 2022-10-05 15:51:09",lt-brs aesara-devs brandonwillard,0
929,1246,Function profiling should use `time.time_ns` instead of `time.time`,michaelosthege,"## Description of your problem or feature request

The function profiling in https://github.com/aesara-devs/aesara/blob/2dc0af2f921e9d8795f270c0ec7a294c0b49a0be/aesara/compile/function/types.py#L827 (and more lines that file) uses `time.time()` which has varying precision on different platforms.

`time.time_ns()` or `time.time_ns() / (10 ** 9)` for float seconds should be used for high-precision performance measurement.

Further reading: https://stackoverflow.com/a/1938096

## Versions and main components

* Aesara version: 2.8.7
* Operating system: mac-OS, Windows
",enhancement good first issue help wanted refactor,,2022-10-09 22:17:46,2022-10-25 01:29:53,"ricardoV94 referenced 2022-10-09 22:19:12,brandonwillard labeled 2022-10-09 22:58:51,brandonwillard labeled 2022-10-09 22:58:51,brandonwillard milestoned 2022-10-09 22:58:55,brandonwillard labeled 2022-10-09 22:59:19,brandonwillard labeled 2022-10-09 22:59:23,michaelosthege referenced 2022-10-09 23:18:54,brandonwillard mentioned 2022-10-18 06:06:13,brandonwillard subscribed 2022-10-18 06:06:13,brandonwillard closed 2022-10-25 01:29:54",anirudhacharya ricardoV94 michaelosthege brandonwillard,3
930,1249,Aesara fails to set correct blas ldflags in some windows installations,lucianopaz,"## Description of your problem or feature request

We’ve seen two independent reports of this happen in pymc. See [here](https://github.com/pymc-devs/pymc/issues/6182), [here](https://github.com/aesara-devs/aesara/discussions/1239) and [here](https://discourse.pymc.io/t/failure-to-link-with-mkl-rt-under-windows/10502/1).

**Please provide a minimal, self-contained, and reproducible example.**

Both @opherdonchin and @pandrich said that they had installed pymc by doing

```bash
conda install pymc -c conda-forge
```

Into a new and clean environment. Then the following snippet failed


```python

import aesara
from aesara import tensor as at

x = at.dvector(“x”)
b = at.dmatrix(“b”)
y = at.dot(b, x)
f = aesara.function([x, b], y)
f([1, 0], [[0.5, 0.5], [0.5, 0.5]])

```

**Please provide the full traceback of any errors.**
```python

You can find the C code in this temporary file: C:\\Users\\OPHERL~1\\AppData\\Local\\Temp\\aesara_compilation_error_cqab8ef9
library mkl_rt is not found.
Traceback (most recent call last):
File ""C:\\miniconda3\\envs\\pymc_env_aesara\\lib\\site-packages\\aesara\\link\\vm.py"", line 1246, in make_all
node.op.make_thunk(node, storage_map, compute_map, [], impl=impl)
File ""C:\\miniconda3\\envs\\pymc_env_aesara\\lib\\site-packages\\aesara\\link\\c\\op.py"", line 131, in make_thunk
return self.make_c_thunk(node, storage_map, compute_map, no_recycling)
File ""C:\\miniconda3\\envs\\pymc_env_aesara\\lib\\site-packages\\aesara\\link\\c\\op.py"", line 96, in make_c_thunk
outputs = cl.make_thunk(
File ""C:\\miniconda3\\envs\\pymc_env_aesara\\lib\\site-packages\\aesara\\link\\c\\basic.py"", line 1202, in make_thunk
cthunk, module, in_storage, out_storage, error_storage = self.compile(
File ""C:\\miniconda3\\envs\\pymc_env_aesara\\lib\\site-packages\\aesara\\link\\c\\basic.py"", line 1122, in compile
thunk, module = self.cthunk_factory(
File ""C:\\miniconda3\\envs\\pymc_env_aesara\\lib\\site-packages\\aesara\\link\\c\\basic.py"", line 1647, in cthunk_factory
module = cache.module_from_key(key=key, lnk=self)
File ""C:\\miniconda3\\envs\\pymc_env_aesara\\lib\\site-packages\\aesara\\link\\c\\cmodule.py"", line 1229, in module_from_key
module = lnk.compile_cmodule(location)
File ""C:\\miniconda3\\envs\\pymc_env_aesara\\lib\\site-packages\\aesara\\link\\c\\basic.py"", line 1546, in compile_cmodule
module = c_compiler.compile_str(
File ""C:\\miniconda3\\envs\\pymc_env_aesara\\lib\\site-packages\\aesara\\link\\c\\cmodule.py"", line 2640, in compile_str
raise CompileError(
aesara.link.c.exceptions.CompileError: Compilation failed (return status=1):
""C:\\miniconda3\\envs\\pymc_env\\Library\\mingw-w64\\bin\\g++.exe"" -shared -g -O3 -fno-math-errno -Wno-unused-label -Wno-unused-variable -Wno-write-strings -march=broadwell -mmmx -mno-3dnow -msse -msse2 -msse3 -mssse3 -mno-sse4a -mcx16 -msahf -mmovbe -maes -mno-sha -mpclmul -mpopcnt -mabm -mno-lwp -mfma -mno-fma4 -mno-xop -mbmi -mbmi2 -mno-tbm -mavx -mavx2 -msse4.2 -msse4.1 -mlzcnt -mno-rtm -mno-hle -mrdrnd -mf16c -mfsgsbase -mrdseed -mprfchw -madx -mfxsr -mxsave -mxsaveopt -mno-avx512f -mno-avx512er -mno-avx512cd -mno-avx512pf -mno-prefetchwt1 -mclflushopt -mxsavec -mxsaves -mno-avx512dq -mno-avx512bw -mno-avx512vl -mno-avx512ifma -mno-avx512vbmi -mno-clwb -mno-pcommit -mno-mwaitx --param l1-cache-size=32 --param l1-cache-line-size=64 --param l2-cache-size=8192 -mtune=generic -DNPY_NO_DEPRECATED_API=NPY_1_7_API_VERSION -m64 -DMS_WIN64 -I""C:\\miniconda3\\envs\\pymc_env_aesara\\lib\\site-packages\\numpy\\core\\include"" -I""C:\\miniconda3\\envs\\pymc_env_aesara\\include"" -I""C:\\miniconda3\\envs\\pymc_env_aesara\\lib\\site-packages\\aesara\\link\\c\\c_code"" -L""C:\\miniconda3\\envs\\pymc_env_aesara\\libs"" -L""C:\\miniconda3\\envs\\pymc_env_aesara"" -o ""C:\\Users\\Opher local\\AppData\\Local\\Aesara\\compiledir_Windows-10-10.0.19043-SP0-Intel64_Family_6_Model_142_Stepping_12_GenuineIntel-3.10.6-64\\tmp8du5yfuh\\md2fc7729a4f0f5eb7d1fa94d7650200f3de6577f147c69a812b2b820fec26507.pyd"" ""C:\\Users\\Opher local\\AppData\\Local\\Aesara\\compiledir_Windows-10-10.0.19043-SP0-Intel64_Family_6_Model_142_Stepping_12_GenuineIntel-3.10.6-64\\tmp8du5yfuh\\mod.cpp"" -lmkl_rt ""C:\\miniconda3\\envs\\pymc_env_aesara\\python310.dll""
C:/miniconda3/envs/pymc_env/Library/mingw-w64/bin/../lib/gcc/x86_64-w64-mingw32/5.3.0/../../../../x86_64-w64-mingw32/bin/ld.exe: cannot find -lmkl_rt
collect2.exe: error: ld returned 1 exit status

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
File ""c:\\Opher\\GitHub\\bsf_donchin_jordan_2022\\tests\\test_aesara_flags.py"", line 10, in 
f = aesara.function([x, b], y)
File ""C:\\miniconda3\\envs\\pymc_env_aesara\\lib\\site-packages\\aesara\\compile\\function_init_.py"", line 317, in function
fn = pfunc(
File ""C:\\miniconda3\\envs\\pymc_env_aesara\\lib\\site-packages\\aesara\\compile\\function\\pfunc.py"", line 371, in pfunc
return orig_function(
File ""C:\\miniconda3\\envs\\pymc_env_aesara\\lib\\site-packages\\aesara\\compile\\function\\types.py"", line 1759, in orig_function
fn = m.create(defaults)
File ""C:\\miniconda3\\envs\\pymc_env_aesara\\lib\\site-packages\\aesara\\compile\\function\\types.py"", line 1652, in create
_fn, _i, _o = self.linker.make_thunk(
File ""C:\\miniconda3\\envs\\pymc_env_aesara\\lib\\site-packages\\aesara\\link\\basic.py"", line 254, in make_thunk
return self.make_all(
File ""C:\\miniconda3\\envs\\pymc_env_aesara\\lib\\site-packages\\aesara\\link\\vm.py"", line 1255, in make_all
raise_with_op(fgraph, node)
File ""C:\\miniconda3\\envs\\pymc_env_aesara\\lib\\site-packages\\aesara\\link\\utils.py"", line 534, in raise_with_op
raise exc_value.with_traceback(exc_trace)
File ""C:\\miniconda3\\envs\\pymc_env_aesara\\lib\\site-packages\\aesara\\link\\vm.py"", line 1246, in make_all
node.op.make_thunk(node, storage_map, compute_map, [], impl=impl)
File ""C:\\miniconda3\\envs\\pymc_env_aesara\\lib\\site-packages\\aesara\\link\\c\\op.py"", line 131, in make_thunk
return self.make_c_thunk(node, storage_map, compute_map, no_recycling)
File ""C:\\miniconda3\\envs\\pymc_env_aesara\\lib\\site-packages\\aesara\\link\\c\\op.py"", line 96, in make_c_thunk
outputs = cl.make_thunk(
File ""C:\\miniconda3\\envs\\pymc_env_aesara\\lib\\site-packages\\aesara\\link\\c\\basic.py"", line 1202, in make_thunk
cthunk, module, in_storage, out_storage, error_storage = self.compile(
File ""C:\\miniconda3\\envs\\pymc_env_aesara\\lib\\site-packages\\aesara\\link\\c\\basic.py"", line 1122, in compile
thunk, module = self.cthunk_factory(
File ""C:\\miniconda3\\envs\\pymc_env_aesara\\lib\\site-packages\\aesara\\link\\c\\basic.py"", line 1647, in cthunk_factory
module = cache.module_from_key(key=key, lnk=self)
File ""C:\\miniconda3\\envs\\pymc_env_aesara\\lib\\site-packages\\aesara\\link\\c\\cmodule.py"", line 1229, in module_from_key
module = lnk.compile_cmodule(location)
File ""C:\\miniconda3\\envs\\pymc_env_aesara\\lib\\site-packages\\aesara\\link\\c\\basic.py"", line 1546, in compile_cmodule
module = c_compiler.compile_str(
File ""C:\\miniconda3\\envs\\pymc_env_aesara\\lib\\site-packages\\aesara\\link\\c\\cmodule.py"", line 2640, in compile_str
raise CompileError(
aesara.link.c.exceptions.CompileError: Compilation failed (return status=1):
""C:\\miniconda3\\envs\\pymc_env\\Library\\mingw-w64\\bin\\g++.exe"" -shared -g -O3 -fno-math-errno -Wno-unused-label -Wno-unused-variable -Wno-write-strings -march=broadwell -mmmx -mno-3dnow -msse -msse2 -msse3 -mssse3 -mno-sse4a -mcx16 -msahf -mmovbe -maes -mno-sha -mpclmul -mpopcnt -mabm -mno-lwp -mfma -mno-fma4 -mno-xop -mbmi -mbmi2 -mno-tbm -mavx -mavx2 -msse4.2 -msse4.1 -mlzcnt -mno-rtm -mno-hle -mrdrnd -mf16c -mfsgsbase -mrdseed -mprfchw -madx -mfxsr -mxsave -mxsaveopt -mno-avx512f -mno-avx512er -mno-avx512cd -mno-avx512pf -mno-prefetchwt1 -mclflushopt -mxsavec -mxsaves -mno-avx512dq -mno-avx512bw -mno-avx512vl -mno-avx512ifma -mno-avx512vbmi -mno-clwb -mno-pcommit -mno-mwaitx --param l1-cache-size=32 --param l1-cache-line-size=64 --param l2-cache-size=8192 -mtune=generic -DNPY_NO_DEPRECATED_API=NPY_1_7_API_VERSION -m64 -DMS_WIN64 -I""C:\\miniconda3\\envs\\pymc_env_aesara\\lib\\site-packages\\numpy\\core\\include"" -I""C:\\miniconda3\\envs\\pymc_env_aesara\\include"" -I""C:\\miniconda3\\envs\\pymc_env_aesara\\lib\\site-packages\\aesara\\link\\c\\c_code"" -L""C:\\miniconda3\\envs\\pymc_env_aesara\\libs"" -L""C:\\miniconda3\\envs\\pymc_env_aesara"" -o ""C:\\Users\\Opher local\\AppData\\Local\\Aesara\\compiledir_Windows-10-10.0.19043-SP0-Intel64_Family_6_Model_142_Stepping_12_GenuineIntel-3.10.6-64\\tmp8du5yfuh\\md2fc7729a4f0f5eb7d1fa94d7650200f3de6577f147c69a812b2b820fec26507.pyd"" ""C:\\Users\\Opher local\\AppData\\Local\\Aesara\\compiledir_Windows-10-10.0.19043-SP0-Intel64_Family_6_Model_142_Stepping_12_GenuineIntel-3.10.6-64\\tmp8du5yfuh\\mod.cpp"" -lmkl_rt ""C:\\miniconda3\\envs\\pymc_env_aesara\\python310.dll""
C:/miniconda3/envs/pymc_env/Library/mingw-w64/bin/../lib/gcc/x86_64-w64-mingw32/5.3.0/../../../../x86_64-w64-mingw32/bin/ld.exe: cannot find -lmkl_rt
collect2.exe: error: ld returned 1 exit status

Apply node that caused the error: CGemv{inplace}(AllocEmpty{dtype='float64'}.0, TensorConstant{1.0}, b, x, TensorConstant{0.0})
Toposort index: 2
Inputs types: [TensorType(float64, (None,)), TensorType(float64, ()), TensorType(float64, (None, None)), TensorType(float64, (None,)), TensorType(float64, ())]

HINT: Use a linker other than the C linker to print the inputs' shapes and strides.
HINT: Re-running with most Aesara optimizations disabled could provide a back-trace showing when this node was created. This can be done by setting the Aesara flag 'optimizer=fast_compile'. If that does not work, Aesara optimizations can be disabled with 'optimizer=None'.
HINT: Use the Aesara flag exception_verbosity=high for a debug print-out and storage map footprint of this Apply node.
```

**Please provide any additional information below.**

I’ll ask both @opherdonchin and @pandrich to post their aesara version numbers. You can see their aesara config in the linked issues/discussions, and note that the blas flags don’t include mkl, even though they are correctly installed by conda

## Versions and main components

* Aesara version:
* Aesara config (`python -c ""import aesara; print(aesara.config)""`)
* Python version:
* Operating system:
* How did you install Aesara: (conda/pip)
",duplicate MWE needed Windows,,2022-10-11 09:24:29,2022-10-11 17:40:55,"pandrich mentioned 2022-10-11 09:24:30,pandrich subscribed 2022-10-11 09:24:30,opherdonchin mentioned 2022-10-11 09:24:30,opherdonchin subscribed 2022-10-11 09:24:30,brandonwillard labeled 2022-10-11 16:01:14,pandrich mentioned 2022-10-11 16:05:09,pandrich subscribed 2022-10-11 16:05:09,opherdonchin mentioned 2022-10-11 16:05:10,opherdonchin subscribed 2022-10-11 16:05:10,brandonwillard labeled 2022-10-11 16:05:22,brandonwillard labeled 2022-10-11 16:08:57,brandonwillard closed 2022-10-11 17:40:55",pandrich lucianopaz opherdonchin brandonwillard,3
932,1255,Fix deprecated `SharedVariable.value` usage in `aesara.tensor.extra_ops.broadcast_shape_iter`,hottwaj,"## Description of your problem or feature request

Hi there, thanks for the great library :)

I have a pymc model (I will try and produce a minimal example when I have time) which creates the following warning, stemming from this [line of code](https://github.com/aesara-devs/aesara/blob/main/aesara/tensor/extra_ops.py#L1496) in aesara.tensor.extra_ops, function broadcast_shape_iter

Note that my pymc model **does** work, this is just a warning (it is repeated several times if I run pymc.find_MAP and many many times if I run pymc.sample())

```python
/..../aesara/tensor/rewriting/shape.py:169: UserWarning: Failed to infer_shape from Op Elemwise{true_div,no_inplace}.
Input shapes: [(periods_dim, TensorConstant{1}), (TensorConstant{1}, TensorConstant{1})]
Exception encountered during infer_shape: <class 'Exception'>
Exception message: sharedvar.value does not exist anymore. Use sharedvar.get_value() or sharedvar.set_value() instead.
Traceback: Traceback (most recent call last):
  File ""/..../aesara/tensor/rewriting/shape.py"", line 145, in get_node_infer_shape
    o_shapes = shape_infer(
  File ""/..../aesara/tensor/elemwise.py"", line 834, in infer_shape
    out_shape = aesara.tensor.broadcast_shape(*i_shapes, arrays_are_shapes=True)
  File ""/..../aesara/tensor/extra_ops.py"", line 1459, in broadcast_shape
    return broadcast_shape_iter(arrays, **kwargs)
  File ""/..../aesara/tensor/extra_ops.py"", line 1492, in broadcast_shape_iter
    array_shapes = [
  File ""/..../aesara/tensor/extra_ops.py"", line 1494, in <listcomp>
    + tuple(
  File ""/..../aesara/tensor/extra_ops.py"", line 1496, in <genexpr>
    if getattr(sh, ""value"", sh) == 1
  File ""/..../aesara/compile/sharedvalue.py"", line 207, in _value_get
    raise Exception(
Exception: sharedvar.value does not exist anymore. Use sharedvar.get_value() or sharedvar.set_value() instead.
```

I tried replacing the line of aesara code linked above with the following:

```python
if hasattr(sh, ""get_value"") and sh.get_value() == 1
```

However, the same model then failed and generated a very long stack trace with the errors below repeated in it many times

```python
/..../src/aesara/aesara/tensor/rewriting/shape.py:169: UserWarning: Failed to infer_shape from Op Elemwise{sub,no_inplace}.
Input shapes: [(TensorConstant{1},), (Subtensor{int64}.0,)]
Exception encountered during infer_shape: <class 'AssertionError'>
Exception message: 
Traceback: Traceback (most recent call last):
  File ""/..../src/aesara/aesara/tensor/rewriting/shape.py"", line 145, in get_node_infer_shape
    o_shapes = shape_infer(
  File ""/..../src/aesara/aesara/tensor/elemwise.py"", line 834, in infer_shape
    out_shape = aesara.tensor.broadcast_shape(*i_shapes, arrays_are_shapes=True)
  File ""/..../src/aesara/aesara/tensor/extra_ops.py"", line 1459, in broadcast_shape
    return broadcast_shape_iter(arrays, **kwargs)
  File ""/..../src/aesara/aesara/tensor/extra_ops.py"", line 1544, in broadcast_shape_iter
    assert const_nb_shape != 1
AssertionError

  warn(msg)
/..../src/aesara/aesara/tensor/rewriting/shape.py:169: UserWarning: Failed to infer_shape from Op Elemwise{true_div,no_inplace}.
Input shapes: [(TensorConstant{2},), (TensorConstant{1},)]
Exception encountered during infer_shape: <class 'ValueError'>
Exception message: Could not broadcast dimensions
Traceback: Traceback (most recent call last):
  File ""/..../src/aesara/aesara/tensor/rewriting/shape.py"", line 145, in get_node_infer_shape
    o_shapes = shape_infer(
  File ""/..../src/aesara/aesara/tensor/elemwise.py"", line 834, in infer_shape
    out_shape = aesara.tensor.broadcast_shape(*i_shapes, arrays_are_shapes=True)
  File ""/..../src/aesara/aesara/tensor/extra_ops.py"", line 1459, in broadcast_shape
    return broadcast_shape_iter(arrays, **kwargs)
  File ""/..../src/aesara/aesara/tensor/extra_ops.py"", line 1540, in broadcast_shape_iter
    raise ValueError(""Could not broadcast dimensions"")
ValueError: Could not broadcast dimensions
```

**Please provide any additional information below.**


## Versions and main components

* Aesara version: 2.8.7
* Aesara config (`python -c ""import aesara; print(aesara.config)""`) - see end of this post
* Python version: 3.10.2
* Operating system: ubuntu 20.04
* How did you install Aesara: (conda/pip) pip

```text
WARNING (aesara.tensor.blas): Using NumPy C-API based implementation for BLAS functions.
floatX ({'float16', 'float32', 'float64'}) 
    Doc:  Default floating-point precision for python casts.

Note: float16 support is experimental, use at your own risk.
    Value:  float64

warn_float64 ({'pdb', 'warn', 'raise', 'ignore'}) 
    Doc:  Do an action when a tensor variable with float64 dtype is created.
    Value:  ignore

pickle_test_value (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7fe08ee67df0>>) 
    Doc:  Dump test values while pickling model. If True, test values will be dumped with model.
    Value:  True

cast_policy ({'numpy+floatX', 'custom'}) 
    Doc:  Rules for implicit type casting
    Value:  custom

deterministic ({'more', 'default'}) 
    Doc:  If `more`, sometimes we will select some implementation that are more deterministic, but slower.  Also see the dnn.conv.algo* flags to cover more cases.
    Value:  default

device (cpu)
    Doc:  Default device for computations. only cpu is supported for now
    Value:  cpu

force_device (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7fe08ecb2e00>>) 
    Doc:  Raise an error if we can't use the specified device
    Value:  False

conv__assert_shape (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7fe08ecb2e30>>) 
    Doc:  If True, AbstractConv* ops will verify that user-provided shapes match the runtime shapes (debugging option, may slow down compilation)
    Value:  False

print_global_stats (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7fe08ecb2e60>>) 
    Doc:  Print some global statistics (time spent) at the end
    Value:  False

assert_no_cpu_op ({'pdb', 'warn', 'raise', 'ignore'}) 
    Doc:  Raise an error/warning if there is a CPU op in the computational graph.
    Value:  ignore

unpickle_function (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7fe08ecb2da0>>) 
    Doc:  Replace unpickled Aesara functions with None. This is useful to unpickle old graphs that pickled them when it shouldn't
    Value:  True

<aesara.configparser.ConfigParam object at 0x7fe08ecb2f80>
    Doc:  Default compilation mode
    Value:  Mode

cxx (<class 'str'>) 
    Doc:  The C++ compiler to use. Currently only g++ is supported, but supporting additional compilers should not be too difficult. If it is empty, no C++ code is compiled.
    Value:  /usr/bin/g++

linker ({'c|py', 'cvm', 'c', 'vm_nogc', 'py', 'cvm_nogc', 'c|py_nogc', 'vm'}) 
    Doc:  Default linker used if the aesara flags mode is Mode
    Value:  cvm

allow_gc (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7fe08ecb2ec0>>) 
    Doc:  Do we default to delete intermediate results during Aesara function calls? Doing so lowers the memory requirement, but asks that we reallocate memory at the next function call. This is implemented for the default linker, but may not work for all linkers.
    Value:  True

optimizer ({'o1', 'o2', 'o4', 'None', 'o3', 'fast_compile', 'unsafe', 'merge', 'fast_run'}) 
    Doc:  Default optimizer. If not None, will use this optimizer with the Mode
    Value:  o4

optimizer_verbose (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7fe08ecb3040>>) 
    Doc:  If True, we print all optimization being applied
    Value:  False

on_opt_error ({'pdb', 'warn', 'raise', 'ignore'}) 
    Doc:  What to do when an optimization crashes: warn and skip it, raise the exception, or fall into the pdb debugger.
    Value:  warn

nocleanup (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7fe08ecb30a0>>) 
    Doc:  Suppress the deletion of code files that did not compile cleanly
    Value:  False

on_unused_input ({'warn', 'raise', 'ignore'}) 
    Doc:  What to do if a variable in the 'inputs' list of  aesara.function() is not used in the graph.
    Value:  raise

gcc__cxxflags (<class 'str'>) 
    Doc:  Extra compiler flags for gcc
    Value:  

cmodule__warn_no_version (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7fe08ecb2f50>>) 
    Doc:  If True, will print a warning when compiling one or more Op with C code that can't be cached because there is no c_code_cache_version() function associated to at least one of those Ops.
    Value:  False

cmodule__remove_gxx_opt (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7fe08ecb2fb0>>) 
    Doc:  If True, will remove the -O* parameter passed to g++.This is useful to debug in gdb modules compiled by Aesara.The parameter -g is passed by default to g++
    Value:  False

cmodule__compilation_warning (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7fe08ecb3160>>) 
    Doc:  If True, will print compilation warnings.
    Value:  False

cmodule__preload_cache (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7fe08ecb3190>>) 
    Doc:  If set to True, will preload the C module cache at import time
    Value:  False

cmodule__age_thresh_use (<class 'int'>) 
    Doc:  In seconds. The time after which Aesara won't reuse a compile c module.
    Value:  2073600

cmodule__debug (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7fe08ecb31f0>>) 
    Doc:  If True, define a DEBUG macro (if not exists) for any compiled C code.
    Value:  False

compile__wait (<class 'int'>) 
    Doc:  Time to wait before retrying to acquire the compile lock.
    Value:  5

compile__timeout (<class 'int'>) 
    Doc:  In seconds, time that a process will wait before deciding to
    override an existing lock. An override only happens when the existing
    lock is held by the same owner *and* has not been 'refreshed' by this
    owner for more than this period. Refreshes are done every half timeout
    period for running processes.
    Value:  120

ctc__root (<class 'str'>) 
    Doc:  Directory which contains the root of Baidu CTC library. It is assumed         that the compiled library is either inside the build, lib or lib64         subdirectory, and the header inside the include directory.
    Value:  

tensor__cmp_sloppy (<class 'int'>) 
    Doc:  Relax aesara.tensor.math._allclose (0) not at all, (1) a bit, (2) more
    Value:  0

tensor__local_elemwise_fusion (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7fe08ecb33a0>>) 
    Doc:  Enable or not in fast_run mode(fast_run optimization) the elemwise fusion optimization
    Value:  True

lib__amblibm (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7fe08ecb3430>>) 
    Doc:  Use amd's amdlibm numerical library
    Value:  False

tensor__insert_inplace_optimizer_validate_nb (<class 'int'>) 
    Doc:  -1: auto, if graph have less then 500 nodes 1, else 10
    Value:  -1

traceback__limit (<class 'int'>) 
    Doc:  The number of stack to trace. -1 mean all.
    Value:  8

traceback__compile_limit (<class 'int'>) 
    Doc:  The number of stack to trace to keep during compilation. -1 mean all. If greater then 0, will also make us save Aesara internal stack trace.
    Value:  0

experimental__local_alloc_elemwise (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7fe08ecb35b0>>) 
    Doc:  DEPRECATED: If True, enable the experimental optimization local_alloc_elemwise. Generates error if not True. Use optimizer_excluding=local_alloc_elemwise to disable.
    Value:  True

experimental__local_alloc_elemwise_assert (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7fe08ecb35e0>>) 
    Doc:  When the local_alloc_elemwise is applied, add an assert to highlight shape errors.
    Value:  True

warn__ignore_bug_before ({'0.9', '1.0.2', '0.10', '0.4.1', '0.8.1', 'None', '0.8', '1.0.1', '0.3', '1.0.3', '0.6', '0.5', '0.4', '0.8.2', 'all', '1.0.4', '0.7', '1.0.5', '1.0'}) 
    Doc:  If 'None', we warn about all Aesara bugs found by default. If 'all', we don't warn about Aesara bugs found by default. If a version, we print only the warnings relative to Aesara bugs found after that version. Warning for specific bugs can be configured with specific [warn] flags.
    Value:  0.9

exception_verbosity ({'high', 'low'}) 
    Doc:  If 'low', the text of exceptions will generally refer to apply nodes with short names such as Elemwise{add_no_inplace}. If 'high', some exceptions will also refer to apply nodes with long descriptions  like:
        A. Elemwise{add_no_inplace}
                B. log_likelihood_v_given_h
                C. log_likelihood_h
    Value:  low

print_test_value (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7fe08ecb3730>>) 
    Doc:  If 'True', the __eval__ of an Aesara variable will return its test_value when this is available. This has the practical conseguence that, e.g., in debugging `my_var` will print the same as `my_var.tag.test_value` when a test value is defined.
    Value:  False

compute_test_value ({'warn', 'raise', 'off', 'pdb', 'ignore'}) 
    Doc:  If 'True', Aesara will run each op at graph build time, using Constants, SharedVariables and the tag 'test_value' as inputs to the function. This helps the user track down problems in the graph before it gets optimized.
    Value:  off

compute_test_value_opt ({'warn', 'raise', 'off', 'pdb', 'ignore'}) 
    Doc:  For debugging Aesara optimization only. Same as compute_test_value, but is used during Aesara optimization
    Value:  off

check_input (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7fe08ecb37c0>>) 
    Doc:  Specify if types should check their input in their C code. It can be used to speed up compilation, reduce overhead (particularly for scalars) and reduce the number of generated C files.
    Value:  True

NanGuardMode__nan_is_error (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7fe08ecb37f0>>) 
    Doc:  Default value for nan_is_error
    Value:  True

NanGuardMode__inf_is_error (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7fe08ecb3820>>) 
    Doc:  Default value for inf_is_error
    Value:  True

NanGuardMode__big_is_error (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7fe08ecb38b0>>) 
    Doc:  Default value for big_is_error
    Value:  True

NanGuardMode__action ({'pdb', 'warn', 'raise'}) 
    Doc:  What NanGuardMode does when it finds a problem
    Value:  raise

DebugMode__patience (<class 'int'>) 
    Doc:  Optimize graph this many times to detect inconsistency
    Value:  10

DebugMode__check_c (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7fe08ecb3940>>) 
    Doc:  Run C implementations where possible
    Value:  True

DebugMode__check_py (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7fe08ecb39d0>>) 
    Doc:  Run Python implementations where possible
    Value:  True

DebugMode__check_finite (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7fe08ecb3a00>>) 
    Doc:  True -> complain about NaN/Inf results
    Value:  True

DebugMode__check_strides (<class 'int'>) 
    Doc:  Check that Python- and C-produced ndarrays have same strides. On difference: (0) - ignore, (1) warn, or (2) raise error
    Value:  0

DebugMode__warn_input_not_reused (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7fe08ecb3a60>>) 
    Doc:  Generate a warning when destroy_map or view_map says that an op works inplace, but the op did not reuse the input for its output.
    Value:  True

DebugMode__check_preallocated_output (<class 'str'>) 
    Doc:  Test thunks with pre-allocated memory as output storage. This is a list of strings separated by "":"". Valid values are: ""initial"" (initial storage in storage map, happens with Scan),""previous"" (previously-returned memory), ""c_contiguous"", ""f_contiguous"", ""strided"" (positive and negative strides), ""wrong_size"" (larger and smaller dimensions), and ""ALL"" (all of the above).
    Value:  

DebugMode__check_preallocated_output_ndim (<class 'int'>) 
    Doc:  When testing with ""strided"" preallocated output memory, test all combinations of strides over that number of (inner-most) dimensions. You may want to reduce that number to reduce memory or time usage, but it is advised to keep a minimum of 2.
    Value:  4

profiling__time_thunks (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7fe08ecb3af0>>) 
    Doc:  Time individual thunks when profiling
    Value:  True

profiling__n_apply (<class 'int'>) 
    Doc:  Number of Apply instances to print by default
    Value:  20

profiling__n_ops (<class 'int'>) 
    Doc:  Number of Ops to print by default
    Value:  20

profiling__output_line_width (<class 'int'>) 
    Doc:  Max line width for the profiling output
    Value:  512

profiling__min_memory_size (<class 'int'>) 
    Doc:  For the memory profile, do not print Apply nodes if the size
                 of their outputs (in bytes) is lower than this threshold
    Value:  1024

profiling__min_peak_memory (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7fe08ecb3c40>>) 
    Doc:  The min peak memory usage of the order
    Value:  False

profiling__destination (<class 'str'>) 
    Doc:  File destination of the profiling output
    Value:  stderr

profiling__debugprint (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7fe08ecb3ca0>>) 
    Doc:  Do a debugprint of the profiled functions
    Value:  False

profiling__ignore_first_call (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7fe08ecb3cd0>>) 
    Doc:  Do we ignore the first call of an Aesara function.
    Value:  False

on_shape_error ({'warn', 'raise'}) 
    Doc:  warn: print a warning and use the default value. raise: raise an error
    Value:  warn

openmp (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7fe08ecb3d30>>) 
    Doc:  Allow (or not) parallel computation on the CPU with OpenMP. This is the default value used when creating an Op that supports OpenMP parallelization. It is preferable to define it via the Aesara configuration file ~/.aesararc or with the environment variable AESARA_FLAGS. Parallelization is only done for some operations that implement it, and even for operations that implement parallelism, each operation is free to respect this flag or not. You can control the number of threads used with the environment variable OMP_NUM_THREADS. If it is set to 1, we disable openmp in Aesara by default.
    Value:  False

openmp_elemwise_minsize (<class 'int'>) 
    Doc:  If OpenMP is enabled, this is the minimum size of vectors for which the openmp parallelization is enabled in element wise ops.
    Value:  200000

optimizer_excluding (<class 'str'>) 
    Doc:  When using the default mode, we will remove optimizer with these tags. Separate tags with ':'.
    Value:  

optimizer_including (<class 'str'>) 
    Doc:  When using the default mode, we will add optimizer with these tags. Separate tags with ':'.
    Value:  

optimizer_requiring (<class 'str'>) 
    Doc:  When using the default mode, we will require optimizer with these tags. Separate tags with ':'.
    Value:  

optdb__position_cutoff (<class 'float'>) 
    Doc:  Where to stop eariler during optimization. It represent the position of the optimizer where to stop.
    Value:  inf

optdb__max_use_ratio (<class 'float'>) 
    Doc:  A ratio that prevent infinite loop in EquilibriumGraphRewriter.
    Value:  8.0

cycle_detection ({'fast', 'regular'}) 
    Doc:  If cycle_detection is set to regular, most inplaces are allowed,but it is slower. If cycle_detection is set to faster, less inplacesare allowed, but it makes the compilation faster.The interaction of which one give the lower peak memory usage iscomplicated and not predictable, so if you are close to the peakmemory usage, triyng both could give you a small gain.
    Value:  regular

check_stack_trace ({'warn', 'raise', 'log', 'off'}) 
    Doc:  A flag for checking the stack trace during the optimization process. default (off): does not check the stack trace of any optimization log: inserts a dummy stack trace that identifies the optimizationthat inserted the variable that had an empty stack trace.warn: prints a warning if a stack trace is missing and also a dummystack trace is inserted that indicates which optimization insertedthe variable that had an empty stack trace.raise: raises an exception if a stack trace is missing
    Value:  off

metaopt__verbose (<class 'int'>) 
    Doc:  0 for silent, 1 for only warnings, 2 for full output withtimings and selected implementation
    Value:  0

metaopt__optimizer_excluding (<class 'str'>) 
    Doc:  exclude optimizers with these tags. Separate tags with ':'.
    Value:  

metaopt__optimizer_including (<class 'str'>) 
    Doc:  include optimizers with these tags. Separate tags with ':'.
    Value:  

profile (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7fe08ecb3fd0>>) 
    Doc:  If VM should collect profile information
    Value:  False

profile_optimizer (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7fe08ece00a0>>) 
    Doc:  If VM should collect optimizer profile information
    Value:  False

profile_memory (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7fe08ece00d0>>) 
    Doc:  If VM should collect memory profile information and print it
    Value:  False

<aesara.configparser.ConfigParam object at 0x7fe08ece0100>
    Doc:  Useful only for the VM Linkers. When lazy is None, auto detect if lazy evaluation is needed and use the appropriate version. If the C loop isn't being used and lazy is True, use the Stack VM; otherwise, use the Loop VM.
    Value:  None

unittests__rseed (<class 'str'>) 
    Doc:  Seed to use for randomized unit tests. Special value 'random' means using a seed of None.
    Value:  666

warn__round (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7fe08ece01c0>>) 
    Doc:  Warn when using `tensor.round` with the default mode. Round changed its default from `half_away_from_zero` to `half_to_even` to have the same default as NumPy.
    Value:  False

numba__vectorize_target ({'cuda', 'cpu', 'parallel'}) 
    Doc:  Default target for numba.vectorize.
    Value:  cpu

numba__fastmath (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7fe08ece0280>>) 
    Doc:  If True, use Numba's fastmath mode.
    Value:  True

numba__cache (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7fe08ece0310>>) 
    Doc:  If True, use Numba's file based caching.
    Value:  True

compiledir_format (<class 'str'>) 
    Doc:  Format string for platform-dependent compiled module subdirectory
(relative to base_compiledir). Available keys: aesara_version, device,
gxx_version, hostname, numpy_version, platform, processor,
python_bitwidth, python_int_bitwidth, python_version, short_platform.
Defaults to compiledir_%(short_platform)s-%(processor)s-
%(python_version)s-%(python_bitwidth)s.
    Value:  compiledir_%(short_platform)s-%(processor)s-%(python_version)s-%(python_bitwidth)s

<aesara.configparser.ConfigParam object at 0x7fe08ece0430>
    Doc:  platform-independent root directory for compiled modules
    Value:  /home/jonathan/.aesara

<aesara.configparser.ConfigParam object at 0x7fe08ece0370>
    Doc:  platform-dependent cache directory for compiled modules
    Value:  /home/jonathan/.aesara/compiledir_Linux-5.15--generic-x86_64-with-glibc2.31-x86_64-3.10.2-64

blas__ldflags (<class 'str'>) 
    Doc:  lib[s] to include for [Fortran] level-3 blas implementation
    Value:  

blas__check_openmp (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7fe08eadb5e0>>) 
    Doc:  Check for openmp library conflict.
WARNING: Setting this to False leaves you open to wrong results in blas-related operations.
    Value:  True

scan__allow_gc (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7fe077e4f340>>) 
    Doc:  Allow/disallow gc inside of Scan (default: False)
    Value:  False

scan__allow_output_prealloc (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7fe077e0d150>>) 
    Doc:  Allow/disallow memory preallocation for outputs inside of scan (default: True)
    Value:  True

```
",bug,,2022-10-13 20:42:21,2022-10-17 22:50:23,"brandonwillard labeled 2022-10-17 17:41:26,brandonwillard renamed 2022-10-17 19:58:47,brandonwillard labeled 2022-10-17 19:59:15,brandonwillard unlabeled 2022-10-17 21:07:30,brandonwillard referenced 2022-10-17 21:21:54,brandonwillard closed 2022-10-17 22:50:23,brandonwillard referenced 2022-10-17 22:50:23",hottwaj brandonwillard,4
934,1259,Add JAX-specific rewrites and use static graph analysis to raise early,rlouf,"As the refactor of JAX `Scan` dispatcher progresses, more and more branching logic is being added throughout the dispatcher. All of it is linked to JAX's restrictions in terms of shapes and slicing. All the graphs I have encountered so far describe computations that can be implemented in JAX, but it is sometimes not obvious based on the graph Aesara produces.

Since we are lowering to JAX from a static computation graph I think we should use the graph to take a _defensive_ approach. The idea is to move the complexity that currently lives in the dispatcher upstream, and raise as early as possible in the transpilation process if necessary:
- Analyse the graph, and raise if we find a pattern that is not admissible for JAX;
- If no such patterns are found then canonicalize the graph in a JAX-friendly format so that it can be lowered with the simplest dispatcher possible.
- In addition, there are known ""tricks"" to make JAX programs compile. They often involve  grouping computations, and rewrites + custom JAX-specific types and dispatch would automate these tricks. Effectively making the Aesara JAX backend easier to use than JAX itself.

Since JAX follows XLA very closely in its semantics and inherits its constraints, this line of work will be useful when we start targeting XLA directly. More generally, using static graph analysis, backend-specific types and graph rewrites to tailor the graph to each backend's specificities is generally a better approach than writing very complex dispatchers.",JAX graph rewriting refactor,,2022-10-15 11:37:36,2022-12-10 07:32:53,"rlouf labeled 2022-10-15 11:37:47,rlouf renamed 2022-10-15 11:47:19,brandonwillard labeled 2022-10-16 00:49:59,brandonwillard labeled 2022-10-16 00:50:04,rlouf closed 2022-12-10 07:32:54",rlouf brandonwillard,1
935,1261,Remove `Type.filter_inplace`,brandonwillard,"### Discussed in https://github.com/aesara-devs/aesara/discussions/1248

<div type='discussions-op-text'>

<sup>Originally posted by **markusschmaus** October 10, 2022</sup>
`Type.filter_inplace` is currently not implemented anywhere in the code and the only place where it is used the `NotImplementedError` is caught and `Type.filter` is used instead. Based on the comment it seems that this is some leftover from some earlier feature. Is there any reason why this should be kept around? Otherwise I am going to remove it in https://github.com/aesara-devs/aesara/pull/1207.</div>",duplicate refactor,,2022-10-16 05:48:27,2022-10-19 10:14:44,"brandonwillard labeled 2022-10-16 05:48:27,brandonwillard milestoned 2022-10-16 05:48:38,ricardoV94 closed 2022-10-19 10:14:44,ricardoV94 labeled 2022-10-19 10:14:50",ricardoV94 brandonwillard,1
937,1263,Issues with Setting up Aesara,anirudhacharya,"I am running into a few issues while setting up `aesara` on my laptop, I am raising this issue to see if others have faced the same problem and if there is a fix, or if I am doing anything wrong while setting it up.

I am using [this guide to setup and test the installation](https://aesara.readthedocs.io/en/latest/dev_start_guide.html#installation-and-configuration).

On the latest main branch commit( `c7ff283de`) following things are failing -

- After running `python setup.py install` I ran `pytest tests/` from the `aesara` home directory and the following tests failed. I recently raised a PR and the CI system passed with all green, so clearly, these tests are only failing locally on my laptop. I am not sure if I am doing anything wrong while running the tests -
```
FAILED tests/test_breakpoint.py::TestPdbBreakpoint::test_infer_shape - aesara.link.c.exceptions.CompileError: Compilation failed (return status=1):
FAILED tests/test_breakpoint.py::TestPdbBreakpoint::test_grad - aesara.link.c.exceptions.CompileError: Compilation failed (return status=1):
FAILED tests/test_breakpoint.py::TestPdbBreakpoint::test_fprop - aesara.link.c.exceptions.CompileError: Compilation failed (return status=1):
FAILED tests/test_gradient.py::TestGrad::test_grad_int - aesara.link.c.exceptions.CompileError: Compilation failed (return status=1):
FAILED tests/test_gradient.py::test_subgraph_grad - aesara.link.c.exceptions.CompileError: Compilation failed (return status=1):
FAILED tests/test_gradient.py::TestConsiderConstant::test_op_removed - aesara.link.c.exceptions.CompileError: Compilation failed (return status=1):
FAILED tests/test_gradient.py::TestConsiderConstant::test_grad - aesara.link.c.exceptions.CompileError: Compilation failed (return status=1):
FAILED tests/test_gradient.py::TestZeroGrad::test_op_removed - aesara.link.c.exceptions.CompileError: Compilation failed (return status=1):
FAILED tests/test_gradient.py::TestZeroGrad::test_grad - aesara.link.c.exceptions.CompileError: Compilation failed (return status=1):
FAILED tests/test_gradient.py::TestDisconnectedGrad::test_op_removed - aesara.link.c.exceptions.CompileError: Compilation failed (return status=1):
FAILED tests/test_gradient.py::TestDisconnectedGrad::test_grad - aesara.link.c.exceptions.CompileError: Compilation failed (return status=1):
FAILED tests/test_gradient.py::test_undefined_grad_opt - aesara.link.c.exceptions.CompileError: Compilation failed (return status=1):
FAILED tests/test_gradient.py::test_jacobian_matrix - aesara.link.c.exceptions.CompileError: Compilation failed (return status=1):
FAILED tests/test_printing.py::test_pydotprint_profile - aesara.link.c.exceptions.CompileError: Compilation failed (return status=1):
FAILED tests/test_printing.py::test_debugprint - aesara.link.c.exceptions.CompileError: Compilation failed (return status=1):
FAILED tests/test_rop.py::TestRopLop::test_max - aesara.link.c.exceptions.CompileError: Compilation failed (return status=1):
FAILED tests/test_rop.py::TestRopLop::test_conv - aesara.link.c.exceptions.CompileError: Compilation failed (return status=1):
FAILED tests/test_rop.py::TestRopLop::test_flatten - aesara.link.c.exceptions.CompileError: Compilation failed (return status=1):
FAILED tests/test_rop.py::TestRopLop::test_sum - aesara.link.c.exceptions.CompileError: Compilation failed (return status=1):
FAILED tests/test_rop.py::TestRopLop::test_multiple_outputs - aesara.link.c.exceptions.CompileError: Compilation failed (return status=1):
...
FAILED tests/tensor/test_elemwise.py::TestBroadcast::test_c - aesara.link.c.exceptions.CompileError: Compilation failed (return status=1):
FAILED tests/tensor/test_elemwise.py::TestBroadcast::test_c_inplace - aesara.link.c.exceptions.CompileError: Compilation failed (return status=1):
FAILED tests/tensor/test_elemwise.py::TestBroadcast::test_fill - aesara.link.c.exceptions.CompileError: Compilation failed (return status=1):
FAILED tests/tensor/test_elemwise.py::TestBroadcast::test_weird_strides - aesara.link.c.exceptions.CompileError: Compilation failed (return status=1):
FAILED tests/tensor/test_elemwise.py::TestBroadcast::test_same_inputs - aesara.link.c.exceptions.CompileError: Compilation failed (return status=1):
FAILED tests/tensor/test_elemwise.py::TestCAReduce::test_c_noopt - aesara.link.c.exceptions.CompileError: Compilation failed (return status=1):
FAILED tests/tensor/test_elemwise.py::TestCAReduce::test_infer_shape - aesara.link.c.exceptions.CompileError: Compilation failed (return status=1):
FAILED tests/tensor/test_elemwise.py::TestBitOpReduceGrad::test_all_grad - aesara.link.c.exceptions.CompileError: Compilation failed (return status=1):
FAILED tests/tensor/test_elemwise.py::TestBitOpReduceGrad::test_any_grad - aesara.link.c.exceptions.CompileError: Compilation failed (return status=1):
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! KeyboardInterrupt !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
/Users/anirudhacharya/miniconda3/envs/aesara-dev/lib/python3.10/codecs.py:309: KeyboardInterrupt
(to show a full traceback on KeyboardInterrupt use --full-trace)
================================================== 231 failed, 1879 passed, 84 skipped, 25 xfailed, 1 xpassed, 2591 warnings in 2875.66s (0:47:55) ===================================================
```
These are not the only failures, there are more which I did not update here, because the list would be too long.

- ~~When I try to install the documentation dependencies with this command `pip install -r requirements-rtd.txt` it throws the following error~~ - no longer an issue due to [this comment](https://github.com/aesara-devs/aesara/issues/1263#issuecomment-1280988826)
```
Collecting pydot2
  Using cached pydot2-1.0.33.tar.gz (19 kB)
  Preparing metadata (setup.py) ... error
  error: subprocess-exited-with-error

  × python setup.py egg_info did not run successfully.
  │ exit code: 1
  ╰─> [1 lines of output]
      error in pydot2 setup command: use_2to3 is invalid.
      [end of output]

  note: This error originates from a subprocess, and is likely not a problem with pip.
error: metadata-generation-failed

× Encountered error while generating package metadata.
╰─> See above for output.

note: This is an issue with the package mentioned above, not pip.
hint: See above for details.
```

## Versions and main components

* Aesara version: 1.0.10
* Python version: Python 3.9.12
* Operating system: macOS Monterey version 12.0.1
* How did you install Aesara: (conda/pip) `python setup.py install` ( built with the latest commit `c7ff283de`)

",,,2022-10-17 10:52:51,2022-10-17 19:52:07,"rlouf mentioned 2022-10-17 12:54:38,rlouf subscribed 2022-10-17 12:54:38,aesara-devs locked 2022-10-17 19:52:07,rlouf converted_to_discussion 2022-10-17 19:52:07",anirudhacharya rlouf aesara-devs,5
938,1264,Issues while setting up aesara,anirudhacharya,"I am facing a few issues while setting up `aesara` on my laptop, I am raising this issue to see if others have faced similar issues or if I am doing something wrong during the setup. 

I am following [this setup guide to build and install aesara locally](https://aesara.readthedocs.io/en/latest/dev_start_guide.html#installation-and-configuration) I have built the repository with the latest commit on the main branch( `c7ff283de`)

- After I install `aesara` I am running `pytest tests/` from the home folder and a lot of tests are failing. I recently raised a PR and the CI passed all the tests for that PR and was completely green. So these failures are happening only locally on my machine, and all of them are failing with the same error `aesara.link.c.exceptions.CompileError: Compilation failed`, which makes me think there is something wrong with my setup, rather than the tests themselves. But I have followed all the instructions from [this guide](https://aesara.readthedocs.io/en/latest/dev_start_guide.html#installation-and-configuration)
```
FAILED tests/test_breakpoint.py::TestPdbBreakpoint::test_infer_shape - aesara.link.c.exceptions.CompileError: Compilation failed (return status=1):
FAILED tests/test_breakpoint.py::TestPdbBreakpoint::test_grad - aesara.link.c.exceptions.CompileError: Compilation failed (return status=1):
FAILED tests/test_breakpoint.py::TestPdbBreakpoint::test_fprop - aesara.link.c.exceptions.CompileError: Compilation failed (return status=1):
FAILED tests/test_gradient.py::TestGrad::test_grad_int - aesara.link.c.exceptions.CompileError: Compilation failed (return status=1):
FAILED tests/test_gradient.py::test_subgraph_grad - aesara.link.c.exceptions.CompileError: Compilation failed (return status=1):
FAILED tests/test_gradient.py::TestConsiderConstant::test_op_removed - aesara.link.c.exceptions.CompileError: Compilation failed (return status=1):
FAILED tests/test_gradient.py::TestConsiderConstant::test_grad - aesara.link.c.exceptions.CompileError: Compilation failed (return status=1):
FAILED tests/test_gradient.py::TestZeroGrad::test_op_removed - aesara.link.c.exceptions.CompileError: Compilation failed (return status=1):
FAILED tests/test_gradient.py::TestZeroGrad::test_grad - aesara.link.c.exceptions.CompileError: Compilation failed (return status=1):
FAILED tests/test_gradient.py::TestDisconnectedGrad::test_op_removed - aesara.link.c.exceptions.CompileError: Compilation failed (return status=1):
FAILED tests/test_gradient.py::TestDisconnectedGrad::test_grad - aesara.link.c.exceptions.CompileError: Compilation failed (return status=1):
FAILED tests/test_gradient.py::test_undefined_grad_opt - aesara.link.c.exceptions.CompileError: Compilation failed (return status=1):
FAILED tests/test_gradient.py::test_jacobian_matrix - aesara.link.c.exceptions.CompileError: Compilation failed (return status=1):
FAILED tests/test_printing.py::test_pydotprint_profile - aesara.link.c.exceptions.CompileError: Compilation failed (return status=1):
FAILED tests/test_printing.py::test_debugprint - aesara.link.c.exceptions.CompileError: Compilation failed (return status=1):
FAILED tests/test_rop.py::TestRopLop::test_max - aesara.link.c.exceptions.CompileError: Compilation failed (return status=1):
FAILED tests/test_rop.py::TestRopLop::test_conv - aesara.link.c.exceptions.CompileError: Compilation failed (return status=1):
FAILED tests/test_rop.py::TestRopLop::test_flatten - aesara.link.c.exceptions.CompileError: Compilation failed (return status=1):
FAILED tests/test_rop.py::TestRopLop::test_sum - aesara.link.c.exceptions.CompileError: Compilation failed (return status=1):
FAILED tests/test_rop.py::TestRopLop::test_multiple_outputs - aesara.link.c.exceptions.CompileError: Compilation failed (return status=1):
...
FAILED tests/tensor/test_casting.py::TestCasting::test_bug_complext_10_august_09 - aesara.link.c.exceptions.CompileError: Compilation failed (return status=1):
FAILED tests/tensor/test_elemwise.py::TestBroadcast::test_c - aesara.link.c.exceptions.CompileError: Compilation failed (return status=1):
FAILED tests/tensor/test_elemwise.py::TestBroadcast::test_c_inplace - aesara.link.c.exceptions.CompileError: Compilation failed (return status=1):
FAILED tests/tensor/test_elemwise.py::TestBroadcast::test_fill - aesara.link.c.exceptions.CompileError: Compilation failed (return status=1):
FAILED tests/tensor/test_elemwise.py::TestBroadcast::test_weird_strides - aesara.link.c.exceptions.CompileError: Compilation failed (return status=1):
FAILED tests/tensor/test_elemwise.py::TestBroadcast::test_same_inputs - aesara.link.c.exceptions.CompileError: Compilation failed (return status=1):
FAILED tests/tensor/test_elemwise.py::TestCAReduce::test_c_noopt - aesara.link.c.exceptions.CompileError: Compilation failed (return status=1):
FAILED tests/tensor/test_elemwise.py::TestCAReduce::test_infer_shape - aesara.link.c.exceptions.CompileError: Compilation failed (return status=1):
FAILED tests/tensor/test_elemwise.py::TestBitOpReduceGrad::test_all_grad - aesara.link.c.exceptions.CompileError: Compilation failed (return status=1):
FAILED tests/tensor/test_elemwise.py::TestBitOpReduceGrad::test_any_grad - aesara.link.c.exceptions.CompileError: Compilation failed (return status=1):
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! KeyboardInterrupt !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
/Users/anirudhacharya/miniconda3/envs/aesara-dev/lib/python3.10/codecs.py:309: KeyboardInterrupt
(to show a full traceback on KeyboardInterrupt use --full-trace)
================================================== 231 failed, 1879 passed, 84 skipped, 25 xfailed, 1 xpassed, 2591 warnings in 2875.66s (0:47:55) ===================================================
```

- When I try to run `pip install -r requirements-rtd.txt` to install documentation dependencies, the following error gets thrown
```
Collecting pydot2
  Using cached pydot2-1.0.33.tar.gz (19 kB)
  Preparing metadata (setup.py) ... error
  error: subprocess-exited-with-error

  × python setup.py egg_info did not run successfully.
  │ exit code: 1
  ╰─> [1 lines of output]
      error in pydot2 setup command: use_2to3 is invalid.
      [end of output]

  note: This error originates from a subprocess, and is likely not a problem with pip.
error: metadata-generation-failed

× Encountered error while generating package metadata.
╰─> See above for output.

note: This is an issue with the package mentioned above, not pip.
hint: See above for details.
```

## Versions and main components

* Aesara version: 1.0.10
* Python version: Python 3.9.12
* Operating system: macOS Monterey version 12.0.1
* How did you install Aesara: (conda/pip) `python setup.py install`
",,,2022-10-17 11:12:29,2022-10-17 11:27:33,anirudhacharya closed 2022-10-17 11:27:33,anirudhacharya,0
944,1274,Math equations in aesara web documents are not rendering properly,ligonliu,"## Description of your problem or feature request

Math equations in aesara web documents are not rendering properly. I tried latest versions of Firefox and Chrome on Linux but both are displaying the TeX code instead of the formulas.

![image](https://user-images.githubusercontent.com/3859493/197402619-35d269ea-bf06-433c-b264-746e86f6fe97.png)
![image](https://user-images.githubusercontent.com/3859493/197402704-8ea3f674-01ad-4b43-a4c0-79621f18a67c.png)

Developers, what do you use to read the docs? Is there a local document reader you recommend with good search functionality? 
I appreciate your help.

**Please provide any additional information below.**

## Versions and main components

* Aesara version: latest
* Python version: ---
* Operating system: Linux, Firefox 106.0.1 (64-bit), Chrome 106.0.5249.119 (Official Build) (64-bit)
* How did you install Aesara: ---

",bug documentation,,2022-10-23 16:07:33,2023-02-20 14:16:21,"rlouf labeled 2022-10-23 19:22:55,rlouf labeled 2022-10-23 19:22:55,rlouf closed 2023-02-20 14:16:21",ligonliu rlouf,2
945,1276,Update remaining uses of `time.time` for profiling,brandonwillard,"As a follow-up to #1262, we need to update all remaining uses of `time.time` to `time.perf_counter` (e.g. in `aesara.scan`, `aesara.link`, etc.)",good first issue help wanted refactor,,2022-10-25 01:33:20,2022-11-25 03:34:14,"brandonwillard labeled 2022-10-25 01:33:21,brandonwillard labeled 2022-10-25 01:33:21,brandonwillard labeled 2022-10-25 01:33:21,gustavomfb assigned 2022-10-25 15:25:24,gustavomfb unassigned 2022-10-27 15:38:42,brandonwillard closed 2022-11-25 03:34:14",gustavomfb brandonwillard,1
953,1292,Update time.clock() to time.perf_counter() in documentation under tutorial/conditions ,anmolsinghbhullar,"On the webpage https://aesara.readthedocs.io/en/latest/tutorial/conditions.html#ifelse-vs-switch,
the code example uses time.clock() which is deprecated as of python 3.3. I believe it should be updated to time.perf_counter().

The block of code in question currently looks like this:

```python

tic = time.clock()
for i in range(n_times):
    f_switch(val1, val2, big_mat1, big_mat2)
print('time spent evaluating both values %f sec' % (time.clock() - tic))

tic = time.clock()
for i in range(n_times):
    f_lazyifelse(val1, val2, big_mat1, big_mat2)
print('time spent evaluating one value %f sec' % (time.clock() - tic))

```
every time.clock() instance should be updated to time.perf_counter() so like this:

```python
tic = time.perf_counter()
for i in range(n_times):
    f_switch(val1, val2, big_mat1, big_mat2)
print('time spent evaluating both values %f sec' % (time.perf_counter() - tic))

tic = time.perf_counter()
for i in range(n_times):
    f_lazyifelse(val1, val2, big_mat1, big_mat2)
print('time spent evaluating one value %f sec' % (time.perf_counter() - tic))
```

As far as I can tell, this is the only instance of time.clock() used in the documentation. ",documentation good first issue help wanted refactor,,2022-11-10 21:24:32,2022-11-11 17:35:22,"brandonwillard labeled 2022-11-10 22:14:16,brandonwillard labeled 2022-11-10 22:14:21,brandonwillard labeled 2022-11-10 22:14:26,brandonwillard labeled 2022-11-10 22:17:28,brandonwillard connected 2022-11-11 17:34:24,brandonwillard closed 2022-11-11 17:35:23",anmolsinghbhullar brandonwillard,1
961,1303,Add numerical stabilization for difference of exponentials,rlouf,"From https://github.com/aesara-devs/aeppl/pull/131

```python
def logdiffexp(a, b):
    """"""log(exp(a) - exp(b))""""""
    return a + at.log1mexp(b - a)
```",enhancement help wanted graph rewriting,,2022-11-18 08:04:58,2023-01-31 15:44:34,"rlouf labeled 2022-11-18 08:04:58,rlouf labeled 2022-11-18 08:04:58,rlouf labeled 2022-11-18 08:04:58,rlouf closed 2023-01-31 15:44:34",rlouf,0
962,1304,Set up scheduled nightly builds for Aesara,rlouf,,CI,,2022-11-18 09:38:06,2022-11-19 03:18:30,"rlouf labeled 2022-11-18 09:38:06,brandonwillard closed 2022-11-19 03:18:30",rlouf brandonwillard,1
965,1308,Remove code style and linting dependencies from `environment.yml`,dgerlanc,"These dependencies (`black`, `isort`, `flake8`, `pep8`, and `pyflakes`) are now covered through `pre-commit` and can be removed from `environment.yml`

See https://github.com/aesara-devs/aesara/pull/960#discussion_r889617077.",good first issue help wanted Conda,,2022-11-21 17:56:45,2022-11-22 04:09:54,"dgerlanc labeled 2022-11-21 17:57:19,dgerlanc labeled 2022-11-21 17:57:19,brandonwillard labeled 2022-11-21 18:24:26,brandonwillard connected 2022-11-22 04:09:42,brandonwillard closed 2022-11-22 04:09:54",dgerlanc brandonwillard,0
970,1314,`test_cache_race_condition` fails on macOS,dgerlanc,"This test is failing because it is pickling a local closure. This may also be a problem on Linux?

```bash
pytest tests/link/c/test_cmodule.py::test_cache_race_condition          
```

```python
tests/link/c/test_cmodule.py:259: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _../../miniconda3/envs/aesara-dev/lib/python3.10/multiprocessing/process.py:121: in start
    self._popen = self._Popen(self)
../../miniconda3/envs/aesara-dev/lib/python3.10/multiprocessing/context.py:288: in _Popen
    return Popen(process_obj)
../../miniconda3/envs/aesara-dev/lib/python3.10/multiprocessing/popen_spawn_posix.py:32: in __init__
    super().__init__(process_obj)
../../miniconda3/envs/aesara-dev/lib/python3.10/multiprocessing/popen_fork.py:19: in __init__
    self._launch(process_obj)
../../miniconda3/envs/aesara-dev/lib/python3.10/multiprocessing/popen_spawn_posix.py:47: in _launch
    reduction.dump(process_obj, fp)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
obj = <SpawnProcess name='SpawnProcess-1' parent=37311 initial>, file = <_io.BytesIO object at 0x11e4187c0>, protocol = None

    def dump(obj, file, protocol=None):
        '''Replacement for pickle.dump() using ForkingPickler.'''
>       ForkingPickler(file, protocol).dump(obj)
E       AttributeError: Can't pickle local object 'test_cache_race_condition.<locals>.f_build'

../../miniconda3/envs/aesara-dev/lib/python3.10/multiprocessing/reduction.py:60: AttributeError

```

The test is running 10 loops and creating 30 concurrent compilation processes. Per @brandonwillard, this was the setting in CI that was sufficient to create the error. In theory, we should only need two separate, concurrent runs, but may require some low level tracing to figure out where the race condition is created.

## Versions and main components

Aesara version: rel-2.8.9
* Python version: 3.10.7
* Operating system: macOS Monterey (12, 6, 1), Apple M1
* How did you install Aesara: `environment-arm.yml`

<details> <summary> Aesara config: </summary>
floatX ({'float32', 'float16', 'float64'}) 
    Doc:  Default floating-point precision for python casts.

Note: float16 support is experimental, use at your own risk.
    Value:  float64

warn_float64 ({'warn', 'pdb', 'ignore', 'raise'}) 
    Doc:  Do an action when a tensor variable with float64 dtype is created.
    Value:  ignore

pickle_test_value (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x11994f3a0>>) 
    Doc:  Dump test values while pickling model. If True, test values will be dumped with model.
    Value:  True

cast_policy ({'numpy+floatX', 'custom'}) 
    Doc:  Rules for implicit type casting
    Value:  custom

deterministic ({'more', 'default'}) 
    Doc:  If `more`, sometimes we will select some implementation that are more deterministic, but slower.  Also see the dnn.conv.algo* flags to cover more cases.
    Value:  default

device (cpu)
    Doc:  Default device for computations. only cpu is supported for now
    Value:  cpu

force_device (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x104e9fdc0>>) 
    Doc:  Raise an error if we can't use the specified device
    Value:  False

conv__assert_shape (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x11994c850>>) 
    Doc:  If True, AbstractConv* ops will verify that user-provided shapes match the runtime shapes (debugging option, may slow down compilation)
    Value:  False

print_global_stats (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x11994cdc0>>) 
    Doc:  Print some global statistics (time spent) at the end
    Value:  False

assert_no_cpu_op ({'warn', 'pdb', 'ignore', 'raise'}) 
    Doc:  Raise an error/warning if there is a CPU op in the computational graph.
    Value:  ignore

unpickle_function (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x11994efe0>>) 
    Doc:  Replace unpickled Aesara functions with None. This is useful to unpickle old graphs that pickled them when it shouldn't
    Value:  True

<aesara.configparser.ConfigParam object at 0x11994cee0>
    Doc:  Default compilation mode
    Value:  Mode

cxx (<class 'str'>) 
    Doc:  The C++ compiler to use. Currently only g++ is supported, but supporting additional compilers should not be too difficult. If it is empty, no C++ code is compiled.
    Value:  /Users/dgerlanc/miniconda3/envs/aesara-dev/bin/clang++

linker ({'cvm', 'c|py', 'c|py_nogc', 'vm_nogc', 'cvm_nogc', 'vm', 'c', 'py'}) 
    Doc:  Default linker used if the aesara flags mode is Mode
    Value:  cvm

allow_gc (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x11994f430>>) 
    Doc:  Do we default to delete intermediate results during Aesara function calls? Doing so lowers the memory requirement, but asks that we reallocate memory at the next function call. This is implemented for the default linker, but may not work for all linkers.
    Value:  True

optimizer ({'o1', 'o4', 'unsafe', 'merge', 'o3', 'fast_run', 'fast_compile', 'o2', 'None'}) 
    Doc:  Default optimizer. If not None, will use this optimizer with the Mode
    Value:  o4

optimizer_verbose (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x11994ceb0>>) 
    Doc:  If True, we print all optimization being applied
    Value:  False

on_opt_error ({'warn', 'pdb', 'raise', 'ignore'}) 
    Doc:  What to do when an optimization crashes: warn and skip it, raise the exception, or fall into the pdb debugger.
    Value:  warn

nocleanup (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x11994f1f0>>) 
    Doc:  Suppress the deletion of code files that did not compile cleanly
    Value:  False

on_unused_input ({'warn', 'ignore', 'raise'}) 
    Doc:  What to do if a variable in the 'inputs' list of  aesara.function() is not used in the graph.
    Value:  raise

gcc__cxxflags (<class 'str'>) 
    Doc:  Extra compiler flags for gcc
    Value:  -Wno-c++11-narrowing

cmodule__warn_no_version (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x11994f1c0>>) 
    Doc:  If True, will print a warning when compiling one or more Op with C code that can't be cached because there is no c_code_cache_version() function associated to at least one of those Ops.
    Value:  False

cmodule__remove_gxx_opt (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x11994f2b0>>) 
    Doc:  If True, will remove the -O* parameter passed to g++.This is useful to debug in gdb modules compiled by Aesara.The parameter -g is passed by default to g++
    Value:  False

cmodule__compilation_warning (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x11994f310>>) 
    Doc:  If True, will print compilation warnings.
    Value:  False

cmodule__preload_cache (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x11994f9a0>>) 
    Doc:  If set to True, will preload the C module cache at import time
    Value:  False

cmodule__age_thresh_use (<class 'int'>) 
    Doc:  In seconds. The time after which Aesara won't reuse a compile c module.
    Value:  2073600

cmodule__debug (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x11994f280>>) 
    Doc:  If True, define a DEBUG macro (if not exists) for any compiled C code.
    Value:  False

compile__wait (<class 'int'>) 
    Doc:  Time to wait before retrying to acquire the compile lock.
    Value:  5

compile__timeout (<class 'int'>) 
    Doc:  In seconds, time that a process will wait before deciding to
    override an existing lock. An override only happens when the existing
    lock is held by the same owner *and* has not been 'refreshed' by this
    owner for more than this period. Refreshes are done every half timeout
    period for running processes.
    Value:  120

ctc__root (<class 'str'>) 
    Doc:  Directory which contains the root of Baidu CTC library. It is assumed         that the compiled library is either inside the build, lib or lib64         subdirectory, and the header inside the include directory.
    Value:  

tensor__cmp_sloppy (<class 'int'>) 
    Doc:  Relax aesara.tensor.math._allclose (0) not at all, (1) a bit, (2) more
    Value:  0

tensor__local_elemwise_fusion (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x11994f8b0>>) 
    Doc:  Enable or not in fast_run mode(fast_run optimization) the elemwise fusion optimization
    Value:  True

lib__amblibm (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x11994f820>>) 
    Doc:  Use amd's amdlibm numerical library
    Value:  False

tensor__insert_inplace_optimizer_validate_nb (<class 'int'>) 
    Doc:  -1: auto, if graph have less then 500 nodes 1, else 10
    Value:  -1

traceback__limit (<class 'int'>) 
    Doc:  The number of stack to trace. -1 mean all.
    Value:  8

traceback__compile_limit (<class 'int'>) 
    Doc:  The number of stack to trace to keep during compilation. -1 mean all. If greater then 0, will also make us save Aesara internal stack trace.
    Value:  0

experimental__local_alloc_elemwise (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x11994f6a0>>) 
    Doc:  DEPRECATED: If True, enable the experimental optimization local_alloc_elemwise. Generates error if not True. Use optimizer_excluding=local_alloc_elemwise to disable.
    Value:  True

experimental__local_alloc_elemwise_assert (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x11994f670>>) 
    Doc:  When the local_alloc_elemwise is applied, add an assert to highlight shape errors.
    Value:  True

warn__ignore_bug_before ({'0.4.1', '0.4', '0.3', '1.0.3', '0.8.2', '1.0.5', '1.0.1', '0.5', '0.8', '1.0.2', '1.0', '0.10', '0.9', '0.8.1', '1.0.4', '0.7', 'all', '0.6', 'None'}) 
    Doc:  If 'None', we warn about all Aesara bugs found by default. If 'all', we don't warn about Aesara bugs found by default. If a version, we print only the warnings relative to Aesara bugs found after that version. Warning for specific bugs can be configured with specific [warn] flags.
    Value:  0.9

exception_verbosity ({'high', 'low'}) 
    Doc:  If 'low', the text of exceptions will generally refer to apply nodes with short names such as Elemwise{add_no_inplace}. If 'high', some exceptions will also refer to apply nodes with long descriptions  like:
        A. Elemwise{add_no_inplace}
                B. log_likelihood_v_given_h
                C. log_likelihood_h
    Value:  low

print_test_value (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x11994f520>>) 
    Doc:  If 'True', the __eval__ of an Aesara variable will return its test_value when this is available. This has the practical conseguence that, e.g., in debugging `my_var` will print the same as `my_var.tag.test_value` when a test value is defined.
    Value:  False

compute_test_value ({'pdb', 'off', 'ignore', 'warn', 'raise'}) 
    Doc:  If 'True', Aesara will run each op at graph build time, using Constants, SharedVariables and the tag 'test_value' as inputs to the function. This helps the user track down problems in the graph before it gets optimized.
    Value:  off

compute_test_value_opt ({'pdb', 'off', 'ignore', 'warn', 'raise'}) 
    Doc:  For debugging Aesara optimization only. Same as compute_test_value, but is used during Aesara optimization
    Value:  off

check_input (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x11994f490>>) 
    Doc:  Specify if types should check their input in their C code. It can be used to speed up compilation, reduce overhead (particularly for scalars) and reduce the number of generated C files.
    Value:  True

NanGuardMode__nan_is_error (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x11994f460>>) 
    Doc:  Default value for nan_is_error
    Value:  True

NanGuardMode__inf_is_error (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x11994f370>>) 
    Doc:  Default value for inf_is_error
    Value:  True

NanGuardMode__big_is_error (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x11994faf0>>) 
    Doc:  Default value for big_is_error
    Value:  True

NanGuardMode__action ({'warn', 'pdb', 'raise'}) 
    Doc:  What NanGuardMode does when it finds a problem
    Value:  raise

DebugMode__patience (<class 'int'>) 
    Doc:  Optimize graph this many times to detect inconsistency
    Value:  10

DebugMode__check_c (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x11994fb80>>) 
    Doc:  Run C implementations where possible
    Value:  True

DebugMode__check_py (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x11994fc10>>) 
    Doc:  Run Python implementations where possible
    Value:  True

DebugMode__check_finite (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x11994fc40>>) 
    Doc:  True -> complain about NaN/Inf results
    Value:  True

DebugMode__check_strides (<class 'int'>) 
    Doc:  Check that Python- and C-produced ndarrays have same strides. On difference: (0) - ignore, (1) warn, or (2) raise error
    Value:  0

DebugMode__warn_input_not_reused (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x11994fca0>>) 
    Doc:  Generate a warning when destroy_map or view_map says that an op works inplace, but the op did not reuse the input for its output.
    Value:  True

DebugMode__check_preallocated_output (<class 'str'>) 
    Doc:  Test thunks with pre-allocated memory as output storage. This is a list of strings separated by "":"". Valid values are: ""initial"" (initial storage in storage map, happens with Scan),""previous"" (previously-returned memory), ""c_contiguous"", ""f_contiguous"", ""strided"" (positive and negative strides), ""wrong_size"" (larger and smaller dimensions), and ""ALL"" (all of the above).
    Value:  

DebugMode__check_preallocated_output_ndim (<class 'int'>) 
    Doc:  When testing with ""strided"" preallocated output memory, test all combinations of strides over that number of (inner-most) dimensions. You may want to reduce that number to reduce memory or time usage, but it is advised to keep a minimum of 2.
    Value:  4

profiling__time_thunks (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x11994fd30>>) 
    Doc:  Time individual thunks when profiling
    Value:  True

profiling__n_apply (<class 'int'>) 
    Doc:  Number of Apply instances to print by default
    Value:  20

profiling__n_ops (<class 'int'>) 
    Doc:  Number of Ops to print by default
    Value:  20

profiling__output_line_width (<class 'int'>) 
    Doc:  Max line width for the profiling output
    Value:  512

profiling__min_memory_size (<class 'int'>) 
    Doc:  For the memory profile, do not print Apply nodes if the size
                 of their outputs (in bytes) is lower than this threshold
    Value:  1024

profiling__min_peak_memory (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x11994fe80>>) 
    Doc:  The min peak memory usage of the order
    Value:  False

profiling__destination (<class 'str'>) 
    Doc:  File destination of the profiling output
    Value:  stderr

profiling__debugprint (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x11994fee0>>) 
    Doc:  Do a debugprint of the profiled functions
    Value:  False

profiling__ignore_first_call (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x11994ff10>>) 
    Doc:  Do we ignore the first call of an Aesara function.
    Value:  False

on_shape_error ({'warn', 'raise'}) 
    Doc:  warn: print a warning and use the default value. raise: raise an error
    Value:  warn

openmp (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x11994ff70>>) 
    Doc:  Allow (or not) parallel computation on the CPU with OpenMP. This is the default value used when creating an Op that supports OpenMP parallelization. It is preferable to define it via the Aesara configuration file ~/.aesararc or with the environment variable AESARA_FLAGS. Parallelization is only done for some operations that implement it, and even for operations that implement parallelism, each operation is free to respect this flag or not. You can control the number of threads used with the environment variable OMP_NUM_THREADS. If it is set to 1, we disable openmp in Aesara by default.
    Value:  False

openmp_elemwise_minsize (<class 'int'>) 
    Doc:  If OpenMP is enabled, this is the minimum size of vectors for which the openmp parallelization is enabled in element wise ops.
    Value:  200000

optimizer_excluding (<class 'str'>) 
    Doc:  When using the default mode, we will remove optimizer with these tags. Separate tags with ':'.
    Value:  

optimizer_including (<class 'str'>) 
    Doc:  When using the default mode, we will add optimizer with these tags. Separate tags with ':'.
    Value:  

optimizer_requiring (<class 'str'>) 
    Doc:  When using the default mode, we will require optimizer with these tags. Separate tags with ':'.
    Value:  

optdb__position_cutoff (<class 'float'>) 
    Doc:  Where to stop eariler during optimization. It represent the position of the optimizer where to stop.
    Value:  inf

optdb__max_use_ratio (<class 'float'>) 
    Doc:  A ratio that prevent infinite loop in EquilibriumGraphRewriter.
    Value:  8.0

cycle_detection ({'fast', 'regular'}) 
    Doc:  If cycle_detection is set to regular, most inplaces are allowed,but it is slower. If cycle_detection is set to faster, less inplacesare allowed, but it makes the compilation faster.The interaction of which one give the lower peak memory usage iscomplicated and not predictable, so if you are close to the peakmemory usage, triyng both could give you a small gain.
    Value:  regular

check_stack_trace ({'warn', 'off', 'raise', 'log'}) 
    Doc:  A flag for checking the stack trace during the optimization process. default (off): does not check the stack trace of any optimization log: inserts a dummy stack trace that identifies the optimizationthat inserted the variable that had an empty stack trace.warn: prints a warning if a stack trace is missing and also a dummystack trace is inserted that indicates which optimization insertedthe variable that had an empty stack trace.raise: raises an exception if a stack trace is missing
    Value:  off

metaopt__verbose (<class 'int'>) 
    Doc:  0 for silent, 1 for only warnings, 2 for full output withtimings and selected implementation
    Value:  0

metaopt__optimizer_excluding (<class 'str'>) 
    Doc:  exclude optimizers with these tags. Separate tags with ':'.
    Value:  

metaopt__optimizer_including (<class 'str'>) 
    Doc:  include optimizers with these tags. Separate tags with ':'.
    Value:  

profile (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x11998c2b0>>) 
    Doc:  If VM should collect profile information
    Value:  False

profile_optimizer (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x11998c2e0>>) 
    Doc:  If VM should collect optimizer profile information
    Value:  False

profile_memory (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x11998c310>>) 
    Doc:  If VM should collect memory profile information and print it
    Value:  False

<aesara.configparser.ConfigParam object at 0x11998c340>
    Doc:  Useful only for the VM Linkers. When lazy is None, auto detect if lazy evaluation is needed and use the appropriate version. If the C loop isn't being used and lazy is True, use the Stack VM; otherwise, use the Loop VM.
    Value:  None

unittests__rseed (<class 'str'>) 
    Doc:  Seed to use for randomized unit tests. Special value 'random' means using a seed of None.
    Value:  666

warn__round (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x11998c400>>) 
    Doc:  Warn when using `tensor.round` with the default mode. Round changed its default from `half_away_from_zero` to `half_to_even` to have the same default as NumPy.
    Value:  False

numba__vectorize_target ({'cpu', 'parallel', 'cuda'}) 
    Doc:  Default target for numba.vectorize.
    Value:  cpu

numba__fastmath (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x11998c4c0>>) 
    Doc:  If True, use Numba's fastmath mode.
    Value:  True

numba__cache (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x11998c550>>) 
    Doc:  If True, use Numba's file based caching.
    Value:  True

compiledir_format (<class 'str'>) 
    Doc:  Format string for platform-dependent compiled module subdirectory
(relative to base_compiledir). Available keys: aesara_version, device,
gxx_version, hostname, numpy_version, platform, processor,
python_bitwidth, python_int_bitwidth, python_version, short_platform.
Defaults to compiledir_%(short_platform)s-%(processor)s-
%(python_version)s-%(python_bitwidth)s.
    Value:  compiledir_%(short_platform)s-%(processor)s-%(python_version)s-%(python_bitwidth)s

<aesara.configparser.ConfigParam object at 0x11998c5e0>
    Doc:  platform-independent root directory for compiled modules
    Value:  /Users/dgerlanc/.aesara

<aesara.configparser.ConfigParam object at 0x11998c520>
    Doc:  platform-dependent cache directory for compiled modules
    Value:  /Users/dgerlanc/.aesara/compiledir_macOS-12.6.1-arm64-arm-64bit-arm-3.10.7-64

blas__ldflags (<class 'str'>) 
    Doc:  lib[s] to include for [Fortran] level-3 blas implementation
    Value:  -L/Users/dgerlanc/miniconda3/envs/aesara-dev/lib -lcblas -lblas -lcblas -lblas -Wl,-rpath,/Users/dgerlanc/miniconda3/envs/aesara-dev/lib

blas__check_openmp (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x119b97ee0>>) 
    Doc:  Check for openmp library conflict.
WARNING: Setting this to False leaves you open to wrong results in blas-related operations.
    Value:  True

scan__allow_gc (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x11d828400>>) 
    Doc:  Allow/disallow gc inside of Scan (default: False)
    Value:  False

scan__allow_output_prealloc (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x11cf05ff0>>) 
    Doc:  Allow/disallow memory preallocation for outputs inside of scan (default: True)
    Value:  True

</details>
",bug testing C-backend MacOS,dgerlanc,2022-11-27 20:42:56,2023-02-22 01:44:15,"dgerlanc labeled 2022-11-27 20:42:56,dgerlanc labeled 2022-11-27 20:42:56,dgerlanc labeled 2022-11-27 20:42:56,dgerlanc labeled 2022-11-27 20:42:56,dgerlanc assigned 2022-11-27 20:42:56,dgerlanc referenced 2022-11-27 20:47:17,dgerlanc referenced 2022-11-29 02:25:35,brandonwillard mentioned 2022-11-29 02:38:24,brandonwillard subscribed 2022-11-29 02:38:24,dgerlanc referenced 2023-02-17 02:08:23,dgerlanc referenced 2023-02-18 14:51:31,dgerlanc referenced 2023-02-22 00:28:20,brandonwillard closed 2023-02-22 01:44:15,brandonwillard referenced 2023-02-22 01:44:15",dgerlanc brandonwillard,0
974,1322,Add JAX implementation for `ChiSquareRV`,rlouf,,enhancement good first issue help wanted JAX random variables,,2022-12-03 20:06:52,2023-03-10 01:50:18,"rlouf labeled 2022-12-03 20:07:10,rlouf labeled 2022-12-03 20:07:10,rlouf labeled 2022-12-03 20:07:10,rlouf labeled 2022-12-03 20:07:10,rlouf labeled 2022-12-03 20:07:10,brandonwillard closed 2023-03-10 01:50:18",rlouf brandonwillard,0
975,1323,Add JAX implementation for `GeometricRV`,rlouf,,enhancement good first issue help wanted JAX random variables,,2022-12-03 20:08:00,2023-02-21 06:29:46,"rlouf labeled 2022-12-03 20:08:17,rlouf labeled 2022-12-03 20:08:17,rlouf labeled 2022-12-03 20:08:17,rlouf labeled 2022-12-03 20:08:17,rlouf labeled 2022-12-03 20:08:17,rlouf closed 2023-02-21 06:29:46",rlouf,0
976,1325,Add JAX implementation for `LogNormalRV`,rlouf,,enhancement good first issue help wanted JAX random variables,,2022-12-03 20:11:43,2022-12-10 09:53:09,"rlouf labeled 2022-12-03 20:11:58,rlouf labeled 2022-12-03 20:11:58,rlouf labeled 2022-12-03 20:11:58,rlouf labeled 2022-12-03 20:11:58,rlouf labeled 2022-12-03 20:11:58,rlouf closed 2022-12-10 09:53:09",rlouf,0
977,1329,Add JAX implementation for `WaldRV`,rlouf,,enhancement good first issue help wanted JAX random variables,,2022-12-03 20:16:12,2023-02-20 17:03:01,"rlouf labeled 2022-12-03 20:16:25,rlouf labeled 2022-12-03 20:16:25,rlouf labeled 2022-12-03 20:16:25,rlouf labeled 2022-12-03 20:16:25,rlouf labeled 2022-12-03 20:16:25,rlouf closed 2023-02-20 17:03:01",rlouf,0
978,1333,Add JAX implementation for `GenGammaRV`,rlouf,,enhancement good first issue help wanted JAX random variables,,2022-12-03 20:20:09,2023-03-10 00:22:54,"rlouf labeled 2022-12-03 20:20:22,rlouf labeled 2022-12-03 20:20:22,rlouf labeled 2022-12-03 20:20:22,rlouf labeled 2022-12-03 20:20:22,rlouf labeled 2022-12-03 20:20:22,brandonwillard connected 2023-02-22 00:04:09,brandonwillard closed 2023-03-10 00:22:55",rlouf brandonwillard,0
979,1334,Add JAX implementation for `HalfCauchyRV`,rlouf,,enhancement good first issue help wanted JAX random variables,,2022-12-03 20:21:09,2022-12-15 17:22:42,"rlouf labeled 2022-12-03 20:21:24,rlouf labeled 2022-12-03 20:21:24,rlouf labeled 2022-12-03 20:21:24,rlouf labeled 2022-12-03 20:21:24,rlouf labeled 2022-12-03 20:21:24,rlouf connected 2022-12-14 16:41:10,rlouf closed 2022-12-15 17:22:43",rlouf,0
980,1335,Add JAX implementation for `HalfNormalRV`,rlouf,,enhancement good first issue help wanted JAX random variables,,2022-12-03 20:21:42,2022-12-14 13:13:13,"rlouf labeled 2022-12-03 20:22:02,rlouf labeled 2022-12-03 20:22:02,rlouf labeled 2022-12-03 20:22:02,rlouf labeled 2022-12-03 20:22:02,rlouf labeled 2022-12-03 20:22:02,rlouf connected 2022-12-12 15:04:44,rlouf closed 2022-12-14 13:13:13",rlouf theorashid,3
984,1339,Create Aesara Mission Statement,dgerlanc,Official mission statement of the Aesara Project!,documentation important,dgerlanc brandonwillard rlouf,2022-12-07 17:07:04,2022-12-07 21:31:54,"dgerlanc labeled 2022-12-07 17:07:04,dgerlanc labeled 2022-12-07 17:07:04,dgerlanc assigned 2022-12-07 17:07:04,brandonwillard assigned 2022-12-07 17:07:04,rlouf assigned 2022-12-07 17:07:04,dgerlanc referenced 2022-12-07 17:25:26,dgerlanc referenced 2022-12-07 17:27:30,brandonwillard referenced 2022-12-07 20:25:02,dgerlanc closed 2022-12-07 21:31:54,dgerlanc referenced 2022-12-07 21:31:55",dgerlanc rlouf brandonwillard,0
987,1344,In JAX random linking splitting should happen before using the key,AdrienCorenflos,"See for example

https://github.com/aesara-devs/aesara/blob/fe3e76d961653ed1180e8badeb7d721861a1c2f5/aesara/link/jax/dispatch/random.py#L126-L130

in order to be internals-agnostic, this should be 

```python
def sample_fn(rng, size, dtype, *parameters): 
     rng_key = rng[""jax_state""] 
     rng_key, sample_key = jax.random.split(rng_key, 2) 
     sample = jax_op(sample_key, *parameters, shape=size, dtype=dtype) 
     rng[""jax_state""] = rng_key
     return (rng, sample) 
```",JAX random variables,rlouf,2022-12-09 17:41:14,2022-12-13 14:08:01,"rlouf assigned 2022-12-09 18:21:05,brandonwillard labeled 2022-12-09 18:21:16,brandonwillard labeled 2022-12-09 18:22:05,rlouf closed 2022-12-13 14:08:01",rlouf AdrienCorenflos brandonwillard,0
991,1350,Add Zipf `RandomVariable`,rlouf,https://numpy.org/doc/stable/reference/random/generated/numpy.random.Generator.zipf.html,enhancement good first issue help wanted random variables NumPy compatibility Op implementation,,2022-12-10 08:03:22,2023-03-02 18:21:50,"rlouf labeled 2022-12-10 08:03:42,rlouf labeled 2022-12-10 08:03:42,rlouf labeled 2022-12-10 08:03:42,rlouf labeled 2022-12-10 08:03:42,rlouf labeled 2022-12-10 08:03:42,rlouf labeled 2022-12-10 08:03:42,brandonwillard closed 2023-03-02 18:21:50",rlouf brandonwillard,0
992,1351,Add Rayleigh `RandomVariable`,rlouf,https://numpy.org/doc/stable/reference/random/generated/numpy.random.Generator.rayleigh.html,enhancement good first issue help wanted random variables NumPy compatibility Op implementation,,2022-12-10 08:05:15,2023-02-14 17:52:51,"rlouf labeled 2022-12-10 08:05:42,rlouf labeled 2022-12-10 08:05:42,rlouf labeled 2022-12-10 08:05:42,rlouf labeled 2022-12-10 08:05:42,rlouf labeled 2022-12-10 08:05:42,rlouf labeled 2022-12-10 08:05:42,brandonwillard closed 2023-02-14 17:52:51",rlouf brandonwillard,0
993,1352,Add power `RandomVariable`,rlouf,https://numpy.org/doc/stable/reference/random/generated/numpy.random.Generator.power.html,enhancement good first issue help wanted random variables NumPy compatibility Op implementation,,2022-12-10 08:06:15,2023-03-02 18:21:49,"rlouf labeled 2022-12-10 08:06:35,rlouf labeled 2022-12-10 08:06:35,rlouf labeled 2022-12-10 08:06:35,rlouf labeled 2022-12-10 08:06:35,rlouf labeled 2022-12-10 08:06:35,rlouf labeled 2022-12-10 08:06:50,brandonwillard closed 2023-03-02 18:21:49",rlouf brandonwillard,0
994,1353,Add random `RandomVariable`,rlouf,https://numpy.org/doc/stable/reference/random/generated/numpy.random.Generator.random.html,enhancement good first issue help wanted random variables NumPy compatibility Op implementation,,2022-12-10 08:08:28,2022-12-15 19:19:50,"rlouf labeled 2022-12-10 08:08:57,rlouf labeled 2022-12-10 08:08:57,rlouf labeled 2022-12-10 08:08:57,rlouf labeled 2022-12-10 08:08:57,rlouf labeled 2022-12-10 08:08:57,rlouf labeled 2022-12-10 08:08:57,brandonwillard closed 2022-12-15 19:19:50",rlouf brandonwillard,0
995,1358,"Add standard_cauchy, standard_gamma, standard_t and standard_exponential `RandomVariable`",rlouf,"We want to re-use the existing `GammaRV`, `CauchyRV`, `StudentTRV` `ExponentialRV`. The implementation of `StandardNormalRV` may need to be updated.",enhancement good first issue help wanted random variables NumPy compatibility Op implementation,,2022-12-10 08:16:56,2023-03-10 01:03:32,"rlouf labeled 2022-12-10 08:17:26,rlouf labeled 2022-12-10 08:17:26,rlouf labeled 2022-12-10 08:17:26,rlouf labeled 2022-12-10 08:17:26,rlouf labeled 2022-12-10 08:17:26,rlouf labeled 2022-12-10 08:17:26,rlouf renamed 2022-12-13 13:35:19,brandonwillard closed 2023-03-10 01:03:32",rlouf brandonwillard,0
1001,1366,Add f `RandomVariable`,rlouf,https://numpy.org/doc/stable/reference/random/generated/numpy.random.f.html,enhancement good first issue help wanted random variables Op implementation,,2022-12-13 13:38:03,2023-09-21 00:03:45,"rlouf labeled 2022-12-13 13:38:03,rlouf labeled 2022-12-13 13:38:03,rlouf labeled 2022-12-13 13:38:03,rlouf labeled 2022-12-13 13:38:03,rlouf labeled 2022-12-13 13:38:04,brandonwillard closed 2023-09-21 00:03:46",rlouf brandonwillard,0
1002,1368,Add JAX implementation for `InvGammaRV`,rlouf,,enhancement good first issue help wanted JAX random variables,,2022-12-13 13:58:25,2023-03-23 23:34:41,"rlouf labeled 2022-12-13 13:58:25,rlouf labeled 2022-12-13 13:58:25,rlouf labeled 2022-12-13 13:58:25,rlouf labeled 2022-12-13 13:58:25,rlouf labeled 2022-12-13 13:58:25,rlouf mentioned 2023-02-25 19:34:35,rlouf subscribed 2023-02-25 19:34:35,manish-p-gupta mentioned 2023-02-25 19:34:35,manish-p-gupta subscribed 2023-02-25 19:34:35,rlouf mentioned 2023-03-07 01:28:32,rlouf subscribed 2023-03-07 01:28:32,brandonwillard mentioned 2023-03-08 01:00:16,brandonwillard subscribed 2023-03-08 01:00:16,brandonwillard mentioned 2023-03-10 02:33:25,brandonwillard subscribed 2023-03-10 02:33:25,brandonwillard closed 2023-03-23 23:34:41",manish-p-gupta rlouf PaulScemama brandonwillard,12
1003,1369,Add JAX implementation for `TriangularRV`,rlouf,,enhancement good first issue help wanted JAX random variables,,2022-12-13 14:00:51,2022-12-13 15:59:33,"rlouf labeled 2022-12-13 14:00:51,rlouf labeled 2022-12-13 14:00:51,rlouf labeled 2022-12-13 14:00:51,rlouf labeled 2022-12-13 14:00:51,rlouf labeled 2022-12-13 14:00:51,rlouf closed 2022-12-13 15:59:34",rlouf AdrienCorenflos,1
1007,1373,Avoid pinning a particular python executable in flake8 hook,maresb,"## Description of your problem or feature request

This stems from the discussion in https://github.com/aesara-devs/aesara/pull/1371#discussion_r1049117400

In the recent commit https://github.com/aesara-devs/aesara/commit/0d28d0b90861ec86c7417987be01e5ebbbff1e0f, the line `language_version: python39` was added to the flake8 pre-commit hook. My system has no `python39` executable, which causes flake8 to fail for me when this line is present.

I am unable to reproduce the issue which this line was intended to solve (https://github.com/python/importlib_metadata/issues/406). I have run

```bash
mamba env remove --name aesara-dev
mamba env create -f environment.yml
conda activate aesara-dev
pip install -e .
pre-commit run --all-files
```

I receive the following error message:

```
[INFO] Installing environment for https://github.com/pycqa/flake8.
[INFO] Once installed this environment will be reused.
[INFO] This may take a few minutes...
An unexpected error has occurred: CalledProcessError: command: ('/home/mares/micromamba/envs/aesara-dev/bin/python', '-mvirtualenv', '/home/mares/.cache/pre-commit/reposfhl0r0a/py_env-python39', '-p', 'python39')
return code: 1
expected return code: 0
stdout:
    RuntimeError: failed to find interpreter for Builtin discover of python_spec='python39'
    
stderr: (none)
Check the log at /home/mares/.cache/pre-commit/pre-commit.log
```

Upon removing the `language_version` line, pre-commit succeeds.

## Versions and main components

* Operating system: Ubuntu 20.04

```
$ mamba list | grep -e importlib-metadata -e flake8 -e python -e aesara
# packages in environment at /home/mares/micromamba/envs/aesara-dev:
_ipython_minor_entry_point 8.7.0                hb6b4a82_0    conda-forge
aesara                    2.8.9+89.gfc0bfdf9e           dev_0    <develop>
importlib-metadata        5.1.0              pyha770c72_0    conda-forge
ipython                   8.7.0              pyh41d4057_0    conda-forge
python                    3.10.8          h4a9ceb5_0_cpython    conda-forge
python_abi                3.10                    3_cp310    conda-forge

```
<details> <summary> Aesara config: </summary>

floatX ({'float16', 'float32', 'float64'}) 
    Doc:  Default floating-point precision for python casts.

Note: float16 support is experimental, use at your own risk.
    Value:  float64

warn_float64 ({'pdb', 'ignore', 'raise', 'warn'}) 
    Doc:  Do an action when a tensor variable with float64 dtype is created.
    Value:  ignore

pickle_test_value (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f04196b7b80>>) 
    Doc:  Dump test values while pickling model. If True, test values will be dumped with model.
    Value:  True

cast_policy ({'custom', 'numpy+floatX'}) 
    Doc:  Rules for implicit type casting
    Value:  custom

deterministic ({'more', 'default'}) 
    Doc:  If `more`, sometimes we will select some implementation that are more deterministic, but slower.  Also see the dnn.conv.algo* flags to cover more cases.
    Value:  default

device (cpu)
    Doc:  Default device for computations. only cpu is supported for now
    Value:  cpu

force_device (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f040a7cace0>>) 
    Doc:  Raise an error if we can't use the specified device
    Value:  False

conv__assert_shape (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f040a7cad10>>) 
    Doc:  If True, AbstractConv* ops will verify that user-provided shapes match the runtime shapes (debugging option, may slow down compilation)
    Value:  False

print_global_stats (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f040a7cad40>>) 
    Doc:  Print some global statistics (time spent) at the end
    Value:  False

assert_no_cpu_op ({'pdb', 'ignore', 'raise', 'warn'}) 
    Doc:  Raise an error/warning if there is a CPU op in the computational graph.
    Value:  ignore

unpickle_function (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f040a7c8d30>>) 
    Doc:  Replace unpickled Aesara functions with None. This is useful to unpickle old graphs that pickled them when it shouldn't
    Value:  True

<aesara.configparser.ConfigParam object at 0x7f040a7cae60>
    Doc:  Default compilation mode
    Value:  Mode

cxx (<class 'str'>) 
    Doc:  The C++ compiler to use. Currently only g++ is supported, but supporting additional compilers should not be too difficult. If it is empty, no C++ code is compiled.
    Value:  /home/mares/micromamba/envs/aesara-dev/bin/g++

linker ({'py', 'vm_nogc', 'cvm_nogc', 'vm', 'c|py_nogc', 'c|py', 'cvm', 'c'}) 
    Doc:  Default linker used if the aesara flags mode is Mode
    Value:  cvm

allow_gc (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f040a7cac80>>) 
    Doc:  Do we default to delete intermediate results during Aesara function calls? Doing so lowers the memory requirement, but asks that we reallocate memory at the next function call. This is implemented for the default linker, but may not work for all linkers.
    Value:  True

optimizer ({'unsafe', 'fast_compile', 'None', 'o4', 'o1', 'merge', 'o3', 'o2', 'fast_run'}) 
    Doc:  Default optimizer. If not None, will use this optimizer with the Mode
    Value:  o4

optimizer_verbose (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f040a7caf20>>) 
    Doc:  If True, we print all optimization being applied
    Value:  False

on_opt_error ({'pdb', 'ignore', 'raise', 'warn'}) 
    Doc:  What to do when an optimization crashes: warn and skip it, raise the exception, or fall into the pdb debugger.
    Value:  warn

nocleanup (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f040a7caf80>>) 
    Doc:  Suppress the deletion of code files that did not compile cleanly
    Value:  False

on_unused_input ({'ignore', 'raise', 'warn'}) 
    Doc:  What to do if a variable in the 'inputs' list of  aesara.function() is not used in the graph.
    Value:  raise

gcc__cxxflags (<class 'str'>) 
    Doc:  Extra compiler flags for gcc
    Value:  

cmodule__warn_no_version (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f040a7cafe0>>) 
    Doc:  If True, will print a warning when compiling one or more Op with C code that can't be cached because there is no c_code_cache_version() function associated to at least one of those Ops.
    Value:  False

cmodule__remove_gxx_opt (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f040a7cae30>>) 
    Doc:  If True, will remove the -O* parameter passed to g++.This is useful to debug in gdb modules compiled by Aesara.The parameter -g is passed by default to g++
    Value:  False

cmodule__compilation_warning (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f040a7cb040>>) 
    Doc:  If True, will print compilation warnings.
    Value:  False

cmodule__preload_cache (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f040a7cb070>>) 
    Doc:  If set to True, will preload the C module cache at import time
    Value:  False

cmodule__age_thresh_use (<class 'int'>) 
    Doc:  In seconds. The time after which Aesara won't reuse a compile c module.
    Value:  2073600

cmodule__debug (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f040a7cb0d0>>) 
    Doc:  If True, define a DEBUG macro (if not exists) for any compiled C code.
    Value:  False

compile__wait (<class 'int'>) 
    Doc:  Time to wait before retrying to acquire the compile lock.
    Value:  5

compile__timeout (<class 'int'>) 
    Doc:  In seconds, time that a process will wait before deciding to
    override an existing lock. An override only happens when the existing
    lock is held by the same owner *and* has not been 'refreshed' by this
    owner for more than this period. Refreshes are done every half timeout
    period for running processes.
    Value:  120

ctc__root (<class 'str'>) 
    Doc:  Directory which contains the root of Baidu CTC library. It is assumed         that the compiled library is either inside the build, lib or lib64         subdirectory, and the header inside the include directory.
    Value:  

tensor__cmp_sloppy (<class 'int'>) 
    Doc:  Relax aesara.tensor.math._allclose (0) not at all, (1) a bit, (2) more
    Value:  0

tensor__local_elemwise_fusion (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f040a7cb280>>) 
    Doc:  Enable or not in fast_run mode(fast_run optimization) the elemwise fusion optimization
    Value:  True

lib__amblibm (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f040a7cb310>>) 
    Doc:  Use amd's amdlibm numerical library
    Value:  False

tensor__insert_inplace_optimizer_validate_nb (<class 'int'>) 
    Doc:  -1: auto, if graph have less then 500 nodes 1, else 10
    Value:  -1

traceback__limit (<class 'int'>) 
    Doc:  The number of stack to trace. -1 mean all.
    Value:  8

traceback__compile_limit (<class 'int'>) 
    Doc:  The number of stack to trace to keep during compilation. -1 mean all. If greater then 0, will also make us save Aesara internal stack trace.
    Value:  0

experimental__local_alloc_elemwise (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f040a7cb490>>) 
    Doc:  DEPRECATED: If True, enable the experimental optimization local_alloc_elemwise. Generates error if not True. Use optimizer_excluding=local_alloc_elemwise to disable.
    Value:  True

experimental__local_alloc_elemwise_assert (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f040a7cb4c0>>) 
    Doc:  When the local_alloc_elemwise is applied, add an assert to highlight shape errors.
    Value:  True

warn__ignore_bug_before ({'0.4.1', '1.0', '1.0.4', '0.8.1', '1.0.2', '0.10', '0.9', '0.3', '1.0.5', '0.8', '1.0.3', '0.7', '0.6', '1.0.1', 'None', '0.5', 'all', '0.8.2', '0.4'}) 
    Doc:  If 'None', we warn about all Aesara bugs found by default. If 'all', we don't warn about Aesara bugs found by default. If a version, we print only the warnings relative to Aesara bugs found after that version. Warning for specific bugs can be configured with specific [warn] flags.
    Value:  0.9

exception_verbosity ({'low', 'high'}) 
    Doc:  If 'low', the text of exceptions will generally refer to apply nodes with short names such as Elemwise{add_no_inplace}. If 'high', some exceptions will also refer to apply nodes with long descriptions  like:
        A. Elemwise{add_no_inplace}
                B. log_likelihood_v_given_h
                C. log_likelihood_h
    Value:  low

print_test_value (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f040a7cb610>>) 
    Doc:  If 'True', the __eval__ of an Aesara variable will return its test_value when this is available. This has the practical conseguence that, e.g., in debugging `my_var` will print the same as `my_var.tag.test_value` when a test value is defined.
    Value:  False

compute_test_value ({'off', 'pdb', 'raise', 'warn', 'ignore'}) 
    Doc:  If 'True', Aesara will run each op at graph build time, using Constants, SharedVariables and the tag 'test_value' as inputs to the function. This helps the user track down problems in the graph before it gets optimized.
    Value:  off

compute_test_value_opt ({'off', 'pdb', 'raise', 'warn', 'ignore'}) 
    Doc:  For debugging Aesara optimization only. Same as compute_test_value, but is used during Aesara optimization
    Value:  off

check_input (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f040a7cb6a0>>) 
    Doc:  Specify if types should check their input in their C code. It can be used to speed up compilation, reduce overhead (particularly for scalars) and reduce the number of generated C files.
    Value:  True

NanGuardMode__nan_is_error (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f040a7cb6d0>>) 
    Doc:  Default value for nan_is_error
    Value:  True

NanGuardMode__inf_is_error (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f040a7cb700>>) 
    Doc:  Default value for inf_is_error
    Value:  True

NanGuardMode__big_is_error (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f040a7cb790>>) 
    Doc:  Default value for big_is_error
    Value:  True

NanGuardMode__action ({'pdb', 'raise', 'warn'}) 
    Doc:  What NanGuardMode does when it finds a problem
    Value:  raise

DebugMode__patience (<class 'int'>) 
    Doc:  Optimize graph this many times to detect inconsistency
    Value:  10

DebugMode__check_c (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f040a7cb820>>) 
    Doc:  Run C implementations where possible
    Value:  True

DebugMode__check_py (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f040a7cb8b0>>) 
    Doc:  Run Python implementations where possible
    Value:  True

DebugMode__check_finite (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f040a7cb8e0>>) 
    Doc:  True -> complain about NaN/Inf results
    Value:  True

DebugMode__check_strides (<class 'int'>) 
    Doc:  Check that Python- and C-produced ndarrays have same strides. On difference: (0) - ignore, (1) warn, or (2) raise error
    Value:  0

DebugMode__warn_input_not_reused (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f040a7cb940>>) 
    Doc:  Generate a warning when destroy_map or view_map says that an op works inplace, but the op did not reuse the input for its output.
    Value:  True

DebugMode__check_preallocated_output (<class 'str'>) 
    Doc:  Test thunks with pre-allocated memory as output storage. This is a list of strings separated by "":"". Valid values are: ""initial"" (initial storage in storage map, happens with Scan),""previous"" (previously-returned memory), ""c_contiguous"", ""f_contiguous"", ""strided"" (positive and negative strides), ""wrong_size"" (larger and smaller dimensions), and ""ALL"" (all of the above).
    Value:  

DebugMode__check_preallocated_output_ndim (<class 'int'>) 
    Doc:  When testing with ""strided"" preallocated output memory, test all combinations of strides over that number of (inner-most) dimensions. You may want to reduce that number to reduce memory or time usage, but it is advised to keep a minimum of 2.
    Value:  4

profiling__time_thunks (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f040a7cb9d0>>) 
    Doc:  Time individual thunks when profiling
    Value:  True

profiling__n_apply (<class 'int'>) 
    Doc:  Number of Apply instances to print by default
    Value:  20

profiling__n_ops (<class 'int'>) 
    Doc:  Number of Ops to print by default
    Value:  20

profiling__output_line_width (<class 'int'>) 
    Doc:  Max line width for the profiling output
    Value:  512

profiling__min_memory_size (<class 'int'>) 
    Doc:  For the memory profile, do not print Apply nodes if the size
                 of their outputs (in bytes) is lower than this threshold
    Value:  1024

profiling__min_peak_memory (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f040a7cbb20>>) 
    Doc:  The min peak memory usage of the order
    Value:  False

profiling__destination (<class 'str'>) 
    Doc:  File destination of the profiling output
    Value:  stderr

profiling__debugprint (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f040a7cbb80>>) 
    Doc:  Do a debugprint of the profiled functions
    Value:  False

profiling__ignore_first_call (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f040a7cbbb0>>) 
    Doc:  Do we ignore the first call of an Aesara function.
    Value:  False

on_shape_error ({'raise', 'warn'}) 
    Doc:  warn: print a warning and use the default value. raise: raise an error
    Value:  warn

openmp (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f040a7cbc10>>) 
    Doc:  Allow (or not) parallel computation on the CPU with OpenMP. This is the default value used when creating an Op that supports OpenMP parallelization. It is preferable to define it via the Aesara configuration file ~/.aesararc or with the environment variable AESARA_FLAGS. Parallelization is only done for some operations that implement it, and even for operations that implement parallelism, each operation is free to respect this flag or not. You can control the number of threads used with the environment variable OMP_NUM_THREADS. If it is set to 1, we disable openmp in Aesara by default.
    Value:  False

openmp_elemwise_minsize (<class 'int'>) 
    Doc:  If OpenMP is enabled, this is the minimum size of vectors for which the openmp parallelization is enabled in element wise ops.
    Value:  200000

optimizer_excluding (<class 'str'>) 
    Doc:  When using the default mode, we will remove optimizer with these tags. Separate tags with ':'.
    Value:  

optimizer_including (<class 'str'>) 
    Doc:  When using the default mode, we will add optimizer with these tags. Separate tags with ':'.
    Value:  

optimizer_requiring (<class 'str'>) 
    Doc:  When using the default mode, we will require optimizer with these tags. Separate tags with ':'.
    Value:  

optdb__position_cutoff (<class 'float'>) 
    Doc:  Where to stop eariler during optimization. It represent the position of the optimizer where to stop.
    Value:  inf

optdb__max_use_ratio (<class 'float'>) 
    Doc:  A ratio that prevent infinite loop in EquilibriumGraphRewriter.
    Value:  8.0

cycle_detection ({'regular', 'fast'}) 
    Doc:  If cycle_detection is set to regular, most inplaces are allowed,but it is slower. If cycle_detection is set to faster, less inplacesare allowed, but it makes the compilation faster.The interaction of which one give the lower peak memory usage iscomplicated and not predictable, so if you are close to the peakmemory usage, triyng both could give you a small gain.
    Value:  regular

check_stack_trace ({'off', 'log', 'raise', 'warn'}) 
    Doc:  A flag for checking the stack trace during the optimization process. default (off): does not check the stack trace of any optimization log: inserts a dummy stack trace that identifies the optimizationthat inserted the variable that had an empty stack trace.warn: prints a warning if a stack trace is missing and also a dummystack trace is inserted that indicates which optimization insertedthe variable that had an empty stack trace.raise: raises an exception if a stack trace is missing
    Value:  off

metaopt__verbose (<class 'int'>) 
    Doc:  0 for silent, 1 for only warnings, 2 for full output withtimings and selected implementation
    Value:  0

metaopt__optimizer_excluding (<class 'str'>) 
    Doc:  exclude optimizers with these tags. Separate tags with ':'.
    Value:  

metaopt__optimizer_including (<class 'str'>) 
    Doc:  include optimizers with these tags. Separate tags with ':'.
    Value:  

profile (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f040a7cbf10>>) 
    Doc:  If VM should collect profile information
    Value:  False

profile_optimizer (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f040a7cbf40>>) 
    Doc:  If VM should collect optimizer profile information
    Value:  False

profile_memory (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f040a7cbf70>>) 
    Doc:  If VM should collect memory profile information and print it
    Value:  False

<aesara.configparser.ConfigParam object at 0x7f040a7cbfa0>
    Doc:  Useful only for the VM Linkers. When lazy is None, auto detect if lazy evaluation is needed and use the appropriate version. If the C loop isn't being used and lazy is True, use the Stack VM; otherwise, use the Loop VM.
    Value:  None

unittests__rseed (<class 'str'>) 
    Doc:  Seed to use for randomized unit tests. Special value 'random' means using a seed of None.
    Value:  666

warn__round (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f040a6000a0>>) 
    Doc:  Warn when using `tensor.round` with the default mode. Round changed its default from `half_away_from_zero` to `half_to_even` to have the same default as NumPy.
    Value:  False

numba__vectorize_target ({'cuda', 'cpu', 'parallel'}) 
    Doc:  Default target for numba.vectorize.
    Value:  cpu

numba__fastmath (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f040a600160>>) 
    Doc:  If True, use Numba's fastmath mode.
    Value:  True

numba__cache (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f040a6001f0>>) 
    Doc:  If True, use Numba's file based caching.
    Value:  True

compiledir_format (<class 'str'>) 
    Doc:  Format string for platform-dependent compiled module subdirectory
(relative to base_compiledir). Available keys: aesara_version, device,
gxx_version, hostname, numpy_version, platform, processor,
python_bitwidth, python_int_bitwidth, python_version, short_platform.
Defaults to compiledir_%(short_platform)s-%(processor)s-
%(python_version)s-%(python_bitwidth)s.
    Value:  compiledir_%(short_platform)s-%(processor)s-%(python_version)s-%(python_bitwidth)s

<aesara.configparser.ConfigParam object at 0x7f040a600280>
    Doc:  platform-independent root directory for compiled modules
    Value:  /home/mares/.aesara

<aesara.configparser.ConfigParam object at 0x7f040a6001c0>
    Doc:  platform-dependent cache directory for compiled modules
    Value:  /home/mares/.aesara/compiledir_Linux-5.15--generic-x86_64-with-glibc2.31-x86_64-3.10.8-64

blas__ldflags (<class 'str'>) 
    Doc:  lib[s] to include for [Fortran] level-3 blas implementation
    Value:  -L/home/mares/micromamba/envs/aesara-dev/lib -lcblas -lblas -lcblas -lblas

blas__check_openmp (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f040a2cfd60>>) 
    Doc:  Check for openmp library conflict.
WARNING: Setting this to False leaves you open to wrong results in blas-related operations.
    Value:  True

scan__allow_gc (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f0406bc4d60>>) 
    Doc:  Allow/disallow gc inside of Scan (default: False)
    Value:  False

scan__allow_output_prealloc (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7f0406b66590>>) 
    Doc:  Allow/disallow memory preallocation for outputs inside of scan (default: True)
    Value:  True

</details>
",testing,,2022-12-15 19:02:15,2022-12-15 22:55:03,"brandonwillard labeled 2022-12-15 19:18:52,maresb closed 2022-12-15 22:55:03",maresb brandonwillard,5
1013,1379,Failing nightly build,maresb,"Success on [Dec. 28](https://github.com/aesara-devs/aesara/actions/runs/3790978838/jobs/6446051005) and failure on [Dec. 29](https://github.com/aesara-devs/aesara/actions/runs/3797620542/jobs/6458717648), and the corresponding [diff on main](https://github.com/aesara-devs/aesara/compare/e9bd02684e4cb7c0f01cb6ec9d30c511057812e7..f874925fecabd50a751d82c6d8a9aad41ce1acbe).

The problem seems to be that the nightly package is being built as `aesara` instead of `aesara-nightly`. Perhaps `pyproject.toml` is overriding `setup.py` when `python -m build --sdist .` is run.",bug CI,,2022-12-31 11:01:45,2022-12-31 21:04:03,"maresb referenced 2022-12-31 12:03:28,maresb referenced 2022-12-31 12:10:26,brandonwillard labeled 2022-12-31 21:02:33,brandonwillard labeled 2022-12-31 21:02:38,brandonwillard closed 2022-12-31 21:04:04,brandonwillard referenced 2022-12-31 21:04:04",maresb brandonwillard,5
1027,1394,Setting up aesara,Smit-create,"I want to set up a local aesara dev environment. I started by creating the `conda` env and tried to make some changes and test aesara. It showed me to build or install aesara to import it so I tried the following:
```console
% python3 -m build
```

This ends up with the following error:
```console
File ""/private/var/folders/43/clc5f7ln5sj3d3zwmnvpmpxm0000gn/T/build-env-23uc9q72/lib/python3.10/site-packages/setuptools_scm/git.py"", line 224, in _git_parse_inner
    return meta(
  File ""/private/var/folders/43/clc5f7ln5sj3d3zwmnvpmpxm0000gn/T/build-env-23uc9q72/lib/python3.10/site-packages/setuptools_scm/version.py"", line 226, in meta
    parsed_version = _parse_tag(tag, preformatted, config)
  File ""/private/var/folders/43/clc5f7ln5sj3d3zwmnvpmpxm0000gn/T/build-env-23uc9q72/lib/python3.10/site-packages/setuptools_scm/version.py"", line 203, in _parse_tag
    assert version is not None
AssertionError

ERROR Backend subprocess exited when trying to invoke get_requires_for_build_sdist
```",,,2023-01-12 10:37:51,2023-01-12 19:06:41,"aesara-devs locked 2023-01-12 19:06:40,brandonwillard converted_to_discussion 2023-01-12 19:06:40",brandonwillard aesara-devs Smit-create,0
1029,1397,Remove doc/ from sdist,maresb,According to https://github.com/aesara-devs/aesara/pull/1384#issuecomment-1385319888 it seems that the `docs/` directory isn't doing anything but taking up space from the sdist. (Deleting reduces the `.tar.gz` sdist from 3.85MB to 1.34MB.),help wanted refactor setup and installation,,2023-01-17 12:08:53,2023-02-19 01:03:17,"brandonwillard labeled 2023-01-17 17:30:11,brandonwillard labeled 2023-01-17 17:30:18,brandonwillard labeled 2023-01-17 18:36:32,maresb renamed 2023-02-05 09:01:06,maresb referenced 2023-02-05 09:08:39,maresb referenced 2023-02-18 10:52:32,brandonwillard closed 2023-02-19 01:03:17,brandonwillard referenced 2023-02-19 01:03:17",maresb brandonwillard,0
1032,1401,Math typo in logistic sigmoid function docstring,larryshamalama,"There are three docstring instances where the sigmoid/expit function is incorrectly displayed as `(1 / (1 + exp(x))` instead of `(1 / (1 + exp(-x))`.

https://github.com/aesara-devs/aesara/blob/2dc759127547d4b836629e927b7af24f840cc91c/aesara/tensor/inplace.py#L318

https://github.com/aesara-devs/aesara/blob/baa6b7fa3f5ba53359bf1180260b0afb6b924287/aesara/tensor/math.py#L1421

https://github.com/aesara-devs/aesara/blob/a7ef6db7535c60911446930b12f35d986c2e0094/aesara/scalar/math.py#L1093",bug documentation good first issue help wanted,,2023-01-24 15:05:35,2023-01-26 12:30:10,"brandonwillard labeled 2023-01-24 18:24:23,brandonwillard labeled 2023-01-24 18:24:23,brandonwillard labeled 2023-01-24 18:24:27,brandonwillard labeled 2023-01-24 18:24:30,vrii14 mentioned 2023-01-26 10:08:56,vrii14 subscribed 2023-01-26 10:08:56,rlouf closed 2023-01-26 12:30:10,vrii14 mentioned 2023-01-26 17:26:51,vrii14 subscribed 2023-01-26 17:26:51",larryshamalama rlouf vrii14 brandonwillard,4
1037,1406,Nightly version numbering issue after package metadata changes,brandonwillard,"Looks like we need some additional changes to address [this dev version issue during the nightly build](https://github.com/aesara-devs/aesara/actions/runs/4094517974/jobs/7060780835#step:7:31).  We can temporarily revert https://github.com/aesara-devs/aesara/pull/1405 in the meantime, if need be.",bug setup and installation,maresb,2023-02-05 02:33:49,2023-02-05 17:16:08,"brandonwillard labeled 2023-02-05 02:33:49,brandonwillard labeled 2023-02-05 02:33:49,maresb assigned 2023-02-05 08:33:59,maresb referenced 2023-02-05 08:48:53,brandonwillard closed 2023-02-05 17:16:09,brandonwillard referenced 2023-02-05 17:16:09",maresb brandonwillard,1
1044,1415,Fix link to the mission statement in the README,rlouf,It should point to `doc/mission.rst`,documentation,,2023-02-08 07:05:26,2023-02-10 06:00:37,"rlouf labeled 2023-02-08 07:05:50,rlouf closed 2023-02-10 06:00:37",rlouf sudarsan2k5,1
1047,1422,Replace `sqr` for `square`,rlouf,"There already is an alias but we should replace `sqr` with `square`, use `sqr` as an alias an add a deprecation warning.

https://numpy.org/doc/stable/reference/generated/numpy.power.html#numpy.power",good first issue NumPy compatibility,,2023-02-13 03:32:29,2023-03-10 01:03:02,"rlouf labeled 2023-02-13 03:32:29,dgerlanc labeled 2023-02-17 03:02:17,brandonwillard closed 2023-03-10 01:03:02",dgerlanc rlouf brandonwillard,0
1057,1446,Cython-generated `Scan` code is invalid for Python 3.11,brandonwillard,"### Discussed in https://github.com/aesara-devs/aesara/discussions/1445

<div type='discussions-op-text'>

<sup>Originally posted by **mgorny** February 20, 2023</sup>
Is Python 3.11 supported by aesara? It seems to be listed in its trove classifier list but when I run the test suite against 3.11, I get 139 test failures. Looking at a few of them, they seem to be caused by some C-level incompatibility, e.g.:

```
E               /tmp/portage/dev-python/aesara-2.8.11/homedir/.aesara/compiledir_140467563507264/scan_perform/mod.cpp: In function ‘void __Pyx_AddTraceback(const char*, int, int, const char*)’:
E               /tmp/portage/dev-python/aesara-2.8.11/homedir/.aesara/compiledir_140467563507264/scan_perform/mod.cpp:438:62: error: invalid use of incomplete type ‘PyFrameObject’ {aka ‘struct _frame’}
E                 438 |   #define __Pyx_PyFrame_SetLineNumber(frame, lineno)  (frame)->f_lineno = (lineno)
E                     |                                                              ^~
E               /tmp/portage/dev-python/aesara-2.8.11/homedir/.aesara/compiledir_140467563507264/scan_perform/mod.cpp:26021:5: note: in expansion of macro ‘__Pyx_PyFrame_SetLineNumber’
E               26021 |     __Pyx_PyFrame_SetLineNumber(py_frame, py_line);
E                     |     ^~~~~~~~~~~~~~~~~~~~~~~~~~~
E               In file included from /usr/include/python3.11/Python.h:42:
E               /usr/include/python3.11/pytypedefs.h:22:16: note: forward declaration of ‘PyFrameObject’ {aka ‘struct _frame’}
E                  22 | typedef struct _frame PyFrameObject;
E                     |                ^~~~~~
```

I could try providing more details but the build log is 1.1G of size.</div>",bug C-backend Scan,,2023-02-21 01:59:32,2023-02-21 18:56:45,"brandonwillard labeled 2023-02-21 01:59:32,brandonwillard labeled 2023-02-21 01:59:32,brandonwillard labeled 2023-02-21 01:59:32,brandonwillard renamed 2023-02-21 01:59:59,brandonwillard closed 2023-02-21 18:56:45",brandonwillard,0
1064,1455,Contributor installation error: Error getting the version from source `vcs`,PaulScemama,"## Description of your problem or feature request
Installation error seemingly due to 
```
[tool.hatch.version]
source = ""vcs""`
```

in `pyproject.toml`

Provide a minimal, self-contained, and reproducible example (i.e. an [MWE](https://en.wikipedia.org/wiki/Minimal_reproducible_example)):

After forking and cloning to local machine:
```python
conda env create -n aesara-dev -f environment-arm.yml
conda activate aesara-dev
pip install -r requirements.txt
```

**Please provide the full tracebacks for any relevant errors and/or warning messages.**


```python
Obtaining file:///Users/pscemama/aesara (from -r requirements.txt (line 1))
  Installing build dependencies ... done
  Checking if build backend supports build_editable ... done
  Getting requirements to build editable ... done
  Preparing editable metadata (pyproject.toml) ... error
  error: subprocess-exited-with-error
  
  × Preparing editable metadata (pyproject.toml) did not run successfully.
  │ exit code: 1
  ╰─> [22 lines of output]
      /private/var/folders/_7/1p7_nqln507d79h4n2d3t2140000gp/T/pip-build-env-lnsii9vt/overlay/lib/python3.10/site-packages/setuptools_scm/version.py:84: UserWarning: tag '0.0' no version found
        warnings.warn(f""tag {tag!r} no version found"")
      Traceback (most recent call last):
        File ""/Users/pscemama/miniforge3/envs/aesara-dev/lib/python3.10/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py"", line 353, in <module>
          main()
        File ""/Users/pscemama/miniforge3/envs/aesara-dev/lib/python3.10/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py"", line 335, in main
          json_out['return_val'] = hook(**hook_input['kwargs'])
        File ""/Users/pscemama/miniforge3/envs/aesara-dev/lib/python3.10/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py"", line 176, in prepare_metadata_for_build_editable
          whl_basename = build_hook(metadata_directory, config_settings)
        File ""/private/var/folders/_7/1p7_nqln507d79h4n2d3t2140000gp/T/pip-build-env-lnsii9vt/overlay/lib/python3.10/site-packages/hatchling/build.py"", line 78, in build_editable
          return os.path.basename(next(builder.build(wheel_directory, ['editable'])))
        File ""/private/var/folders/_7/1p7_nqln507d79h4n2d3t2140000gp/T/pip-build-env-lnsii9vt/overlay/lib/python3.10/site-packages/hatchling/builders/plugin/interface.py"", line 93, in build
          self.metadata.validate_fields()
        File ""/private/var/folders/_7/1p7_nqln507d79h4n2d3t2140000gp/T/pip-build-env-lnsii9vt/overlay/lib/python3.10/site-packages/hatchling/metadata/core.py"", line 243, in validate_fields
          _ = self.version
        File ""/private/var/folders/_7/1p7_nqln507d79h4n2d3t2140000gp/T/pip-build-env-lnsii9vt/overlay/lib/python3.10/site-packages/hatchling/metadata/core.py"", line 128, in version
          self._version = self._get_version()
        File ""/private/var/folders/_7/1p7_nqln507d79h4n2d3t2140000gp/T/pip-build-env-lnsii9vt/overlay/lib/python3.10/site-packages/hatchling/metadata/core.py"", line 226, in _get_version
          version = self.hatch.version.cached
        File ""/private/var/folders/_7/1p7_nqln507d79h4n2d3t2140000gp/T/pip-build-env-lnsii9vt/overlay/lib/python3.10/site-packages/hatchling/metadata/core.py"", line 1412, in cached
          raise type(e)(message) from None
      AssertionError: Error getting the version from source `vcs`:
      [end of output]
  
  note: This error originates from a subprocess, and is likely not a problem with pip.
error: metadata-generation-failed

× Encountered error while generating package metadata.
╰─> See above for output.

note: This is an issue with the package mentioned above, not pip.
hint: See above for details.
```

**Please provide any additional information below.**
Machine details:
```
Model Name: MacBook Pro
Kernel Version: Darwin 21.6.0
Chip: Apple M1 Pro
```

## Versions and main components

* Aesara version: **2.8.12** (lastest)
* Python version: **3.10.9**
* Operating system: **MacOS**
* How did you install Aesara: (conda/pip) **via contribution list.**

",,,2023-02-25 20:33:53,2023-02-27 23:14:25,"aesara-devs locked 2023-02-27 23:14:24,brandonwillard converted_to_discussion 2023-02-27 23:14:24",PaulScemama aesara-devs brandonwillard,0
1065,1457,Mention the need to pull tags when performing a development install,brandonwillard,"We need to mention that users should pull the tags (e.g. `git fetch -t`) when attempting a developer build; otherwise, they could get `vcs` errors.

_Originally posted by @brandonwillard in https://github.com/aesara-devs/aesara/discussions/1456#discussioncomment-5133445_
      ",documentation good first issue help wanted,,2023-02-27 23:43:17,2023-03-09 22:42:36,"brandonwillard mentioned 2023-02-27 23:43:18,brandonwillard subscribed 2023-02-27 23:43:18,brandonwillard labeled 2023-02-27 23:43:31,brandonwillard labeled 2023-02-27 23:43:31,brandonwillard labeled 2023-02-27 23:43:31,brandonwillard mentioned 2023-02-28 01:42:51,brandonwillard subscribed 2023-02-28 01:42:51,dgerlanc mentioned 2023-03-03 03:36:16,dgerlanc subscribed 2023-03-03 03:36:16,dgerlanc mentioned 2023-03-03 14:00:27,dgerlanc subscribed 2023-03-03 14:00:27,brandonwillard mentioned 2023-03-03 14:00:28,brandonwillard subscribed 2023-03-03 14:00:28,brandonwillard closed 2023-03-09 22:42:36",dgerlanc PaulScemama brandonwillard,6
1066,1458,Error when `pip install -r requirements-rtd.txt` for contributing to docs,PaulScemama,"When running the command `pip install -r requirements-rtd.txt`, an issue arises with `use_2to3`. [This](https://stackoverflow.com/questions/69100275/error-while-downloading-the-requirements-using-pip-install-setup-command-use-2) says it is due to `setuptools>=58` breaking support for `use_2to3`. The changelog for `setuptools` notes:

> ""[#2086](https://github.com/pypa/setuptools/issues/2086): Removed support for 2to3 during builds. Projects should port to a unified codebase or pin to an older version of Setuptools using [PEP 518](https://peps.python.org/pep-0518/) build-requires.""

I resolved the issue by `pip install setuptools==58` and then `pip install -r requirements-rtd.txt`. 

Not entirely sure what the best option is to fix this.",,,2023-02-28 00:36:54,2023-02-28 01:19:41,"aesara-devs locked 2023-02-28 01:19:40,brandonwillard converted_to_discussion 2023-02-28 01:19:41",PaulScemama aesara-devs brandonwillard,0
1069,1462,RTD build failure,brandonwillard,"We're getting a new [build failure](https://readthedocs.org/api/v2/build/19656140.txt).  If I'm reading the output correctly, the issue might be due to the `modes` entry [here](https://github.com/aesara-devs/aesara/blob/055ceb18eb4f702050f131bd6b05f9b76949f56e/doc/compile/index.rst?plain=1#L17).

We also need to address the Sphinx compilation issues in that output.",bug documentation help wanted,,2023-03-02 20:28:08,2023-03-08 00:33:47,"brandonwillard labeled 2023-03-02 20:28:27,brandonwillard labeled 2023-03-02 20:28:27,brandonwillard labeled 2023-03-02 20:28:27,brandonwillard closed 2023-03-08 00:33:48",brandonwillard,1
1070,1463,Random sampling JAX failures caused by `jax>=0.4.5`,brandonwillard,It looks like a new version of JAX has resulted in [errors in at least one sampling test](https://github.com/aesara-devs/aesara/actions/runs/4334414386/jobs/7568310584#step:6:2041) and it's blocking CI runs.,bug help wanted JAX important,,2023-03-05 05:22:44,2023-03-07 23:58:01,"brandonwillard labeled 2023-03-05 05:22:44,brandonwillard labeled 2023-03-05 05:22:44,brandonwillard labeled 2023-03-05 05:22:44,brandonwillard labeled 2023-03-05 05:22:45,brandonwillard connected 2023-03-05 20:21:53,brandonwillard closed 2023-03-07 23:58:01",brandonwillard,0
1074,1467,DeprecationWarning: numpy conversion of out-of-bound python integers,eganster,"After a fresh install of aesara (in a new venv) I started noticing DeprecationWarnings from numpy regarding out-of-bound conversion of python integers.
I'm pretty sure that this is related/started with NumPy 1.24: https://numpy.org/doc/stable/release/1.24.0-notes.html#conversion-of-out-of-bound-python-integers

```python
>>> import warnings
>>> warnings.filterwarnings('always', category=DeprecationWarning)
>>> from aesara import tensor
>>> tensor.gt(127, 2)
Elemwise{gt,no_inplace}.0
>>> tensor.gt(128, 2)
/home/USER/virtualenvs/test/lib/python3.10/site-packages/aesara/misc/safe_asarray.py:35: DeprecationWarning: NumPy will stop allowing conversion of out-of-bound Python integers to integer arrays.  The conversion of 128 to int8 will fail in the future.
For the old behavior, usually:
    np.array(value).astype(dtype)`
will give the desired result (the cast overflows).
  rval = np.asarray(a, dtype=dtype, order=order)
Elemwise{gt,no_inplace}.0
>>> tensor.gt(32768, 2)
/home/USER/virtualenvs/test/lib/python3.10/site-packages/aesara/misc/safe_asarray.py:35: DeprecationWarning: NumPy will stop allowing conversion of out-of-bound Python integers to integer arrays.  The conversion of 32768 to int8 will fail in the future.
For the old behavior, usually:
    np.array(value).astype(dtype)`
will give the desired result (the cast overflows).
  rval = np.asarray(a, dtype=dtype, order=order)
/home/USER/virtualenvs/test/lib/python3.10/site-packages/aesara/misc/safe_asarray.py:35: DeprecationWarning: NumPy will stop allowing conversion of out-of-bound Python integers to integer arrays.  The conversion of 32768 to int16 will fail in the future.
For the old behavior, usually:
    np.array(value).astype(dtype)`
will give the desired result (the cast overflows).
  rval = np.asarray(a, dtype=dtype, order=order)
Elemwise{gt,no_inplace}.0
```

## Versions and main components

* Aesara version: 2.8.12
* Python version: 3.10
* Operating system:
* How did you install Aesara: pip
",NumPy compatibility unexpected behavior,,2023-03-08 12:02:37,2023-03-10 15:50:48,"brandonwillard labeled 2023-03-08 18:57:17,brandonwillard labeled 2023-03-08 18:57:17,brandonwillard closed 2023-03-10 15:50:48",eganster brandonwillard,2
1077,1472,"Possible small error in `GenGammaRV`: `""rng_state""` instead of `""jax_state""`",PaulScemama,"My `JAX` internals knowledge is less than stellar at the moment, so I haven't dug deep and this seems like a transparent bug. 

I noticed in `aesara/link/jax/dispatch/random.py` the `GenGammaRV` implementation is:

```python
@jax_sample_fn.register(aer.GenGammaRV)
def jax_sample_fn_gengamma(op):
    r""""""Provide a JAX implementation of `GenGammaRV`.

    Samples are obtained from inverse sampling using the following:

    .. math::

        F^{-1}(q; a, d, p) = a \\left( G^{-1}(q) \\right)^{1/p}

    where :math:`G` is the CDF of a gamma distribution with
    :math:`\\alpha = d/p` and :math:`\\beta = 1`.

    .. note::

        Here we use the parametrization :math:`\\alpha = d/p`.

    """"""

    def sample_fn(rng, size, dtype, *parameters):
        rng_key = rng[""jax_state""]
        rng_key, sampling_key = jax.random.split(rng_key, 2)

        alpha, lam, p = parameters
        d = alpha / p
        samples = jax.random.gamma(sampling_key, d, size, dtype)
        samples = lam * samples ** (1.0 / p)

        rng[""rng_state""] = rng_key
        return (rng, samples)

    return sample_fn

```

Notice at the end we have `rng[""rng_state""] = rng_key`. I noticed that in every other implementation it is intead `rng[""jax_state""] = rng_key`. I just wanted to point it out, it's a very very tiny error! 
",bug help wanted JAX,,2023-03-13 23:21:11,2023-03-15 00:20:27,"brandonwillard labeled 2023-03-14 14:41:19,brandonwillard labeled 2023-03-14 14:41:19,brandonwillard labeled 2023-03-14 14:41:19,brandonwillard closed 2023-03-15 00:20:28,soraros unsubscribed 2023-03-16 13:02:38",PaulScemama soraros brandonwillard,1
1078,1473,"Error with git pre-hooks: Module ""setuptools._distutils.errors"" has no attribute ""CompileError""",PaulScemama,"## Description of your problem or feature request

I made some changes on my local branch; then when committing to my remote fork of the repository, I am getting a git pre-hook error that I really have no clue what it's about 😅. 

```python
git commit -m ""commit message""
```

**Please provide the full tracebacks for any relevant errors and/or warning messages.**


```python
debug statements (python)................................................Passed
check for merge conflicts................................................Passed
black....................................................................Passed
flake8...................................................................Passed
isort....................................................................Passed
autoflake................................................................Passed
mypy.....................................................................Failed
- hook id: mypy
- exit code: 1

aesara/link/c/exceptions.py:1: error: Module ""setuptools._distutils.errors"" has no attribute ""CompileError""  [attr-defined]
aesara/graph/destroyhandler.py:361: note: By default the bodies of untyped functions are not checked, consider using --check-untyped-defs  [annotation-unchecked]
aesara/graph/destroyhandler.py:368: note: By default the bodies of untyped functions are not checked, consider using --check-untyped-defs  [annotation-unchecked]
aesara/graph/destroyhandler.py:379: note: By default the bodies of untyped functions are not checked, consider using --check-untyped-defs  [annotation-unchecked]
aesara/graph/destroyhandler.py:387: note: By default the bodies of untyped functions are not checked, consider using --check-untyped-defs  [annotation-unchecked]
aesara/graph/destroyhandler.py:390: note: By default the bodies of untyped functions are not checked, consider using --check-untyped-defs  [annotation-unchecked]
aesara/graph/destroyhandler.py:392: note: By default the bodies of untyped functions are not checked, consider using --check-untyped-defs  [annotation-unchecked]
aesara/graph/destroyhandler.py:394: note: By default the bodies of untyped functions are not checked, consider using --check-untyped-defs  [annotation-unchecked]
aesara/graph/destroyhandler.py:396: note: By default the bodies of untyped functions are not checked, consider using --check-untyped-defs  [annotation-unchecked]
aesara/graph/destroyhandler.py:399: note: By default the bodies of untyped functions are not checked, consider using --check-untyped-defs  [annotation-unchecked]
aesara/graph/destroyhandler.py:401: note: By default the bodies of untyped functions are not checked, consider using --check-untyped-defs  [annotation-unchecked]
aesara/sparse/type.py:94: note: By default the bodies of untyped functions are not checked, consider using --check-untyped-defs  [annotation-unchecked]
aesara/graph/rewriting/basic.py:1166: note: By default the bodies of untyped functions are not checked, consider using --check-untyped-defs  [annotation-unchecked]
aesara/graph/rewriting/basic.py:1167: note: By default the bodies of untyped functions are not checked, consider using --check-untyped-defs  [annotation-unchecked]
aesara/graph/rewriting/basic.py:1168: note: By default the bodies of untyped functions are not checked, consider using --check-untyped-defs  [annotation-unchecked]
aesara/tensor/rewriting/elemwise.py:614: note: By default the bodies of untyped functions are not checked, consider using --check-untyped-defs  [annotation-unchecked]
aesara/tensor/rewriting/shape.py:521: note: By default the bodies of untyped functions are not checked, consider using --check-untyped-defs  [annotation-unchecked]
aesara/tensor/rewriting/shape.py:522: note: By default the bodies of untyped functions are not checked, consider using --check-untyped-defs  [annotation-unchecked]
aesara/tensor/rewriting/shape.py:523: note: By default the bodies of untyped functions are not checked, consider using --check-untyped-defs  [annotation-unchecked]
aesara/scalar/basic.py:917: note: By default the bodies of untyped functions are not checked, consider using --check-untyped-defs  [annotation-unchecked]
aesara/link/basic.py:334: note: By default the bodies of untyped functions are not checked, consider using --check-untyped-defs  [annotation-unchecked]
aesara/link/basic.py:359: note: By default the bodies of untyped functions are not checked, consider using --check-untyped-defs  [annotation-unchecked]
aesara/printing.py:1121: note: By default the bodies of untyped functions are not checked, consider using --check-untyped-defs  [annotation-unchecked]
aesara/printing.py:1122: note: By default the bodies of untyped functions are not checked, consider using --check-untyped-defs  [annotation-unchecked]
aesara/link/numba/dispatch/scan.py:79: note: By default the bodies of untyped functions are not checked, consider using --check-untyped-defs  [annotation-unchecked]
aesara/link/numba/dispatch/scan.py:102: note: By default the bodies of untyped functions are not checked, consider using --check-untyped-defs  [annotation-unchecked]
aesara/link/numba/dispatch/scan.py:120: note: By default the bodies of untyped functions are not checked, consider using --check-untyped-defs  [annotation-unchecked]
aesara/link/numba/dispatch/scan.py:128: note: By default the bodies of untyped functions are not checked, consider using --check-untyped-defs  [annotation-unchecked]
aesara/link/numba/dispatch/scan.py:141: note: By default the bodies of untyped functions are not checked, consider using --check-untyped-defs  [annotation-unchecked]
aesara/link/numba/dispatch/scan.py:145: note: By default the bodies of untyped functions are not checked, consider using --check-untyped-defs  [annotation-unchecked]
aesara/link/numba/dispatch/scan.py:192: note: By default the bodies of untyped functions are not checked, consider using --check-untyped-defs  [annotation-unchecked]
aesara/link/numba/dispatch/scan.py:202: note: By default the bodies of untyped functions are not checked, consider using --check-untyped-defs  [annotation-unchecked]
Found 1 error in 1 file (checked 2 source files)
```

**Please provide any additional information below.**


## Versions and main components

* Aesara version: 2.8.12 (latest)
* Python version: 3.10.5
* Operating system: Mac M1
* How did you install Aesara: (conda/pip) conda then pip via developer guide.

<details> <summary> Aesara config: </summary>

Place the results of `python -c ""import aesara; print(aesara.config)""` here.

```
WARNING (aesara.tensor.blas): Using NumPy C-API based implementation for BLAS functions.
floatX ({'float32', 'float64', 'float16'}) 
    Doc:  Default floating-point precision for python casts.

Note: float16 support is experimental, use at your own risk.
    Value:  float64

warn_float64 ({'raise', 'warn', 'pdb', 'ignore'}) 
    Doc:  Do an action when a tensor variable with float64 dtype is created.
    Value:  ignore

pickle_test_value (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x11cee3c10>>) 
    Doc:  Dump test values while pickling model. If True, test values will be dumped with model.
    Value:  True

cast_policy ({'numpy+floatX', 'custom'}) 
    Doc:  Rules for implicit type casting
    Value:  custom

deterministic ({'default', 'more'}) 
    Doc:  If `more`, sometimes we will select some implementation that are more deterministic, but slower.  Also see the dnn.conv.algo* flags to cover more cases.
    Value:  default

device (cpu)
    Doc:  Default device for computations. only cpu is supported for now
    Value:  cpu

force_device (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x11cf24970>>) 
    Doc:  Raise an error if we can't use the specified device
    Value:  False

conv__assert_shape (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x11cf243d0>>) 
    Doc:  If True, AbstractConv* ops will verify that user-provided shapes match the runtime shapes (debugging option, may slow down compilation)
    Value:  False

print_global_stats (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x11cf24f10>>) 
    Doc:  Print some global statistics (time spent) at the end
    Value:  False

assert_no_cpu_op ({'raise', 'warn', 'pdb', 'ignore'}) 
    Doc:  Raise an error/warning if there is a CPU op in the computational graph.
    Value:  ignore

unpickle_function (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x11cf25000>>) 
    Doc:  Replace unpickled Aesara functions with None. This is useful to unpickle old graphs that pickled them when it shouldn't
    Value:  True

<aesara.configparser.ConfigParam object at 0x11cf25030>
    Doc:  Default compilation mode
    Value:  Mode

cxx (<class 'str'>) 
    Doc:  The C++ compiler to use. Currently only g++ is supported, but supporting additional compilers should not be too difficult. If it is empty, no C++ code is compiled.
    Value:  /Users/pscemama/miniforge3/envs/aesara-dev/bin/clang++

linker ({'c|py_nogc', 'cvm', 'c|py', 'vm', 'vm_nogc', 'py', 'cvm_nogc', 'c'}) 
    Doc:  Default linker used if the aesara flags mode is Mode
    Value:  cvm

allow_gc (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x11cf25390>>) 
    Doc:  Do we default to delete intermediate results during Aesara function calls? Doing so lowers the memory requirement, but asks that we reallocate memory at the next function call. This is implemented for the default linker, but may not work for all linkers.
    Value:  True

optimizer ({'o1', 'None', 'o2', 'fast_compile', 'fast_run', 'o3', 'o4', 'unsafe', 'merge'}) 
    Doc:  Default optimizer. If not None, will use this optimizer with the Mode
    Value:  o4

optimizer_verbose (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x11cf25300>>) 
    Doc:  If True, we print all optimization being applied
    Value:  False

on_opt_error ({'raise', 'warn', 'pdb', 'ignore'}) 
    Doc:  What to do when an optimization crashes: warn and skip it, raise the exception, or fall into the pdb debugger.
    Value:  warn

nocleanup (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x11cf25960>>) 
    Doc:  Suppress the deletion of code files that did not compile cleanly
    Value:  False

on_unused_input ({'raise', 'warn', 'ignore'}) 
    Doc:  What to do if a variable in the 'inputs' list of  aesara.function() is not used in the graph.
    Value:  raise

gcc__cxxflags (<class 'str'>) 
    Doc:  Extra compiler flags for gcc
    Value:  

cmodule__warn_no_version (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x11cf25060>>) 
    Doc:  If True, will print a warning when compiling one or more Op with C code that can't be cached because there is no c_code_cache_version() function associated to at least one of those Ops.
    Value:  False

cmodule__remove_gxx_opt (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x11cf25180>>) 
    Doc:  If True, will remove the -O* parameter passed to g++.This is useful to debug in gdb modules compiled by Aesara.The parameter -g is passed by default to g++
    Value:  False

cmodule__compilation_warning (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x11cf25240>>) 
    Doc:  If True, will print compilation warnings.
    Value:  False

cmodule__preload_cache (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x11cf259f0>>) 
    Doc:  If set to True, will preload the C module cache at import time
    Value:  False

cmodule__age_thresh_use (<class 'int'>) 
    Doc:  In seconds. The time after which Aesara won't reuse a compile c module.
    Value:  2073600

cmodule__debug (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x11cf25990>>) 
    Doc:  If True, define a DEBUG macro (if not exists) for any compiled C code.
    Value:  False

compile__wait (<class 'int'>) 
    Doc:  Time to wait before retrying to acquire the compile lock.
    Value:  5

compile__timeout (<class 'int'>) 
    Doc:  In seconds, time that a process will wait before deciding to
    override an existing lock. An override only happens when the existing
    lock is held by the same owner *and* has not been 'refreshed' by this
    owner for more than this period. Refreshes are done every half timeout
    period for running processes.
    Value:  120

ctc__root (<class 'str'>) 
    Doc:  Directory which contains the root of Baidu CTC library. It is assumed         that the compiled library is either inside the build, lib or lib64         subdirectory, and the header inside the include directory.
    Value:  

tensor__cmp_sloppy (<class 'int'>) 
    Doc:  Relax aesara.tensor.math._allclose (0) not at all, (1) a bit, (2) more
    Value:  0

tensor__local_elemwise_fusion (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x11cf257b0>>) 
    Doc:  Enable or not in fast_run mode(fast_run optimization) the elemwise fusion optimization
    Value:  True

lib__amblibm (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x11cf25720>>) 
    Doc:  Use amd's amdlibm numerical library
    Value:  False

tensor__insert_inplace_optimizer_validate_nb (<class 'int'>) 
    Doc:  -1: auto, if graph have less then 500 nodes 1, else 10
    Value:  -1

traceback__limit (<class 'int'>) 
    Doc:  The number of stack to trace. -1 mean all.
    Value:  8

traceback__compile_limit (<class 'int'>) 
    Doc:  The number of stack to trace to keep during compilation. -1 mean all. If greater then 0, will also make us save Aesara internal stack trace.
    Value:  0

experimental__local_alloc_elemwise (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x11cf255a0>>) 
    Doc:  DEPRECATED: If True, enable the experimental optimization local_alloc_elemwise. Generates error if not True. Use optimizer_excluding=local_alloc_elemwise to disable.
    Value:  True

experimental__local_alloc_elemwise_assert (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x11cf25570>>) 
    Doc:  When the local_alloc_elemwise is applied, add an assert to highlight shape errors.
    Value:  True

warn__ignore_bug_before ({'1.0.1', '0.10', '1.0.5', '0.4', 'all', '0.6', '0.8.2', '0.4.1', '0.8', '1.0.4', '0.3', '0.7', '1.0.2', '0.8.1', '1.0', 'None', '0.5', '1.0.3', '0.9'}) 
    Doc:  If 'None', we warn about all Aesara bugs found by default. If 'all', we don't warn about Aesara bugs found by default. If a version, we print only the warnings relative to Aesara bugs found after that version. Warning for specific bugs can be configured with specific [warn] flags.
    Value:  0.9

exception_verbosity ({'low', 'high'}) 
    Doc:  If 'low', the text of exceptions will generally refer to apply nodes with short names such as Elemwise{add_no_inplace}. If 'high', some exceptions will also refer to apply nodes with long descriptions  like:
        A. Elemwise{add_no_inplace}
                B. log_likelihood_v_given_h
                C. log_likelihood_h
    Value:  low

print_test_value (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x11cf25420>>) 
    Doc:  If 'True', the __eval__ of an Aesara variable will return its test_value when this is available. This has the practical conseguence that, e.g., in debugging `my_var` will print the same as `my_var.tag.test_value` when a test value is defined.
    Value:  False

compute_test_value ({'warn', 'pdb', 'off', 'ignore', 'raise'}) 
    Doc:  If 'True', Aesara will run each op at graph build time, using Constants, SharedVariables and the tag 'test_value' as inputs to the function. This helps the user track down problems in the graph before it gets optimized.
    Value:  off

compute_test_value_opt ({'warn', 'pdb', 'off', 'ignore', 'raise'}) 
    Doc:  For debugging Aesara optimization only. Same as compute_test_value, but is used during Aesara optimization
    Value:  off

check_input (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x11cf25a20>>) 
    Doc:  Specify if types should check their input in their C code. It can be used to speed up compilation, reduce overhead (particularly for scalars) and reduce the number of generated C files.
    Value:  True

NanGuardMode__nan_is_error (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x11cf25a50>>) 
    Doc:  Default value for nan_is_error
    Value:  True

NanGuardMode__inf_is_error (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x11cf25a80>>) 
    Doc:  Default value for inf_is_error
    Value:  True

NanGuardMode__big_is_error (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x11cf25b10>>) 
    Doc:  Default value for big_is_error
    Value:  True

NanGuardMode__action ({'raise', 'warn', 'pdb'}) 
    Doc:  What NanGuardMode does when it finds a problem
    Value:  raise

DebugMode__patience (<class 'int'>) 
    Doc:  Optimize graph this many times to detect inconsistency
    Value:  10

DebugMode__check_c (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x11cf25ba0>>) 
    Doc:  Run C implementations where possible
    Value:  True

DebugMode__check_py (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x11cf25c30>>) 
    Doc:  Run Python implementations where possible
    Value:  True

DebugMode__check_finite (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x11cf25c60>>) 
    Doc:  True -> complain about NaN/Inf results
    Value:  True

DebugMode__check_strides (<class 'int'>) 
    Doc:  Check that Python- and C-produced ndarrays have same strides. On difference: (0) - ignore, (1) warn, or (2) raise error
    Value:  0

DebugMode__warn_input_not_reused (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x11cf25cc0>>) 
    Doc:  Generate a warning when destroy_map or view_map says that an op works inplace, but the op did not reuse the input for its output.
    Value:  True

DebugMode__check_preallocated_output (<class 'str'>) 
    Doc:  Test thunks with pre-allocated memory as output storage. This is a list of strings separated by "":"". Valid values are: ""initial"" (initial storage in storage map, happens with Scan),""previous"" (previously-returned memory), ""c_contiguous"", ""f_contiguous"", ""strided"" (positive and negative strides), ""wrong_size"" (larger and smaller dimensions), and ""ALL"" (all of the above).
    Value:  

DebugMode__check_preallocated_output_ndim (<class 'int'>) 
    Doc:  When testing with ""strided"" preallocated output memory, test all combinations of strides over that number of (inner-most) dimensions. You may want to reduce that number to reduce memory or time usage, but it is advised to keep a minimum of 2.
    Value:  4

profiling__time_thunks (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x11cf25d50>>) 
    Doc:  Time individual thunks when profiling
    Value:  True

profiling__n_apply (<class 'int'>) 
    Doc:  Number of Apply instances to print by default
    Value:  20

profiling__n_ops (<class 'int'>) 
    Doc:  Number of Ops to print by default
    Value:  20

profiling__output_line_width (<class 'int'>) 
    Doc:  Max line width for the profiling output
    Value:  512

profiling__min_memory_size (<class 'int'>) 
    Doc:  For the memory profile, do not print Apply nodes if the size
                 of their outputs (in bytes) is lower than this threshold
    Value:  1024

profiling__min_peak_memory (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x11cf25ea0>>) 
    Doc:  The min peak memory usage of the order
    Value:  False

profiling__destination (<class 'str'>) 
    Doc:  File destination of the profiling output
    Value:  stderr

profiling__debugprint (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x11cf25f00>>) 
    Doc:  Do a debugprint of the profiled functions
    Value:  False

profiling__ignore_first_call (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x11cf25f30>>) 
    Doc:  Do we ignore the first call of an Aesara function.
    Value:  False

on_shape_error ({'raise', 'warn'}) 
    Doc:  warn: print a warning and use the default value. raise: raise an error
    Value:  warn

openmp (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x11cf25f90>>) 
    Doc:  Allow (or not) parallel computation on the CPU with OpenMP. This is the default value used when creating an Op that supports OpenMP parallelization. It is preferable to define it via the Aesara configuration file ~/.aesararc or with the environment variable AESARA_FLAGS. Parallelization is only done for some operations that implement it, and even for operations that implement parallelism, each operation is free to respect this flag or not. You can control the number of threads used with the environment variable OMP_NUM_THREADS. If it is set to 1, we disable openmp in Aesara by default.
    Value:  False

openmp_elemwise_minsize (<class 'int'>) 
    Doc:  If OpenMP is enabled, this is the minimum size of vectors for which the openmp parallelization is enabled in element wise ops.
    Value:  200000

optimizer_excluding (<class 'str'>) 
    Doc:  When using the default mode, we will remove optimizer with these tags. Separate tags with ':'.
    Value:  

optimizer_including (<class 'str'>) 
    Doc:  When using the default mode, we will add optimizer with these tags. Separate tags with ':'.
    Value:  

optimizer_requiring (<class 'str'>) 
    Doc:  When using the default mode, we will require optimizer with these tags. Separate tags with ':'.
    Value:  

optdb__position_cutoff (<class 'float'>) 
    Doc:  Where to stop eariler during optimization. It represent the position of the optimizer where to stop.
    Value:  inf

optdb__max_use_ratio (<class 'float'>) 
    Doc:  A ratio that prevent infinite loop in EquilibriumGraphRewriter.
    Value:  8.0

cycle_detection ({'regular', 'fast'}) 
    Doc:  If cycle_detection is set to regular, most inplaces are allowed,but it is slower. If cycle_detection is set to faster, less inplacesare allowed, but it makes the compilation faster.The interaction of which one give the lower peak memory usage iscomplicated and not predictable, so if you are close to the peakmemory usage, triyng both could give you a small gain.
    Value:  regular

check_stack_trace ({'off', 'warn', 'log', 'raise'}) 
    Doc:  A flag for checking the stack trace during the optimization process. default (off): does not check the stack trace of any optimization log: inserts a dummy stack trace that identifies the optimizationthat inserted the variable that had an empty stack trace.warn: prints a warning if a stack trace is missing and also a dummystack trace is inserted that indicates which optimization insertedthe variable that had an empty stack trace.raise: raises an exception if a stack trace is missing
    Value:  off

metaopt__verbose (<class 'int'>) 
    Doc:  0 for silent, 1 for only warnings, 2 for full output withtimings and selected implementation
    Value:  0

metaopt__optimizer_excluding (<class 'str'>) 
    Doc:  exclude optimizers with these tags. Separate tags with ':'.
    Value:  

metaopt__optimizer_including (<class 'str'>) 
    Doc:  include optimizers with these tags. Separate tags with ':'.
    Value:  

profile (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x11cf26290>>) 
    Doc:  If VM should collect profile information
    Value:  False

profile_optimizer (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x11cf262c0>>) 
    Doc:  If VM should collect optimizer profile information
    Value:  False

profile_memory (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x11cf262f0>>) 
    Doc:  If VM should collect memory profile information and print it
    Value:  False

<aesara.configparser.ConfigParam object at 0x11cf26320>
    Doc:  Useful only for the VM Linkers. When lazy is None, auto detect if lazy evaluation is needed and use the appropriate version. If the C loop isn't being used and lazy is True, use the Stack VM; otherwise, use the Loop VM.
    Value:  None

unittests__rseed (<class 'str'>) 
    Doc:  Seed to use for randomized unit tests. Special value 'random' means using a seed of None.
    Value:  666

warn__round (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x11cf263e0>>) 
    Doc:  Warn when using `tensor.round` with the default mode. Round changed its default from `half_away_from_zero` to `half_to_even` to have the same default as NumPy.
    Value:  False

numba__vectorize_target ({'cuda', 'cpu', 'parallel'}) 
    Doc:  Default target for numba.vectorize.
    Value:  cpu

numba__fastmath (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x11cf264a0>>) 
    Doc:  If True, use Numba's fastmath mode.
    Value:  True

numba__cache (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x11cf26530>>) 
    Doc:  If True, use Numba's file based caching.
    Value:  True

compiledir_format (<class 'str'>) 
    Doc:  Format string for platform-dependent compiled module subdirectory
(relative to base_compiledir). Available keys: aesara_version, device,
gxx_version, hostname, numpy_version, platform, processor,
python_bitwidth, python_int_bitwidth, python_version, short_platform.
Defaults to compiledir_%(short_platform)s-%(processor)s-
%(python_version)s-%(python_bitwidth)s.
    Value:  compiledir_%(short_platform)s-%(processor)s-%(python_version)s-%(python_bitwidth)s

<aesara.configparser.ConfigParam object at 0x11cf265c0>
    Doc:  platform-independent root directory for compiled modules
    Value:  /Users/pscemama/.aesara

<aesara.configparser.ConfigParam object at 0x11cf26500>
    Doc:  platform-dependent cache directory for compiled modules
    Value:  /Users/pscemama/.aesara/compiledir_macOS-13.2.1-arm64-arm-64bit-arm-3.10.9-64

blas__ldflags (<class 'str'>) 
    Doc:  lib[s] to include for [Fortran] level-3 blas implementation
    Value:  

blas__check_openmp (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x11d177580>>) 
    Doc:  Check for openmp library conflict.
WARNING: Setting this to False leaves you open to wrong results in blas-related operations.
    Value:  True

scan__allow_gc (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x12feeaf20>>) 
    Doc:  Allow/disallow gc inside of Scan (default: False)
    Value:  False

scan__allow_output_prealloc (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x12feb06d0>>) 
    Doc:  Allow/disallow memory preallocation for outputs inside of scan (default: True)
    Value:  True
```
",,,2023-03-14 00:00:31,2023-03-14 14:42:54,"soraros unsubscribed 2023-03-14 01:06:27,aesara-devs locked 2023-03-14 14:42:53,brandonwillard converted_to_discussion 2023-03-14 14:42:54",aesara-devs PaulScemama soraros brandonwillard,0
1081,1478,Consider making `RandomStream`-generated `RandomVariable`s update RNGs in-place,brandonwillard,"@aesara-devs/core, should we make `RandomStream` return `RandomVariable`s with `RandomVariable.inplace == True`, instead of setting `SharedVariable.default_update`s on the generated `RandomTypeSharedVariable`s?  

The reason we set `SharedVariable.default_update` is so that the `aesara.function`-compiled results will generate different samples between calls, as one would expect in a normal NumPy scenario.  To do that, some form of global RNG state is required, and that's provided by the `RandomTypeSharedVariable` objects; however, those objects need to be updated in-place each time a sample is drawn, and it's the `aesara.function` updates mechanism that provides this in a very general way.  In the case of `RandomVariable` `Op`s, there's a `RandomVariable.inplace` attribute that also provides this and is considerably more efficient to use, because it removes the `copy` performed on the RNG before sampling.  Since samples drawn from `RandomStream` are always intended to be updated in-place&mdash;albeit using the updates mechanisms&mdash;it seems like we should just use the `Op`-level in-placing and avoid the additional overhead and fundamentally problematic `SharedVariable.default_update`s attribute altogether.

(N.B. We have an `aesara.tensor.random.rewriting.basic.random_make_inplace` rewrite that replaces `RandomVariable` `Op`s with in-placed versions under the `FAST_RUN` compilation mode.)

The underlying problem is that by [setting `SharedVariable.default_update` in `RandomStream.gen`](https://github.com/aesara-devs/aesara/blob/b75eed9392c68961c74e66c4c48be80dff6dceaa/aesara/tensor/random/utils.py#L282), we're adding ""state"" to the `RandomTypeSharedVariable`s we produce (i.e. state that unnecessarily associates `RandomTypeSharedVariable`s with specific sample graphs), and this state makes it difficult to reuse existing `RandomTypeSharedVariable`s in rewrites.  Plus, it can easily lead to the introduction of old, unwanted update graphs&mdash;the end result of which is that we end up compiling and sampling from a completely different graph (i.e. than the one we intended) just to update a shared RNG object.  A full illustration of the problem is provided [here](https://github.com/aesara-devs/aeppl/pull/239#issuecomment-1469106585).

In general, we should completely remove `SharedVariable.default_update`, because it severely complicates more than a couple things in Aesara.  The only reason we haven't removed it is due to its use in this one case.


N.B. This idea has come up once before, but I guess we didn't have an explicit issue for it.",question important refactor random variables request discussion,,2023-03-17 22:53:59,2023-03-18 23:55:34,"brandonwillard labeled 2023-03-17 22:53:59,brandonwillard labeled 2023-03-17 22:53:59,brandonwillard labeled 2023-03-17 22:54:00,brandonwillard labeled 2023-03-17 22:54:00,brandonwillard labeled 2023-03-17 22:54:00,dgerlanc subscribed 2023-03-17 22:54:00,brandonwillard subscribed 2023-03-17 22:54:00,rlouf subscribed 2023-03-17 22:54:01,kc611 subscribed 2023-03-17 22:54:01,brandonwillard renamed 2023-03-17 22:54:23,brandonwillard renamed 2023-03-17 23:21:13,dgerlanc subscribed 2023-03-18 20:01:28,brandonwillard subscribed 2023-03-18 20:01:28,rlouf subscribed 2023-03-18 20:01:28,kc611 subscribed 2023-03-18 20:01:28,aesara-devs locked 2023-03-18 23:55:34,brandonwillard converted_to_discussion 2023-03-18 23:55:34,soraros unsubscribed 2023-03-20 02:11:15",rlouf aesara-devs soraros kc611 brandonwillard dgerlanc,4
1090,1488,"""error in pydot2 setup command: use_2to3 is invalid"" while installing requirements-rtd.txt dependencies",Ankit-Dhankhar,"## Description of your problem or feature request

While setting up Aesaro from latest commit (`3eb7cdfaa7b97c9791d8303ae440a5f141c6d813`) by following this [guide] (https://aesara.readthedocs.io/en/latest/dev_start_guide.html). On running `pip install -r requirements-rtd.txt` following error is thrown:

```
Obtaining file:///home/ankit/Desktop/Github/Aesara (from -r requirements-rtd.txt (line 1))
  Installing build dependencies ... done
  Checking if build backend supports build_editable ... done
  Getting requirements to build editable ... done
  Preparing editable metadata (pyproject.toml) ... done
Requirement already satisfied: sphinx>=1.3.0 in /home/ankit/miniconda3/envs/aesara-dev/lib/python3.10/site-packages (from -r requirements-rtd.txt (line 2)) (6.1.3)
Collecting sphinx-book-theme
  Using cached sphinx_book_theme-1.0.1-py3-none-any.whl (396 kB)
Collecting sphinx-design
  Using cached sphinx_design-0.3.0-py3-none-any.whl (2.2 MB)
Collecting jinja2<3.1.0
  Using cached Jinja2-3.0.3-py3-none-any.whl (133 kB)
Requirement already satisfied: pygments in /home/ankit/miniconda3/envs/aesara-dev/lib/python3.10/site-packages (from -r requirements-rtd.txt (line 6)) (2.14.0)
Requirement already satisfied: pytest in /home/ankit/miniconda3/envs/aesara-dev/lib/python3.10/site-packages (from -r requirements-rtd.txt (line 7)) (7.2.2)
Requirement already satisfied: numpy in /home/ankit/miniconda3/envs/aesara-dev/lib/python3.10/site-packages (from -r requirements-rtd.txt (line 8)) (1.22.4)
Collecting gnumpy
  Using cached gnumpy-0.2.tar.gz (28 kB)
  Preparing metadata (setup.py) ... done
Requirement already satisfied: pydot in /home/ankit/miniconda3/envs/aesara-dev/lib/python3.10/site-packages (from -r requirements-rtd.txt (line 10)) (1.4.2)
Collecting pydot2
  Using cached pydot2-1.0.33.tar.gz (19 kB)
  Preparing metadata (setup.py) ... error
  error: subprocess-exited-with-error
  
  × python setup.py egg_info did not run successfully.
  │ exit code: 1
  ╰─> [1 lines of output]
      error in pydot2 setup command: use_2to3 is invalid.
      [end of output]
  
  note: This error originates from a subprocess, and is likely not a problem with pip.
error: metadata-generation-failed

× Encountered error while generating package metadata.
╰─> See above for output.

note: This is an issue with the package mentioned above, not pip.
hint: See above for details.
```

Same issue can be reproduced by running `pip install pydot2`

This issue is previously reported in [this](https://github.com/aesara-devs/aesara/issues/1263#issuecomment-12809888260) and [this](https://github.com/aesara-devs/aesara/discussions/1459#discussion-4906904) discussion thread which was fixed by removing pydot2 from `requirement-rtd.txt` and pinning the version of sepuptools to older version (latest in this case v67.6.1) respectively.

## Versions and main components

* Aesara version: git commit - 3eb7cdfaa7b97c9791d8303ae440a5f141c6d813
* Python version: Python 3.10.10
* Operating system: GNU/Linux
* How did you install Aesara: (git + conda) by following this [guide] (https://aesara.readthedocs.io/en/latest/dev_start_guide.html)

Opening this issue to start discussion for the holistic solution of above issue.
- solution 1 (preferred) : Removed dependency from `pydot2` in `requirement-rtd.txt` 
- solution 2 : Add a message in [guide] (https://aesara.readthedocs.io/en/latest/dev_start_guide.html) to pin the version of setuptool to older version.

I would be glad to submit the PR for fixing this issue.",bug documentation setup and installation,,2023-04-03 19:38:01,2023-04-17 19:24:16,"Ankit-Dhankhar renamed 2023-04-04 03:05:28,brandonwillard labeled 2023-04-06 19:49:40,brandonwillard labeled 2023-04-06 19:49:40,brandonwillard labeled 2023-04-06 19:56:44,brandonwillard closed 2023-04-17 19:24:17",Ankit-Dhankhar brandonwillard,1
1097,1499,aesara.scan IndexError,davipatti,"Using `aesara.scan` to make values 0 if any three preceding values in a column are non-zero.

Output is as expected, but errors are logged, and `IndexError` gets raised.

```python
import aesara as ae
import numpy as np

np.random.seed(42)
m, n = 10, 12
arr = np.random.choice([0, 1], p=[0.75, 0.25], size=m * n).reshape(m, n)

print(""\\ninput:"")
print(arr)


def mask_prev_three(arr: ae.tensor.TensorLike, n: int) -> ae.tensor.TensorVariable:
    taps = -3, -2, -1
    initial = ae.tensor.zeros((3, n), ""int8"")
    masked, _ = ae.scan(
        lambda i0, im3, im2, im1: ae.tensor.switch(im3 | im2 | im1, 0, i0),
        sequences=arr,
        outputs_info=dict(taps=taps, initial=initial),
    )
    return masked


masked = mask_prev_three(ae.tensor.as_tensor(arr, dtype=""int8""), n).eval()

print(""\\noutput:"")
print(masked)
```

```
input:
[[0 1 0 0 0 0 0 1 0 0 0 1]
 [1 0 0 0 0 0 0 0 0 0 0 0]
 [0 1 0 0 0 0 0 0 0 1 1 1]
 [0 0 0 0 0 0 0 1 0 0 0 0]
 [0 0 1 1 1 1 0 1 0 0 0 0]
 [0 0 1 0 0 0 0 1 0 1 1 0]
 [0 1 0 0 1 0 0 0 1 0 0 0]
 [0 0 0 0 1 0 0 0 1 0 1 0]
 [0 0 0 0 0 0 0 0 1 0 0 1]
 [0 0 0 0 1 1 0 1 1 0 1 0]]
ERROR (aesara.graph.rewriting.basic): Rewrite failure due to: save_mem_new_scan
ERROR (aesara.graph.rewriting.basic): node: for{cpu,scan_fn}(TensorConstant{10}, TensorConstant{[[0 1 0 0 .. 1 0 1 0]]}, IncSubtensor{Set;:int64:}.0)
ERROR (aesara.graph.rewriting.basic): TRACEBACK:
ERROR (aesara.graph.rewriting.basic): Traceback (most recent call last):
  File ""/Users/pattinson/.virtualenvs/aesara-env/lib/python3.10/site-packages/aesara/graph/rewriting/basic.py"", line 1926, in process_node
    replacements = node_rewriter.transform(fgraph, node)
  File ""/Users/pattinson/.virtualenvs/aesara-env/lib/python3.10/site-packages/aesara/graph/rewriting/basic.py"", line 1086, in transform
    return self.fn(fgraph, node)
  File ""/Users/pattinson/.virtualenvs/aesara-env/lib/python3.10/site-packages/aesara/scan/rewriting.py"", line 1431, in save_mem_new_scan
    nw_input = expand_empty(_nw_input, tmp_idx)
  File ""/Users/pattinson/.virtualenvs/aesara-env/lib/python3.10/site-packages/aesara/scan/utils.py"", line 239, in expand_empty
    new_shape = [size + shapes[0]] + shapes[1:]
IndexError: list index out of range


output:
[[0 1 0 0 0 0 0 1 0 0 0 1]
 [1 0 0 0 0 0 0 0 0 0 0 0]
 [0 0 0 0 0 0 0 0 0 1 1 0]
 [0 0 0 0 0 0 0 0 0 0 0 0]
 [0 0 1 1 1 1 0 1 0 0 0 0]
 [0 0 0 0 0 0 0 0 0 0 0 0]
 [0 1 0 0 0 0 0 0 1 0 0 0]
 [0 0 0 0 0 0 0 0 0 0 1 0]
 [0 0 0 0 0 0 0 0 0 0 0 1]
 [0 0 0 0 1 1 0 1 0 0 0 0]]
```

## Versions and main components

* Aesara version: 2.9.0
* Python version: 3.10.4
* Operating system: macos ventura 13.1
* How did you install Aesara: pip

<details> <summary> Aesara config: </summary>

floatX ({'float16', 'float64', 'float32'}) 
    Doc:  Default floating-point precision for python casts.

Note: float16 support is experimental, use at your own risk.
    Value:  float64

warn_float64 ({'ignore', 'pdb', 'warn', 'raise'}) 
    Doc:  Do an action when a tensor variable with float64 dtype is created.
    Value:  ignore

pickle_test_value (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x106663c10>>) 
    Doc:  Dump test values while pickling model. If True, test values will be dumped with model.
    Value:  True

cast_policy ({'custom', 'numpy+floatX'}) 
    Doc:  Rules for implicit type casting
    Value:  custom

deterministic ({'default', 'more'}) 
    Doc:  If `more`, sometimes we will select some implementation that are more deterministic, but slower.  Also see the dnn.conv.algo* flags to cover more cases.
    Value:  default

device (cpu)
    Doc:  Default device for computations. only cpu is supported for now
    Value:  cpu

force_device (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x106662710>>) 
    Doc:  Raise an error if we can't use the specified device
    Value:  False

conv__assert_shape (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x106663c40>>) 
    Doc:  If True, AbstractConv* ops will verify that user-provided shapes match the runtime shapes (debugging option, may slow down compilation)
    Value:  False

print_global_stats (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x106663160>>) 
    Doc:  Print some global statistics (time spent) at the end
    Value:  False

assert_no_cpu_op ({'ignore', 'pdb', 'warn', 'raise'}) 
    Doc:  Raise an error/warning if there is a CPU op in the computational graph.
    Value:  ignore

unpickle_function (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x106663880>>) 
    Doc:  Replace unpickled Aesara functions with None. This is useful to unpickle old graphs that pickled them when it shouldn't
    Value:  True

<aesara.configparser.ConfigParam object at 0x1066633d0>
    Doc:  Default compilation mode
    Value:  Mode

cxx (<class 'str'>) 
    Doc:  The C++ compiler to use. Currently only g++ is supported, but supporting additional compilers should not be too difficult. If it is empty, no C++ code is compiled.
    Value:  /usr/bin/clang++

linker ({'c', 'cvm', 'py', 'cvm_nogc', 'vm', 'c|py_nogc', 'c|py', 'vm_nogc'}) 
    Doc:  Default linker used if the aesara flags mode is Mode
    Value:  cvm

allow_gc (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x106663cd0>>) 
    Doc:  Do we default to delete intermediate results during Aesara function calls? Doing so lowers the memory requirement, but asks that we reallocate memory at the next function call. This is implemented for the default linker, but may not work for all linkers.
    Value:  True

optimizer ({'merge', 'unsafe', 'None', 'o2', 'o1', 'fast_run', 'fast_compile', 'o3', 'o4'}) 
    Doc:  Default optimizer. If not None, will use this optimizer with the Mode
    Value:  o4

optimizer_verbose (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x106663a60>>) 
    Doc:  If True, we print all optimization being applied
    Value:  False

on_opt_error ({'ignore', 'pdb', 'warn', 'raise'}) 
    Doc:  What to do when an optimization crashes: warn and skip it, raise the exception, or fall into the pdb debugger.
    Value:  warn

nocleanup (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x106663b80>>) 
    Doc:  Suppress the deletion of code files that did not compile cleanly
    Value:  False

on_unused_input ({'ignore', 'warn', 'raise'}) 
    Doc:  What to do if a variable in the 'inputs' list of  aesara.function() is not used in the graph.
    Value:  raise

gcc__cxxflags (<class 'str'>) 
    Doc:  Extra compiler flags for gcc
    Value:  

cmodule__warn_no_version (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x106663c70>>) 
    Doc:  If True, will print a warning when compiling one or more Op with C code that can't be cached because there is no c_code_cache_version() function associated to at least one of those Ops.
    Value:  False

cmodule__remove_gxx_opt (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x106663a00>>) 
    Doc:  If True, will remove the -O* parameter passed to g++.This is useful to debug in gdb modules compiled by Aesara.The parameter -g is passed by default to g++
    Value:  False

cmodule__compilation_warning (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x106663e80>>) 
    Doc:  If True, will print compilation warnings.
    Value:  False

cmodule__preload_cache (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x106663e50>>) 
    Doc:  If set to True, will preload the C module cache at import time
    Value:  False

cmodule__age_thresh_use (<class 'int'>) 
    Doc:  In seconds. The time after which Aesara won't reuse a compile c module.
    Value:  2073600

cmodule__debug (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x106663dc0>>) 
    Doc:  If True, define a DEBUG macro (if not exists) for any compiled C code.
    Value:  False

compile__wait (<class 'int'>) 
    Doc:  Time to wait before retrying to acquire the compile lock.
    Value:  5

compile__timeout (<class 'int'>) 
    Doc:  In seconds, time that a process will wait before deciding to
    override an existing lock. An override only happens when the existing
    lock is held by the same owner *and* has not been 'refreshed' by this
    owner for more than this period. Refreshes are done every half timeout
    period for running processes.
    Value:  120

ctc__root (<class 'str'>) 
    Doc:  Directory which contains the root of Baidu CTC library. It is assumed         that the compiled library is either inside the build, lib or lib64         subdirectory, and the header inside the include directory.
    Value:  

tensor__cmp_sloppy (<class 'int'>) 
    Doc:  Relax aesara.tensor.math._allclose (0) not at all, (1) a bit, (2) more
    Value:  0

tensor__local_elemwise_fusion (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x106663fa0>>) 
    Doc:  Enable or not in fast_run mode(fast_run optimization) the elemwise fusion optimization
    Value:  True

lib__amblibm (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x106663fd0>>) 
    Doc:  Use amd's amdlibm numerical library
    Value:  False

tensor__insert_inplace_optimizer_validate_nb (<class 'int'>) 
    Doc:  -1: auto, if graph have less then 500 nodes 1, else 10
    Value:  -1

traceback__limit (<class 'int'>) 
    Doc:  The number of stack to trace. -1 mean all.
    Value:  8

traceback__compile_limit (<class 'int'>) 
    Doc:  The number of stack to trace to keep during compilation. -1 mean all. If greater then 0, will also make us save Aesara internal stack trace.
    Value:  0

experimental__local_alloc_elemwise (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x1066ac1f0>>) 
    Doc:  DEPRECATED: If True, enable the experimental optimization local_alloc_elemwise. Generates error if not True. Use optimizer_excluding=local_alloc_elemwise to disable.
    Value:  True

experimental__local_alloc_elemwise_assert (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x1066ac220>>) 
    Doc:  When the local_alloc_elemwise is applied, add an assert to highlight shape errors.
    Value:  True

warn__ignore_bug_before ({'0.9', '0.4.1', '1.0.4', '1.0.3', '1.0', '1.0.2', 'all', '0.8.1', '0.8', '0.5', '0.7', '0.8.2', 'None', '0.4', '0.3', '0.6', '0.10', '1.0.5', '1.0.1'}) 
    Doc:  If 'None', we warn about all Aesara bugs found by default. If 'all', we don't warn about Aesara bugs found by default. If a version, we print only the warnings relative to Aesara bugs found after that version. Warning for specific bugs can be configured with specific [warn] flags.
    Value:  0.9

exception_verbosity ({'low', 'high'}) 
    Doc:  If 'low', the text of exceptions will generally refer to apply nodes with short names such as Elemwise{add_no_inplace}. If 'high', some exceptions will also refer to apply nodes with long descriptions  like:
        A. Elemwise{add_no_inplace}
                B. log_likelihood_v_given_h
                C. log_likelihood_h
    Value:  low

print_test_value (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x1066ac370>>) 
    Doc:  If 'True', the __eval__ of an Aesara variable will return its test_value when this is available. This has the practical conseguence that, e.g., in debugging `my_var` will print the same as `my_var.tag.test_value` when a test value is defined.
    Value:  False

compute_test_value ({'warn', 'off', 'pdb', 'raise', 'ignore'}) 
    Doc:  If 'True', Aesara will run each op at graph build time, using Constants, SharedVariables and the tag 'test_value' as inputs to the function. This helps the user track down problems in the graph before it gets optimized.
    Value:  off

compute_test_value_opt ({'warn', 'off', 'pdb', 'raise', 'ignore'}) 
    Doc:  For debugging Aesara optimization only. Same as compute_test_value, but is used during Aesara optimization
    Value:  off

check_input (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x1066ac400>>) 
    Doc:  Specify if types should check their input in their C code. It can be used to speed up compilation, reduce overhead (particularly for scalars) and reduce the number of generated C files.
    Value:  True

NanGuardMode__nan_is_error (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x1066ac430>>) 
    Doc:  Default value for nan_is_error
    Value:  True

NanGuardMode__inf_is_error (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x1066ac460>>) 
    Doc:  Default value for inf_is_error
    Value:  True

NanGuardMode__big_is_error (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x1066ac4f0>>) 
    Doc:  Default value for big_is_error
    Value:  True

NanGuardMode__action ({'pdb', 'warn', 'raise'}) 
    Doc:  What NanGuardMode does when it finds a problem
    Value:  raise

DebugMode__patience (<class 'int'>) 
    Doc:  Optimize graph this many times to detect inconsistency
    Value:  10

DebugMode__check_c (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x1066ac580>>) 
    Doc:  Run C implementations where possible
    Value:  True

DebugMode__check_py (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x1066ac610>>) 
    Doc:  Run Python implementations where possible
    Value:  True

DebugMode__check_finite (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x1066ac640>>) 
    Doc:  True -> complain about NaN/Inf results
    Value:  True

DebugMode__check_strides (<class 'int'>) 
    Doc:  Check that Python- and C-produced ndarrays have same strides. On difference: (0) - ignore, (1) warn, or (2) raise error
    Value:  0

DebugMode__warn_input_not_reused (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x1066ac6a0>>) 
    Doc:  Generate a warning when destroy_map or view_map says that an op works inplace, but the op did not reuse the input for its output.
    Value:  True

DebugMode__check_preallocated_output (<class 'str'>) 
    Doc:  Test thunks with pre-allocated memory as output storage. This is a list of strings separated by "":"". Valid values are: ""initial"" (initial storage in storage map, happens with Scan),""previous"" (previously-returned memory), ""c_contiguous"", ""f_contiguous"", ""strided"" (positive and negative strides), ""wrong_size"" (larger and smaller dimensions), and ""ALL"" (all of the above).
    Value:  

DebugMode__check_preallocated_output_ndim (<class 'int'>) 
    Doc:  When testing with ""strided"" preallocated output memory, test all combinations of strides over that number of (inner-most) dimensions. You may want to reduce that number to reduce memory or time usage, but it is advised to keep a minimum of 2.
    Value:  4

profiling__time_thunks (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x1066ac730>>) 
    Doc:  Time individual thunks when profiling
    Value:  True

profiling__n_apply (<class 'int'>) 
    Doc:  Number of Apply instances to print by default
    Value:  20

profiling__n_ops (<class 'int'>) 
    Doc:  Number of Ops to print by default
    Value:  20

profiling__output_line_width (<class 'int'>) 
    Doc:  Max line width for the profiling output
    Value:  512

profiling__min_memory_size (<class 'int'>) 
    Doc:  For the memory profile, do not print Apply nodes if the size
                 of their outputs (in bytes) is lower than this threshold
    Value:  1024

profiling__min_peak_memory (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x1066ac880>>) 
    Doc:  The min peak memory usage of the order
    Value:  False

profiling__destination (<class 'str'>) 
    Doc:  File destination of the profiling output
    Value:  stderr

profiling__debugprint (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x1066ac8e0>>) 
    Doc:  Do a debugprint of the profiled functions
    Value:  False

profiling__ignore_first_call (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x1066ac910>>) 
    Doc:  Do we ignore the first call of an Aesara function.
    Value:  False

on_shape_error ({'warn', 'raise'}) 
    Doc:  warn: print a warning and use the default value. raise: raise an error
    Value:  warn

openmp (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x1066ac970>>) 
    Doc:  Allow (or not) parallel computation on the CPU with OpenMP. This is the default value used when creating an Op that supports OpenMP parallelization. It is preferable to define it via the Aesara configuration file ~/.aesararc or with the environment variable AESARA_FLAGS. Parallelization is only done for some operations that implement it, and even for operations that implement parallelism, each operation is free to respect this flag or not. You can control the number of threads used with the environment variable OMP_NUM_THREADS. If it is set to 1, we disable openmp in Aesara by default.
    Value:  False

openmp_elemwise_minsize (<class 'int'>) 
    Doc:  If OpenMP is enabled, this is the minimum size of vectors for which the openmp parallelization is enabled in element wise ops.
    Value:  200000

optimizer_excluding (<class 'str'>) 
    Doc:  When using the default mode, we will remove optimizer with these tags. Separate tags with ':'.
    Value:  

optimizer_including (<class 'str'>) 
    Doc:  When using the default mode, we will add optimizer with these tags. Separate tags with ':'.
    Value:  

optimizer_requiring (<class 'str'>) 
    Doc:  When using the default mode, we will require optimizer with these tags. Separate tags with ':'.
    Value:  

optdb__position_cutoff (<class 'float'>) 
    Doc:  Where to stop eariler during optimization. It represent the position of the optimizer where to stop.
    Value:  inf

optdb__max_use_ratio (<class 'float'>) 
    Doc:  A ratio that prevent infinite loop in EquilibriumGraphRewriter.
    Value:  8.0

cycle_detection ({'fast', 'regular'}) 
    Doc:  If cycle_detection is set to regular, most inplaces are allowed,but it is slower. If cycle_detection is set to faster, less inplacesare allowed, but it makes the compilation faster.The interaction of which one give the lower peak memory usage iscomplicated and not predictable, so if you are close to the peakmemory usage, triyng both could give you a small gain.
    Value:  regular

check_stack_trace ({'off', 'log', 'warn', 'raise'}) 
    Doc:  A flag for checking the stack trace during the optimization process. default (off): does not check the stack trace of any optimization log: inserts a dummy stack trace that identifies the optimizationthat inserted the variable that had an empty stack trace.warn: prints a warning if a stack trace is missing and also a dummystack trace is inserted that indicates which optimization insertedthe variable that had an empty stack trace.raise: raises an exception if a stack trace is missing
    Value:  off

metaopt__verbose (<class 'int'>) 
    Doc:  0 for silent, 1 for only warnings, 2 for full output withtimings and selected implementation
    Value:  0

metaopt__optimizer_excluding (<class 'str'>) 
    Doc:  exclude optimizers with these tags. Separate tags with ':'.
    Value:  

metaopt__optimizer_including (<class 'str'>) 
    Doc:  include optimizers with these tags. Separate tags with ':'.
    Value:  

profile (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x1066acc70>>) 
    Doc:  If VM should collect profile information
    Value:  False

profile_optimizer (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x1066acca0>>) 
    Doc:  If VM should collect optimizer profile information
    Value:  False

profile_memory (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x1066accd0>>) 
    Doc:  If VM should collect memory profile information and print it
    Value:  False

<aesara.configparser.ConfigParam object at 0x1066acd00>
    Doc:  Useful only for the VM Linkers. When lazy is None, auto detect if lazy evaluation is needed and use the appropriate version. If the C loop isn't being used and lazy is True, use the Stack VM; otherwise, use the Loop VM.
    Value:  None

unittests__rseed (<class 'str'>) 
    Doc:  Seed to use for randomized unit tests. Special value 'random' means using a seed of None.
    Value:  666

warn__round (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x1066acdc0>>) 
    Doc:  Warn when using `tensor.round` with the default mode. Round changed its default from `half_away_from_zero` to `half_to_even` to have the same default as NumPy.
    Value:  False

numba__vectorize_target ({'parallel', 'cuda', 'cpu'}) 
    Doc:  Default target for numba.vectorize.
    Value:  cpu

numba__fastmath (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x1066ace80>>) 
    Doc:  If True, use Numba's fastmath mode.
    Value:  True

numba__cache (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x1066acf10>>) 
    Doc:  If True, use Numba's file based caching.
    Value:  True

compiledir_format (<class 'str'>) 
    Doc:  Format string for platform-dependent compiled module subdirectory
(relative to base_compiledir). Available keys: aesara_version, device,
gxx_version, hostname, numpy_version, platform, processor,
python_bitwidth, python_int_bitwidth, python_version, short_platform.
Defaults to compiledir_%(short_platform)s-%(processor)s-
%(python_version)s-%(python_bitwidth)s.
    Value:  compiledir_%(short_platform)s-%(processor)s-%(python_version)s-%(python_bitwidth)s

<aesara.configparser.ConfigParam object at 0x1066acfd0>
    Doc:  platform-independent root directory for compiled modules
    Value:  /Users/pattinson/.aesara

<aesara.configparser.ConfigParam object at 0x1066acf70>
    Doc:  platform-dependent cache directory for compiled modules
    Value:  /Users/pattinson/.aesara/compiledir_macOS-13.1-arm64-arm-64bit-arm-3.10.4-64

blas__ldflags (<class 'str'>) 
    Doc:  lib[s] to include for [Fortran] level-3 blas implementation
    Value:  -lblas

blas__check_openmp (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x106973040>>) 
    Doc:  Check for openmp library conflict.
WARNING: Setting this to False leaves you open to wrong results in blas-related operations.
    Value:  True

scan__allow_gc (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x139c466b0>>) 
    Doc:  Allow/disallow gc inside of Scan (default: False)
    Value:  False

scan__allow_output_prealloc (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x139a07ac0>>) 
    Doc:  Allow/disallow memory preallocation for outputs inside of scan (default: True)
    Value:  True




</details>
",bug graph rewriting Scan,,2023-06-02 20:43:42,2023-06-03 23:48:31,"brandonwillard labeled 2023-06-02 20:46:55,brandonwillard labeled 2023-06-02 21:28:57,brandonwillard labeled 2023-06-02 21:28:57,brandonwillard closed 2023-06-03 23:48:31",davipatti brandonwillard,0
1103,1511,Import aesara fails with numpy 1.26,oscarbenjamin,"This is showing up in SymPy's CI but only for Python 3.11.
```console
$ python3.11 -m venv venv
$ source venv/bin/activate
$ pip install numpy aesara
...
Successfully installed aesara-2.9.1 cons-0.4.6 etuples-0.3.9 filelock-3.12.4 logical-unification-0.4.6 minikanren-1.0.3 multipledispatch-1.0.0 numpy-1.26.0 scipy-1.11.2 toolz-0.12.0 typing-extensions-4.8.0
$ python -c 'import aesara'
Traceback (most recent call last):
  File ""/home/oscar/current/active/sympy/tmp/venv/lib/python3.11/site-packages/aesara/configparser.py"", line 234, in fetch_val_for_key
    return self._aesara_cfg.get(section, option)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/oscar/.pyenv/versions/3.11.3/lib/python3.11/configparser.py"", line 797, in get
    d = self._unify_values(section, vars)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/oscar/.pyenv/versions/3.11.3/lib/python3.11/configparser.py"", line 1168, in _unify_values
    raise NoSectionError(section) from None
configparser.NoSectionError: No section: 'blas'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/home/oscar/current/active/sympy/tmp/venv/lib/python3.11/site-packages/aesara/configparser.py"", line 350, in __get__
    val_str = cls.fetch_val_for_key(self.name, delete_key=delete_key)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/oscar/current/active/sympy/tmp/venv/lib/python3.11/site-packages/aesara/configparser.py"", line 238, in fetch_val_for_key
    raise KeyError(key)
KeyError: 'blas__ldflags'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
  File ""/home/oscar/current/active/sympy/tmp/venv/lib/python3.11/site-packages/aesara/__init__.py"", line 120, in <module>
    from aesara import scalar, tensor
  File ""/home/oscar/current/active/sympy/tmp/venv/lib/python3.11/site-packages/aesara/tensor/__init__.py"", line 106, in <module>
    from aesara.tensor import (  # noqa
  File ""/home/oscar/current/active/sympy/tmp/venv/lib/python3.11/site-packages/aesara/tensor/blas.py"", line 162, in <module>
    from aesara.tensor.blas_headers import blas_header_text, blas_header_version
  File ""/home/oscar/current/active/sympy/tmp/venv/lib/python3.11/site-packages/aesara/tensor/blas_headers.py"", line 1015, in <module>
    if not config.blas__ldflags:
           ^^^^^^^^^^^^^^^^^^^^
  File ""/home/oscar/current/active/sympy/tmp/venv/lib/python3.11/site-packages/aesara/configparser.py"", line 354, in __get__
    val_str = self.default()
              ^^^^^^^^^^^^^^
  File ""/home/oscar/current/active/sympy/tmp/venv/lib/python3.11/site-packages/aesara/link/c/cmodule.py"", line 2725, in default_blas_ldflags
    blas_info = np.__config__.get_info(""blas_opt"")
                ^^^^^^^^^^^^^^^^^^^^^^
AttributeError: module 'numpy.__config__' has no attribute 'get_info'
```
The problem is caused by the release of numpy 1.26.0 two days ago.

Downgrading numpy fixes the problem:
```console
$ pip install 'numpy<1.26'
...
Successfully installed numpy-1.25.2
$ python -c 'import aesara'
WARNING (aesara.tensor.blas): Using NumPy C-API based implementation for BLAS functions.
```",,,2023-09-18 10:52:27,2023-09-20 22:20:46,brandonwillard closed 2023-09-20 22:20:46,oscarbenjamin brandonwillard,2
1105,1517,Memory leak with a particular combination of operations,mattearllongshot,"## Description of your problem or feature request

With a particular combination of subtensor / matrix multiplication / scalar multiplication / gradient operations (see below for details) approximately 128 bytes of memory are leaked per function evaluation.

Setup a fresh conda environment with these steps:

```bash
conda create -n leak
conda activate leak
conda install -c conda-forge aesara
pip install psutil
```

Then run the following script:

```python
import aesara
import aesara.tensor as at
import numpy as np
import psutil


mask_shape = (3,)
mulvec_shape = (2,)
matrix_shape = (3, 2)


def mk_fun():
    mask = aesara.shared(np.zeros(mask_shape, dtype=bool))
    mulvec = aesara.shared(np.zeros(mulvec_shape))
    matrix = aesara.shared(np.zeros(matrix_shape))

    scale = at.as_tensor(np.ones(mask_shape))

    invec = at.vector()
    invec_masked = invec[mask]
    masked_output = (
        (invec_masked[:, None] * matrix) @ mulvec
        * scale[mask]
    )
    output = at.zeros_like(invec)
    output = at.set_subtensor(output[mask], masked_output)

    fun_and_grad = aesara.function([invec], at.grad(at.sum(output), invec))

    mask.set_value(np.ones(mask_shape, dtype=bool))
    mulvec.set_value(np.ones(mulvec_shape))
    matrix.set_value(np.ones(matrix_shape))

    return fun_and_grad


fun = mk_fun()


proc = psutil.Process()

print('memory usage before', proc.memory_info().rss / (1 << 20), 'MB')
for i in range(1_000_000):
    fun(np.zeros(mask_shape))
print('memory usage after', proc.memory_info().rss / (1 << 20), 'MB')
```

This produces the following output:

```python
memory usage before 119.4296875 MB
memory usage after 241.703125 MB
```

I've tried to reduce the example even further but this is as small as I could make it.  Hopefully you can make some sense of it!


## Versions and main components

* Aesara version: 2.9.2
* Python version: Python 3.11.6
* Operating system: Ubuntu 18.04.6 LTS
* How did you install Aesara: conda

<details> <summary> Aesara config: </summary>

Place the results of `python -c ""import aesara; print(aesara.config)""` here.
```
floatX ({'float32', 'float16', 'float64'})
    Doc:  Default floating-point precision for python casts.

Note: float16 support is experimental, use at your own risk.
    Value:  float64

warn_float64 ({'warn', 'raise', 'ignore', 'pdb'})
    Doc:  Do an action when a tensor variable with float64 dtype is created.
    Value:  ignore

pickle_test_value (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7fb9fcc511d0>>)
    Doc:  Dump test values while pickling model. If True, test values will be dumped with model.
    Value:  True

cast_policy ({'custom', 'numpy+floatX'})
    Doc:  Rules for implicit type casting
    Value:  custom

deterministic ({'default', 'more'})
    Doc:  If `more`, sometimes we will select some implementation that are more deterministic, but slower.  Also see the dnn.conv.algo* flags to cover more cases.
    Value:  default

device (cpu)
    Doc:  Default device for computations. only cpu is supported for now
    Value:  cpu

force_device (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7fba5d428850>>)
    Doc:  Raise an error if we can't use the specified device
    Value:  False

conv__assert_shape (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7fb9fbee2750>>)
    Doc:  If True, AbstractConv* ops will verify that user-provided shapes match the runtime shapes (debugging option, may slow down compilation)
    Value:  False

print_global_stats (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7fb9fbee2850>>)
    Doc:  Print some global statistics (time spent) at the end
    Value:  False

assert_no_cpu_op ({'warn', 'raise', 'ignore', 'pdb'})
    Doc:  Raise an error/warning if there is a CPU op in the computational graph.
    Value:  ignore

unpickle_function (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7fb9fbee2810>>)
    Doc:  Replace unpickled Aesara functions with None. This is useful to unpickle old graphs that pickled them when it shouldn't
    Value:  True

<aesara.configparser.ConfigParam object at 0x7fb9fbee29d0>
    Doc:  Default compilation mode
    Value:  Mode

cxx (<class 'str'>)
    Doc:  The C++ compiler to use. Currently only g++ is supported, but supporting additional compilers should not be too difficult. If it is empty, no C++ code is compiled.
    Value:  /mnt/data/conda/envs/aesarabug/bin/g++

linker ({'vm', 'py', 'c|py_nogc', 'c', 'c|py', 'cvm', 'cvm_nogc', 'vm_nogc'})
    Doc:  Default linker used if the aesara flags mode is Mode
    Value:  cvm

allow_gc (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7fba04e81310>>)
    Doc:  Do we default to delete intermediate results during Aesara function calls? Doing so lowers the memory requirement, but asks that we reallocate memory at the next function call. This is implemented for the default linker, but may not work for all linkers.
    Value:  True

optimizer ({'merge', 'o3', 'fast_compile', 'None', 'o1', 'o4', 'unsafe', 'fast_run', 'o2'})
    Doc:  Default optimizer. If not None, will use this optimizer with the Mode
    Value:  o4

optimizer_verbose (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7fb9fcd61150>>)
    Doc:  If True, we print all optimization being applied
    Value:  False

on_opt_error ({'pdb', 'ignore', 'raise', 'warn'})
    Doc:  What to do when an optimization crashes: warn and skip it, raise the exception, or fall into the pdb debugger.
    Value:  warn

nocleanup (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7fb9fbee2b90>>)
    Doc:  Suppress the deletion of code files that did not compile cleanly
    Value:  False

on_unused_input ({'ignore', 'raise', 'warn'})
    Doc:  What to do if a variable in the 'inputs' list of  aesara.function() is not used in the graph.
    Value:  raise

gcc__cxxflags (<class 'str'>)
    Doc:  Extra compiler flags for gcc
    Value:

cmodule__warn_no_version (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7fb9fcdd12d0>>)
    Doc:  If True, will print a warning when compiling one or more Op with C code that can't be cached because there is no c_code_cache_version() function associated to at least one of those Ops.
    Value:  False

cmodule__remove_gxx_opt (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7fb9fbee2a10>>)
    Doc:  If True, will remove the -O* parameter passed to g++.This is useful to debug in gdb modules compiled by Aesara.The parameter -g is passed by default to g++
    Value:  False

cmodule__compilation_warning (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7fb9fbee2f50>>)
    Doc:  If True, will print compilation warnings.
    Value:  False

cmodule__preload_cache (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7fb9fbee2f90>>)
    Doc:  If set to True, will preload the C module cache at import time
    Value:  False

cmodule__age_thresh_use (<class 'int'>)
    Doc:  In seconds. The time after which Aesara won't reuse a compile c module.
    Value:  2073600

cmodule__debug (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7fb9fbee3110>>)
    Doc:  If True, define a DEBUG macro (if not exists) for any compiled C code.
    Value:  False

compile__wait (<class 'int'>)
    Doc:  Time to wait before retrying to acquire the compile lock.
    Value:  5

compile__timeout (<class 'int'>)
    Doc:  In seconds, time that a process will wait before deciding to
    override an existing lock. An override only happens when the existing
    lock is held by the same owner *and* has not been 'refreshed' by this
    owner for more than this period. Refreshes are done every half timeout
    period for running processes.
    Value:  120

ctc__root (<class 'str'>)
    Doc:  Directory which contains the root of Baidu CTC library. It is assumed         that the compiled library is either inside the build, lib or lib64         subdirectory, and the header inside the include directory.
    Value:

tensor__cmp_sloppy (<class 'int'>)
    Doc:  Relax aesara.tensor.math._allclose (0) not at all, (1) a bit, (2) more
    Value:  0

tensor__local_elemwise_fusion (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7fb9fbee3550>>)
    Doc:  Enable or not in fast_run mode(fast_run optimization) the elemwise fusion optimization
    Value:  True

lib__amblibm (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7fb9fbee3590>>)
    Doc:  Use amd's amdlibm numerical library
    Value:  False

tensor__insert_inplace_optimizer_validate_nb (<class 'int'>)
    Doc:  -1: auto, if graph have less then 500 nodes 1, else 10
    Value:  -1

traceback__limit (<class 'int'>)
    Doc:  The number of stack to trace. -1 mean all.
    Value:  8

traceback__compile_limit (<class 'int'>)
    Doc:  The number of stack to trace to keep during compilation. -1 mean all. If greater then 0, will also make us save Aesara internal stack trace.
    Value:  0

experimental__local_alloc_elemwise (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7fb9fbee3810>>)
    Doc:  DEPRECATED: If True, enable the experimental optimization local_alloc_elemwise. Generates error if not True. Use optimizer_excluding=local_alloc_elemwise to disable.
    Value:  True

experimental__local_alloc_elemwise_assert (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7fb9fbee3ad0>>)
    Doc:  When the local_alloc_elemwise is applied, add an assert to highlight shape errors.
    Value:  True

warn__ignore_bug_before ({'all', 'None', '1.0.1', '0.9', '0.7', '0.10', '1.0.2', '0.4.1', '0.8.1', '0.8.2', '0.3', '1.0.4', '0.8', '0.5', '0.6', '0.4', '1.0', '1.0.5', '1.0.3'})
    Doc:  If 'None', we warn about all Aesara bugs found by default. If 'all', we don't warn about Aesara bugs found by default. If a version, we print only the warnings relative to Aesara bugs found after that version. Warning for specific bugs can be configured with specific [warn] flags.
    Value:  0.9

exception_verbosity ({'high', 'low'})
    Doc:  If 'low', the text of exceptions will generally refer to apply nodes with short names such as Elemwise{add_no_inplace}. If 'high', some exceptions will also refer to apply nodes with long descriptions  like:
        A. Elemwise{add_no_inplace}
                B. log_likelihood_v_given_h
                C. log_likelihood_h
    Value:  low

print_test_value (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7fb9fbee3c50>>)
    Doc:  If 'True', the __eval__ of an Aesara variable will return its test_value when this is available. This has the practical conseguence that, e.g., in debugging `my_var` will print the same as `my_var.tag.test_value` when a test value is defined.
    Value:  False

compute_test_value ({'raise', 'warn', 'ignore', 'pdb', 'off'})
    Doc:  If 'True', Aesara will run each op at graph build time, using Constants, SharedVariables and the tag 'test_value' as inputs to the function. This helps the user track down problems in the graph before it gets optimized.
    Value:  off

compute_test_value_opt ({'raise', 'warn', 'ignore', 'pdb', 'off'})
    Doc:  For debugging Aesara optimization only. Same as compute_test_value, but is used during Aesara optimization
    Value:  off

check_input (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7fba5d2bdbd0>>)
    Doc:  Specify if types should check their input in their C code. It can be used to speed up compilation, reduce overhead (particularly for scalars) and reduce the number of generated C files.
    Value:  True

NanGuardMode__nan_is_error (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7fb9fcb813d0>>)
    Doc:  Default value for nan_is_error
    Value:  True

NanGuardMode__inf_is_error (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7fb9fbef40d0>>)
    Doc:  Default value for inf_is_error
    Value:  True

NanGuardMode__big_is_error (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7fb9fbef4090>>)
    Doc:  Default value for big_is_error
    Value:  True

NanGuardMode__action ({'pdb', 'raise', 'warn'})
    Doc:  What NanGuardMode does when it finds a problem
    Value:  raise

DebugMode__patience (<class 'int'>)
    Doc:  Optimize graph this many times to detect inconsistency
    Value:  10

DebugMode__check_c (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7fb9fbef42d0>>)
    Doc:  Run C implementations where possible
    Value:  True

DebugMode__check_py (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7fba04e81390>>)
    Doc:  Run Python implementations where possible
    Value:  True

DebugMode__check_finite (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7fb9fbef45d0>>)
    Doc:  True -> complain about NaN/Inf results
    Value:  True

DebugMode__check_strides (<class 'int'>)
    Doc:  Check that Python- and C-produced ndarrays have same strides. On difference: (0) - ignore, (1) warn, or (2) raise error
    Value:  0

DebugMode__warn_input_not_reused (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7fb9fbef4790>>)
    Doc:  Generate a warning when destroy_map or view_map says that an op works inplace, but the op did not reuse the input for its output.
    Value:  True

DebugMode__check_preallocated_output (<class 'str'>)
    Doc:  Test thunks with pre-allocated memory as output storage. This is a list of strings separated by "":"". Valid values are: ""initial"" (initial storage in storage map, happens with Scan),""previous"" (previously-returned memory), ""c_contiguous"", ""f_contiguous"", ""strided"" (positive and negative strides), ""wrong_size"" (larger and smaller dimensions), and ""ALL"" (all of the above).
    Value:

DebugMode__check_preallocated_output_ndim (<class 'int'>)
    Doc:  When testing with ""strided"" preallocated output memory, test all combinations of strides over that number of (inner-most) dimensions. You may want to reduce that number to reduce memory or time usage, but it is advised to keep a minimum of 2.
    Value:  4

profiling__time_thunks (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7fb9fbef4890>>)
    Doc:  Time individual thunks when profiling
    Value:  True

profiling__n_apply (<class 'int'>)
    Doc:  Number of Apply instances to print by default
    Value:  20

profiling__n_ops (<class 'int'>)
    Doc:  Number of Ops to print by default
    Value:  20

profiling__output_line_width (<class 'int'>)
    Doc:  Max line width for the profiling output
    Value:  512

profiling__min_memory_size (<class 'int'>)
    Doc:  For the memory profile, do not print Apply nodes if the size
                 of their outputs (in bytes) is lower than this threshold
    Value:  1024

profiling__min_peak_memory (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7fb9fbef4c50>>)
    Doc:  The min peak memory usage of the order
    Value:  False

profiling__destination (<class 'str'>)
    Doc:  File destination of the profiling output
    Value:  stderr

profiling__debugprint (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7fb9fcc3ac50>>)
    Doc:  Do a debugprint of the profiled functions
    Value:  False

profiling__ignore_first_call (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7fb9fbef4f10>>)
    Doc:  Do we ignore the first call of an Aesara function.
    Value:  False

on_shape_error ({'raise', 'warn'})
    Doc:  warn: print a warning and use the default value. raise: raise an error
    Value:  warn

openmp (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7fb9fbef4f50>>)
    Doc:  Allow (or not) parallel computation on the CPU with OpenMP. This is the default value used when creating an Op that supports OpenMP parallelization. It is preferable to define it via the Aesara configuration file ~/.aesararc or with the environment variable AESARA_FLAGS. Parallelization is only done for some operations that implement it, and even for operations that implement parallelism, each operation is free to respect this flag or not. You can control the number of threads used with the environment variable OMP_NUM_THREADS. If it is set to 1, we disable openmp in Aesara by default.
    Value:  False

openmp_elemwise_minsize (<class 'int'>)
    Doc:  If OpenMP is enabled, this is the minimum size of vectors for which the openmp parallelization is enabled in element wise ops.
    Value:  200000

optimizer_excluding (<class 'str'>)
    Doc:  When using the default mode, we will remove optimizer with these tags. Separate tags with ':'.
    Value:

optimizer_including (<class 'str'>)
    Doc:  When using the default mode, we will add optimizer with these tags. Separate tags with ':'.
    Value:

optimizer_requiring (<class 'str'>)
    Doc:  When using the default mode, we will require optimizer with these tags. Separate tags with ':'.
    Value:

optdb__position_cutoff (<class 'float'>)
    Doc:  Where to stop eariler during optimization. It represent the position of the optimizer where to stop.
    Value:  inf

optdb__max_use_ratio (<class 'float'>)
    Doc:  A ratio that prevent infinite loop in EquilibriumGraphRewriter.
    Value:  8.0

cycle_detection ({'fast', 'regular'})
    Doc:  If cycle_detection is set to regular, most inplaces are allowed,but it is slower. If cycle_detection is set to faster, less inplacesare allowed, but it makes the compilation faster.The interaction of which one give the lower peak memory usage iscomplicated and not predictable, so if you are close to the peakmemory usage, triyng both could give you a small gain.
    Value:  regular

check_stack_trace ({'raise', 'off', 'warn', 'log'})
    Doc:  A flag for checking the stack trace during the optimization process. default (off): does not check the stack trace of any optimization log: inserts a dummy stack trace that identifies the optimizationthat inserted the variable that had an empty stack trace.warn: prints a warning if a stack trace is missing and also a dummystack trace is inserted that indicates which optimization insertedthe variable that had an empty stack trace.raise: raises an exception if a stack trace is missing
    Value:  off

metaopt__verbose (<class 'int'>)
    Doc:  0 for silent, 1 for only warnings, 2 for full output withtimings and selected implementation
    Value:  0

metaopt__optimizer_excluding (<class 'str'>)
    Doc:  exclude optimizers with these tags. Separate tags with ':'.
    Value:

metaopt__optimizer_including (<class 'str'>)
    Doc:  include optimizers with these tags. Separate tags with ':'.
    Value:

profile (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7fb9fc2fafd0>>)
    Doc:  If VM should collect profile information
    Value:  False

profile_optimizer (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7fb9fbef54d0>>)
    Doc:  If VM should collect optimizer profile information
    Value:  False

profile_memory (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7fb9fbee3310>>)
    Doc:  If VM should collect memory profile information and print it
    Value:  False

<aesara.configparser.ConfigParam object at 0x7fb9fce06050>
    Doc:  Useful only for the VM Linkers. When lazy is None, auto detect if lazy evaluation is needed and use the appropriate version. If the C loop isn't being used and lazy is True, use the Stack VM; otherwise, use the Loop VM.
    Value:  None

unittests__rseed (<class 'str'>)
    Doc:  Seed to use for randomized unit tests. Special value 'random' means using a seed of None.
    Value:  666

warn__round (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7fb9fbef5710>>)
    Doc:  Warn when using `tensor.round` with the default mode. Round changed its default from `half_away_from_zero` to `half_to_even` to have the same default as NumPy.
    Value:  False

numba__vectorize_target ({'cuda', 'cpu', 'parallel'})
    Doc:  Default target for numba.vectorize.
    Value:  cpu

numba__fastmath (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7fb9fbef5950>>)
    Doc:  If True, use Numba's fastmath mode.
    Value:  True

numba__cache (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7fb9fbef5890>>)
    Doc:  If True, use Numba's file based caching.
    Value:  True

compiledir_format (<class 'str'>)
    Doc:  Format string for platform-dependent compiled module subdirectory
(relative to base_compiledir). Available keys: aesara_version, device,
gxx_version, hostname, numpy_version, platform, processor,
python_bitwidth, python_int_bitwidth, python_version, short_platform.
Defaults to compiledir_%(short_platform)s-%(processor)s-
%(python_version)s-%(python_bitwidth)s.
    Value:  compiledir_%(short_platform)s-%(processor)s-%(python_version)s-%(python_bitwidth)s

<aesara.configparser.ConfigParam object at 0x7fb9fc29ca90>
    Doc:  platform-independent root directory for compiled modules
    Value:  /home/matthew/.aesara

<aesara.configparser.ConfigParam object at 0x7fb9fc1cd190>
    Doc:  platform-dependent cache directory for compiled modules
    Value:  /home/matthew/.aesara/compiledir_Linux-4.15--generic-x86_64-with-glibc2.27-x86_64-3.11.6-64

blas__ldflags (<class 'str'>)
    Doc:  lib[s] to include for [Fortran] level-3 blas implementation
    Value:  -L/mnt/data/conda/envs/aesarabug/lib -lcblas -lblas -lcblas -lblas

blas__check_openmp (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7fb9fbb83dd0>>)
    Doc:  Check for openmp library conflict.
WARNING: Setting this to False leaves you open to wrong results in blas-related operations.
    Value:  True

scan__allow_gc (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7fb9eeed5710>>)
    Doc:  Allow/disallow gc inside of Scan (default: False)
    Value:  False

scan__allow_output_prealloc (<bound method BoolParam._apply of <aesara.configparser.BoolParam object at 0x7fb9fcc39690>>)
    Doc:  Allow/disallow memory preallocation for outputs inside of scan (default: True)
    Value:  True

```
</details>
",bug important C-backend performance concern,,2023-10-06 17:39:52,2023-10-25 14:19:10,"brandonwillard labeled 2023-10-07 00:00:19,brandonwillard labeled 2023-10-07 00:01:46,brandonwillard labeled 2023-10-07 00:01:46,brandonwillard labeled 2023-10-07 00:01:46,mattearllongshot mentioned 2023-10-12 02:01:08,mattearllongshot subscribed 2023-10-12 02:01:08,brandonwillard closed 2023-10-25 14:19:11",brandonwillard andrejmuhic mattearllongshot,5
