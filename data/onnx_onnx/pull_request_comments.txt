ezyang(2017-09-08 00:20:31):LGTM. (We might need to adjust the conda packaging a little, but we'll handle it next time it's needed.)
prasanthpul(2017-09-08 19:06:20):The document is actually auto-generated. Will also need to fix in https://github.com/onnx/onnx/blob/master/onnx/defs/nn/defs.cc
andreh7(2017-09-08 19:19:03):@prasanthpul should I close this pull request and create another one for `onnx/defs/nn/defs.cc` instead ? Or add the change for `onnx/defs/nn/defs.cc` this one ?

(I guess this will also have an impact on existing serialized models ? but it's probably better to fix this now rather than later..)
Yangqing(2017-09-08 19:21:14):Feel free to add the change for onnx/defs/nn/defs.cc .
ezyang(2017-09-10 03:11:29):Thanks!
bddppq(2017-09-12 17:38:57):#16 
ezyang(2017-09-12 17:48:07):Should we also record git hash information when it's available? As seen at: https://github.com/pytorch/pytorch/blob/master/setup.py#L555 (not blocking this patch)

I'm a bit surprised that setuptools doesn't have any built-in knobs for doing this. I guess not, since PyTorch has to reimplement it from scratch!
ezyang(2017-09-12 17:49:08):LGTM.
bddppq(2017-09-15 21:08:32):#28 
ezyang(2017-09-16 05:19:09):Let's make a follow up task about NumInputs so we don't forget about it.
jerryzh168(2017-09-16 00:15:43):Can we keep NumInputs(2,3) for now? We'll need to make changes in caffe2 and pytorch side to make this working, otherwise it will break our tests. We can change it after the corresponding changes has been made.
ebarsoum(2017-09-16 01:57:54):Sure.
ezyang(2017-09-17 01:55:03):Perhaps now is a good time to add a proper docblock here ;)
ezyang(2017-09-17 01:59:41):Especially because `raw` changes the expected type of what you pass for `vals`.

Speaking of which, we don't have a utility function for taking a numpy ndarray and making a TensorProto from it. Might be useful.
bddppq(2017-09-17 03:30:00):Sure will add a docstring.

There is one in numpy_helper https://github.com/onnx/onnx/blob/master/onnx/numpy_helper.py#L94 .
ezyang(2017-09-18 21:25:56):LGTM, I think Constant should be fairly noncontroversial.

However, since you mentioned network parameters, I'd also like to point out that we snuck in initializers into the graph format (https://github.com/onnx/onnx/blob/master/onnx/onnx.proto#L97) for the very reason of adding parameters. This may be more suitable for parameters than Constant, since it's easier to swap out initializers if you have different trained weights you want to use.
bddppq(2017-09-18 21:41:02):Feel free to merge
bddppq(2017-09-19 21:57:59):Thanks!
ezyang(2017-09-20 00:20:15):This TODO can be removed now :)
ezyang(2017-09-20 00:21:38):Note: It's going to be a bit difficult to write tests for things like convolution using bare numpy.  One good approach is to use tensor comprehensions but we still need a reference implementation of TCs we can roll into ONNX.
bddppq(2017-09-20 03:25:25):Yeah I'm tempting to create a new place for hosting all these reference implementations instead of spreading them in a test file. :-)
ezyang(2017-09-20 00:25:17):Operators.md is an autogenerated file, so what actually needs to be updated is the source in https://github.com/onnx/onnx/tree/master/onnx/defs (and maybe also the doc generator)
brettkoonce(2017-09-20 23:51:47):moving to #44
ghost(2017-09-22 16:19:05):Rather than discuss this design choice for GEMM specifically, I continued the discussion started in #24.
dzhulgakov(2017-09-25 23:10:58):From offline discussion: add broadcasting to C
ebarsoum(2017-09-27 16:14:09):Why it isn't experiment initially?
bddppq(2017-09-27 16:29:19):@ebarsoum It was experimental initially in this PR, in my most recent commit to this PR I have moved it out of experimental.
ebarsoum(2017-09-27 16:59:32):@bddppq why we move it out of experiment?
bddppq(2017-09-27 17:07:59):@ebarsoum From offline discussion with @yuanbyu @prasanthpul and @dzhulgakov, we will add Gemm (as non-experimental) and remove FC
bddppq(2017-09-23 07:00:24):They are not disjoint. Each initializer needs to have a name that must be one of the inputs name so the backend can initialize the corresponding input.
ezyang(2017-09-23 15:18:21):@bddppq Wait, are we really not handling the initializer list positionally? News to me!
bddppq(2017-09-23 15:25:25):@ezyang yep, since there could be input value not filled in initialization
gramalingam(2017-09-23 16:12:34):May be I am missing something: the disjoint representation has the same information, so what is the problem? Instead of input = [x,p1,y,p2,z], initializer = [(p1,value1), (p2,value2)], we have input = [x,y,z] and initializer = [(p1,value1), (p2,value2)].
donbox(2017-09-25 00:41:01):This thread indicates that the current spec is somewhat confusing. On my first read, I had the following questions:

1. What happens if two or more tensors with the same name appear in the initializer field?
2. Does the presence of an initializer with name "foo" mean that the input named "foo" should not appear as an external input parameter when the graph is evaluated?

Honestly, I prefer Rama's approach of just making the initializers a disjoint set of named constants that can be referred to in the Tensor/Value namespace. Way less confusing IMO. 

BTW, if we reject this PR, someone needs to answer #1 and #2 above in the spec, lest folks get confused.

And yes, Ed, as spec'ed, position is immaterial if I'm reading it correctly.
yuanbyu(2017-09-25 14:09:01):This is what I believe the properties of the names in graph inputs and initializers:

1. The names of the graph inputs are distinct with each other.
2. The names of the initializers are distinct with each other.
3. The names of the graph inputs and the initializers must be the name of some input of some node in the graph.

Before Rama's PR:
4.  The names of initializers are a subset of the names of the graph inputs.
Rama's PR proposed:
4. The names of initializers and the names of graph inputs are disjoint. 
ezyang(2017-09-25 16:36:50):OK, let me give one justification for the current design (where initializer is a subset of inputs): if you decide you didn't actually want the pretrained parameters that shipped with the network, you can just drop the initializers on the floor, and you still have a valid network. With the proposed PR, you now have to add extra inputs for everything that was an initializer previously. Also, if initializers actually "define" the tensors, why not just implement them as constant nodes? (One answer: so that the backend can load the constants first, before the inputs are available.)

OTOH, I do agree that the current representation has too many degrees of freedom. For example, the Caffe2 backend currently only accepts inputs positionally. If initializers don't cover a prefix of the inputs, what are the intended semantics? Totally unclear.

In any case, if we accept Rama's PR, we also need to adjust the behavior of the frontends and backends, because Rama's semantics are definitely *not* the current semantics, and they are BC-breaking.
gramalingam(2017-09-25 17:09:00):Ah, so the first paragraph does give a different semantics (that is, different from what I understood earlier). The important thing is to agree on the semantics, and we can fix the documentation accordingly. The proposed semantics, as I understand it, is that if a name "X" has a value in the initializer list *and* if the user specifies a value for "X", "X" takes the user-specified value. This is reasonable.

On a related but different (implementation oriented) aspect: is there any thought about tensor-values (which could be large) being stored in separate files from the model itself? In the scenario you describe, if you don't need the initializer values, why read/load them?
gramalingam(2017-09-25 17:11:41):If we go with the above semantics, I would recommend that "input" be a list of (name, optional value) pairs.
bddppq(2017-09-25 18:13:09):Looks to me this PR does two things:
1. clearly stated the tensor name in initializer should be unique
2. change the representation of input and initializer

The first part is good, and I think there are still many other places we could improve in the documentation. Regarding to second part, it's better not to change it. As @ezyang has pointed out, the current representation is more flexible when you wanna drop the initializer and preserve the graph structure. Actually, this is where the current representation came from. In Caffe2, weights and graph structure are separated into two pb files (init_net.pb and predict_net.pb).

In onnx-caffe2 backend, there are three ways users can provide weights:
1. embed the weights in the initializer
2. use a separated init_net.pb
3. pass in-memory values

gramalingam(2017-09-26 20:39:11):Ok, I have changed the documentation to clarify what happens when a name appears in both initializer and input.
yuanbyu(2017-09-24 17:25:12):a) In GraphProto, ArgInfoProto is repeated and we don't require its presence, so it is optional. 
b) We support protobuf v2.6.1 and up. So one-of is allowed but map is not allowed.

donbox(2017-09-24 18:40:06):Yuan, your first comment is at odds with your last comment.

I believe that declaration of input and output types is required on graph (required for type inference) and but we CAN live with it optional on node assuming the implementation has knowledge of the declared signature for the referenced operator. 
yuanbyu(2017-09-24 19:05:39):Oops it was a typo. It should have said "In **NodeProto**, ArgInfoProto is repeated ...". Thanks Don for pointing it out.
ezyang(2017-09-25 16:55:23):I apologize if this has been written elsewhere, but it would be really helpful to know what use-cases we expect backends to take advantage of this type information for. For example, Caffe2 cannot make use of this type information at all, because it always waits until it knows what the concrete inputs are, at which point it can recompute all of the information. Do we have any short term use cases where this truly is untenable? 

For example, you might think that having type information for every value tensor flowing through a network would be helpful for a hypothetical optimizer. But actually, from experience working with the PyTorch JIT, you *still* need some sort of type inference engine, because if you're going to introduce new nodes you need to give them accurate types, but the only way you're going to get them is via inference. So, from a tool perspective, it would be *easier* if you didn't have to specify types for EVERY value flowing through the network, as that makes it mandatory for the frontend to implement type inference.
donbox(2017-09-26 11:09:35):We have scenarios where we will import a model and deploy it with an app, and that the type/shape of inputs is statically known.

The ability to do a validity check at model import against the known signature of operators helps us catch bad models earlier in the developer work flow (hence the desire for the operator library stuff).

The ability to cache the results of type/shape inference into the rep directly (hence the desire for putting type/shape annotation on Node) reduces load time as we know we’ve already checked the model.  On client configurations, load time matters. 

DB



Sent from Mail for Windows 10
[16299 - 16.0.8600.4040]

From: Edward Z. Yang
Sent: Monday, September 25, 2017 9:55 AM
To: onnx/onnx
Cc: Don Box; Comment
Subject: Re: [onnx/onnx] Resurrect type and shape (#51)

I apologize if this has been written elsewhere, but it would be really helpful to know what use-cases we expect backends to take advantage of this type information for. For example, Caffe2 cannot make use of this type information at all, because it always waits until it knows what the concrete inputs are, at which point it can recompute all of the information. Do we have any short term use cases where this truly is untenable?
—
You are receiving this because you commented.
Reply to this email directly, view it on GitHub, or mute the thread.


yuanbyu(2017-09-26 22:03:17):@dzhulgakov Based on your suggestion, I moved the type and shape out of NodeProto and into GraphProto. Please take another look?
ezyang(2017-10-02 16:20:13):@fmassa has been working on a TensorFlow importer for ONNX, and the TensorFlow frontend *requires* shape and type information, so we'll need this for that.
fmassa(2017-10-02 16:22:15):I just found out that TF requires `type` information, but the `shape` is optional.
dzhulgakov(2017-09-25 01:00:16):Why do we need a separation (especially of "oneof" kind) for sparse vs dense tensors? Element type is present in either Tensor or SparseTensor.values. Maybe put TensorProto.DataType as member of ArgInfoProto directly?

Or do you imagine some other fields appearing in TypeProto struct?
dzhulgakov(2017-09-25 01:02:29):another (less descriptive) option would be to use negative values in "shape" as symbolic references, e.g. "-1", "-2", etc. It'd be less descriptive but would simplify the format and use the same `repeated int64 dims` for both fully-realized shape and partially specified shape.
dzhulgakov(2017-09-25 01:04:36):Should we also support arbitrary list of dimensions? E.g. we can have "named range of dimensions". One option to specify it would be to put `string dim_list` above that can cover 0 or more arbitrary dimensions preserved "as is".

For example `Gather` operator would be taking shapes `[N, ...dims...]` and `[K]` and returning `[K, ...dims...]`.
dzhulgakov(2017-09-25 01:14:10):I'm a little confused what is the role of specification at the individual Node level. In particular all inputs of the node are some other Node's outputs, so the type/shape information is redundant. If we wanted to preserve types and shapes of intermediate value it feel more natural to just record them in "value_name -> ArgInfoProto" directly in the graph.

Second, how does it connect with the type specifications and inferences #55 ? If we're moving towards full automatic shape inference there is little need to record the shapes of the intermediate values in the model proto.
donbox(2017-09-25 02:54:13):On your first point, you are correct that the type declarations on Node require redundant specification on both the input and output side. If we keep this rep, we need to define the rules for what happens when they TypeProto is not identical.   I'll defer to Yuan as to whether encoding a single table that declares the type of each edge/named-value is better or worse.

On your second point, one could take a graph proto and the library protos for all referenced operators and either (a) fill in absent Node type decls or (b) check the ones that are there.  Note that in Yuan's PR, he's only requiring type decls on Graph input/output, not Node (which as you state can be inferred).
donbox(2017-09-25 02:56:11):See #55 for other TypeProto variants, which are needed to properly schematize attribute values (which can be scalars or sequences, not just tensors)

.
donbox(2017-09-25 02:58:03):That would work, however using symbolic names has two advantages:

1. It coins a name one can refer to in doc_string.

2. If the same name appears two or more times on a given graph or node, it's value must be the same, which addresses correlating dimensions across input and output parameters.. 
donbox(2017-09-25 03:00:13):I've hit the need for something like this as well. If I understand correctly, you are saying we'd effectively add a rest arg/variatic arg and allow it to be named, again, to support correlation?
gramalingam(2017-09-25 03:27:43):Agree with Don. On the second point, this allows the possibility of a type-inference being a stand-alone tool, that takes a graph-proto as input and writes a (fully type-annotated) graph-proto as output.
dzhulgakov(2017-09-25 06:10:34):On the second point - what is the use case of recording intermediate tensor shapes in the serialized model? I'm a bit biased towards keeping the minimum necessary functionality for model interchange in ONNX whereas inference of shapes can be handled by corresponding frameworks. As you said, we can always infer shapes based on Node definitions, operator specs and graph inputs.
yuanbyu(2017-09-25 06:11:13):Yes, TypeProto is an extension point, and we would like to have the flexibility to extend it t introduce more data types in the future.
dzhulgakov(2017-09-25 06:12:02):Agree, symbolic names are indeed more readable. Probably it's fine at expense of protobuf brevity.
dzhulgakov(2017-09-25 06:12:55):Yes, adding a "variadic placeholder" that can capture zero or more dimensions
yuanbyu(2017-09-25 06:14:35):It could also be used for more efficient memory reuse, when we for example statically plan for the memory allocation/reuse.
yuanbyu(2017-09-25 06:18:18):Yes, using negative numbers would work, but somewhat less elegant in my opinion.
yuanbyu(2017-09-25 06:36:14):On the first point, I am fine with encoding them as "value_name -> ArgInfoProto" in graph. 

On the second point, I don't think that this PR and Don's PR commit to fully automatic shape inference. We can discuss if that is the direction we should go and the issues we might run into.
For example, do we really want to require all participating frameworks to perform fully automatic shape inference? Even for type inference, while we will definitely d it, it is uncertain to me if we want to require it for all frameworks.

yuanbyu(2017-09-25 07:20:52):TF has a more general `Gather` where the axis to gather is an input. Do we want to handle it?
jerryzh168(2017-09-25 18:22:57):For type/shape inference, is it possible to do it in the onnx level so that participating frameworks don't need to implement it again?
jerryzh168(2017-09-25 18:24:20):We can do something like:
1. provide a set of default type/shape inference functions
2. offer a way to override the default inference rules.
gramalingam(2017-09-25 19:01:06):If we are willing to do a BC breaking change, it would be better to bundle information about an input (or output) together: name, type, shape, and even default-values (which are currently in the initializer list), instead of correlating across lists by position or name etc. (with the possibility of missing or mismatching entries).
dzhulgakov(2017-09-26 05:52:22):It's actually a perfect example why static shape inference might be limiting and providing C++ inference function is beneficial. E.g. that's exactly what TF does in this case: https://github.com/tensorflow/tensorflow/blob/3c0bcf8442a107b635cca64440d05c390a35d177/tensorflow/core/ops/array_ops.cc#L1396

I think for now we can avoid "vararg" expressions. For inputs/outputs specification even simple symbolic names like in this PR should be sufficient.
dzhulgakov(2017-09-26 05:56:24):On the first point, imho it makes more sense to put it into `name -> ArgInfoProto` map on the top level to avoid duplication. What do you think?

As for type/shape inference, it'd be very interesting to include "standard implementation" of the inference library in ONNX at some point, but it's slightly lower priority. If you already have library available - it'd be awesome if you could contribute it.
dzhulgakov(2017-09-26 05:58:23):I don't have strong preference. @ezyang ?
donbox(2017-09-26 10:09:51):Dmytro, we should definitely collapse the GraphProto.input/GraphProto.input_arg_info fields for both compactness and to make it less likely that folks don't screw it up..

I suspect once we write down the type inference rules, someone will contribute a reference implementation. I agree that would be pure goodness and will help us on interop.
donbox(2017-09-26 10:13:47):I do have a strong preference to at least put the arginfo and name stuff together.

I'm less convicted on putting initializer in there given the other threads going on. 

Ed?
dzhulgakov(2017-09-27 18:51:29):this would be a breaking change for PyTorch which spits out the proto in C++: https://github.com/pytorch/pytorch/blob/5deacb5bce8bfb41662b57a61ea635c44d2a347f/torch/csrc/onnx/onnx.h#L249

Just FYI for @ezyang 
dzhulgakov(2017-09-27 18:53:02):I think it MUST be present only if it's graph inputs/outputs. Let's clarify that if it's used as a part of internal values definitions (`value_info` below) then it's fine to have type optional
dzhulgakov(2017-09-27 18:56:54):describe that it's optional?
dzhulgakov(2017-09-27 18:57:49):we should probably bump IR version because it's indeed a breaking change (just following the protocol)
dzhulgakov(2017-09-27 18:59:34):as we discussed in a chat - this is not safe. Maybe add fields with another id?
donbox(2017-09-27 20:29:01):I think we should just bump the IR version and take this change along with #53 as a single BC.

If we get this to master this week, I think we are fine. 
donbox(2017-09-27 20:31:55):Yup. See my last comment.
yuanbyu(2017-09-27 20:35:36):It is optional for a value to appear in `value_info`. So when a value appears in `value_info`, the PR currently does require type.  Do you think we should relax it further in this case?
yuanbyu(2017-09-27 20:36:12):Will do.
yuanbyu(2017-09-27 20:39:54):Done.
yuanbyu(2017-09-27 20:56:33):It certainly makes things less clean. I will assign new ids so we can look at it again.
ezyang(2017-10-02 16:29:47):I have been thinking about this some more, and I don't think shape should be besides the TypeProto. The shape specification can depend on whether or not you have a dense or sparse tensor; for example, PyTorch sparse tensors have both sparse dimensions and dense dimensions, and that should be required in the shape.
ezyang(2017-10-02 16:31:56):Whether or not we actually rename Tensor to Value is, I think, blocked on figuring out what happens in https://github.com/onnx/onnx/pull/67
bddppq(2017-10-02 22:36:17):Since we have decided to make `type` and `shape` as required in the graph input and output, the argument `inputs` and `outputs` in this `make_graph` function should not be a list of string anymore, otherwise, this `make_graph` will actually produce an invalid graph.
yuanbyu(2017-10-02 22:44:30):Good point. Any proposal how we should change it?
bddppq(2017-10-02 22:53:13):inputs and outputs should be a list of corresponding proto structs (like initializer), so the original code is fine (you just need to revert the changes in this file).
yuanbyu(2017-10-02 23:43:05):OK. I will add a MakeType() method to make it easier to construct.
yuanbyu(2017-10-03 03:07:25):@bddppq Thanks for fixing my bug. I was going to look at after dinner, but pleasantly found it fixed.
dzhulgakov(2017-09-25 01:25:15):Are you planning to export this format based on operator definitions in C++?

I'm trying to understand the motivation for serializable Operator definitions at this point. Alternative would be to expand operator schema to include type and shape specifications whenever necessary (similarly to NumInput/NumOutput). We can use those specification to validate the networks and generate Operators.md.

The benefit of C++ definitions is that we can support arbitrary function for shape inference/validation in case our symbolic language is not powerful enough.
donbox(2017-09-25 02:48:27):@dzhulgakov, both import from and export to C++ is definitely something we are looking at. We also need it for the input/output declarations on graph. 

We also are looking at having a machine rep we can use to check a model without running python or c++ code.
dzhulgakov(2017-09-25 06:15:37):How about putting it into a separate .proto file? onnx.proto is a core functionality for model export. Operator library is more of an additional resource, assuming that we keep using C++ decls as the source of truth.

cc @bddppq 
ezyang(2017-09-25 16:46:58):I generally like the idea of having the schema data in exportable form (in fact, I asked about this at https://github.com/ProjectToffee/ToffeeIR/issues/28), but it is extra work, and closes off some avenues for specifying operators (e.g., writing custom C++ code to do shape inference.) I think this pushes us on a good design evolutionary path, but we should let others comment.

Some other comments:

* I think putting the operator signature format in the same protobuf as the actual protobuf for model export is bound to be confusing, because, if I understand correctly, you're never actually supposed to send a `LibraryProto` over the wire with your network with a model.
* I'd also advise against inserting attributes (e.g., `model_version` in `LibraryProto`; misnamed anyway) which imply that there will be multiple versions of library. Right now, it is important for ONNX to provide a *single* set of standardized operators and avoid fragmentation. If there is support for extra libraries, it should be as a mechanism for frameworks to provide experimental operators for themselves; but this should be clearly marked as such.
* It would be really good if the proposed format could be validated against the existing schema code, so that we know that it's enough to store all of the information we have already.
gramalingam(2017-09-25 17:50:12):As I read it, the intention is that operator-signatures are in a different file. Only the file-name (of the file containing the signatures) is mentioned in the model ... sort of an include/import "onnx.standard".
donbox(2017-09-27 06:16:42):Once we land on a design, moving this into a separate file is trivial.

I just made a few changes, specifically to clear up the fact that the types allowed for attributes are distinct from those allowed for parameters.  I also mimicked Yuan's typeshape and flipped the graph input/output to ParameterDeclProtos and killed the use of arg_info there.

I manually validated against schema.h but am putting off writing a converter until we get closer to consensus.

Known gaps relative to schema.h are:
  - No support for dynamically computed paramter counts (by design).
  - No support for min/max parameter count.
  - No support for file/line info. Unclear if this is needed. How does it get used?
dzhulgakov(2017-09-27 19:16:24):A few questions to understand a bigger picture:
- could you please share a list of types you're planning to add for CoreML support
- clarify what are the expected semantics of library discoverability. E.g. there's now "imported libraries" field, but it's unclear what frameworks should do with it. I'd rather tend not to include it and let it up to a framework to manage dependencies

Also some on details:
- for optional inputs/outputs (like min/max you mentioned) is the desired setup to inject multiple instances of SignatureDeclProto with different numbers of inputs? might make sense to clarify that
- do you think codifying default values for arguments make sense to add?

If it's not too big work, I'd actually ask to split out into a separate file in this PR. This would make separation more clear as they are semantically different and make review and iteration easier.
donbox(2017-10-05 06:50:39):@dzhulgakov :

The list of types is what's described in PR67/ I've separated that stuff from this PR to get a clean read on the library mechanism independent of the type changes.

The semantics of library discoverability is that the URI of the referenced library can either be recognized as a well-known ID (that an implementation recognizes either because it cached the library def or is hard coded (a la C2 with the C2 operators).  We're open to a better alternative - the other obvious approach is to punt the explicit reference and rely on external mechanisms to specify the set of model/library files to process. 

Yes, the repeated part of SignatureDecl is to support different # of inputs or different types of inputs (e.g., overloads on Tensor<float> vs. Tensor<double>). 

Yes, we should tackle default arg values/optional-vs-required et al. 

As for separating out into a distinct file, let's reconcile this design and the map/record/sequence design and we can stratify accordingly. 

One note - as part of absorbing the various BC's into this branch, I took the liberty of cribbing the ModelProto schema for both single-graph/"main" models and libraries. The fields are identical other than having a single graph vs. a collection of externs + functions (which are just graphs that can be referenced by name from a node just like an extern operator).   BTW, we may want to hoist out the function thing as a general way to do name subgraph everywhere. Right now we have graph-valued attributes that have no defined meaning.

ezyang(2017-11-03 20:19:46):This is obsoleted by the new OperatorSet design.
dzhulgakov(2017-09-26 06:17:54):I don't think it's good to mix the scalars and sequences here with Tensor/SparseTensor types which are used in type inference. They are semantically different categories and are a bit confusing. For example, from seeing this at first I assumed that you propose introducing scalar types flowing through the graph which is already covered by 0-dim tensors
dzhulgakov(2017-09-26 06:31:57):you might need to add a notion of optional / default attributed. Otherwise optional args might easily lead to combinatorial explosion of OperatorDecls (assuming that you intend to produce a separate one for each combination)
dzhulgakov(2017-09-27 19:04:16):Let's not use ParameterDecl here. In my opinion onnx.proto should not depend on operator_lib.proto or whatever we will call it. Yuan's diff just uses ArgInfo Proto here.
dzhulgakov(2017-09-27 19:08:41):Could you please rebase on Yuan's diff properly. Currently TypeProto doesn't seem to be defined in this file :)
ezyang(2017-09-27 21:55:10):Based on IRL discussion, this line is going away now, correct? (Or am I misunderstanding?)
donbox(2017-09-24 23:52:01):Note that the representation used for complex128 and complex64 is to simply pack the two float/double values next to one another in the raw_data field (little-endian, IEEE754, real followed by imaginary component).

Another equally valid mapping would be to simply use the float_data and double_data fields and encode using odd # elements for reals, even # elements for complex.  That mapping might be more convenient as it doesn't involve raw buffer manipulating.
ezyang(2017-09-25 16:22:15):It is possible that IR.md wasn't supposed to mention complex64/complex128 as supported types. Which frameworks support these kinds of numbers? I know PyTorch (https://github.com/pytorch/pytorch/issues/755) and Caffe2 do not. Does CNTK? If no one supports it, it would be better to not put it into the IR (and just make sure we have enough extensibility points to add it later, if it becomes a thing.)
donbox(2017-09-26 10:00:10):Ed, I wasn't part of the initial ONNX group, so I can't speak to how complex64 and complex128 made it into the spec docs.

I'll let Yuan speak to the CNTK usage.

TF supports it: https://www.tensorflow.org/versions/r0.12/api_docs/python/math_ops/complex_number_functions

I do know that numpy also supports it, so I can believe it gets used elsewhere in the wild.

BTW, TF has a few other scalar types that we don't support (quantized ints, truncated float16).
nouiz(2017-09-26 12:31:06):In Theano, we have basic complexe support. But we are seeing an increasing
need for this. More models use that or mimic it with manual implementation.
So if this is already working in this PR, I would think it is good to
include it now.

On mar. 26 sept. 2017 06:00 Don Box <notifications@github.com> wrote:

> Ed, I wasn't part of the initial ONNX group, so I can't speak to how
> complex64 and complex128 made it into the spec docs.
>
> I'll let Yuan speak to the CNTK usage.
>
> TF supports it:
> https://www.tensorflow.org/versions/r0.12/api_docs/python/math_ops/complex_number_functions
>
> I do know that numpy also supports it, so I can believe it gets used
> elsewhere in the wild.
>
> BTW, TF has a few other scalar types that we don't support (quantized
> ints, truncated float16).
>
> —
> You are receiving this because you are subscribed to this thread.
> Reply to this email directly, view it on GitHub
> <https://github.com/onnx/onnx/pull/57#issuecomment-332149552>, or mute
> the thread
> <https://github.com/notifications/unsubscribe-auth/AALC-1xsXagGg_3BP5aKj22w6lVU5Skbks5smMsrgaJpZM4PiErP>
> .
>

ezyang(2017-09-27 02:01:38):In the model of "tiered" support, we should be clear what expectations we have about frameworks supporting complex numbers, which is that, they are unlikely to be very good support by most backends, at least in the short (even mid) term.
nouiz(2017-09-27 13:37:37):That seem a pretty good to me.

On Tue, Sep 26, 2017 at 10:01 PM Edward Z. Yang <notifications@github.com>
wrote:

> In the model of "tiered" support, we should be clear what expectations we
> have about frameworks supporting complex numbers, which is that, they are
> unlikely to be very good support by most backends, at least in the short
> (even mid) term.
>
> —
> You are receiving this because you commented.
>
>
> Reply to this email directly, view it on GitHub
> <https://github.com/onnx/onnx/pull/57#issuecomment-332386945>, or mute
> the thread
> <https://github.com/notifications/unsubscribe-auth/AALC-w06G3TBuzgQuDpKyHlmeHa9Vuz8ks5smayDgaJpZM4PiErP>
> .
>

yuanbyu(2017-09-28 14:41:42):@donbox the data types were initially in but removed at the end to keep things minimal for v1. Currently CNTK doesn't support complex64 and complex128, but we have started to see needs for them. 

donbox(2017-10-04 15:28:18):@dzhulgakov - I've updated helper.py and numpy_helper.py to support the new data types.
dzhulgakov(2017-09-25 01:28:00):BOOL should be stored in int32_data above (see comment there)
dzhulgakov(2017-09-25 01:28:24):for double it probably  makes sense to create double_data field separately
dzhulgakov(2017-09-25 01:28:58):why not use float_data directly and enable convention there for "real component first"
donbox(2017-09-25 02:36:29):Check the original PR. It's field 10. 
donbox(2017-09-25 02:41:38):That is consistent with the spec and is explicitly called out as a supported DataType on int32_data. . 

The use of raw_data is optional but supported for all data types except for string. The comment text simply makes it unambiguous how things get encoded when the raw_data field is used. 
donbox(2017-09-25 02:44:17):Thanks - I agree (see my comment at the top of this PR).

I updated the PR to support this. 

dzhulgakov(2017-10-03 21:58:29):if you're changing this - could you please change a few places in onnx library? E.g. onnx/numpy_helper.py and a few others (just do the search)
dzhulgakov(2017-10-03 22:00:27):do we have concrete use cases for unsigned and high-precision numbers as attributes? I'm not opposed to them, but just curious whether we really need them at this point (few more parts for a switch and also less natural as now float-valued ones need to be looked up in both `f` and `d`).
dzhulgakov(2017-09-25 06:04:40):cc @bddppq to see which serialized files we'd need to regenerate
yuanbyu(2017-09-25 13:39:50):Once we make `producer_version` a string, it can also be used to encode `producer_tag`, like in SemVer. The `producer_tag` string was introduced mainly because we couldn't encode it with an int `producer_version`. 
ezyang(2017-09-25 16:18:03):Reading this PR made me also realize that we are mixing up the semantics of `GraphProto`. Let me explain.

Remember that `GraphProto` can be embedded *within* a `GraphProto` via the `g`  attribute type. But does it make sense to respecify things like `ir_version` inside a nested `GraphProto`? No. So this is a pretty clear indication that we messed things up.

It seems like there should be a minimal `GraphProto` that doesn't have all of this extra info, and then a *true* top-level struct, like `OnnxProto`, which contains version info, extra metadata, and then the actual graph.

What do people think?
gramalingam(2017-09-25 16:50:35):Agree with the problem about embedded GraphProto. I think this is also a good opportunity to reconcile this with the proposal for Functions that is coming up separately. The minimal GraphProto you refer to and a function-body share most things in common. 
bddppq(2017-09-25 17:20:06):@dzhulgakov There are some models (translated from Caffe2) uploaded to aws, I will regenerate them once all the proto format changes are settled.
donbox(2017-09-26 09:52:46):Ed, I agree that we conflated the "file/package/container format" stuff with the core graph semantics. 

I would also argue that while we can shove a GraphProto into an attribute, we have no declared semantics for what one can do with it once it's there. Specifically, is their any way that the "main" graph can refer to the graph stored in its attributes or is it just a way to shove a bunch of graphs into a single file that someone needs to figure out how to break apart and do something with?

We landed on defining FunctionDefProto and making it a peer to GraphProto.node:

    message GraphProto {
    // generic file format stuff
        optional int64 ir_version;
        optional string producer_name;
        optional string producer_version;
        optional string name;  
        optional string domain;
        optional int64 model_version;
        optional string doc_string;

    // graph stuff
        repeated string input; // untyped intput declarations
        repeated string output; // untyped output declarations
        repeated TensorProto initializer;
        repeated Node node;.  // graph "body"

        repeated FunctionDefProto function; // named sub-graphs that can be referenced via Node.op_type
    // ...
    }
    message FunctionDefProto {
        optional string name;
        repeated ParameterDeclProto input_param; // typed input declarations
        repeated ParameterDeclProto output_param; // typed output declarations
        repeated ParameterDeclProto input_attribute; // typed attribute declarations

        Node node; // function body, value namespace distinct from GraphProto.node and other function bodies
    }

Essentially, FunctionDefProto is just the "graph" part of GraphProto and the "file format" stuff is inherited from the parent.

Picking up on your suggestion, I'd have rather written it like this:

    message ModelProto {
    // generic file format stuff
        optional int64 ir_version;
        optional string producer_name;
        optional string producer_version;
        optional string name;  
        optional string domain;
        optional int64 model_version;
        optional string doc_string;

        repeated TensorProto initializer;
    
        optional FunctionDefProto main; // the primary graph for this model
        repeated FunctionDefProto function; // named sub-graphs that can be referenced via Node.op_type
    }
    message FunctionDefProto {
        optional string name;
        repeated ParameterDeclProto input_param; // typed input declarations
        repeated ParameterDeclProto output_param; // typed output declarations
        repeated ParameterDeclProto input_attribute; // typed attribute declarations

        Node node; // function body, value namespace distinct from GraphProto.node and other function bodies
    }

Not married to names but you get the drift. 

Note that we didn't change GraphProto.input and output in order to maintain wire compat. This would address that.

Thoughts?



binarybana(2017-09-26 18:56:49):We should probably move the `Function`/computation semantics vs file container separation discussion to #9 (or perhaps a new issue). But I agree that there are improvements to be made.

I'm not sure if all graphs/functions need to be included in a flat list at the top like @donbox suggested, or if hierarchies make more sense there. (eg: if I want to take someone else's model and pack it into mine, then I'd have to flatten that sub-model's functions into my model's subfunction rather than reusing it whole).
ezyang(2017-09-26 19:00:13):I like `ModelProto` as the top-level protobuf name, but I think it will be clearer for people if we keep `GraphProto` name as it is. If we change it, you would also need to change the `g` and `graphs` attribute names to be consistent; otherwise, you would be in the awkward situation where the "g" field refers to a "FunctionDefProto" rather than a "GraphProto". And honestly, despite the kind of oblique name, I really liked thinking these as NOT function definitions, because that's really what they were: anonymous functions!

Did we end up discussing what was going to happen with functions? I'd like to get the BC breaking change with top level ModelProto in without also dragging in the function definition changes, which are there own set of things to talk about. So, in concrete code, I am suggesting that for just format, we just have something like:

```
message ModelProto {
  // The version of the IR this model targets. See Version enum below.
  optional int64 ir_version = 1;

  // The  producer of this model
  optional string producer = 2;

  // Domain of the model.
  // We use reverse domain names as name space indicators. For example:
  // `com.facebook.fair` or `com.microsoft.cognitiveservices`
  //
  // Together with `name` and `version`, this forms the unique identity of
  // the graph.
  optional string domain = 3;

  // The actual graph defining the model
  optional Graph graph = 4

  // An optional list of initial values for tensors in the list of input above.
  // Used to pass serialized parameters for networks.
  // Each entry specifies a value for the input whose TensorProto.name matches
  // a name in the input. This list is not required to be the same length of as
  // input since many tensors might not have initial values.
  repeated TensorProto initializer = 5;

  // A human-readable documentation for this graph. Markdown is allowed.
  optional string doc_string = 6;
}

// GraphProto defines a series of nodes to form a directed acyclic graph.
// This is the equivalent of the "network" and "graph" in many deep learning
// frameworks.
message GraphProto {
  // The nodes in the graph.
  repeated NodeProto node = 1;

  // The name of the graph.
  optional string name = 2;   // namespace Graph

  // The inputs and outputs of the graph.
  repeated string input = 3;  // namespace Tensor
  repeated string output = 4; // namespace Tensor
};
```

And maybe also a name for models...
donbox(2017-09-26 20:34:24):Happy to decouple the top-level format shift from function.  Two Q's:

1. Where do initializers go?  ModelProto or GraphProto?
2. I'd like us to stage this so the parameter declarations for graph gets rolled out at the same time.
3. We have a high level understanding of whether we want to reuse ModelProto for the library stuff by adding an additional optional field for operators, or whether we are going to have two top-level protos (one for library, one for model). I prefer the latter.

BTW, shall I prep the PR to reflect Ed's suggestion tonight?


donbox(2017-09-27 04:21:38):I've committed the change to onnx.proto to reflect the Model/Graph split.

I am confident that this will break our tests and am staging the changes to the various python files that touch GraphProto.

I will need help handing regenerating the protobufs that are on AWS. 

BTW, now is the time to throw comments at the refactored proto file, as propagating these changes are not cheap..

ezyang(2017-09-27 14:02:45):The proto changes LGTM.
donbox(2017-09-27 16:13:05):Thanks Ed. 

What's the protocol on this project to actually push this to master?  This is my first PR  with y'all :-)
dzhulgakov(2017-09-27 16:56:21):The split looks good, thanks!

For Library vs Model - I agree it's better to have it as a separate. I'd even argue that OpLibrary definitions should go to a separate .proto file as the semantics are different (defining model vs codifying framework support of operators).

As for initializers - let's keep them in GraphProto for now. Though we'd need to define semantics of "initializers" for inline GraphProto better. (cc @ezyang @bddppq for their opinion)

@bddppq can help regenerating stuff on AWS (maybe we should put instructions somewhere? :) )
ezyang(2017-09-25 16:13:04):This comment is out of date; now that we have a free form string field, we should just put the tag in `producer_version`. So, the `producer_tag` should just be something like `pytorch` or `cntk`.

With this in mind, it might be better to rename this to just `producer` (rather than `producer_tag`). I also wonder if we shouldn't also fold in the version number with producer, so that we have one less field here (I'm ambivalent about this though.)
donbox(2017-09-25 22:27:57):Agreed on the rename and changing the comment text to move any versiony stuff to producer_version. I thought about doing this earlier but I was trying to minimize the BC.  As a nit, I'd use "producer_name" rather than "producer" but I'm not super convicted. 

Ambivalent about combining the fields. 

Unless someone objects, I'll push the producer_tag->producer_name rename.
donbox(2017-09-26 09:53:36):I just pushed the change suggested above.
bddppq(2017-09-27 22:53:58):@donbox Is there any reason to not use 7 here? :-)
bddppq(2017-09-27 21:18:27):I vaguely remember on my macbook I need to export this as "10.12"
ezyang(2017-09-28 02:21:43):10.9 empirically worked on my macOS Sierra 10.12.6, so I am not sure what is going on here.
bddppq(2017-09-28 06:54:06):Oh actually it was installing pytorch, not onnx.

**Update**: I see the same error when installing onnx:

`error: $MACOSX_DEPLOYMENT_TARGET mismatch: now "10.9" but "10.12" during configure`
My python (2.7.13) was installed with brew and I'm using a virtualenv. The installation actually works when `$MACOSX_DEPLOYMENT_TARGET` is not set.

ezyang(2017-10-02 03:28:31):Could it be that you are affected by https://bugs.python.org/issue9516 ? This is inconsistent with the version number you reported which should have had this bug fixed. Could you post a longer trace about this issue?

Note that the semantics of `MACOSX_DEPLOYMENT_TARGET` is that it specifies the *minimum* Mac OS X version that we want to be able to run an executable on. So lower is better.
bddppq(2017-10-03 06:49:24):Unfortunately the stack trace doesn't have any useful information:

```
$ MACOSX_DEPLOYMENT_TARGET=10.9 python setup.py develop
zip_safe flag not set; analyzing archive contents...

Installed /private/tmp/onnx/.eggs/pytest_runner-2.12.1-py2.7.egg
running develop
running create_version
running egg_info
creating onnx.egg-info
writing requirements to onnx.egg-info/requires.txt
writing onnx.egg-info/PKG-INFO
writing top-level names to onnx.egg-info/top_level.txt
writing dependency_links to onnx.egg-info/dependency_links.txt
writing manifest file 'onnx.egg-info/SOURCES.txt'
reading manifest file 'onnx.egg-info/SOURCES.txt'
reading manifest template 'MANIFEST.in'
writing manifest file 'onnx.egg-info/SOURCES.txt'
running build_ext
running build_proto
compiling /private/tmp/onnx/onnx/onnx.proto
building 'onnx.onnx_cpp2py_export' extension
clang -fno-strict-aliasing -fno-common -dynamic -g -O2 -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -I/private/tmp/onnx -I/private/tmp/onnx/third_party/pybind11/include -I/usr/local/Cellar/python/2.7.13/Frameworks/Python.framework/Versions/2.7/include/python2.7 -I/usr/local/Cellar/python/2.7.13/Frameworks/Python.framework/Versions/2.7/include/python2.7 -c /private/tmp/onnx/onnx/cpp2py_export.cc -o build/temp.macosx-10.12-x86_64-2.7/private/tmp/onnx/onnx/cpp2py_export.o -std=c++11 -stdlib=libc++
error: $MACOSX_DEPLOYMENT_TARGET mismatch: now "10.9" but "10.12" during configure
```

I suspect it's my python (which is from brew) was configured to "MACOSX_DEPLOYMENT_TARGET=10.12" when it was built.
dzhulgakov(2017-09-28 03:41:22):What's the general procedure for updating the op specs? It means that we'd need to change backend implementations (like in Pow case)
ebarsoum(2017-10-03 15:51:34):I updated this PR, based on all feed backs. Can I get a sign off? 
bddppq(2017-10-03 16:20:38):How about the "axes", "starts" and "ends" attribute in the Slice op? Shouldn't they be deleted since now they are inputs?
jerryzh168(2017-10-04 22:23:36):@bddppq I think it's better to remove inputs and keep attributes, what's the argument here to keep/remove either one? I just did a translation of Crop in C1 to Slice, and used attributes because inputs requires the type matching and somehow I didn't manage to get it right.(it's complaining that it was given a long long type but I have explicitly cast it to int), So I think there will be less pain to move these things to attribute.
bddppq(2017-10-04 22:44:12):@jerryzh168 I actually like to keep both too, it's much more practical for frontends (no need to create extra constant tensor to host values that are already known). But there was an offline discussion that come to the conclusion we should keep only one of input & attribute.
@ezyang @ebarsoum @yuanbyu
ezyang(2017-10-05 16:22:24):I summarized the offline discussion, and future things we have to do, here; https://github.com/onnx/onnx/issues/76
dzhulgakov(2017-09-28 03:38:15):actually C2 implementation allows to specify starts and ends as arguments or inputs. Thus support for (3) here. @ezyang - do we rely on it anywhere?
dzhulgakov(2017-09-28 03:39:25):probably we don't want to allow for the second argument to be consumed
bddppq(2017-09-28 05:21:31):I would prefer keeping this attribute. Frontends could already know the exponent and it will be inconvenient (also inefficient) to always create an extra input tensor).
bddppq(2017-09-28 05:47:42):Just to make sure I understand this, so `axes=[1, 0]` means `data[starts[1]:ends[1], starts[0]:ends[0]]`? If so, since these are attributes, so I assume they are already known at the time frontends export onnx, why couldn't frontends shuffle starts and ends?
ebarsoum(2017-09-28 16:07:49):Why do we need both: attribute and input?
ebarsoum(2017-09-28 16:08:01):Sure
ebarsoum(2017-09-28 16:09:32):The problem is we have case in which the exponent can change, specially for custom learner. We added exponent as input in order to support Keras backend and other scenario. Atteribute will be limiting in this case.
ebarsoum(2017-09-28 16:13:47):This isn't for shuffle, actually the above won't work. Let's say you have 5D tensor as input, and you want to Slice the 2 in the middle, you can say: axes = (2,3), start=(1,1), end=(4,4). So axes, specify the list of axes in which you apply the start and end indices, len(axes) = len(starts) = len(ends). This is better, then specifying all the axes in starts and ends, and most of the starts and ends are the entire dimension.
bddppq(2017-09-28 16:37:54):I'm not against adding exponent as Input for dynamic cases, I'm saying let's keep the attribute for static cases.
bddppq(2017-09-28 16:38:13):I see that makes senses
ebarsoum(2017-09-28 20:28:22):@bddppq  what the advantage? static case could use the argument also? What if the user specify both?
bddppq(2017-09-29 02:58:28):Copied from my first comment:

> Frontends could already know the exponent and it will be inconvenient (also inefficient) to always create an extra input tensor).

In caffe2 we (will) disallow user specifying both.
bddppq(2017-09-29 03:00:23):The reason is exactly like adding `exponent` as input in Pow op: for dynamic use cases :-)
dzhulgakov(2017-09-29 04:56:41):Another way is to allow -1 to denote "the rest" like in Python and always specify the sizes. TF does the same: https://www.tensorflow.org/api_docs/python/tf/slice

Same with C2: https://github.com/caffe2/caffe2/blob/01827c153db96349745a544148c1ff0386c5ef9e/caffe2/operators/slice_op.h#L59

I feel it's more natural. What do you think?
ebarsoum(2017-09-29 05:20:52):It look weird to me to have one function support both instead of split them, and not all OPs require dynamic input. If that the case, we can promote all our attributes to input.s
ebarsoum(2017-09-29 05:50:42):-1 mean read from the end.
ezyang(2017-10-02 14:51:07):PyTorch's export which uses slice always uses slice's inputs as constants, and we currently fake up a tensor in order to support it. There are other operators that PyTorch supports which would make use of the dynamic behavior.
fmassa(2017-10-02 15:01:26):Note that TF uses inputs for both `Transpose` and `Reshape` instead of attributes for the permutation / new shape.
bddppq(2017-10-02 23:37:03):@ebarsoum Since there will be usecases that needs `starts` and `ends` to be dynamic, we need to add them back to Input (and remove them from attributes). Since "axes" is tied to "starts" and "ends', I believe it should be converted to input as well.
jerryzh168(2017-10-04 22:32:14):As an fyi, in c2, slice don't have "axis" and user needs to specify the starts and ends of each dimension, and currently c2 only supports slicing one of the dimensions. We might need to change c2 implementation when this is merged.
dzhulgakov(2017-10-04 23:09:54):Yeah, I mean that instead of `axis` you can fill other dimensions with [0, -1]

In other words for 5D tensor and axes = (2,3), start=(1,1), end=(4,4) it'd become
`start=(0,0,1,1,0), end=(-1,-1,4,4,-1)` and it's more explicit.
dzhulgakov(2017-10-04 23:11:03):clarify whether the end is inclusive or exclusive?
jerryzh168(2017-10-06 04:01:04):Also in numpy/matlab we need to specify all the dimensions in array slicing. I think the definition of c2 slice op is probably borrowed from numpy.
donbox(2017-09-29 00:40:39):LGTM
donbox(2017-09-29 00:53:45):Actually, in helper.py, you are adding attributes to ModelProto, which doesn't have them.

We only have defined semantics for attributes on NodeProto, which has the attribute field and has the kws parameter on make_node already.. 
bddppq(2017-09-29 02:30:18):@donbox The `attr` in `setattr` is different from the `attribute` in NodeProto. Also I added a test to make sure no garbage fields will be set on ModelProto: https://github.com/onnx/onnx/pull/66/files#diff-ddf8b839a977a7a48f41526fc7a82dbcR197
dzhulgakov(2017-10-02 04:27:47):Thanks a lot for putting it together.

I'm still a little bit concerned that we're trying to jam the generic type system (pretty much protobuf implemented in protobuf) into the spec. The main concern is that it makes it harder to understand for framework authors and vendors trying to implement ONNX. It seems that vast majority of them (especially vendors) would target only dense tensors for foreseeable future. On the other hand I understand your to express data preprocessing and traditional ML operations as part of the same graph.

Thus, it'd be really interesting to figure out a way to arrange the spec in "tiered" fashion. The "onnx minimal" would include effectively the current spec - with dense (and maybe sparse) tensors only and would be something vendors can initially target. The expanded spec would have all the types we'd want to include.

Second concern is that ValueProto effectively becomes a struct replicating arbitrary Protobuf. Using it this way is less convenient (as instead of writing `foo.x = 2` it'd be something like `foo.fields.append({key = 'x', value = ValueProto(TensorProto(2))})`) and also less space efficient (as we'd need to serialize record fields over and over. In some sense, having such a generic spec doesn't really standardize much.

With two issues in mind we can try to figure out some way of splitting them. Below some of the ideas.

**Approach A**. Separate non-tensor types to a separate proto for readability purposes. Basically means moving all internal pieces of ValueProto (except for TensorProto) to `onnx-extended.proto` and importing it in the main `onnx.proto` file. We can say in the spec that everything from `onnx-extended` belongs to the extended tier of the spec. Note, that it doesn't change serialization at all, just the .proto readability.

**Approach B**. Codegen script that would take the source file and produce binary-compatible `onnx.proto` and `onnx-extended.proto`. The simple way to implement it would be to add comments in the proto file like `//EXTENDED BEGIN` and `//EXTENDED END` and cut them out in the simplified version. We might actually  require some script to autogen .proto files anyway, because of proto2/proto3 syntax support and the fact that C# generator requires proto3.

This approach improves readability and makes it clear which part needs to be implemented. The tricky part would be figuring out which version of .proto the client code depends. We could have separate compilation targets for simple and extended versions and have two modes of compilation for the associated C++ library.

**Approach C**. Do type-erasure for extended types. I.e. have `ValueProto`/`TypeProto` being a `oneof` of Tensor, SparseTensor or `value (bytes)` + `typeid (string)`. Then for most common cases like tensors we'd serialize the value directly. For extended types, framework would first serialize the value into a string and just stick it to the serialized value. We wouldn't enforce much of what serialization format looks like, but can describe it for most common types.

This approach is actually adopted by Caffe2. It has BlobProto (equivalent of ValueProto) that has built-in support for Tensor (as the most common type) and opaque `contents` field paired with `type`: https://github.com/caffe2/caffe2/blob/master/caffe2/proto/caffe2.proto#L302 There is support for arbitrary objects serialization (e.g. maps) that can "registered" with the type name (see e,g. https://github.com/caffe2/caffe2/blob/01827c153db96349745a544148c1ff0386c5ef9e/caffe2/operators/map_ops.cc#L35 though it's not very-well designed map support).

**Approach D**. Type erasure + using protobufs for serialization. It's effectively the Approach C but with additional details in the spec clarifying that the serialization of values is codified as individual protobuf messages, also present in the onnx repo. In this case we wouldn't need generic ValueProto and instead would have operator-family-specific types when necessary (e.g. maybe having ImageProto in a separate file which is still part of the repo).

**What about record types?** One example that seemed to be brought up above is support for "record types", e.g. for describing image + pixel type as a single struct from input. In order to answer whether we need this type or not, we need to decide which level of IR onnx represent. For simple fixed records, one can always break down the batch of record into several separate tensor inputs and use graph operations directly. For example, instead of `list<record<field1, field2>>` we'd have two tensors `field1` and `field2` with batch size as the first dimension. This is usually a standard representation of things in DL frameworks anyway. The benefit is that we don't have to have generic operators like GetField and can leave to the application to decide how to prepare the input. In my opinion, feature preprocessing pipelines are diverse enough and it's rather hard to codify all of them in the single spec and graph definition.

What do you think of the above reasoning? I'd personally go with approach D if possible (or at the very least B, to keep things tiered).

cc @donbox @ezyang @soumith @yuanbyu 
soumith(2017-10-02 04:43:10):I haven't been following the protobuf changes as closely as Dmytro and YQ, but looking at this inception style proposal, it seems like we're going full steam to make sure most of the hardware vendors never implement ONNX in-whole, and hence it becomes a broken / fragmented spec.

Can we figure out a tiered and cleanly isolatable "lite" spec or "level-1" spec that doesn't demoralize hardware vendors too much?
That's my high level ask, since I haven't been participating in the low-level how-to-do-this, I wont join in now.

If folks have any doubts about the need for a lighter more narrowly scoped spec to not scare away vendors, I think we can provide testimonials / needs directly from vendors. Most such partners are going for ConvNets + RNNs, or in some cases only ConvNets.
donbox(2017-10-02 16:25:53):@dzhulgakov, great analysis of the proposal. Thanks for writing up your thoughts.

FWIW, we considered your proposal D above before putting this up here (and I would definitely do D over C). 

The upside of D over the current payload is:
1. The spec is *way* simpler - no "protobuf in protobuf." 
1. The PB files will be smaller - especially for dense graphs. 

The downside of D over the current payload is: 
1. Attribute values become completely opaque to frameworks/runtimes. Not fatal, but this definitely limits the ability to do any pre-processing/validation of attribute values prior to initializing the operator, which is the only piece of code that can parse the blob.
2. When we eventually do an operator schema, we can address the previous issue, but that will potentially push the complexity around, not reduce overall complexity. 
3. If we look at the DNN operators in CoreML, a bunch of the attributes will become blobs.  For better or worse, one framework/runtime made a different stylistic choice from Caffe2. 
4. Once we allow non-tensor values as input/output values, we are going to wind up needing at least the TypeProto additions. If we allow non-tensor values as constants and/or initializer values, we'll wind up with ValueProto.

I'm sensitive to the desire to stratify that spec based on DNN vs. non-DNN operators. That's not going to save us from solving the structured attribute problem and, at least for image, some form of additional typing on input/output values.

donbox(2017-10-02 16:26:45):@soumith - it would be good to triangulate which individuals at NV et al are balking.  I'm not hearing it from our side.
donbox(2017-10-02 22:50:33):I just added a comment that makes it clear that for non-tensor input/output values, DNN-only implementations MAY elect to not implement them and that new DNN operators SHOULD not use them .
dzhulgakov(2017-10-25 07:29:44):Based on our offline discussions - let's figure out a way to do cosmetic split into a separate file. Doing simple preprocessor and committing the final result might be a good option. Do you want to take a stab at it?

One thing I wanted to get better idea of (out of curiosity) - what is the target use case for arbitrary-valued attributes? I understand allowing of non-tensor values flowing through the graph, but when would you need arbitrary nested attributes?
ezyang(2017-10-30 14:12:10):Obsoleted by #131.
gramalingam(2017-09-29 05:34:47):Why not move TensorShapeProto into TensorTypeProto (as an optional field)?
gramalingam(2017-09-29 05:42:23):Why "distinctly"?
gramalingam(2017-09-29 05:51:57):Is that a NameValuePairProto? Didn't see a NamedValueProto ... maybe I missed it.
gramalingam(2017-09-29 06:01:02):I am not sure how to interpret the elem_shape. I feel moving TensorShapeProto into TensorTypeProto will eliminate the need for this.
gramalingam(2017-09-29 06:02:01):Same comment as for elem_shape above.
gramalingam(2017-09-29 06:04:40):This will help address the issue of how the shape is described when we have, e.g., a sequence of records with one field of type tensor.
NiklasGustafsson(2017-10-01 03:19:07):(BTW, new to this thread, but I've been working with Yuan, Don, and other at Microsoft discussing ONNX).

I agree with Rama, moving shape into the tensor type declaration would remove the need to have it appear wherever 'TypeProto' appears. It's really only applicable to tensors. In this case, it would appear that the shape is an attribute of the sequence, when in fact, it is an attribute of the element type.
NiklasGustafsson(2017-10-01 03:24:56):I presume it indicates that the field value types are heterogenous, unlike a map where value types are homogeneous. Is that what 'distinctly typed' refers to?
NiklasGustafsson(2017-10-01 03:27:12):Where is this enum used? I couldn't find a reference to it.
donbox(2017-10-01 04:43:51):Yes. Heterogeneous.  If someone has better language, party on.

donbox(2017-10-01 04:45:44):Notationally, moving TensorShapeProto inside of TensorTypeProto makes a ton of sense, as I can't imagine anywhere one would use TensorTypeProto and not have a TensorShapeProto on the side.

Yuan makes a valid point that shape inference can and will often happen distinctly from type inference. I don't think the notational change that you guys suggest impacts that.
donbox(2017-10-01 04:46:25):It gets emitted by the protoc compiler and is available as a constant in language bindings.  Look fro IR_VERSION in the .py files.
NiklasGustafsson(2017-10-01 14:50:49):That's right, this is a syntactic change only, but it's one that makes it clear that shapes go with tensors, not with other types. Shape is a property of tensors, so it belongs in the type.

That shape inference and element type inference use separate logic and may flow independently should not be impacted by this, as you say, notational change
dzhulgakov(2017-10-02 03:41:23):it's best to separate Attributes because they'd rather have very simple types (hopefully)
yuanbyu(2017-10-02 15:56:26):I wouldn't want to put both under the name of `TypeProto`, as it is more than type in the traditional sense. So if we want to put them together, how about naming it `TypeShapeProto`?
gramalingam(2017-10-02 15:58:18):I am fine with either name, but bundling them together into one unit makes sense to me
NiklasGustafsson(2017-10-02 15:58:48):That's fine with me. The name is less significant to me than having the two together as a unit.
NiklasGustafsson(2017-10-02 16:18:40):BTW, I opened issue #68 to track a decision on this.
dzhulgakov(2017-10-25 19:21:48):delete the comment?
dzhulgakov(2017-10-03 17:47:36):you need to gate only the type part with `type_required`, similarly for `shape`
dzhulgakov(2017-10-03 17:48:53):we should also validate value_info, I guess
bddppq(2017-10-03 17:50:19):@dzhulgakov Both elem_type and shape are stored together in the `type` field. `type_required` here actually means elem_type is required to present.

Update:
Oh I see what you are saying, ignore my comments above.
bddppq(2017-10-03 17:07:37):typo
bddppq(2017-10-03 20:01:20):`ft`
ezyang(2017-10-05 13:25:55):ONNX doesn't mandate implementation, right? So I'd reword this as, "This operator is usually implemented via CuDNN"
ezyang(2017-10-05 13:26:23):Add a space here?
bddppq(2017-10-03 16:40:57):LGTM
dzhulgakov(2017-10-03 17:45:46):you need to describe in which order the pads are listed. Is it axis_0_begin, axis_0_end, axis_1_begin, ... ?
houseroad(2017-10-03 17:47:48):Yeah, I will add it to the doc. 
dzhulgakov(2017-10-05 03:59:43):Looks good
houseroad(2017-10-06 19:35:26):LGTM. :-)
bddppq(2017-10-13 00:34:54):ChannelShuffle can be represented with Reshape + Transpose
ebarsoum(2017-10-10 18:50:13):Please add the semantic and behavior of this OP in the Doc, It is not acceptable to just reference a paper.
bddppq(2017-10-10 02:48:49):I think onnx-caffe2 doesn't need to be updated because you have aggregated the test cases into  `self.tests` for backward compatibility.
bddppq(2017-10-10 02:37:25):It's better to be consistent with `run_model`, so either make `device` a required args or make `run_model` also has the default device. I understand the former is a breaking change, but hey we have a lot of breaking changes already right :-)
bddppq(2017-10-10 02:45:39):Please add 'device' keyword arg to this too.
dzhulgakov(2017-10-10 03:15:36):Let me actually make device an optional arg, it's a better choice imho
bddppq(2017-10-10 03:29:23):nit: missed a `,` in the tuple, but `numpy.empty` can take single int as shape as well.
ezyang(2017-10-11 15:44:36):Aaawkward :) (Awkward in the sense that there is a special case for string here.)
brettkoonce(2017-10-11 20:53:45):See also #107.
bddppq(2017-10-11 21:10:05):@brettkoonce The `Operators.md` file is generated, could you run `python onnx/defs/gen_doc.py` to regenerate it?
brettkoonce(2017-10-11 22:37:31):Can't figure out where these are coming from:

usethe => use the
GlobalMaxPooll => extra l?
flatenned => flattened
brettkoonce(2017-10-14 22:28:47):@bddppq thanks for the tips!
bddppq(2017-10-16 09:07:27):Looks good! Could you also check in the `*defs.cc` source files changes?
brettkoonce(2017-10-16 17:56:32):@bddppq I tried, I don't seen to be able to find any corresponding code?
bddppq(2017-10-17 04:16:51):I merged master to your PR and then the changes in `Operators.md`disappeared.
bddppq(2017-10-17 04:17:26):Thanks for your contribution!
brettkoonce(2017-10-17 17:26:24):@bddppq Ahh, that makes sense.
bddppq(2017-10-12 05:18:47):This doesn't match the current `AveragePool` doc at https://github.com/onnx/onnx/blob/master/onnx/defs/nn/defs.cc#L35. Is it possible your working copy is outdated? Note after changing C++ files you need to reinstall in order to get the changes effective.
ezyang(2017-10-12 15:14:27):This sounds like a good reason to move the spec files out of C++ :)
bddppq(2017-10-12 17:42:57):On the other hand you want to have these operators schema in c++ for being able to do validation on any environment. :-)
brettkoonce(2017-10-13 18:18:58):which files do I need to re-install/what is being pulled in exactly here?
bddppq(2017-10-14 04:45:57):How did you install onnx at the beginning? The procedure of changing docs in `Operators.md` is first update the `*.defs.cc` file, and then install the new sources into your python environment (there is a "Source" section in our repo frontpage telling you how to install from source) and then run `python onnx/defs/gen_doc.py` to get the new `Operators.md` file.
bddppq(2017-10-22 00:39:13):Continue in #127 
dzhulgakov(2017-10-13 04:42:21):can we clarify explicitly that it's symmetrical padding? (or did we land on asymetrical)?
ebarsoum(2017-10-15 00:21:26):@dzhulgakov  what you mean adding SAME/VALID setting? I assume that will replace current setting and not in addition, right?

When I added 'pads' attribute initially, I meant to have 1/0 per dimension, 1 mean SAME and 0 mean VALID. However, I didn't mean upper and lower per dimension. I have no problem to use SAME/VALID settings. I don't won't end user to specify asymmetric padding, if needed they can use ONNX.pad OP.

dzhulgakov(2017-10-17 08:11:42):So I think there's actually discrepancy here. Both C2 and PyTorch just specify the list of padding in pixels for each dimension.

Specifying the VALID/SAME for each dimension independently is a bit unnatural and I don't think any framework supports it except for CNTK. Do you see a lot of use cases for it actually?

I think the reasonable support would be to have either:
1) arbitrary padding in pixels per dimension. This is the most general and matches for example TensorRT/PyTorch/C2.
2) one global setting of SAME to keep compatibility with TF.

That's the kind of setup we discussed in person last Wednesday on the sync. What do you think about it?
gramalingam(2017-10-12 16:44:38):A naïve question: is axis numbering zero-based or one-based? The preceding lines (318,319) start with d_1, but this line implies zero-based. Doesn't one of them have to be changed?
bddppq(2017-10-12 17:18:40):Oh it should be zero based, will update the doc.
prasanthpul(2017-11-07 01:47:25):LogSoftmax is now also in #203 
houseroad(2017-11-07 03:50:40):"Ah, I was dropped" 

:-)
ezyang(2017-10-14 03:50:34):I wish we had a way of not copy pasting things like this. It will make the spec harder to edit if we need to change how these mechanics work.
bddppq(2017-10-16 09:04:28):Thanks!
krpopo(2017-10-18 04:13:26):I confirm that this patch works for me.
ebarsoumMS(2017-10-18 21:46:17):Hi @ezyang what you mean by don't hardcode Windows, some setup logic is different between Windows and Linux?
linkerzhang(2017-10-18 21:48:44):@ezyang, "/MT" compile option is only needed for windows, I think. Copying /MT description as below,

Causes the application to use the multithread, static version of the run-time library. Defines _MT and causes the compiler to place the library name LIBCMT.lib into the .obj file so that the linker will use LIBCMT.lib to resolve external symbols.

Make sense?



ezyang(2017-10-19 00:20:18):@ebarsoum Do any folks at Microsoft want to setup Windows CI build for onnx? Would be good to prevent this from bitrotting.

@linkerzhang Thank you for this! It all looks good to me.
linkerzhang(2017-10-19 04:44:27):@ezyang Good point! we should set up windows CI build. We'll have someone to do it :). 
bddppq(2017-10-16 19:49:19):Don't make it required. If a user has installed protobuf to system paths, it's legit to not set it.
bddppq(2017-10-16 20:09:32):`\` is not cross platform. `os.path.join` can handle this if you split it into separated parts.
Shared library suffixes are different on varies platforms (e.g. `.so`, `.dylib` ...), so it's better to let distutils/setuptools to handle it for you. (i.e. setting `self.libraries`, `self.lib_dirs`and `self.rpaths`).
linkerzhang(2017-10-17 22:21:09):Make sense! Thank you!

Fixed it accordingly.
ezyang(2017-10-18 04:16:02):This is not portable. You should test if you're on Windows before doing this.
ezyang(2017-10-18 04:16:12):Ditto here.
linkerzhang(2017-10-18 05:37:43):Sorry, what do you mean "not portable" please? This is similar with the line above, but adding one more include dir which contains protobuf header files. 
linkerzhang(2017-10-18 05:42:56):it works in windows btw :).
krpopo(2017-10-18 14:06:33):It works in Windows, but it breaks for other platforms. If you build in Linux conda environment, `self.libraries` should be `['protobuf']`, not `[os.path.join(os.getenv('CONDA_PREFIX'), "Library", "lib", "libprotobuf")]`.

Your `if` only checks for conda, but you should also check for platform
linkerzhang(2017-10-18 15:57:35):Thank you very much! Limiting it only setting include dir and library for windows only now.
ezyang(2017-10-19 03:11:09):This looks good to me, speaking for PyTorch.
ebarsoum(2017-10-20 01:42:49):Any update? The above was actually a bug in the spec, arg op with multiple axes is meaningless.
bddppq(2017-10-19 22:24:21):If we wanna go fancy, use ```<sub>`Experimental`</sub>``` to get <sub>`Experimental`</sub> :-)
brettkoonce(2017-10-19 21:30:23):Is this still true?
bddppq(2017-10-19 22:20:10):Good catch, not true anymore after this change :-)
houseroad(2017-10-20 03:55:28):LGTM :-)
dzhulgakov(2017-10-20 01:19:41):magic! :)
houseroad(2017-10-20 04:00:08):Minor: slice(None) may be more concise. 
bddppq(2017-10-20 18:39:19):Overall LGTM
fmassa(2017-10-21 00:27:53):Do you plan to follow the same semantics of TF for the SAME case? Because it's not equivalent to just add `pads = (kernel_size - 1 )/ 2`, but it depends instead on the input size being even or odd as well.
ebarsoum(2017-10-21 01:12:41):@fmassa that is a good question, it primarily depends if the resulted computation is symmetric or non-symmetric. @Yangqing and @yuanbyu any opinion on this part? Should the behavior be similar to TF (append the extra pixel to the end part of the dimension)?
dzhulgakov(2017-10-25 05:37:35):I think the point of adding SAME is exactly to emulate TF behavior. Otherwise we could have expressed it with `pads` property
ebarsoum(2017-10-26 23:26:15):@dzhulgakov not exactly, the main reason is to cover most cases easily.
bddppq(2017-10-20 18:13:55):Typo: "thecorresponding"

Could you also clearly state that the added pixels will be padded as 0?
bddppq(2017-10-20 18:17:45):I agree with this change, but please point it out in the PR, it's so easy to overlook. 
bddppq(2017-10-20 18:18:47):"and SAME"
bddppq(2017-10-20 18:38:49):Do we want to bikeshed the attribute name here? :-) @ezyang @dzhulgakov 
ebarsoum(2017-10-20 20:50:43):I don't care about the name, it just need to be different than pads.  So let me know which name you guys prefer.
bddppq(2017-10-20 18:42:58):It surprises me that a linter can find this out
houseroad(2017-10-20 18:51:52):Yup, quite helpful.
bddppq(2017-10-20 23:04:31):Could you follow the instructions of how to install from source on the front page, and then run `python onnx/defs/gen_doc.py` to update the auto generated `Operators.md` file?
bddppq(2017-10-22 00:35:11):Thanks!
gramalingam(2017-10-20 22:47:41):I have a basic question: what is the compatibility goal/requirement between the two protobuf definitions? 
ezyang(2017-10-21 03:30:39):@gramalingam They need to be binary compatible. We can reverse engineer this using the backward compatibility rules (https://developers.google.com/protocol-buffers/docs/proto) which say: (1) don't change any numeric tags, (2) make all fields optional. So, as long as the transformation from ONNX to ONNX-ML is a "backwards-compatible change", the two protobufs are compatible.
donbox(2017-10-25 18:41:15):Sorry for the delayed reply - I've been sick and am just now starting to be lucid enough to do any technical work.

I just updated dbox/add_map_record_sequence (a.k.a. PR67) to clearly delinate what we'd trim or not depending on the target.  

I simply used #if/#endif inside of a proto // comment as a placeholder.

I'm less worried about the python generator (which as you say, is a no-op).

What we should get agreement on is:

1. Is onnx.proto.in a valid proto file, at least syntactically?   As someone who edits the file, I'd prefer that my text editor's language services don't break if we can achieve that.

2. For conditional inclusion of specific declarations (e.g., messages, fields, etc), I think we can use an embedded-in-comments ifdef syntax. Honestly, that will keep us honest wrt colliding field IDs.  We should decide whether we want #if or #iddef semantics. For our purposes, I'd strongly favor #ifdef semantics.

3. I don't have a good answer for how to deal with the optional keyword. If we are going to treat proto2-vs-proto3 as a first-class thing, I assume we could simply bulk inject it (or bulk remove it).  I'm loath to add a general #define.
donbox(2017-10-25 18:42:34):I think we should really use the proto2/proto3 thing to vet this design.   @ezyang  are you up for combining that with this PR?   Solving for the PR67/ONNX-ML stuff is trivial compared to solving that problem. 
dzhulgakov(2017-10-25 19:24:17):One suggestion - add verification to Travis similarly to #136?
ezyang(2017-10-26 16:43:00):OK, it is now wired up to do ONNX/ONNX-ML output, as well as proto2/proto3 output. I based this on top of PR #67. CI is not done yet and will be added.
ezyang(2017-10-26 16:45:19):We also need to add tests that the proto2-proto3 files are binary compatible. This is kind of annoying to do since you can't link both versions of the protobuf in the same process (they clash in the protobuf library's global namespace.)
prasanthpul(2017-10-26 20:15:39):@ezyang are you adding those tests or you want someone else to contribute them?
ezyang(2017-10-26 20:31:51):I spent some time trying to add them, but it seems to be irritating to do this in a robust way. I wanted the test to be invariant to proto changes; i.e., you could change your proto, and if you introduced a breaking feature, the test would automatically flag it; you wouldn't have to remember to update some test files or something like that. But this rules out having some pre-computed model files that you test for input in proto2 and proto3 (since, definitionally, they won't test new features.) I experimented with https://github.com/trailofbits/protofuzz for randomly generating protobuf, but their generation strategy is not robust enough (for one, it infinite loops on recursive structs.) At this point I'm happy to punt this to someone else.

Alternately, we can just do a few informal tests and call it a day.
ezyang(2017-10-26 20:42:42):@prasanthpul Nice catch! I pushed a commit that fixes it.
prasanthpul(2017-10-27 20:00:03):@ezyang let's keep more automated tests for proto3 as a separate item
ezyang(2017-10-30 14:11:16):As soon as CI is green I intend to merge this.
prasanthpul(2017-10-26 20:22:13):nit: when "optional" is stripped the trailing whitespace is left behind causing different indentation for proto3 vs proto2 versions.
bddppq(2017-10-23 17:52:22):Thanks!
bddppq(2017-10-23 18:20:13):This is not the only (nor officially recommended by google) way to install protobuf, we should instead point users to the protobuf installation guide.
linkerzhang(2017-10-23 18:22:47):So adding same line as "You will need an install of protobuf and numpy to build ONNX. One easy way to get these dependencies is via Anaconda:" looks good to you please?
mkolod(2017-10-26 16:49:22):@bddppq could you look at this please?
gramalingam(2017-10-25 21:30:53):Should we preserve the tag-values of the previously existing fields, and use "AttributeType type = 12" instead?
gramalingam(2017-10-25 21:32:40):Gives us a bit more compatibility
ezyang(2017-10-26 19:01:44):This LGTM, assuming we decide hex semver is what we ultimately want.
ezyang(2017-10-30 14:37:50):I force pushed a rebase, and also the necessary changes to work on top of onnx_ml.
ezyang(2017-10-30 14:37:59):I plan to merge after CI is green.
dzhulgakov(2017-10-25 22:11:41):doesn't really match the comment?
dzhulgakov(2017-10-25 22:12:14):there's already https://github.com/onnx/onnx/blob/b8ab0ed286ffd0b0020a367e078f66bdc3303db5/onnx/defs/schema.h#L151 - maybe use the enum from here and update the library code?
dzhulgakov(2017-10-25 22:12:57):even with BC we shouldn't break proto compatibility (i.e. be able to deserialize). Let's keep the ids for existing fields.
donbox(2017-10-25 22:57:09):Fixed.  Also, I switched to hex constants, as using octal is pretty esoteric. 
donbox(2017-10-25 23:13:40):Good catch. 

I just changed the proto definition to have the same enum values as schema.h for now. 

The simplest way to actually do this right is to change:

    enum class AttrType {

to 

    typedef <identifier-generated-by-protoc> AttrType;

Let me know if you have a better idea.  I'd rather not scrub all of the call sites to pull on the pb.h identifier directly.


donbox(2017-10-25 23:21:34):Good point. 

I think what you are advocating is this:
    1. Allow pre-1.1 implementations to simply infer the discriminator value for 1.0 files.
    2. Require 1.1 or later files to have the field written.  Absent (which defaults to 0 which is FLOAT) would always be treated as a FLOAT and any has_field heuristics would be illegal.

If we do this, then technically this isn't a breaking change (which I like!).

Given that, I'd suggest we simply bump the bugfix number in the version.

donbox(2017-10-26 04:56:24):Actually, I looked at doing the above, and it's pretty grotesque.

The way enum identifiers get mangled by protoc for both Python and C++ is suboptimal.

To get reasonable Python bindings, I moved the enum to nest inside of AttributeProto. Otherwise, we wind up with top-level identifiers like FLOAT and TENSORS, which feels pretty bad.

However, the generated C++ is pretty bad either way.

If I don't nest AttributeType, then the generated C++ looks like this:

    enum AttributeType {
      AttributeType_FLOAT = 0,
      AttributeType_INT = 1,
      AttributeType_STRING = 2,
     ...

If we do nest AttributeType to make the python language binding reasonable, then the generated C++ looks like this:

    enum AttributeProto_AttributeType {
      AttributeProto_AttributeType_FLOAT = 0,
      AttributeProto_AttributeType_INT = 1,
      AttributeProto_AttributeType_STRING = 2,
     ...

Either way, I'd need to touch a lot of lines of C++, most of which is relatively immune from the proto file.

To that end, I'll add a warning in both files indicating that they should be revved in sync and NOT try to have a single definition.



ezyang(2017-10-26 18:51:15):So, in #78 I mentioned that we might want to stop using protobuf nesting. Does this apply to enums as well?
linkerzhang(2017-10-27 20:35:18):Should we add an "Undefined" = 0 as the default one here? 
linkerzhang(2017-10-27 20:36:38):the version format was xx.xx.xxxx, so the version should be updated as 0x00010000 (0.1.0), no?
donbox(2017-10-28 22:25:57):For message types, we should absolutely stop nesting.

For enums, I would argue that we should nest unless we want to inject the individual enum identifiers into the global/package namespace.

For the example above, when I had it at the top level, the identifier STRING shows up as a onnx.STRING in python.  It's actually worse - if you look at AttributeType and the DataType enum from TensorProto, we would have two conflicting identifiers (that would at best accidentally have the same value).

I think our style should be NO nesting of messages and ALWAYS nesting of enums.  I'd leave IR_VERSION as is just for historical reasons.

Objections?
donbox(2017-10-28 22:26:52):Edward, until we decide to switch off of semver, let's not keep this PR out of master for that reason. I hate having hanging PRs we need to keep dealing with merges on. 
donbox(2017-10-28 22:27:47):@linkerzhang, We could, but why?  
donbox(2017-10-28 22:29:06):I'm assuming we are parsing little-endian based on the initial value we used (000000001), which I assumed was 1.0.0.  This PR simply bumps the version to 1.0.1, which is 0x00010001
linkerzhang(2017-10-28 22:50:31):If not, for the models generated with old version will always have "float" as attribute type by default, which may be not true.
linkerzhang(2017-10-28 22:53:23):I think the current version is 0.0.1 :)
linkerzhang(2017-10-28 22:53:25):I think the current version is 0.0.1 :)
linkerzhang(2017-10-28 22:53:27):I think the current version is 0.0.1 :)
ezyang(2017-10-29 01:07:10):Nope, you've convinced me. Nested enums it is!
donbox(2017-10-29 01:11:56):We resolved offline that the spec in master is at 0.0.1.

I'm writing a proposal (that will get checked in as part of the ONNX spec) for how we will deal with all of this so we don't have to have this discussion multiple times :-)
donbox(2017-10-29 01:15:26):Good catch.  Let me write down the versioning spec (I'm going to write the IR rules first since we are close - op versioning needs more bake time).

For all of our enums, we will need to steal 0 as undefined, which is a protobuf best practice.  @dzhulgakov , when I make this change, do you want me to add an undefined enum in the corresponding C++ enum?

gramalingam(2017-10-27 21:40:03):It seems like "op_def" is meant more to support an implementation, but doesn't have any formal or semantic content. (I don't have an objection to having it, but just checking if this name has any other meaning.)

Adding a descriptive "documentation" string would likely be more useful.

Now that we have this: this seems like a convenient place to start adding other meta-data about the operator (like its type-signature, etc.). Of course, we need to agree on a type-system first (about which there is a discussion elsewhere, I guess), but once we do so, would that be reasonable?

I guess this all boils to one question (discussed earlier): on the role of operator-registry in onnx/defs vs. explicitly capturing as much meta-data as possible in protobuf format.

gramalingam(2017-10-27 22:02:10):Ok, please ignore part of my question, I read this before reading the new versioning proposal! At least I understand that op_def serves as a unique id. It's meaning is currently captured in the operator.md file, I guess. I still think it will be useful to expand the role of this definition to capture more meta-data about the operator right in the proto file, over time.
ezyang(2017-10-28 02:41:27):> I still think it will be useful to expand the role of this definition to capture more meta-data about the operator right in the proto file, over time.

Yes, I definitely think we're headed this direction. Actually, I would propose we put meta-data about operator definitions in a different proto file; one could definitely make use of the operator namespace proto without using any of the other type information, etc.
donbox(2017-10-31 08:13:53):Sorry I'm late to the party.  I've now swapped in a bunch of context.

Here are my basic quesoins:

1. We need to anticipate organizations defining their own operator libraries for internal use that may or may not ever be published externally.  We will always have ONNX as the repository for operators that frameworks can claim support for, but at least in our world, we also know that there will be private operator repos that will have the same version requirements as we do.

2. I worry that the name mangling will have a lot of cognitive overhead AND requires a chain of change logs in order to find the trail of versions. If we just used SemVer per operator, we'd have a much simpler conceptual model AND the ability to have compatibility rules defined once for both graphs and ops.

3. It's not at all obvious wher ethe ChangeLogProto goes. Is it a separate top-level file format? If so how do we version that format? How does one acquire them and do I need all of them to complete the chain?

4. I saw you added op_version to model proto.  What's the format? SemVer? That assumes we are sticking with SemVer for ONNX for the duration, which we should talk about.  Our versioning discussions have been around the components, not the whole.  If we are IN ANY WAY refering to the release version, then we need to define the rules for versioning releases as a whole. Yikes!!
ezyang(2017-10-31 15:29:46):Things to do:

1. Introduce vendor namespaces, so that vendors can also make use of the versioning mechanism for their private operators
2. Lay out the intent of defining machine readable format for the operator library
ezyang(2017-11-03 20:20:00):Obsoleted.
linkerzhang(2017-10-27 15:56:02):question please. For semantic changes of an existing operator (say, Slice you mentioned :) ), will we only have it in "added" (overwrite way) or we also want to have it firstly in "removed" and then "added"?
ezyang(2017-10-27 17:42:52):Yep. So let's say you have `_OldSlice` and `_NewSlice`. Going in, you'll have a version of ONNX with `Slice` mapped to `_OldSlice`. Then, to move to NewSlice, you write this changelog entry:

```
ChangelogEntry {
  removed = "Slice"
  added = OperatorBinding {
    op_type = "Slice",
    op_def = "_NewSlice",
  }
}
```

We could introduce a modify field if this is a little oblique.
linkerzhang(2017-10-27 21:05:27):We may just have a "new" to contain both "added" and "changed". Say
ChangelogEntry {
  new = OperatorBinding {
    op_type = "Slice",
    op_def = "_NewSlice",
  }
}

In the "new", the key is always over-written, if the key exists, then it's removing the old one and change to new one, if the key does not exist, it's purely adding a new one, and "removed" contains keys totally removed.
linkerzhang(2017-10-27 21:05:48):the name "new" may be not good although :) 
ezyang(2017-10-27 21:14:05):That's OK by me! My only wondering is whether or not we should be more explicit if an override occurs. "Adding" is a BC-compatible change, but "overriding" is not.
gramalingam(2017-10-27 22:14:12):So, I guess this is a MUST-be-present field?
ezyang(2017-10-28 02:39:29):Yep, definitely mandatory. But we could grandfather models missing this parameter by setting it to version 1.
yuanbyu(2017-10-31 15:45:47):@dzhulgakov As I understand it, this one is blocked due to the optional inputs issue #151? 
dzhulgakov(2017-10-31 23:51:30):Yep, let me hack #151.

In the meantime we also need to figure out what to do with peephole connections weights. If you have better suggestions on how to layout the weight matrices - please share.
yuanbyu(2017-11-03 16:49:11):@dzhulgakov @raduk @ebarsoum I have updated this PR as we discussed:

1. Removed OptimizedRNN and replaced it with 4 simpler ops: SimpleRNN, GRU, LSTM, BidirectionalLSTM. 
2. The current version doesn't support multi-layer RNNs. We will add it in a separate PR with a new version number.
AppVeyorBot(2017-11-05 23:53:06)::white_check_mark: [Build onnx 0.3.12 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.12) (commit https://github.com/onnx/onnx/commit/444e1898aa by @yuanbyu)
AppVeyorBot(2017-11-05 23:53:07)::white_check_mark: [Build onnx 0.3.12 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.12) (commit https://github.com/onnx/onnx/commit/444e1898aa by @yuanbyu)
AppVeyorBot(2017-11-05 23:59:26)::white_check_mark: [Build onnx 0.3.14 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.14) (commit https://github.com/onnx/onnx/commit/60f0e0d5ac by @yuanbyu)
AppVeyorBot(2017-11-05 23:59:26)::white_check_mark: [Build onnx 0.3.14 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.14) (commit https://github.com/onnx/onnx/commit/60f0e0d5ac by @yuanbyu)
AppVeyorBot(2017-11-06 00:40:43)::white_check_mark: [Build onnx 0.3.16 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.16) (commit https://github.com/onnx/onnx/commit/6b2c1049e5 by @yuanbyu)
AppVeyorBot(2017-11-06 00:40:44)::white_check_mark: [Build onnx 0.3.16 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.16) (commit https://github.com/onnx/onnx/commit/6b2c1049e5 by @yuanbyu)
AppVeyorBot(2017-11-06 00:44:54)::white_check_mark: [Build onnx 0.3.18 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.18) (commit https://github.com/onnx/onnx/commit/32e43e314b by @yuanbyu)
AppVeyorBot(2017-11-06 00:44:55)::white_check_mark: [Build onnx 0.3.18 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.18) (commit https://github.com/onnx/onnx/commit/32e43e314b by @yuanbyu)
yuanbyu(2017-11-06 14:44:55):@dzhulgakov I made a round of edits based on the comments. PTAL. Thanks.
AppVeyorBot(2017-11-06 19:31:00)::white_check_mark: [Build onnx 0.3.21 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.21) (commit https://github.com/onnx/onnx/commit/591158623e by @yuanbyu)
dzhulgakov(2017-10-26 22:46:29):actually, why did you remove `batch_size`? Cudnn allows to initialize different state per example and it in general makes sense (if you want to continue previous sequence)
dzhulgakov(2017-10-26 22:46:54):shouldn't we merge it into the global block of weights above?
dzhulgakov(2017-10-26 22:49:28):you need to describe the order of activation functions (probably the same as the gates order in weights blob below). And say that default version just assumes stuff from the formula above.
dzhulgakov(2017-10-26 22:51:06):edit the comment for weights to describe that bias is optional in this case?
dzhulgakov(2017-10-26 22:51:58):add more comments in the spec above regarding where exactly 'clip' gets plugged in?
dzhulgakov(2017-10-26 22:53:16):you'd need to update the comment here too - for example change 'relu' / 'tanh' to 'simple' and update layout order
dzhulgakov(2017-10-26 22:54:35):with several optional args the ordering is unfortunate. Maybe we should allow for 'None' (or empty string) in inputs for this purpose
yuanbyu(2017-10-27 16:53:21):This is related to the layout of the weights: We may have to pad if the layout is not right. Also, it is optional. So I made it a separate input.  It is fine with me to pack it into weights.
yuanbyu(2017-10-27 16:53:23):Reverted.
yuanbyu(2017-10-27 16:53:27):I don't know it. Reverted.
yuanbyu(2017-10-27 16:53:30):Thanks. Done.
Regarding the layout order in the weights input, I don't have a complete picture. Take a simple example, for LSTM, it is not completely clear to me how we pack Wi, Wf, Wo, and Wc in the weight tensor. Is it [4*hidden_size, input_size] or [hidden_size, 4*input_size]?  Perhaps I missed something obvious.
yuanbyu(2017-10-27 16:53:32):Done.
yuanbyu(2017-10-27 16:53:35):Biases are still included in input(0) but not used if use_bias is false.
yuanbyu(2017-10-27 16:53:39):Done.
raduk(2017-10-27 18:33:35):This is not needed anymore. We may also comment that forget weights are ignored for this type of cell
raduk(2017-10-27 18:36:32):We may add the clip to all the formulas: ReLU(clip(Wi*Xt + Ri*Ht-1 + Wbi + Rbi))
yuanbyu(2017-10-27 19:39:29):Clip is optional and disabled as default. There are comments on the clip attribute.
yuanbyu(2017-10-27 19:39:31):Good catch. Removed.
dzhulgakov(2017-10-31 23:50:22):It's actually more complicated as we're trying to pack several layers of e.g. LSTM together. The first layer would have different input size from later ones, the input gates might not have inputs at all, etc. Right now I just concatenated all the weight matrices together. It's not pretty but solves the problem of too many inputs.

Thus for peep_hole connection I'd just modify the message to say that there are more weight matrices in the internal order (R being "input-hidden", "hidden-hidden" and "peephole").
yuanbyu(2017-11-01 22:32:38):@dzhulgakov As we discussed, I will try to update this PR to have 4 RNN ops: SimpleRNN, GRU, LSTM, and BirectionalLSTM. I think that this would make things simpler.
dzhulgakov(2017-11-02 21:23:32):Yes, adding separate ops would make it simpler. The only problem is that stacked nature - for example the input-hidden matrix has different sizes for the first and subsequent layers.
dzhulgakov(2017-11-03 22:06:40):you need to list the full list of activation functions we want to support. I think tanh and relu are fine for now but we should be more explicit.
dzhulgakov(2017-11-03 22:09:42):I wonder whether we should allow implicit broadcasting here, i.e. allow batch_size to be 1 and to expansion
raduk(2017-11-03 22:27:44):shouldn't this be a vector ? same shape as R ?
dzhulgakov(2017-11-03 22:43:00):should it be optional too? then it becomes directly the cudnn's skip_input setting
dzhulgakov(2017-11-03 22:44:16):it's a matter of style, but why not [3, hidden_size, input_size]?
dzhulgakov(2017-11-03 22:44:32):and this then can be [2, 3, hidden_size]
dzhulgakov(2017-11-03 22:57:35):you need to refer to peepholes here too
dzhulgakov(2017-11-03 22:59:34):Why bidirectional applies to LSTM only? How about just adding bidirectional support to all ops here and just adding [2] as an extra dimension to all inputs weight matrices if that's the case? We can even generalize "direction" attributed to be eight "forward", "reverse" or "bidirectional"
dzhulgakov(2017-11-03 23:11:13):for repetitive arguments like this you may find FillUsing useful and extract common args to a subfunction. See for example https://github.com/onnx/onnx/blob/ea6d6578d9d2e566be69077a8c5b43663b0649c7/onnx/defs/reduction/defs.cc#L36
dzhulgakov(2017-11-03 23:12:05):so if you couple them the input weight would be different? should we instead create a yet another op instead?
yuanbyu(2017-11-05 23:49:13):Thanks. Done.
yuanbyu(2017-11-05 23:49:19):I would prefer to keep it not optional for this PR, and do it in the next one.
yuanbyu(2017-11-05 23:49:27):I have a slight preference to keep it simple as is.
yuanbyu(2017-11-05 23:49:32):It seems that both CoreML and PyTorch use this notation so I will keep it as is.
yuanbyu(2017-11-05 23:49:36):See above.
yuanbyu(2017-11-05 23:49:43):Added the peephole term in the equation.
yuanbyu(2017-11-05 23:49:47):Good point. Done.
yuanbyu(2017-11-05 23:49:52):Changed to have bidirectional support for all 3 ops.
yuanbyu(2017-11-05 23:49:58):My understanding is W_i is ignored in this case. I am inclined not to create a new op for it. 
raduk(2017-11-06 17:28:01):shouldn't this be a vector ? same shape as R ?
raduk(2017-11-06 17:29:56):Coming back to this: it feels a bit weird to have the default defined by the absence. And it is not consistent with other attributes that have defaults.
raduk(2017-11-06 17:31:47):Also, from the description it may not be clear where this is applied (I've seen people mistakenly clipping after the activation for instance, or only on parts of the equation). I'd add the clip in the equations.
raduk(2017-11-06 17:32:58):I'd add this: the forget gate is computed as f = 1-i. The weights for the forget gate are ignored.
raduk(2017-11-06 17:33:52):[num_directions, 3*hidden_size] ?
yuanbyu(2017-11-06 17:47:47):Good point. Fixed.

yuanbyu(2017-11-06 17:47:50):Yes. Thanks.
raduk(2017-11-06 18:10:24):If bidirectional we need more of these
yuanbyu(2017-11-06 18:44:46):Good point. I will update the spec.
dzhulgakov(2017-11-07 01:22:43):sorry for nitpicking, but the order of arguments is weird in this case:

X,W,R,B
initial_h
seq_lengths
initial_c
P

Maybe group them together? At least seq_len shouldn't separate out init_c and init_h. As for P it's fine as it's rarely used, so we can keep it last
dzhulgakov(2017-11-07 01:23:16):denote that it's optional
dzhulgakov(2017-11-07 03:30:25):@yuanbyu - would you mind to address this comment? 'Conditional accept' assumes that issues are addressed before merging :)
yuanbyu(2017-11-07 03:39:34):Sure. I will have a small PR for this coming your way. :-)
yuanbyu(2017-11-07 04:39:48):This is for notation. We make it clear it is optional when talking about input `P`.
jamesr66a(2017-10-27 00:17:40):Hi Emad, Can you provide an example of the case [C, M, K] * [C, K, N] -> [C, M, N] with the Numpy dot api?
ebarsoum(2017-10-27 01:01:40):The current Dot is wrong, in our latest discussion we decided to replace it with MatMul that support numpy semantic. In numpy more than 2D, it treated as stacked matrices.  So what you provided should work.

Here the proposal, remove dot and rename BatchMatMul to MatMul, and make it behave like numpy for up to 3D. No need to support more than that.

If both inputs are <= 2D, then it is simply a matrix multiplication. If one or both are 3D, then the third dimension is the batch axis and will act as simply stacking multiple 2D tensors. If we want a batch of 1D, then we can reshape them to  [C, 1, K] * [C, K, 1] -> [C, 1, 1]. This will cover the case, where one has batch axis and the other not.


ezyang(2017-10-28 03:58:44):@ebarsoum So, if I understand you correctly, you don't want a BatchMatMul in ONNX (whose semantics would be equivalent to `np.matmul` even for N > 3), because you want exporters to reshape, matrix multiply, and then unreshape?

Now that I am thinking about this, there is one practical problem to doing it this way. Something that we've been trying to support is ONNX models which are flexible with regards to their batch size, i.e., one of these dynamic dimensions we've been talking about. But now we are in trouble: a reshape operation takes hard coded integers for the shape to reshape. So the initial reshape is OK because you can use -1 to infer the dimensions you need, but you're a bit stuck for restoring the shape, because you need to know what the batch size was to write it in, but that's dynamic.

Have you guys thought about how to solve this problem at all?
ebarsoum(2017-10-28 19:58:52):@ezyang how BatchMatMul is equivalent to np.matmul for N > 3, I through BatchMatMul always treat only first dimension as batch, right and only support 3D? np.matmul always treat last 2 dimension as matrix.

What I am asking is that is there a reason to have both MatMul and BatchMatMul (I am reading their behavior from Caffe 2) instead of combining them into one. If we follow numpy semantic it should support both cases, right? especially the case, where one operand have batch axis and the other doesn't (W*x). The reason that I said we can stop at 3D, because we don't have a case for more than 1D batch axes (sequences is another story) and we can expand them in the future.

Regarding the reshape problem this is why in CNTK batch axis isn't index based, we treat it specially and directly reference it by name. Which help is the case that you mentioned. We actually don't support batch axis with fixed size.
ezyang(2017-10-29 01:49:03):@ebarsoum  I'm sorry! I was totally misinformed about the semantics of `torch.bmm`, from whom the proposed `BatchMatMul` inherited its semantics; I thought bmm supported > 3D, but it doesn't. So, the situation I described, is actually for the *opposite* situation of what `MatMul` is: the MatMul that supports n-dimensional batching is the GOOD situation, and having only the 3D MatMul is the BAD situation.

@jamesr66a So, actually, the motivation for `BatchMatMul` is a bit different than what we originally expected. I thought that `BatchMatMul` wasn't going to be supported by `MatMul`, but a `BatchMatMul` call is translatable to `MatMul` in the obvious way (just swap `BatchMatMul` with `MatMul`

The actual problem is onnx-caffe2 side, for *implementing* generalized MatMul (cc @bddppq). The trouble is Caffe2's `BatchMatMul` only supports 3D inputs. So, to implement the general numpy style MatMul, we must reshape the tensor to 3D, and then shape it back. But the exporter doesn't have the info needed to actually do this conversion.

I don't know if "Caffe2 doesn't implement general Numpy semantics" is a good reason to change the ONNX spec, but it is definitely a technical problem on our side.
bddppq(2017-10-29 06:54:29):Adding support for >3D in Caffe2 BatchMatMul:  https://github.com/caffe2/caffe2/pull/1399
ebarsoum(2017-10-30 16:42:04):@bddppq I am ok with more than 3D, the reason that I said up to 3D initially, because you already have implementation for it in both Caffe 2 and PyTorch.

Are you guys all ok, to have one MatMul that matches numpy semantic? So no Dot, No BatchMatMul.
jamesr66a(2017-10-30 22:37:50):@ebarsoum Your proposal sounds reasonable, the only thing I'm concerned about is that MatMul in caffe2 has (undocumented) semantics whereby leftmost dimensions are collapsed (also rightmost dimensions are collapsed but that's immaterial to the batching situtation). So for example I can have `[a, b, c] * [a, b, c]` with axis_a=2, axis_b=2, trans_b=1 yield `[a*b, c] * [c, a*b] -> [a*b, a*b]`. This behavior is convenient because reshapes happen implicitly and allows for the case of `a` or `b` being dynamically shaped, which becomes a problem when you have to Reshape back with an op that only supports statically-defined shapes.

How about this: we have a single MatMul operator that supports N-dimensional A and B tensors. But as arguments, we have `axis_a`, `axis_b`, `batch_a`, and `batch_b`. `axis` arguments specify where to partition left and right dimensions (e.g. [a, b, c] axis=2 specifies [a*b, c]), and as a layer on top of that `batch` arguments specify the right-most dimension that is included in the batch (default -1). So [a, b, c] axis=2, batch=0 treats the tensor as a batch of `b*c` sized tensors, containing `a` of them. Afaict this should cover batching as well as collapsing as well as obviate the need for preserving dimensions to un-reshape.
dzhulgakov(2017-10-31 06:39:02):Hm, actually C2 behavior is inconsistent between FC and MatMul:
https://github.com/caffe2/caffe2/blob/master/caffe2/operators/matmul_op.cc#L47
https://github.com/caffe2/caffe2/blob/master/caffe2/operators/fully_connected_op.h#L87

-> MatMul doesn't reshape back.

In this particular case I'm inclined not to overcomplicate things and actually drop the axis part from MatMul in ONNX - it's probably not used that often. What do you think?
bddppq(2017-10-31 07:25:48):I agree onnx MatMul should not have axis attributes and should simply follow the numpy semantic.
In Caffe2, for backward compatibility we can not change MatMul to behave the same, but actually BatchMatMul's current functionality is a strict subtset of numpy matmul, we can extend it to be the numpy matmul equivalent so onnx-caffe2 can simply pass through.
ebarsoum(2017-10-31 17:41:25):Agree with @bddppq , let's follow numpy semantic.
dzhulgakov(2017-10-26 23:20:56):nit: not ndim but len(starts)
dzhulgakov(2017-10-26 23:21:19):while we're on it - add some examples?
bddppq(2017-10-26 23:38:13):Good catch!
bddppq(2017-10-28 07:56:51):We can probably blindly do `attrs.ints.extend(int(v) for v in value)`.

btw. Did some googling and found the related discussion here: https://github.com/numpy/numpy/issues/2951. Turns out in Python3, none of the numpy integers are instances of Python native `int`, and starting from numpy 1.9.0, all of the numpy integers are instances of `numbers.Integral`

bddppq(2017-10-28 07:58:27):Make `parent_module` optional (default to `None`) and set the `__module__` only if this is not `None`
houseroad(2017-10-28 18:14:03):Good catch.
ezyang(2017-10-29 01:45:42):I'm wondering, does putting the relevant identifier in `__all__` silence the linter? `__all__` is used to specify what the public symbols of a module do and I wonder if it solves this problem.

I suppose another way to silence the warning without noqa is to say something like `__version__ = onnx.version.version`
ezyang(2017-10-30 02:17:50):This looks good to me, and aligns with my understanding of our versioning policies.
ezyang(2017-10-31 15:19:53):I'm going to merge this for now, so we can work off it for more.
ezyang(2017-10-30 00:29:39):Haha, "for historical reasons." One good reason for representing this using a bit-packed 32-bit integer is that it's modestly simpler to implement the comparison.

This raises a question in my mind, however. Suppose we release ONNX IR version 0.0.10 (in decimal). Are we representing this as `0x00000010` or `0x0000000A`?  Better be the latter, right? In that case, will we conventionally refer to the IR version in hex or decimal? Maybe the IP address conventions will be unambiguous enough...

Also, one last bit of snark: "65536 ought to be more than enough for anyone." ;)
linkerzhang(2017-10-30 02:21:09):.input our .output => .input or .output?
linkerzhang(2017-10-30 02:25:41):removing default value of an existing input may be a breaking change, not sure whether it's a normal case though.
linkerzhang(2017-10-30 02:27:00):Adding/Removing initializers may be a breaking change too.
linkerzhang(2017-10-30 02:33:41):typo? "and will only increment the MINOR version" => "MAJOR version"?
linkerzhang(2017-10-30 02:41:19):Shall we document "updating checker tool to follow these "MUST" policies" please?
linkerzhang(2017-10-30 02:52:44):A shared/centralized library (c++ and c#, for example) to wrap generated codes into internal representation, with user friendly APIs, may resolve this issue to some extent. In this way, producers or consumers will program against the shared library. Format changes do not break data compatibility but break code compatibility will be handled by the shared library itself. 
dzhulgakov(2017-10-30 03:59:59):I'd advocate for adding a statement here that we guarantee forever backward-compatibility for protobuf-level deserialization. It's how you're supposed to use protobufs anyway. If we ever need to change the field type we'd introduce a new field and deprecate the old one
dzhulgakov(2017-10-30 04:01:51):if we don't provide any tooling for handling model versioning we might as well just say that it's up to using application to handle. In my opinion it really depends on the particular use case of onnx and it'd be different in different organizations
dzhulgakov(2017-10-30 04:03:18):Again, this section makes sense only if we standartize particular models as part of the onnx repo (i.e. being more serious of model zoo). Is it what you have in mind?
donbox(2017-10-30 07:09:19):Yes, it's the latter. I updated the example to be 1.2.345 -> 0x01020157

And yes, it's like IP addresses only there are three parts, not 4 (as in IPv4).
donbox(2017-10-30 07:11:30):I love using a 32-bit int btw. It's just we are going to need a place to stash the release and build metadata info.  For example, I just noticed that we've done two releases (v0.1 and v0.2) yet there's no way to identify whether a PB came from v0.1 or v0.2.
donbox(2017-10-30 07:12:39):Fixed. Thx
donbox(2017-10-30 07:20:42):Right now, our only way to specify default parameters is for graph inputs via the initializer list.  I update the spec to make that clear.

Once we write the rules for operator signatures, we will have to tackle this again in the previous section on Operator versioning. 
donbox(2017-10-30 07:26:37):Not a typo.  It only applies to the pre-1.0.0 policy, and we need a way to snap releases and still patch them.
donbox(2017-10-30 07:27:00):And I added text to make that somewhat clearer.
donbox(2017-10-30 07:29:09):Done. Thx

donbox(2017-10-30 07:30:36):That was my very first sentence, which makes this section non-normative.

I think having us collectively document how we THINK it should work is strictly goodness.

I also think that some of this is text that we will need for operators, since an operator is just a graph who's nodes are opaque :-)
donbox(2017-10-30 07:32:47):That stated, I added more prose to make that clear.  
donbox(2017-10-30 07:33:09):See comments above. Same idea.
donbox(2017-10-30 07:48:55):First, Ke's characterization of data compat vs. code compat is spot on. They are two different things, and the vast majority of protobuf versioning rules are focused on the former and IMO ignore the latter.  If you read the [protobuf versioning rules](https://developers.google.com/protocol-buffers/docs/proto3#updating), it say it's totally OK to switch from int32 to bool, but that would break A TON of code if one were to regenerate their language bindings using protoc against the new .proto file. 

As for Ke's proposed solution, while we may elect to provide a shared library, the specification cannot mandate or really assume it will be used everywhere. We have to assume a reasonable number of people will just run protoc on the latest .proto file and have to absorb our "data compat/code incompat) changes (assuming we every let them happen).  BTW, we are some of those people so we get to live with whatever policy we apply. 

As for @dzhulgakov's suggestion of keeping a field type immutable, I'm OK with that PROVIDED we use a new name for the field. Otherwise, we get the worst of both worlds: we cause the same breaking change to consumers of the .proto file AND we add versioning complexity to consumers as the field's value can now appear in two different places based on version.

BTW, once we hit V1.0.0, I want to adopt @dzhulgakov's approach. Leading up to that, I'm more liberal, as the blast radius of a BC is still fairly contained.  That honeymoon will end soon enough, so maybe we just adopt that policy now (with my addendum of not reusing the same field NAME as well as field NUMBER).  

Dmytro, are you OK with this?


ezyang(2017-10-30 14:40:56):@dzhulgakov A good analogy is to imagine we were building an actual programming language, and are considering to add a package ecosystem. It is best if the designers of package ecosystem specify a normative specification on model definitions.
linkerzhang(2017-10-30 15:36:58):Thank you! Understood.
prasanthpul(2017-10-30 16:48:03):nit: unnecessary "are" in "the MAJOR version numbers _are_ have not changed "
prasanthpul(2017-10-30 16:54:53):Are versions applied across profiles (i.e. ONNX and ONNX-ML) or each profile has it's own versioning? Can users expect that ONNX-ML V1.8 is a superset of ONNX V1.8? Would we ever allow ONNX-ML V1.9 to be a superset of ONNX V1.5?
dzhulgakov(2017-10-30 19:34:00):I actually didn't realize that Protobuf is permissive with some type changes (I'm somewhat biased with Thrift's experience). In that case expanding the type to broader type if we need it should be permissible.

For non-compatible changes - I totally agree - using the new field with different name AND number.
marcelolr(2017-11-11 03:24:13):This was superseded by #216 and is being closed for the same reason.
tjingrant(2017-10-30 21:20:17):I cannot build the latest master. I get the following error:
```
  Running setup.py develop for onnx
    Complete output from command /localhd/tjin/tensorflow_latest/bin/python2 -c "import setuptools, tokenize;__file__='/localhd/tjin/onnx_dev/onnx/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\r\n', '\n');f.close();exec(compile(code, __file__, 'exec'))" develop --no-deps:
    running develop
    running create_version
    running egg_info
    writing requirements to onnx.egg-info/requires.txt
    writing onnx.egg-info/PKG-INFO
    writing top-level names to onnx.egg-info/top_level.txt
    writing dependency_links to onnx.egg-info/dependency_links.txt
    reading manifest file 'onnx.egg-info/SOURCES.txt'
    reading manifest template 'MANIFEST.in'
    warning: no files found matching '*' under directory 'third_party'
    writing manifest file 'onnx.egg-info/SOURCES.txt'
    running build_ext
    running build_proto
    running build_proto_in
    compiling onnx.proto.in
    compiling /localhd/tjin/onnx_dev/onnx/onnx/onnx-ml.proto
    building 'onnx.onnx_cpp2py_export' extension
    powerpc64le-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -Wstrict-prototypes -fno-strict-aliasing -Wdate-time -D_FORTIFY_SOURCE=2 -g -fstack-protector-strong -Wformat -Werror=format-security -fPIC -I/localhd/tjin/onnx_dev/onnx -I/localhd/tjin/onnx_dev/onnx/third_party/pybind11/include -I/usr/include/python2.7 -I/usr/include/python2.7 -c /localhd/tjin/onnx_dev/onnx/onnx/onnx-ml.pb.cc -o build/temp.linux-ppc64le-2.7/localhd/tjin/onnx_dev/onnx/onnx/onnx-ml.pb.o -std=c++11
    cc1plus: warning: command line option '-Wstrict-prototypes' is valid for C/ObjC but not for C++
    powerpc64le-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -Wstrict-prototypes -fno-strict-aliasing -Wdate-time -D_FORTIFY_SOURCE=2 -g -fstack-protector-strong -Wformat -Werror=format-security -fPIC -I/localhd/tjin/onnx_dev/onnx -I/localhd/tjin/onnx_dev/onnx/third_party/pybind11/include -I/usr/include/python2.7 -I/usr/include/python2.7 -c /localhd/tjin/onnx_dev/onnx/onnx/cpp2py_export.cc -o build/temp.linux-ppc64le-2.7/localhd/tjin/onnx_dev/onnx/onnx/cpp2py_export.o -std=c++11
    cc1plus: warning: command line option '-Wstrict-prototypes' is valid for C/ObjC but not for C++
    /localhd/tjin/onnx_dev/onnx/onnx/cpp2py_export.cc:1:31: fatal error: pybind11/pybind11.h: No such file or directory
    compilation terminated.
    error: command 'powerpc64le-linux-gnu-gcc' failed with exit status 1
```

Any idea what's going on?
bddppq(2017-10-30 21:34:52):`pybind11/pybind11.h: No such file or directory`
Looks like the submodules are not fetched in your working copy. Please do `git submodule update --init --recursive`
bddppq(2017-10-30 21:37:09):There is another PR doing this change already: https://github.com/onnx/onnx/pull/127/files/e43f2825951834a0197053788cccd90e8e87fdbb#diff-baadf6fd14a4a80f0fe30bb743d33357L101
tjingrant(2017-10-30 21:39:55):Great! It installs now. Looking forward to its merging!
linkerzhang(2017-10-31 03:07:21):This is not needed as ValueProto will be removed :).
tjingrant(2017-10-31 04:12:49):Sorry to cut into this, but we are very interested in this patch as well. 

I have a question, is there any rule specifying the behavior when both auto_pad and pads are specified? This may eliminate some undefined behavior.
ebarsoum(2017-10-31 04:58:09):@tjingrant  regarding "I have a question, is there any rule specifying the behavior when both auto_pad and pads are specified? This may eliminate some undefined behavior."

Yes, and it is mentioned in the doc, we will only look at `pads` attribute iff auto_pad="VALID". In other word, if auto pad is on manually, then we will read the values in the pads attribute.
doru1004(2017-11-06 17:26:09):@ebarsoum I have a few questions regarding your answer above:

> we will only look at pads attribute iff auto_pad="VALID"

Isn't it the case that if auto_pad is VALID the padding can be resolved automatically without the aid of "pads"?

Are you saying that in the most general case, when pads is specified, auto_pad is being ignored?

Thanks a lot!
ebarsoum(2017-11-07 00:53:23):Hi @doru1004 ,

If auto_pad = 'VALID' that mean not auto_padding, in this case if pads is specified then use the value in `pads` attribute, otherwise no padding is applied.

Here is the logic, 
    If auto_pad = SAME_LOWER or SAME_UPPER, then ignore `pads` attribute.
    If auto_pad = VALID, then check `pads` attribute. If pads attribute isn't empty, then use it. Otherwise no padding.


tjingrant(2017-11-07 01:21:00):@ebarsoum This could be very confusing for the Tensorflow world as 'VALID' = no padding as opposed to no **auto** padding. On the other hand, we will try to put aside my presumptions from Tensorflow and explain the scheme that I think is the most intuitive:

You specify either autopad or pads. When both are specified, we should have a precedence established from discussion and take the one with higher precedence and abandon the one with lower precedence. 

Your current specification has the following aspects that we cannot understand/ do not feel good about:

1. The interaction of auto_pad and pads are asymmetrical. Meaning that in some cases you go down to pads after looking at auto_pad whereas in some other cases you dont. Such asymmetrical behavior will lead to user having a hard time to memorize your specification and results in time wasted in looking up documentation and debugging.

2. I do not see why anyone would specify audo_pad to be 'VALID' and then specify pads to be non-empty. Can you give me a specific use case? Why use 'VALID'  and nothing in the pads attribute in any case if you can just leave pads empty? 

3. On a conceptually level, VALID is an automatic padding scheme (as manifested by its naming). You seem to take it as a partial padding scheme this is problematic because VALID option currently serves two purposes:
- specify no padding is needed. (in which case no information is gained by having this 'VALID' paramter since we can leave pads empty).
- specify users' intent that padding should be done according to the pads attribute (again, no information is gained by having this 'VALID' parameter since it is overiden).
- So my question is why would we have this 'VALID' option? What information can be gained?

I hope you can understand our confusion coming from a Tensorflow world.
tjingrant(2017-10-31 04:11:19):lower and upper side?
tjingrant(2017-10-31 04:14:08):missing space between `values` and `from` near the end.
ebarsoum(2017-10-31 04:55:17):Yes, lower or upper, or left and right, or whatever we want to call it. So for each axis you will have a pair of numbers (lower, upper), which has the number of pixels on each side of the correspoding axis.
tjingrant(2017-10-31 04:57:41):Sorry for being unclear, I'm specifically pointing to the first sentence "Padding for upper and lower side along each axis". For consistency, it's better to use lower and upper here.
donbox(2017-10-31 16:40:55):Folks, this is my first PR since we moved to onnx.proto.in, hence the build break.
prasanthpul(2017-10-31 17:30:32):I'm not sure we need this PR. KeyValuePairProto and Dimension already use oneof
donbox(2017-10-31 17:48:54):I believe that we need it even for oneof cases given the HasField issues in Proto3

<sub>Sent with <a href="http://githawk.com">GitHawk</a></sub>
gramalingam(2017-10-31 17:58:01):I think "oneof key {...}" will generate a corresponding enum in Proto3. I think the problematic case is "optional" fields of scalar-type (which are not in an oneof) ... these will generate "HasField" in Proto2 but not in Proto3.
prasanthpul(2017-10-31 18:02:09):https://developers.google.com/protocol-buffers/docs/reference/python-generated#oneof 

You should be able to use HasField on oneof in proto3
jywumsft(2017-11-01 17:10:06):For C# proto3 does not generate *any* HasField() methods. 

submessages will be null if they were not set. 

For oneof, an enum will be generated so it can be used to test presence. e.g. 
TypeProto.Types.TensorShapeProto.Types.Dimension.ValueOneofCase

For primitive types, they get filled in with default values if not explicitly set,
thus we need the discriminator (i.e. what was done for AttributeProto)

here, because the primitive types are wrapped in a oneof, we can test presence using the enum generated.
prasanthpul(2017-11-01 21:06:24):Thanks for clarifying @jywumsft! Looks like we don't need this PR. @donbox do you still feel otherwise?
linkerzhang(2017-10-31 17:29:59):We don't need the enum in this case, as we have already used oneof there. There will be a "enum" generated with oneof usage.
linkerzhang(2017-10-31 17:30:24):Same as above please.
houseroad(2017-10-31 21:23:03):So we don't need to yield on onnx_ml_pb2.py.
ezyang(2017-11-02 06:50:21):I was thinking about how to simplify this proposal more, and I had (what I think is) a good idea: what if we get rid of the "operator definition" name (the thing that has a leading underscore in my proposal), and instead uniquely identify operators using the operator name (`op_type`) and the *first ONNX operator version they were introduced.* I propose the syntax is `OpName:GlobalOpVer`, e.g., `Add:5` or `Conv:27`.

This is NOT introducing per operator versions, because the "version" tagged on each operator reflects the global operator version, e.g., it's very unlikely you'll have `Add:1`, `Add:2`, `Add:3`, rather, it will look more like `Add:1`, `Add:55`, `Add:123`. You get the point. And like before, operator definitions are still immutable, so you can always talk about `Add:55` and it will always be the same implementation of `Add` it always was.

I think there a few merits to this proposal:

1. It reduces the number of concepts (no more leading underscore names)
2. It reduces the amount of naming we ONNX specifiers have to do (no more having to pick a new operator definition name for conv with bias, the numbering is mandated!)
3. It makes it clearer how to write operator definitions; an operator definition looks as it does today, but we add a new field "Since" which records the earliest ONNX operator version which shipped with this operator (conveniently, this coincides with the tag!)
4. We need ONLY record operator names in the  "Added/Modified/Removed" fields in the changelog, since you can infer the operator version number that was added based on the version of the changelog entry corresponds to.
5. The recorded tag gives you some hints about what the underling implementation should be. It's not as good as a changelog, but it can get you somewhat far.

The downside is you lose the (original) flexibility of this versioning policy, which permitted you to specify a mapping however you might want (e.g., you could bring an old version of an operator back into scope by simply introducing a mapping to it, and this would be automatically supported by frameworks that supported the old version, so long as they had a sufficiently recent changelog.)
ezyang(2017-11-02 15:05:22):> As to whether we call these things namespaces, libraries, modules, or packages, I'm not married to any one term. To avoid picking a name, I will use LNMP throughout my comments as a placeholder for whichever one we land on.

I think in the spec text, I used the term "vendor (operator) extension", to make it clear that the third parties that are most likely to define extra operators are the backend vendors, who would actually be in the business of implementing them. The reason I don't like library/module/package is because it brings the intuition that *users* could write libraries of operators, composed out of more primitive ones, that could portably be reused over many backends (and then you'd have some sort of package manager, etc. etc. etc.) This might be a legitimate problem to solve, but it's not the one we're trying to solve with operator versioning.

> I prefer declaration, but am fine with specification or contract. Definition implies a specific implementation, which I'm not a fan of.

I'm absolutely fine with declaration/specification. They are *solely* called definitions right now, because that is the internal naming convention in ONNX library. I can rename `def` to `spec` in the library to make this clear.
donbox(2017-11-03 05:25:10):Edward and I worked through my comments today and we have a new, simpler design that I am writing up.

We agreed to do it in a separate branch (pr/operator-versioning) and PR186. 

Proto with comments is there now.  

Please move commentary over there, as this PR is now defunct.
linkerzhang(2017-11-01 05:27:46):Change "operator implementation" to "operator definition" or "operator specification"? For "operator implementation", it means "kernel codes" normally?
linkerzhang(2017-11-01 05:49:51):My understanding of this design is, the operator implementation store will never be cleaned up. 

One personal thinking please. I'm in favor of having the capability to clean up operator implementation store. The operator implementation store is part of onnx standard.  We may want to clean up the standard when we feel too old versions do not make sense and we want to enforce an upgrade of vendors' components (importers/exporters). Without the clean up capability, it's always up to vendors to decide the range of operator implementation store they'll support. Does this make sense please? 



prasanthpul(2017-11-01 05:56:26):we should clarify whether ONNX and ONNX-ML (and any other profile) have independent versioning for IR and operators
ezyang(2017-11-01 15:04:18):The proposal, as I understand it, is compatible with what you want. The point is that the operator version standard doesn't *mandate* that an ONNX importer actually have implementations for all operator implementations in the store; they can do whatever they want. We can give guidelines about what we recommend they implement fully to make this clearer.
ezyang(2017-11-01 16:25:31):I think that it must version separately, it doesn't work reasonably otherwise.
prasanthpul(2017-11-01 20:24:06):then we need a field in ModelProto to specify the profile
donbox(2017-11-02 13:39:58):Because ONNX-ML actually augments the type system (and adds proto constructs to represent them), I'm fine adding a profile field to ModleProto. If it were just an optional set of operators, I wouldn't bother.



Edward and I talked on Monday, and I think we agreed that we would support multiple "namespaces" to be used by a given model and that they would (a) version independently from one another and (b) would also give us a disambiguating prefix to avoid identifier conflicts, especially when 3rd parties start defining operators outside of the ONNX spec (which is inevitable and we should design for).  

As to whether we call these things namespaces, libraries, modules, or packages, I'm not married to any one term. To avoid picking a name, I will use LNMP throughout my comments as a placeholder for whichever one we land on.

As to whether we want profiles WITHIN onnx to version independently isn't obvious. My default intuition is no, but I'd love to hear Edward's opinion.  Put another way, do we have one or two LNMPs in onnx?


donbox(2017-11-02 13:43:09):At a high level, I'm still wrestling with monotonically increasing int vs. semver. The details matter, but for now, I will use V as the placeholder typename for whatever scheme we land on. 

For me, the biggest challenge with the design as specified is changelog management. If we find a workable model for managing changelogs (which I don't know if we have, especially as we scale out), then integer is absolutely the right choice.

BTW, what I love about where we are landing is:

1. A given operator specification is immutable both in semantics and signature (modulo where we land on signature compatibility, which we need to write down as part of this doc). 
2. Models declare the collection of immutable operator declarations they depend on.

The details that have me worried are only around identifier management. 

1. I don't know that the changelog design can scale. It puts a lot of burden on implementations, and losing a changelog means you are guessing at whether you are binding to the right specification.
2. We need a disambiguator for operator names to avoid collisions.



donbox(2017-11-02 13:50:26):Terminology note:  

1. Whenever we use the term "operator semantics" in this doc, we need to say "operator signature and semantics" as they are both compatibility-impacting.

2. Depending on what name we pick for LNMP, where we use "ONNX operator version", we should use "LNMP version"


donbox(2017-11-02 14:05:01):I would prefer that we simply snap to CS conventions and leverage that intuition.

Operators are declared - their signature and semantics are specified, but their implementation is opaque.
Graphs are defined - their signature and semantics are specified, and their implementation is transparent.

This is consistent with other language specs. In C:
     extern float add(float a, float b);              // declares add as a name that can be referenced
     float add(float a, float b) { return a + b; } // defines the implementation of add 

I prefer declaration, but am fine with specification or contract. Definition implies a specific implementation, which I'm not a fan of.

postrational(2017-11-02 10:58:43):`np.dot` supports matrices with higher dimensions. There may be current framework limitations, but should the "1- to 3-dimension" limitation be added to the ONNX spec?
bddppq(2017-11-02 20:37:04):Remove the <=3D restriction?
bddppq(2017-11-14 22:58:10):LGTM

@ebarsoum
silaskuo(2018-04-20 18:11:02):Hello, do you have ETA to move this op from experimental to default. Thanks
houseroad(2018-04-20 18:12:47):@silaskuo right on time, I will move it soon. PR is under construction.
silaskuo(2018-04-20 18:21:57):Thanks for the quick reply. Look forward to it! 
silaskuo(2018-04-25 18:31:10):Is it moved from experimental to default? If yes, then is https://github.com/onnx/onnx/blob/master/docs/Operators.md going to be updated accordingly? Thanks.
bddppq(2017-11-02 20:37:52):Please either mark the attribute as required or document the default value.
bddppq(2017-11-02 20:42:31):Maybe add some more description? It's not clear what values of the new pixels in the output.
houseroad(2017-11-02 20:51:31):Sure
houseroad(2017-11-02 20:51:49):Will add more description, and an example here.
bddppq(2017-11-02 23:16:46):Are we also going to add optional output?
dzhulgakov(2017-11-03 08:37:55):For optional output it's less critical - we can always map to a random name and that way there's less to handle in backend. With inputs the situation is that sometimes we need to independently enable/disable several inputs and it's hard to express otherwise.
yuanbyu(2017-11-03 14:43:37):For outputs, I think we are good: If an output is not used in the graph, it is optional. A runtime may choose not to generate an output if it can detect the output is not used.     
bddppq(2017-11-02 23:11:42):nit: there is no need to store `schema_`
dzhulgakov(2017-11-03 08:37:52):I kept it just in case somebody would want to access schemas through global variables (probably not)
bddppq(2017-11-02 23:16:46):Are we also going to add optional output?
dzhulgakov(2017-11-03 08:37:55):For optional output it's less critical - we can always map to a random name and that way there's less to handle in backend. With inputs the situation is that sometimes we need to independently enable/disable several inputs and it's hard to express otherwise.
yuanbyu(2017-11-03 14:43:37):For outputs, I think we are good: If an output is not used in the graph, it is optional. A runtime may choose not to generate an output if it can detect the output is not used.     
bddppq(2017-11-02 23:11:42):nit: there is no need to store `schema_`
dzhulgakov(2017-11-03 08:37:52):I kept it just in case somebody would want to access schemas through global variables (probably not)
donbox(2017-11-03 12:51:24):THis was a scratch area. The real action is at PR186
ezyang(2017-11-03 19:38:27):Commandeering, I need this to land for some of my stuff.
dzhulgakov(2017-11-03 08:14:44):wrap everything into `do {} while(0)` otherwise it'd misfire if used in an if statement
dzhulgakov(2017-11-03 08:16:51):nit: put `return` here - it'd be more readable
dzhulgakov(2017-11-03 08:17:30):I think checker should always take the latest version and we should have a converter before that
dzhulgakov(2017-11-03 08:18:23):since we're doing exceptions now - let's do exceptions inside schema checker too. Right now it spits out the message to std::cerr and it's confusing. Or at least let's make it a TODO.
dzhulgakov(2017-11-03 08:19:41):we should add here proper checking of the graph, i.e. that there are no unconnected inputs, duplicated names, etc.
dzhulgakov(2017-11-03 08:20:30):ValueError?
dzhulgakov(2017-11-03 08:21:02):magic :)
dzhulgakov(2017-11-03 08:22:11):when you do this - does the python side inherit from Exception class?
dzhulgakov(2017-11-03 08:22:32):call it string_utils?
marcelolr(2017-11-03 08:59:11):indent
marcelolr(2017-11-03 09:01:58):Do we have stronger requirements here around names? Not all-whitespace, no embedded nulls/control characters?
marcelolr(2017-11-03 09:07:21):Do we think there's a reasonable size we'd like to set as a high limit? Even if we don't care theoretically, it might be handy for implementations to know there's a limit (and what that limit might be). Even C++ compilers will barf on template depth and the like.
linkerzhang(2017-11-03 15:56:05):we may relax the shape check? for internal nodes' outputs, the shape may be unknown at the beginning, and it may be inferred via a shape inference through the whole graph after loading a graph.
linkerzhang(2017-11-03 15:57:38):If enforced to have a empty shape initialized, then the tensor itself may be mis-recognized as a scalar later as it has shape with zero dimension.
bddppq(2017-11-03 20:23:47):will fix it
bddppq(2017-11-03 20:23:54):will fix it
bddppq(2017-11-03 20:24:45):will fix it
bddppq(2017-11-03 20:25:35):hmm I don't think it's worth to find out a reasonable limit right now
bddppq(2017-11-03 20:25:39):good catch
bddppq(2017-11-03 20:27:41):it's not called to check the internal nodes right now. but I think even we do in the future, it makes senses the ValueInfo of an internal node to be missing in the model pb file instead of having an incomplete ValueInfo
bddppq(2017-11-03 20:28:25):I think right now the policy is as long as it's non-empty, then it's fine. 
bddppq(2017-11-03 20:28:38):makes senses
bddppq(2017-11-03 20:29:17):yep
bddppq(2017-11-03 20:30:05):yep indeed ValueError should be raised for bad arguments
gramalingam(2017-11-04 05:14:17):Looks good to me
linkerzhang(2017-11-04 23:43:28):@dzhulgakov , Thank you very much all the comments! I resolved all the comments. You may let me know if you have more comments please.
AppVeyorBot(2017-11-05 04:30:02)::white_check_mark: [Build onnx 0.3.4 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.4) (commit https://github.com/onnx/onnx/commit/0dfbf958b8 by @)
AppVeyorBot(2017-11-05 04:40:24)::white_check_mark: [Build onnx 0.3.5 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.5) (commit https://github.com/onnx/onnx/commit/ac37b72c1e by @)
prasanthpul(2017-11-03 04:39:06):nit: mismatched comment
linkerzhang(2017-11-03 04:50:44):good catch! :).
gramalingam(2017-11-03 17:34:21):We don't really need to use a type-variable T if it is constrained to take a single value, do we? This looks fine too, but just want to check that it would be equivalent to saying "tensor(int32)" in-place without "T".
gramalingam(2017-11-03 17:41:19):Or, is the string/type parameter of Input/output always interpreted as a type-variable?
gramalingam(2017-11-03 18:05:55):The grammar specified in schema.h says that the parameter of Input/output is given by "<type> ::= <data_type> | tensor(<data_type>) | sparse(<data_type>) | <type_parameter>". Is there a corresponding parsing function? The function DataTypeUtils::FromString does not seem to consider the <type_parameter> case.
gramalingam(2017-11-03 18:07:18):One possible fix is to say that parameter of Input/output is always a type-variable and cannot be a type-constant like "tensor(float)"
linkerzhang(2017-11-03 19:22:27):tensor(int32) should be put in-place. Fixed in new iteration. Thanks!
linkerzhang(2017-11-03 19:32:14):fixed. Thanks!
yuanbyu(2017-11-03 20:08:52):Could you clarify the difference between <data_type> and tensor<data_type>?
yuanbyu(2017-11-03 20:11:02):Could we also take care of ONNX-ML data types here?
gramalingam(2017-11-03 21:58:46):This seems like an interesting example that may require further extensions to the type system to capture fully. We need to capture that "dtype" is the same as "T" (which we could do by using the same name for both). Something to think about as a future extension.
gramalingam(2017-11-03 22:33:38):The generated documentation doesn't look right. Is it because of the extra indentation? I don't know markdown, but some change might be required.
gramalingam(2017-11-03 22:47:53):The output doesn't have to be the same type as the input. It should be type T2, a different type variable. I assume that "to" is an attribute that specifies the type T2?
yuanbyu(2017-11-03 23:08:22):Line42: Is dtype AttrType::INT?
yuanbyu(2017-11-03 23:14:45):for DataType enum?
yuanbyu(2017-11-03 23:16:45):It seems that the output can be one of float, int32, int64, and bool?
linkerzhang(2017-11-03 23:58:39):@yuanbyu , I put // NOTE: <type> ::= <data_type> means the data is scalar (zero dimension) to clarify the <data_type> as scalar. Is it clear enough please?
for covering ONNX-ML data types, I'd add it later. Sounds good to you please?
linkerzhang(2017-11-04 00:04:28):good catch! fixed. Thanks!
linkerzhang(2017-11-04 00:40:48):Format fixed now. It's more readable now. :). Thanks!
dzhulgakov(2017-11-04 00:55:09):I know it's a bit picky, but do we really need it for a fairly simple parser? Maybe we should just use the plain c++ apis for now
linkerzhang(2017-11-04 04:20:42):@dzhulgakov , I feel it's needed. the current tensor and sparse tensor it fairly simple. We will have more types probably, am I right? at least to support onnx-ml. I don't really want to have two piece of codes to support onnx and onnx-ml. Make sense please?
dzhulgakov(2017-11-04 07:53:51):assert depends on the compilation settings. Maybe regular exception?
dzhulgakov(2017-11-04 07:54:52):better not use assert - it becomes moot in non-debug mode
dzhulgakov(2017-11-04 07:56:54):I think it can be moved to .cc file to have the header cleaner
dzhulgakov(2017-11-04 07:58:09):maybe just put it in a map somewhere and process it generically? right now there are 4 places you need to change if new data types is added. Ideally there should be only one.
dzhulgakov(2017-11-04 07:58:48):capitals usually mean macroses. Maybe DataType? or DataTypeSignature?
linkerzhang(2017-11-04 17:18:56):Sure. Will change it to DataType. Thanks!
linkerzhang(2017-11-04 17:24:22):I'd like to keep using it unless there're strong preferences please.
1. it only takes effect as debug build, which is good enough to catch the errors in our CI. It means it should never be hit in release (production) bits.
2. protobuf generated files like onnx.pb.h are using asserts :).
3. using exception is debatable I personally think :).
Thoughts please?
linkerzhang(2017-11-04 17:24:41):Replied in next comment :). Thank you very much!
linkerzhang(2017-11-04 17:27:28):All moved to .cc now. Thanks!
linkerzhang(2017-11-04 17:30:40):OK. Put in one place now. Thanks!
dzhulgakov(2017-11-04 22:17:44):We already use exceptions in schema validation (e.g. see https://github.com/onnx/onnx/pull/192). I'd rather catch these problems even in non-debug builds (as perf matters less here). But it's a minor comment, up to you
dzhulgakov(2017-11-05 03:13:58):Oh, I just meant that there are 4 places in the .cc file that do remapping back and forth. So it might be easier to put them into a map once and reuse it everywhere
dzhulgakov(2017-11-05 03:20:40):I'd still make it something more strict than assert() here. What if somebody tries to call FromDataTypeString() with invalid string? I'd still prefer exceptions (or just crash but including release mode) here.

Another reason for exceptions - we can append the name of the schema later at the higher level
dzhulgakov(2017-11-05 03:21:12):thanks!
dzhulgakov(2017-11-05 03:23:44):.empty()
dzhulgakov(2017-11-05 03:26:00):maybe say that parameter = input/output
dzhulgakov(2017-11-05 03:27:36):maybe use a proper little struct? tuples of many elements are harder to consume
dzhulgakov(2017-11-05 03:28:12):same
dzhulgakov(2017-11-05 03:30:07):skip registering if that's the case
linkerzhang(2017-11-05 04:34:20):Hmmm, OK, let me fix this in next PR. Basically, I'd do it in two points:
1. For operator registering, I'll keep using assert, which is ok, since it will introduce build break, I believe.
2. For other public calls (someone calls ToType), I'm thinking of add a "Status" class to return the result with detail error message. The reason that I still may not want to throw exception is it may be not good for windows to call a SDK which throws exception.
linkerzhang(2017-11-05 04:36:33):will fix in next PR. Thanks!
linkerzhang(2017-11-05 04:36:45):will fix in next PR. Thanks!
donbox(2017-11-03 15:13:54):I believe the design that Edward and I worked on is now fully specified in both onnx.in.proto and versioning.md.

Unleash the review hounds, ideally before Edward gets too far into implementation.

ezyang(2017-11-03 16:08:21):@donbox I'm going to move the proto stuff around a little, no substantive changes.
gramalingam(2017-11-03 18:30:31):Re. "As a rough approximation, the serialized model plays the role of an API's *callee*; the consumer of the seriaized model plays the role of the API's *caller.*". Wouldn't the producer of a serialized model play the role of the "caller" and the consumer (the runtime that executes) a model play the role of the "callee" ?
gramalingam(2017-11-03 19:08:49):A single model can reference and make use of multiple OperatorSets. Are there any constraints here? Can a model use both "com.acme.CNN:3" and "com.acme.RNN:5" mixing different versions of the same OperatorSet (domain)? Or, are such questions outside the scope of this? If an implementation claims to support some specific OperatorSets, does it mean that it should be able to handle models that mix operators from these different OperatorSets, or is that outside the scope of this standard?
ezyang(2017-11-03 20:18:08):> Wouldn't the producer of a serialized model play the role of the "caller" and the consumer (the runtime that executes) a model play the role of the "callee" ?

Yeah, I agree with this.

> A single model can reference and make use of multiple OperatorSets. Are there any constraints here? Can a model use both "com.acme.CNN:3" and "com.acme.RNN:5" mixing different versions of the same OperatorSet (domain)? Or, are such questions outside the scope of this?

You are NOT allowed to mix together multiple versions of an operator set. While technically nothing is stopping us from adding this to spec, it's bad behavior and we shan't encourage it.

One extension Don and I debated was whether or not to allow specifying a version on individual NodeProto. This would short circuit the normal resolution process and let you refer to a legacy operator. On balance, we decided not to add it, although it is a coherent extension.

> If an implementation claims to support some specific OperatorSets, does it mean that it should be able to handle models that mix operators from these different OperatorSets, or is that outside the scope of this standard?

Yes, mixes of different operator sets (e.g., *domains*) should be handled.
gramalingam(2017-11-03 23:08:28):How do "experimental" operators fit within this scheme? Will they move into another OperatorSet, like "experimental.onnx"?
ezyang(2017-11-04 00:24:23):> Will they move into another OperatorSet, like "experimental.onnx"?

The ONNX checker will move to only checking NodeProto in the default domain (domain is missing); and vendors can implement their own checkers for custom operator sets. We won't try to record vendor extensions here because, well, they're vendor extensions.
ezyang(2017-11-04 00:53:07):To see an example of operator versioning in implementation, see: https://github.com/onnx/onnx-caffe2/pull/64
ezyang(2017-11-04 01:02:28):The checker probably needs some more in-depth checks for the new versioning.
ezyang(2017-11-04 01:14:40):PyTorch operator versioning here: https://github.com/pytorch/pytorch/pull/3485
ezyang(2017-11-08 12:24:47):We definitely need more checker support; I'm going to go ahead and merge this for now but there are some follow up tasks to do (I'll open a task to track it.)
ezyang(2017-11-03 18:01:41):I think it would be more unambiguous to say something like, "The set of operators that may be referenced by a given ONNX graph. We refer to this as the operator set version."

I'd actually like to expunge ALL references to "operator version" from the standard (replacing them with "operator set version"), I think this will help prevent common mistakes from our users. The basic problem is that "operator version" implies that an operator has a version associated with it. But this is not the model; rather, an operator is canonically defined by a particular operator *set* version that it was added in (slash, the operator set version when it broke compatibility with the old version.) So I really want to avoid people forgetting to rev the major version, and I think being more precise with the terminology here will help.
ezyang(2017-11-03 18:03:27):I'd like to propose some alternative nomenclature here, going further along the lines of "op version is confusing, because people will think ops version individually."

Let's think about what `op_version` actually means: it is the version of the opset in which this operator was added / had its last breaking change. So it is not an "operator version", it is an "operator set version". And what is it's relation with the operator; we've had the operator *since* that opset version. So a long and wordy, but *clear* (IMO) identifier is `since_opset_version`. This highlights the unusualness of the proposal: normally you don't identify operators by the version they were added, but this is exactly what we are doing here.
ezyang(2017-11-03 18:11:28):s/enventory/inventory/

And notice that with the "since_opset_version" change, one doesn't have to call out that the opset version is "independent" of the op versions, because, well, obviously some ops are going to carry over from older ones.
ezyang(2017-11-03 18:11:49):s/compatibilitey/compatibility/
s/robustnes/robustness/
donbox(2017-11-03 18:15:23):Thank goodness you are part of the mechanical turk of typo corrections.
donbox(2017-11-03 18:20:45):After our call, it's clear to me that the prose and name around op_version is super confusing.

Per our chat, I'm working up a fix.
donbox(2017-11-03 18:41:33):See comment above. We definitely need a different name. Look for a checkin in 30 min.
donbox(2017-11-03 18:41:42):See above
gramalingam(2017-11-03 19:12:15):s/oroto/proto/
s/enventory/inventory/
s/have have have/have/
s/seriaized/serialized/

gramalingam(2017-11-03 19:14:16):s/therefor/therefore/

linkerzhang(2017-11-06 03:11:17):ses -> sets?
linkerzhang(2017-11-06 15:02:01):Confirm my understanding please. So a type change of an existing field will introduce a IR major version change, am I right? (although it's data compatible in some cases). Thank you!
linkerzhang(2017-11-06 15:05:22):"operators represent" -> "an operator represents"? :)
linkerzhang(2017-11-06 15:10:57):Small suggestion please. we may mark the definition "operator id" above a little bit more explicitly, when we say the tuple of (domain, op_type, op_version) is the identifier (operator id).
linkerzhang(2017-11-06 15:18:19):Confirm my understanding please. So now, an operator set (operator specification) has a op_set_version, and each operator has a "since_op_set_version" to clarify when it's added into an operator set. Am I right? Thank you!
linkerzhang(2017-11-06 15:25:05):The "have have have a compatible operator declaration..." for example is asking that "modol author should not use operators with same domain, same op_type, but different version in one model", am I right? if yes, we may list these policies explicitly here.
For the policies that we don't list here, I'm assuming that different runtime (importers) will have different way to handle (out of our control), make sense please?
linkerzhang(2017-11-06 15:28:46):My last question was answered with "yes"  by "Implementations of ONNX MAY elect  
to introduce more sophisticated operator declaration/implementation binding modes to appeal to the liberal clause of the robustness principle.  "
ezyang(2017-11-07 01:36:46):This is a good question. I think as written here, it would be a major version change. However, I am not convinced this is necessarily what we should do; I'd rather revisions to major versions indicating something like, "Hey! Consumer! You need to do something different!" Which wouldn't be the case here.
ezyang(2017-11-07 01:37:10):Yep.
prasanthpul(2017-11-07 18:44:40):I'd like to see a description of how profile (ONNX, ONNX-ML) versioning works.
prasanthpul(2017-11-07 22:07:05):What are the standard domains for ONNX and ONNX-ML ops?
ezyang(2017-11-08 02:48:57):For ONNX, the standard domain is empty; the implicit domain corresponds to ONNX.

ONNX-ML domain needs to be decided; may I suggest `ai.onnx.ml`?
ezyang(2017-11-08 02:54:58):Here is one way to make it work:
* ONNX ML has a separate IR version number
* If a BC-breaking change is made in the ONNX ML fragment of the proto language, only the ONNX ML IR version number is bumped
* If a BC-breaking change is made in the ONNX fragment, the ONNX IR and ONNX ML IR version numbers are bumped.

Thus, ONNX only clients don't see any IR version changes from ONNX ML, and ONNX ML clients see a coherent version number profile by looking at the ONNX ML number only.
dzhulgakov(2017-11-08 04:21:48):how about calling it domain_version?
dzhulgakov(2017-11-08 04:23:52):Do we also need "last changed in version"?
dzhulgakov(2017-11-08 04:24:16):make it -1 and you can enforce that it's set in Finalize() call. But then you'd need to codemod all operator defs
prasanthpul(2017-11-08 04:26:54):Sounds reasonable to me since ONNX-ML is a superset of ONNX. Let's include this in the document.
ezyang(2017-11-08 12:48:57):This method definitely needs some renaming (and doc-updating); it should be "LastUpdatedVersion" or something like that. The name here comes from when I was originally planning to have the ONNX repo continuously keep all information about operators, both old ones (who have since been updated in a BC-breaking way) and the up-to-date ones.

Don and I have discussed a bit about how exactly want to go about representing history change in defs. A more traditional model is that the state of the Git repository at any point in time is the definitive description of the operator state at this point in time. So, for example, if the current ONNX operator version is 34 and you want 12, you'll have to check out the ONNX repo as it was at that version to get the canonical description of that ONNX operator state. When the only description of operators is C++ code, this is not a great state of affairs; so an important further component of this story is a normative protobuf description of the operators at some version, made available for download in some way. The good thing about this approach is that it closely matches ordinary software development practice, and it doesn't require us to change very much our model for operator development; plus, you can't "edit" history.
bddppq(2017-11-04 00:29:42):Default protobuf size limit is 64mb, while most of the onnx files exceed this limit. Increasing it to 1GB when parsing.
marcelolr(2017-11-04 00:31:09):I'd rename proto_utils.h to py_utils.h; the Python dependency is more interesting than the protobuf one, I think. Over time I'm sure we'll find plenty of things that we'll want to put in a "proto_utils.h" file, without pulling in pybind.
bddppq(2017-11-04 00:44:45):Good point, we should separate protobuf && python. I will add another file py_utils.h which will include this proto_utils.h.
marcelolr(2017-11-05 02:49:23):Note that the badge isn't correct, but I'm putting it in as a placeholder. I don't think I have the right access to configure AppVeyor for ONNX account. If someone could set it up and update the badge in the README.md, I'd be quite grateful - the .yml file has been tested and appears to work correctly.
AppVeyorBot(2017-11-05 07:18:38)::white_check_mark: [Build onnx 0.3.8 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.8) (commit https://github.com/onnx/onnx/commit/d5a28fdbb8 by @marcelolr)
AppVeyorBot(2017-11-05 07:18:39)::white_check_mark: [Build onnx 0.3.8 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.8) (commit https://github.com/onnx/onnx/commit/d5a28fdbb8 by @marcelolr)
prasanthpul(2017-11-05 03:47:53):Use: [![Build status](https://ci.appveyor.com/api/projects/status/lm50cevk2hmrll98?svg=true)](https://ci.appveyor.com/project/onnx/onnx)
AppVeyorBot(2017-11-05 20:21:39)::white_check_mark: [Build onnx 0.3.10 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.10) (commit https://github.com/onnx/onnx/commit/ef195f22c8 by @)
AppVeyorBot(2017-11-05 20:21:40)::white_check_mark: [Build onnx 0.3.10 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.10) (commit https://github.com/onnx/onnx/commit/ef195f22c8 by @)
bddppq(2017-11-06 18:21:49):`git-clang-format` (or the same thing `git clang-format`) will only format the lines you touched.
AppVeyorBot(2017-11-07 00:08:23)::white_check_mark: [Build onnx 0.3.27 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.27) (commit https://github.com/onnx/onnx/commit/3486b62e96 by @)
AppVeyorBot(2017-11-07 00:12:07)::white_check_mark: [Build onnx 0.3.28 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.28) (commit https://github.com/onnx/onnx/commit/7a567b2490 by @)
yuanbyu(2017-11-07 00:07:01):This is arguably not quite right, but I don't see an easy fix. Could you add a comment?
AppVeyorBot(2017-11-06 16:21:09)::white_check_mark: [Build onnx 0.3.19 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.19) (commit https://github.com/onnx/onnx/commit/e5710f2d0d by @tjingrant)
bddppq(2017-11-06 18:19:33):Good point. How about both? Like `atol=1e-4, rtol=1e-4`
tjingrant(2017-11-07 19:34:51):At this point it might be better for you to just do it here https://github.com/onnx/onnx/pull/207 . As this current update is becoming cumbersome without a helper function.

I'm sure you probably noticed that `np.testing.assert_allclose(.... rtol=1e-4, atol=1e-4)` is not what you want.
AppVeyorBot(2017-11-07 19:34:58)::white_check_mark: [Build onnx 0.3.50 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.50) (commit https://github.com/onnx/onnx/commit/3b5cb74c9d by @tjingrant)
bddppq(2017-11-07 19:41:36):> I'm sure you probably noticed that np.testing.assert_allclose(.... rtol=1e-4, atol=1e-4) is not what you want.

Sorry, I have not. Could you explain why?
tjingrant(2017-11-07 19:45:59):Quote form numpy documentation about assert_allclose:
> It compares the difference between actual and desired to atol + rtol * abs(desired).

I assumed you want to check for **both** absolute and relative deviation, the above quote from documentation means that using np.testing.assert_allclose(.... rtol=1e-4, atol=1e-4) only checks a necessary condition for what you want to assert, not a sufficient one.
bddppq(2017-11-07 20:50:16):Ah sorry after reread your first comment I found that I have misunderstood your original motivation. I thought you were saying the current checking being too strict and want to relax it. But turns out you were saying the current check is too loose. :-)

Hmm, I think it's a matter of defining how strict we want it to be. But my opinion is if later there are models with big output values, it's infeasible to require 0 atol tolerance. Besides, backend tests also have node tests, and their outputs values could be big too.
tjingrant(2017-11-07 21:06:39):@bddppq Yes indeed, I was having trouble understanding the motivation behind your suggestion as well but I would not object more stringent test conditions thus I concurred. So I think we should probably go with the first commit where only rtol of 1e-3 or 1e-4 is required? Because for big output values, maintaining a ~1e-4 absolute difference (which is the original check in onnx backend tests) is going to be much more difficult (infeasible, as you put it) than maintaining a ~1e-3 or ~1e-4 relative difference.
bddppq(2017-11-07 21:18:38):Yeah sounds good to me. I think numpy set`atol` defaults to 0 for good reason :-).
bddppq(2017-11-07 21:22:57):cc @opedge 
AppVeyorBot(2017-11-06 20:05:40)::white_check_mark: [Build onnx 0.3.22 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.22) (commit https://github.com/onnx/onnx/commit/28054898c6 by @bddppq)
AppVeyorBot(2017-11-06 21:01:04)::white_check_mark: [Build onnx 0.3.25 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.25) (commit https://github.com/onnx/onnx/commit/e90157a8d2 by @yuanbyu)
AppVeyorBot(2017-11-07 17:05:10)::white_check_mark: [Build onnx 0.3.44 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.44) (commit https://github.com/onnx/onnx/commit/ce9b66daf4 by @yuanbyu)
yuanbyu(2017-11-07 17:45:54):@dzhulgakov Yes we should definitely keep it consistent with the other element-wise ops. Thanks for pointing it out. Updated.

I accidentally based this branch on the RNN branch. 
prasanthpul(2017-11-13 20:05:05):Replaced by #233 
AppVeyorBot(2017-11-06 22:13:16)::white_check_mark: [Build onnx 0.3.26 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.26) (commit https://github.com/onnx/onnx/commit/29f43ddc6e by @jamesr66a)
AppVeyorBot(2017-11-07 01:47:37)::white_check_mark: [Build onnx 0.3.30 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.30) (commit https://github.com/onnx/onnx/commit/26a324de7e by @)
SherlockNoMad(2017-11-07 02:09:08):@ebarsoum @bddppq 
Appreciate it if you can review this.
AppVeyorBot(2017-11-07 02:24:50)::white_check_mark: [Build onnx 0.3.33 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.33) (commit https://github.com/onnx/onnx/commit/d5f2debaab by @)
SherlockNoMad(2017-11-08 23:21:56):@dzhulgakov 
I have remove the ops that you had concern. Here's the update list 

HardSigmoid
LogSoftmax
Hardmax
Softsign
Softplus

Appreciate it if you can merge this. 

SherlockNoMad(2017-11-11 00:09:55):All check passed. Please merge. Thank you. 
dzhulgakov(2017-11-07 03:03:00):can we merge ParametricSoftplus and Softplus? They are less frequently used so fewer ops - the better
dzhulgakov(2017-11-07 03:06:50):so what exactly does this op do? I can see you say about softmax here, but I don't get it. The TF one with the same name has but does something different (effectively argmax)
dzhulgakov(2017-11-07 03:07:11):nice, we also had #114 but it was dropped on the floor somehow :)
dzhulgakov(2017-11-07 03:07:47):how about to avoid copy-pasting this chunk we concat strings? There's FillUsing trick (search repo for it) or simple concat might work too.
dzhulgakov(2017-11-07 03:08:47):for ops like this would be cool to provide "lowering" pass that can translate into more elemental ops if necessary (min/max/arithmetics in this case). But probably not for now
dzhulgakov(2017-11-07 03:16:18):Again, do we really need it as a fused op? Few options: add scalar to the original Tanh or just ask people to use 2 ops. Given that there are so many of the ops added here, I'm a bit worried that we'd overload the spec too much.
dzhulgakov(2017-11-07 03:16:55):Again, should it be Relu with parameter? (and backends and bail out if they don't support it)
SherlockNoMad(2017-11-07 18:58:23):Hardmax([0.1, 0.2, 0.3]) == [0, 0, 1]. The largest entry will be mapped to 1 and other entries will be zero. 
This is not the same as argmax, where argmax only gives the index of largest entry. 
SherlockNoMad(2017-11-07 19:33:59):Right. Probably not for now
dzhulgakov(2017-11-08 00:53:34):So it's basically argmax + onehot. Makes sense. Probably deserves a separate op
AppVeyorBot(2017-11-07 01:58:54)::white_check_mark: [Build onnx 0.3.32 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.32) (commit https://github.com/onnx/onnx/commit/e344a44817 by @)
SherlockNoMad(2017-11-07 02:08:58):@ebarsoum @bddppq 
Appreciate it if you can review this.
dzhulgakov(2017-11-07 03:25:22):why do we need it? If it's identity, it can be just the name in the graph
dzhulgakov(2017-11-07 03:28:47):The name is bad as it can be confused with the fully connected layer. Maybe Scale or Affine? 

Also I wonder whether we should just use add and multiply instead if that's not perf-critical
dzhulgakov(2017-11-09 21:35:27):still, why on earth do you need identity? :) isn't it just an edge in the graph?
dzhulgakov(2017-11-09 21:35:53):name is still confusing. Maybe LinearTransform or Affine?
SherlockNoMad(2017-11-11 00:41:09):Identity op is present in 
TensorFlow,  https://www.tensorflow.org/api_docs/python/tf/identity
Keras, https://keras.io/backend/#identity 

From my understanding, the main purpose is for creating a copy of the tensor. 
AppVeyorBot(2017-11-07 04:42:45)::white_check_mark: [Build onnx 0.3.38 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.38) (commit https://github.com/onnx/onnx/commit/f158ad140d by @yuanbyu)
AppVeyorBot(2017-11-07 07:01:09)::white_check_mark: [Build onnx 0.3.41 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.41) (commit https://github.com/onnx/onnx/commit/5ea8326c57 by @ebarsoum)
ebarsoum(2017-11-11 04:33:50):@dzhulgakov  that the reason that I merged pool_h and pool_w to pool_shaoe for future expansion to more than 2D.
dzhulgakov(2017-11-09 21:32:58):the idea of FillUsing and Generator is to share some code between similar ops. Do you mind changing the code above to actually share a piece of common descriptions with MaxPooling?
ebarsoum(2017-11-11 04:27:54):Initially I tried that, but the description between Max and average is a little different.
AppVeyorBot(2017-11-07 11:24:51)::white_check_mark: [Build onnx 0.3.42 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.42) (commit https://github.com/onnx/onnx/commit/0df752afd0 by @bddppq)
AppVeyorBot(2017-11-07 17:53:36)::white_check_mark: [Build onnx 0.3.45 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.45) (commit https://github.com/onnx/onnx/commit/83eabea693 by @bddppq)
bddppq(2017-11-07 19:26:38):Hmm, not sure whether it's a github bug, tried [another renderer](https://jbt.github.io/markdown-editor/#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) and it displays fine. Will play with the github renderer to figure it out.
AppVeyorBot(2017-11-07 19:39:21)::white_check_mark: [Build onnx 0.3.51 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.51) (commit https://github.com/onnx/onnx/commit/69cd41211b by @bddppq)
AppVeyorBot(2017-11-07 19:54:09)::white_check_mark: [Build onnx 0.3.52 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.52) (commit https://github.com/onnx/onnx/commit/cbaed260e0 by @bddppq)
AppVeyorBot(2017-11-07 21:34:22)::white_check_mark: [Build onnx 0.3.55 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.55) (commit https://github.com/onnx/onnx/commit/9833e21ac8 by @bddppq)
ezyang(2017-11-08 20:30:53):SHIP IT
dzhulgakov(2017-11-07 18:47:08):you can use pkgutil to make it look nicer, e.g.: https://github.com/caffe2/caffe2/blob/master/caffe2/python/layers/__init__.py
bddppq(2017-11-07 19:27:29):wow it's much better, will use it.
ezyang(2017-11-08 13:34:47):We really should get in the habit of specifying which version of the operator we are testing ;)
AppVeyorBot(2017-11-07 18:31:18)::white_check_mark: [Build onnx 0.3.47 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.47) (commit https://github.com/onnx/onnx/commit/b8d17c0694 by @yuanbyu)
AppVeyorBot(2017-11-07 19:07:58)::white_check_mark: [Build onnx 0.3.49 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.49) (commit https://github.com/onnx/onnx/commit/2cf378be1a by @yuanbyu)
dzhulgakov(2017-11-09 21:24:12):Btw, we're looking forward to future test cases contributions :)
dzhulgakov(2017-11-08 09:33:30):Maybe use generic op version that doesn't hard-code data layout (e.g. something like https://www.tensorflow.org/api_docs/cc/class/tensorflow/ops/space-to-batch-n-d#classtensorflow_1_1ops_1_1_space_to_batch_n_d). In general ONNX doesn't hard code NCHW format unless necessary.

Just curious, for which model do we need it? Or is it because of CoreML?
ebarsoum(2017-11-08 19:03:59):@dzhulgakov  We can added the generic one in the feature, the reason I added this one is because it is already implemented in Caffe 2 and yes for CoreML compact.
ebarsoum(2017-11-10 07:32:49):@dzhulgakov  Change it to SpaceToDepth and its opposite instead, which is what we want. I agree for your comment for the previous OP.
ebarsoum(2017-11-11 04:26:18):@dzhulgakov  here the TF version:

https://www.tensorflow.org/api_docs/python/tf/depth_to_space

Why extend it require an extra attribute? Isn't just a description update?
dzhulgakov(2017-11-12 00:18:38):you're right, if we're talking about square blocks it's just a description update
dzhulgakov(2017-11-09 21:21:31):you're probably missing padding arguments...
tjingrant(2017-11-08 16:04:49):ping @bddppq @dzhulgakov @ebarsoum 
ebarsoum(2017-11-08 19:06:27):So the proposed behavior change is that `pads` always override `auto_padding`, right?
tjingrant(2017-11-08 19:15:15):@ebarsoum Yes, that way I believe it's a bit cleaner. Will change the other conv and pooling op spec in another commit if consensus can be reached here.
bddppq(2017-11-08 19:16:03):How about do not allow setting both "auto_padding" and "pads"?
tjingrant(2017-11-08 19:18:50):@bddppq that's even better, if onnx schema checker can automatically verify that (although I'm not sure if this is possible).
gramalingam(2017-11-08 19:19:00):Yes, it seems cleaner to not allow both "auto_padding" and "pads". 
bddppq(2017-11-08 22:41:43):OpSchema and checker don't support declaring exclusive attributes yet. For now let's just document it.
bddppq(2017-11-09 18:53:42):LGTM. @ebarsoum 
ebarsoum(2017-11-10 02:06:17):I am ok with this. So pads and auto_pad are exclusive... 
dzhulgakov(2017-11-12 00:12:56):Btw, does moving nested messages outside changes on-the-wire format?
ezyang(2017-11-12 00:22:10):No; message names never occur on the wire.
ezyang(2017-11-14 00:45:04):Rebased.
NiklasGustafsson(2017-11-08 22:45:17):Could we add a reserved properties block here, such as 100-200?
NiklasGustafsson(2017-11-08 22:46:53):In fact, systematically reserving blocks everywhere should help future-proof, I believe.
marcelolr(2017-11-11 03:22:45):This change proved somewhat controversial. The build information can be embedded in model metadata as needed (it has no enforceable semantics) and the prerelease information is easy to miss when applying semver rules. The simpler major/minor/patch scheme will stand then - I'm closing without merging.
ezyang(2017-11-08 23:05:25):Don't forget to run `python setup.py develop` to rebuild the proto files.
linkerzhang(2017-11-08 23:09:28):@ezyang ,Ya, I forgot that and just ran that and added files auto-changed accordingly. Thank you!
bddppq(2017-11-09 23:35:45):protobuf has no guarantee on putting the first field at the beginning bytes (meaning you still need to parse the whole file before being able to access the fields). Could you just grant this magic_prefix a new field number to keep binary compatible? Also the checker should only require this magic_prefix if the ir_version is new enough, like in `check_attribute`, I hope we don't invalidate all the existing model files.
jywumsft(2017-11-10 06:19:32):https://developers.google.com/protocol-buffers/docs/encoding#order
messages are written sequentially by field number.
bddppq(2017-11-10 07:01:55):@jywumsft No it's not guaranteed. If you read further in your linked page:

> However, protocol buffer parsers must be able to parse fields in any order, as not all messages are created by simply serializing an object – for instance, it's sometimes useful to merge two messages by simply concatenating them.
jywumsft(2017-11-10 07:28:53):@bddppq 
I don't think there's a contradiction. it is talking about implementing a general protobuf parser, which this PR is not trying to be. protobuf parsers have to handle cases where the field order is undeterministic (e.g. maps) or if messages are created not using serialization - e.g. 2 distinct messages concatenated together produces valid message. both don't apply here i think.

see this written by google engineer:
https://groups.google.com/forum/#!msg/protobuf/7do1mgl-5LM/y1ydFiwwDgAJ 
bddppq(2017-11-10 07:49:30):@jywumsft  There is. 

> it is talking about implementing a general protobuf parser, which this PR is not trying to be.

Yes ∑it's not creating a general full-functional pb parser, but sniffing the first bytes is definitely parsing the pb file.

It's not a guarantee (otherwise why does protobuf doc bother to make such a requirement on parsers?) And note onnx pb files are not going to be generated only using the `make_*` functions in this repo, exporters have flexibilities to produce valid pb files (which has no absolute guarantee on the order).
ezyang(2017-11-10 23:12:50):@bddppq The working assumption from MS (which I think is reasonable) is that most serializer implementations do serialize in numeric order. The magic is advisory; it's not mandated to exist.
marcelolr(2017-11-11 03:21:27):The idea was not terrible (I'd like to think) but it seems to be confusing enough and not enforce-able enough that it'll be more distracting than anything else. Rather than fine print here, I'd rather people used a different mechanism - care in managing files might be good enough. I'll close this without merging - thanks for the good discussion!
prasanthpul(2017-11-09 08:27:12):Why ONXF instead of ONNX? 
jywumsft(2017-11-09 20:25:44):why field number 13 (gap between 8 and 13)
also, my understanding was ir_version was previously designed to be the first field parsed. so if
we were to move it, would it make more sense as field 2?
jywumsft(2017-11-09 20:26:00):comment refers to field 12 ? but below it's 13?
dzhulgakov(2017-11-09 21:09:19):so if I'm not mistaken it's no backward compatible change: https://developers.google.com/protocol-buffers/docs/proto - fixed32 and int64 are not interchangeable

did you try to see what happens to existing models?
marcelolr(2017-11-09 22:06:24):Keeping every byte distinct - just good practice to figure out byte ordering problems and serializers and whatnot. Not a particular problem when we all use protobuf, but good general practice.
marcelolr(2017-11-09 22:07:17):This was done because there are other PRs in flight that are using those. The particular number isn't important per se. Is there any particular reason folks want to pick one ir_version sooner? I can renumber things to go in text order if there's a preference for that.
marcelolr(2017-11-09 22:09:14):Correct - I mentioned this would break existing files, as well as other implementations that don't pick up the new definition (as they will misidentify the field). But everything is stamped with major version of zero today, so implementations will start rejecting that soon one way or another.
ezyang(2017-11-09 23:15:02):int64 to fixed32 is a BC breaking change 
ezyang(2017-11-10 23:13:44):NB: This is testing an implementation detail of the protobuf library we're using; the library is allowed not to do this.
ebarsoum(2017-11-10 03:13:00):@bddppq  I renamed and updated Normalzie to LpNormalization to be consistent and more flexible.
Yangqing(2017-11-10 17:57:02):Travis seems to have passed but it's not updated back to github yet: https://travis-ci.org/onnx/onnx/builds/299981704?utm_source=github_status&utm_medium=notification consider it already passing.
bddppq(2017-11-09 18:41:59):Is this covered by Sub (by putting the attribute "image" as the second input)?
ebarsoum(2017-11-10 03:12:13):Agree removed.
bddppq(2017-11-10 07:16:58):If only 2.0 is supported, how about renaming the op to L2Normalization and remove this attribute? :-)
bddppq(2017-11-10 07:19:15):Is it equivalent to Scale followed by Add?
ebarsoum(2017-11-10 07:28:55):I am planning to move it from experiment to core, and I want to make it generic so in the future we can add p=1.0.
ebarsoum(2017-11-10 07:34:48):Yes, but we have one fused OP that does both at once. I put it in experiment with the attention to remove it in the future.
bddppq(2017-11-09 10:43:06):Example report: https://gist.github.com/bddppq/741063fa3d778631593fb8c975a1fc8b
dzhulgakov(2017-11-10 19:06:36):This is awesome, thanks! :)
ebarsoum(2017-11-10 07:27:00):Leave FC for now for CoreML.
bddppq(2017-11-10 07:53:03):lol
linkerzhang(2017-11-10 22:07:38):@bddppq Thank you very much for all the great comments! Let me know if there're any more comments please.

I'm going to check-in this, so that I could work on my next PR of adding sequence/map types' parsing logic for onnx-ml. Any comments are still welcomed and I could resolve that later.
ezyang(2017-11-10 23:11:30):Now that we have two ways to "build" onnx, we need CI for both of them.
marcelolr(2017-11-10 08:58:16):These should also check for onnx-operators.pb.cc and its ml variant; otherwise the linker will catch it and complain about duplicate symbols. More generally, it would also be nice to side-by-side some of this (so be able to build both projects in a single install), and to doc somewhere that ```pip install -e <dir> --install-option="--onnxml=1"``` is how the magic happens, but this change seems like a step in the right direction in terms of getting a good repeatable thing going.
bddppq(2017-11-10 17:22:58):Maybe move this `ifdef` chunk into a separated file so other files can use it as a proxy to the correct pb.h file?
bddppq(2017-11-10 17:28:56):Use user_options to add this flag.
bddppq(2017-11-10 17:29:38):You are using `ONNX_ML` as a global variable, so no need to store it into separate command.
bddppq(2017-11-10 17:30:11):ONNX_ML (and `self.onnxml`) is boolean, do `if ONNX_ML`
bddppq(2017-11-10 17:31:23):Access `ONNX_ML` inside create_extension instead of passing it as argument. It's a global setting.
bddppq(2017-11-10 17:33:19):Yes agree, please pick out the correct 'pb.cc' file instead of raising errors.
And like above, do not use `0 == ONNX_ML`, it's a boolean already.
linkerzhang(2017-11-10 17:34:57):sure, will add the build into regular build later and will also update the doc accordingly. Thanks!
linkerzhang(2017-11-10 20:04:51):Great suggestion! Will do it and send new iteration.
linkerzhang(2017-11-10 22:07:59):fixed. Thanks!
linkerzhang(2017-11-10 22:08:10):fixed. Thanks!
linkerzhang(2017-11-10 22:08:17):fixed. Thanks!
linkerzhang(2017-11-10 22:11:20):I'm using --install-option="--onnxml=1". 
"user_options", you mean options for a command, right? I tried that and it needs to send the flag around, and I'd use it as a global setting. I'll keep it as it is if there's no strong concern. Thank you!
ezyang(2017-11-10 23:10:58):This seems really questionable. Let's at least document it as such. You probably got this from the second answer to the top listed SO question on this subject?
linkerzhang(2017-11-10 23:17:55):OK. let me document it. I also don't like two ways of building onnx. However, as long as we have onnx-ml and onnx separately, it has to be there, right? :). 
bddppq(2017-11-11 01:57:10):@linkerzhang Yeah I think @ezyang is not questioning about adding a flag, but the way you do command line parsing here (e.g. it won't show up in the help message, user can not do `--onnxml` or `--onnxml 1`).
bddppq(2017-11-11 02:09:55):no need to install test requires again
linkerzhang(2017-11-11 02:12:40):Fixed and Thanks!
linkerzhang(2017-11-11 04:02:08):This is same as PR#201. On behalf of Yuan, am going to merge it. was signed-off by @dzhulgakov .
ebarsoum(2017-11-11 06:03:18):Shouldn't we call it InstantNormalization to match the same naming convention for other norm OP?
dzhulgakov(2017-11-12 00:01:49):since we're not in training mode - let's remove it for now
dzhulgakov(2017-11-12 00:02:20):let's kill it as it's framework internals for grad computation (even if we target training)
dzhulgakov(2017-11-12 00:02:36):Let's use NCHW format everwhere without creating more confusion as per our earlier discussions
dzhulgakov(2017-11-12 00:03:52):would you be kind to also include math equation instead of just referencing the paper? (I know Caffe2 docs are guilty of that but let's be better)
SherlockNoMad(2017-11-12 01:01:41):done.
SherlockNoMad(2017-11-12 01:01:50):Done.
SherlockNoMad(2017-11-12 01:03:23):OK, I can do that. But I guess BatchNomarlization would also need  the similar update ? 
dzhulgakov(2017-11-13 04:18:49):scale
dzhulgakov(2017-11-13 04:19:00):computed 'per instance per channel'. I'd appreciate more detailed notation too
SherlockNoMad(2017-11-13 06:42:29):Fixed.
SherlockNoMad(2017-11-13 06:42:34):fixed.
ezyang(2017-11-11 11:18:01):This is your friendly neighborhood op versioning czar, reminding you that this is a BC breaking change to 'split' and thus we will need to bump the ONNX operator version. (Yes I know we haven't "released" yet but this is a good test run of our procedures for handling BC-breaking changes.)

If you want to get the new ops in earlier, I'd recommend splitting up the PR.
CLAassistant(2017-11-15 01:46:03):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=235) <br/>All committers have signed the CLA.
ebarsoum(2017-11-15 18:28:49):Superseded by https://github.com/onnx/onnx/pull/262
ezyang(2017-11-16 13:59:17):FYI #262 only has the Split change, not the other bits.
dzhulgakov(2017-11-12 00:20:17):Also, could you also fix rendering a bit? Seems like for RNN ops it's not perfect: https://github.com/onnx/onnx/blob/master/docs/Operators.md#lstm
yuanbyu(2017-11-13 19:49:02):@dzhulgakov @raduk Thanks for the comments. I have updated the PR. PTAL.
dzhulgakov(2017-11-11 23:51:33):put them in the common block?
dzhulgakov(2017-11-11 23:55:54):I guess the correct representation should be Xt * Wi^T. (or keep the original one since for non-batched version it's correct). The input matrices are [hidden_size, input_size] according to the docs (and my previous reference implementation). Which means that you'd need either Wi*Xt or Xt*Wi^T (assuming Xt is a vector and doesn't need to be explicitly transposed)
dzhulgakov(2017-11-11 23:56:41):let's use lowercase for everything to avoid confusion?
dzhulgakov(2017-11-11 23:57:59):I'd say that this whole argument is optional and implementations don't have to support it (including overriding standard activations with tanh/sigmoid.
dzhulgakov(2017-11-11 23:58:26):so, how does user specify alpha/beta?
raduk(2017-11-13 02:04:20):What is the packing for bidirectional ?

Another option would be to have alphas and betas in a single float list; if only two of them, then is [alpha, beta], otherwise [alpha[0],..,alpha[n], beta[0],..,beta[n]]. You pack together activations for forward and backward, you might as well pack the arguments.
raduk(2017-11-13 02:07:36):Add the clip to the common block, it is needed for RNN as well.
raduk(2017-11-13 02:35:02):STRINGS
raduk(2017-11-13 02:42:03):I'd replace sigmoid with f(), and tanh with g(). This way we understand better why 2 / 4 functions are needed.
raduk(2017-11-13 04:30:26):Do we actually expect this to be [1, hidden_size, input_size] for forward only pass ? If yes, this will require expand_dimensions / squeeze operations in the graph to deal with the 1 dimension. If no, and we actually expect [hidden_size, input_size] for forward and [2, hidden_size, input_size] for bidirectional, then the spec should reflect this.
raduk(2017-11-13 05:09:10):Another option is to make it [num_directions*hidden_size, input_size]
yuanbyu(2017-11-13 19:47:48):I tried it using replaceAll(). It looked quite awkward. I will keep it as is, unless you have a better way of doing it.
yuanbyu(2017-11-13 19:47:50):Oops. My mistake. It should be Xt * Wi^T.  Thanks.
yuanbyu(2017-11-13 19:47:55):OK. Done.
yuanbyu(2017-11-13 19:47:57):Using the two optional attributes alpha and beta.
yuanbyu(2017-11-13 19:48:02):Yes, it is optional. I will make it more explicit.
yuanbyu(2017-11-13 19:48:17):I would keep them separate to make it more intuitive and easier to explain.
yuanbyu(2017-11-13 19:48:25):Done. Thanks.
yuanbyu(2017-11-13 19:48:31):Done.
yuanbyu(2017-11-13 19:48:37):Done.
yuanbyu(2017-11-13 19:48:51):Let us leave it as is for now.
dzhulgakov(2017-11-14 06:35:46):it's unclear how they are specified. I.e. if I have `activations='tanh,prelu'`, should `alpha` be `0,0.1` or just `0.1`

also should be called `activations_alpha` for clarity?
dzhulgakov(2017-11-14 06:38:02):the description is not clear - and you probably need to change the Output description too
yuanbyu(2017-11-14 17:30:56):It should just be `0.1`. I updated the doc to make it more clear hopefully.

Changed to `activation_alpha` and `activation_beta`.
yuanbyu(2017-11-14 17:30:58):Updated.
ebarsoum(2017-11-14 19:07:39):@bddppq I removed the p=2 restriction, also p was float by mistake so I change it to int.
bddppq(2017-11-14 19:43:48):LG
bddppq(2017-11-12 06:17:04):Since now it's moved to official, should we remove the "only 2.0" restriction?
gramalingam(2017-11-13 17:40:34):Sounds good to remove the unused ones for now.
prasanthpul(2017-11-14 05:05:46):Can you make "docs/versioning.md" a hyperlink?

"as well as a propose how models" => some words missing or some extra words
prasanthpul(2017-11-14 05:07:57):there is no experimental domain. it's just a flag on the regular domain.
prasanthpul(2017-11-14 05:09:07):Not sure what this means in practice. Experimental ops are documented with the regular ops. where are these experimental fields documented? How do we avoid conflicts between different experimenters?
marcelolr(2017-11-14 16:04:50):fixing
marcelolr(2017-11-14 16:11:03):Well, the could do that ;) Sorry, these are really about extensions not versioning, I've started a section over in IR and they belong there.
marcelolr(2017-11-14 16:11:05):See above. I mixed versioning with extending here, I'm removing that verbiage.
CLAassistant(2017-11-16 23:59:32):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=244) <br/>All committers have signed the CLA.
marcelolr(2017-11-17 00:01:28):Less is more
dzhulgakov(2017-11-14 02:21:30):maybe link to versioning doc or proto file directly? Or do we have a place that describes how opset_import works?
dzhulgakov(2017-11-14 02:24:30):It's not a common practice to have reserved ranges of protobuf fields. I'm not really sure about "reserved for private use" - do we expect people to write their own extensions of ONNX format? If so - what's the way of doing so?

I'm not opposing to this formulation, I'm just really not sure whether and how it'd be used.
marcelolr(2017-11-14 03:20:46):That's described in Versioning.md but it really belongs here. I can do a followup where I pull in the non-versioning aspects of Versioning.md into IR.md; it's a bit odd that the mechanism for binding operators is described there and not here. But I don't feel particularly strong about it - do you have a preference?
marcelolr(2017-11-14 03:26:18):Really, this is to help people that want to try extending ONNX for some reason to distinguish between "I need a field for my own purposes" (private use, say some "your department gets charged `n` credits per evaluation on company servers") vs. "I'm trying out something that I'd like to share with others" (experimentation) (say, some sort of proposal for handling differential updates). Implements can get traction on this without reaching out to the community, and minimizing their chances of colliding with someone else's extensions.

I don't think it's going to get much usage, but if it ever does, it helps to have this guidance in place.
prasanthpul(2017-11-14 05:12:07):I can understand the private use. But I don't see a need for experimentation range. When you are trying something out, it's private use and can use field #s in the private range. When you are ready to contribute it back, you would submit it as a PR with new field #s in the production range. 
dzhulgakov(2017-11-14 06:15:51):My concern is that in order to extend protobufs one would need to fork the spec. Protobuf has builtin extension mechanism but unfortunately it's different between proto2 and proto3.
marcelolr(2017-11-14 16:24:36):@prasanthpul the experimentation range can be used to make sure you can at some point show your work to someone else without immediately colliding with someone's private usage.

@dzhulgakov do you have a different proposal? I imagined that a private extension would indeed fork the spec, if nothing else because it requires a different implementation to actually do something with the new data (and that thing wouldn't just rev transparently with new ONNX updates), so I think it's expected for the scenario not a bug.

An alternative is to kill the protobuf field extensions verbiage to further discourage that. Given how easy it is in practice to drop some new fields in the .proto distribution and get an implementation that is backward compatible with what exists out there, I suspect people will definitely play around with that.

I don't have much personal experience using .proto in production, so I'm happy to hear how this tends to play out and adjust accordingly.
prasanthpul(2017-11-14 20:10:37):@marcelolr if you use experimentation range and share it with someone else who is also using experimentation range, you will have a collision since we are not assigning different experimentation ranges to different parties. 
marcelolr(2017-11-14 20:17:30):@prasanthpul in the experimentation range, you would expect active development so one or another can be updated to coexist. It's also easier to discover because you're sharing your development with others, as opposed to the private range.

People extending a system in a distributed manner are at risk of creating conflicting extensions. Other than discouraging extensions in the first place, I don't see how avoiding any guidance will lead to better outcomes, unless you're arguing for a more sophisticated approach?
marcelolr(2017-11-14 22:52:29):Discussed with @prasanthpul. We'll just comment on private use range and set up by example the practice of contributing to the community for extensions/experiments.
dzhulgakov(2017-11-16 04:49:49):The only other proposal I can think of is to just add `extra_data` string and let people serialize what they want there. In some case by even talking of field ranges we're encouraging extensions. But I don't feel strongly about it and the current solution is fine.
prasanthpul(2017-11-16 22:53:36):let's leave it out for now. its too early for extension
linkerzhang(2017-11-14 16:44:32):Reminder: we may also put the proposals as part of onnx "coding guidelines".
dzhulgakov(2017-11-15 21:30:22):Let's make attributes names all lower-case
prasanthpul(2017-11-15 21:30:50):decision is to make all attributes lowercase
bddppq(2017-11-14 21:52:19):hmm, using "bias" for bias and then "W" for weights sounds inconsistent to me
prasanthpul(2017-11-14 21:55:37):I'd be okay changing 'bias' to 'B' in a separate PR
ezyang(2017-11-15 01:15:47):If we have a philosophy like this, it is worth documenting somewhere. Maybe add a PHILOSOPHY.md
prasanthpul(2017-11-16 18:13:57):fyi - there is a separate pr for bias (#269) and another pr for OpConventions (#270)
AppVeyorBot(2017-11-14 08:21:33)::white_check_mark: [Build onnx 0.3.211 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.211) (commit https://github.com/onnx/onnx/commit/c8dab3a2a9 by @Yangqing)
bddppq(2017-11-14 08:24:24):@onnxbot add to whitelist
linkerzhang(2017-11-14 16:40:56):You may need to run the gen_doc.py to update Operation.md.
Yangqing(2017-11-14 17:54:16):@linkerzhang yup, done. Was on my home computer where the python setup was nuked.
ezyang(2017-12-11 17:02:54):I'd feel better if we had some sort of reference implementation, e.g., in onnx. Also, given that URIs are a logical extension point for string reference IDs, is the nested message necessary?
postrational(2018-03-29 15:01:11):This idea was superseded by https://github.com/onnx/onnx/pull/678
jamesr66a(2017-11-14 18:39:56):Awesome! I was going to do this but didn't get around to implementing it
bddppq(2017-11-14 19:51:36):The original error message will be lost.
bddppq(2017-11-14 19:52:48):How about making it a vector so each level can put its own helpful debug message?
bddppq(2017-11-14 20:00:31):fancy
dzhulgakov(2017-11-14 20:36:25):the original message is not lost - we append it here. The trickery is because what() needs to returns char* and thus we should store the entire message together.
dzhulgakov(2017-11-14 20:36:52):Each level can call AppendContext - they will be appended continuously
dzhulgakov(2017-11-14 20:37:11):figured it's faster to do this way than iterate over all fields manually :)
bddppq(2017-11-14 21:28:49):Ah sorry didn't carefully look into this function :-)
SherlockNoMad(2017-11-21 21:29:39):I have split this PR. Let's discuss the embedding in another PR. 
This PR is a just renaming some attributes in the ops.

AppVeyorBot(2017-11-21 21:39:22)::x: [Build onnx 0.3.448 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.448) (commit https://github.com/onnx/onnx/commit/e49d92fc0b by @SherlockNoMad)
AppVeyorBot(2017-11-21 21:49:39)::white_check_mark: [Build onnx 0.3.449 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.449) (commit https://github.com/onnx/onnx/commit/f1076a379f by @SherlockNoMad)
dzhulgakov(2017-11-23 03:42:15):Looks good but please rebase - there are some merge conflicts
dzhulgakov(2017-11-15 21:37:10):Isn't it Gather
SherlockNoMad(2017-11-15 23:38:17):Taking 2D weights as an example, 
Gather picks row by rowID, Embedding pick column by columnID. 

And yes, embedding can be done by transpose + gather + transpose, but it's very commonly used and deserves a separate op. 
prasanthpul(2017-11-16 02:02:23):weights => W per naming guidelines
prasanthpul(2017-11-16 02:03:24):weights => W per naming guidelines
dzhulgakov(2017-11-16 04:24:14):Can you clarify when this op is actually useful in this setting? ONNX pretty much assumes row-major order so any sensible lookup table should be indexed by rows if it's to get any reasonable perf.
SherlockNoMad(2017-11-17 20:05:07):OK
SherlockNoMad(2017-11-17 21:09:53):Embedding is commonly used for NLP's word embedding lookup.

https://apple.github.io/coremltools/coremlspecification/sections/NeuralNetwork.html#embeddinglayerparams 
prasanthpul(2017-11-17 23:53:32):since it's in experimental mode, let's keep this. seems like there is some use for it. sound okay @dzhulgakov ?
ebarsoum(2017-11-18 00:01:13):This isn't the same as Gather, embed multi the input with the embed matrix, it is only similar to gather if the input is in one hot presentation, right?
dzhulgakov(2017-11-18 00:26:41):Ok, seems like it's equivalent to Gather except that the weights matrix is transposed: it takes a list of indices and returns looks them up in the matrix. I'd argue that it never makes sense to keep matrix transposed (not sure why Apple did it this way) because of memory locality. Can't you just use Gather directly then?

Even though it's experimental I'd like to avoid duplication of operators with nearly-identical semantics
SherlockNoMad(2017-11-18 02:02:42):Thank you on reviewing this. Could you please sign off? 
ebarsoum(2017-11-18 02:14:12):@dzhulgakov  if the input to embed isn't one hot vector, how this is gather?
SherlockNoMad(2017-11-20 19:06:53):@dzhulgakov  As we have explained above. Embedding is not the same as Gather. 
bddppq(2017-11-14 22:44:29):This attribute is not part of the standard definition of softplus (it doesn't appear in coreml either). Is there some real use cases that need this? Can it be implemented by applying Scale to the input first?
AppVeyorBot(2017-11-14 23:14:50)::x: [Build onnx 0.3.244 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.244) (commit https://github.com/onnx/onnx/commit/a2ea159e48 by @houseroad)
AppVeyorBot(2017-11-16 05:33:21)::x: [Build onnx 0.3.299 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.299) (commit https://github.com/onnx/onnx/commit/5df0a6d878 by @houseroad)
AppVeyorBot(2017-11-16 05:45:19)::x: [Build onnx 0.3.300 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.300) (commit https://github.com/onnx/onnx/commit/4ea4a759b5 by @houseroad)
AppVeyorBot(2017-11-16 06:07:33)::x: [Build onnx 0.3.301 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.301) (commit https://github.com/onnx/onnx/commit/55e378744c by @houseroad)
AppVeyorBot(2017-11-16 06:23:35)::x: [Build onnx 0.3.302 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.302) (commit https://github.com/onnx/onnx/commit/72f53686f1 by @houseroad)
AppVeyorBot(2017-11-16 08:57:00)::x: [Build onnx 0.3.303 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.303) (commit https://github.com/onnx/onnx/commit/e4b904a162 by @dzhulgakov)
bddppq(2017-11-17 01:18:45):@onnxbot retest this please
bddppq(2017-11-14 23:17:52):f is already a file object, no need to open its name.
bddppq(2017-11-14 23:23:51):f should be already a file object, no need to open it.
bddppq(2017-11-14 23:26:50):Doesn't look good in a test.
bddppq(2017-11-14 23:28:03):Maybe not to add a separated function but use `os.path.exists` in `load` to distinguish a path vs. a string?
houseroad(2017-11-15 00:04:00):I think it's safer to separate it from load. :-)
bddppq(2017-11-15 00:20:33):> Whether the name can be used to open the file a second time, while the named temporary file is still open, varies across platforms (it can be so used on Unix; it cannot on Windows NT or later)

https://docs.python.org/2/library/tempfile.html#tempfile.NamedTemporaryFile
bddppq(2017-11-15 21:12:20):I was ok with adding a separate function yesterday :-) But today there is this PR #265 adding an additional check in the `load` function, which makes me prefer to merge this into `load`.
houseroad(2017-11-15 21:15:40):Sure, with this check, it is safer to do so.
dzhulgakov(2017-11-16 04:37:36):Actually it's better to have it as a separate function imho. It's weird that load('typo in my file name') will try to parse the string as protobuf :)
bddppq(2017-11-17 01:21:19):this will cause the re-open issue on windows, because inside `onnx.load` it will try to open the file again.
houseroad(2017-11-17 01:22:41):Thanks! Yes, I saw it. Let me think of a solution. :-)
bddppq(2017-11-17 03:06:44):You can create `tempfile.NamedTemporaryFile(delete=False)` and then close it and pass its name to `onnx.load`
CLAassistant(2017-11-15 01:50:24):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=257) <br/>All committers have signed the CLA.
AppVeyorBot(2017-11-15 01:56:57)::x: [Build onnx 0.3.252 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.252) (commit https://github.com/onnx/onnx/commit/fa2a966a9f by @lutzroeder)
AppVeyorBot(2017-11-15 02:04:18)::x: [Build onnx 0.3.255 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.255) (commit https://github.com/onnx/onnx/commit/c92933bd95 by @lutzroeder)
AppVeyorBot(2017-11-15 02:19:54)::x: [Build onnx 0.3.257 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.257) (commit https://github.com/onnx/onnx/commit/da4ba98219 by @lutzroeder)
AppVeyorBot(2017-11-15 02:32:09)::white_check_mark: [Build onnx 0.3.258 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.258) (commit https://github.com/onnx/onnx/commit/61c3aae232 by @lutzroeder)
AppVeyorBot(2017-11-15 02:34:57)::x: [Build onnx 0.3.259 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.259) (commit https://github.com/onnx/onnx/commit/57c13b25f4 by @lutzroeder)
AppVeyorBot(2017-11-15 02:38:22)::white_check_mark: [Build onnx 0.3.260 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.260) (commit https://github.com/onnx/onnx/commit/624b1a0bf1 by @lutzroeder)
AppVeyorBot(2017-11-15 02:43:45)::x: [Build onnx 0.3.262 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.262) (commit https://github.com/onnx/onnx/commit/44f7d6d3a6 by @lutzroeder)
AppVeyorBot(2017-11-15 02:46:49)::white_check_mark: [Build onnx 0.3.263 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.263) (commit https://github.com/onnx/onnx/commit/357df1afa1 by @lutzroeder)
gramalingam(2017-11-15 22:22:13):I think having a machine-readable version of the operator specification is good. The one suggestion I have is that it would be nice to have such a specification in one place. Currently, parts of the specification exist in the OperatorSetProto file. If the same could be expanded to include more metadata about operators, that would be advantageous. (E.g., see issue [https://github.com/onnx/onnx/issues/188])
lutzroeder(2017-11-15 23:16:06):Yes, having a OperatorSetProto file would do the job assuming all the information like docs and samples get included. Is the idea to serialize this to JSON to make it readable and editable (which is probably preferable as such a file might become the master that other code is generated from) or will it be a binary protobuf? Who is working on this and is there some code generating this file?
linkerzhang(2017-11-15 06:42:21):There's also ci build setup for windows https://github.com/onnx/onnx/blob/master/appveyor.yml. which needs to be changed.
linkerzhang(2017-11-15 06:53:35):May I know the point/usage of generating operator.json please? Do you want to check in this operator.json like operator.md? I think we don't want to check it in, if that's the case, then please set the default format running without generating operator.json.
lutzroeder(2017-11-15 06:57:54):To goal is to generate and check-in both. Operator.md is human readable, Operator.json is machine readable. There is no way to get this information in a machine readable form without installing and running the onnx Python code otherwise. The .proto file and this info would be needed to write tools that depend on the data in a model.db but do not run the actual onnx Python code.
lutzroeder(2017-11-15 08:21:47):Updated...
linkerzhang(2017-11-15 15:02:59):Sorry, what information do you need from this .json doc please? I think you may call OperatorSchemaRegistry to access all those information programatically instead of parsing a json doc. We have both python and c++ API available there. 
lutzroeder(2017-11-15 19:46:54):Those APIs only work if the onnx packages are built and installed on a machine. If onnx is not installed these APIs are not available. The goal is to have some machine processable "schema" that can be used by tools to annotate a model.db graph with more information without requiring onnx installed. This file could be generated on a dev machine but ideally onnx.proto and Operator.json should represent a consistent snapshot for each commit and gen_doc.py seems to be the current approach to make sure they are. What information is needed... pretty much all of it. Think about it in terms of xmldoc: There is documentation in the code, it gets written to a machine readable file and then the html/markdown docs get generated from such data file. Intellisense in an IDE can be driven from that data file without being tied to a specific API. Another example would be java class files, which contain metadata that can be processed without calling a specific API, there is a data format that can be read and processed. Right now in onnx, operator type information and docs are buried in a specific binary. Can it be made available in a machine readable data file and how is that file being shared...
linkerzhang(2017-11-15 21:53:42):Thank you for detail clarification. Hmmm, I personally have some concern of having this json (maintained) checked in as another representation of ONNX "standard". I'd hear some other voices. 
dzhulgakov(2017-11-16 04:34:50):Depends on what kind of tooling you're thinking about. It's important to make clear that we don't want backend implementations to consume this file and instead they should depend on onnx lib itself if possible. But if you need to build some javascript functionality (e.g. Atom plugin) it might be a reasonable approach.

Putting it in repo vs generating on-the-fly doesn't matter that much. If you're putting it in repo - do you plan to package it in the pip distribution too?
lutzroeder(2017-11-16 13:56:16):Tooling I was thinking about is of the javascript kind. Based on the thread @gramalingam mentioned below it sounds like the right approach is to abandon this PR for now. I can build the JSON file locally as a stop gap and wait for proper support via OperatorProto. Ideally the OperatorProto file should be serialized to JSON as well (to be readable) and get checked in to a known location but getting the proto definitions right probably requires some coordination.
CLAassistant(2017-11-15 12:04:07):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=262) <br/>All committers have signed the CLA.
gramalingam(2017-11-15 18:41:52):Looks good to me (modulo preceding comments by Ke)
linkerzhang(2017-11-15 21:30:36):@ezyang , as you're on vacation, if you agree with the comments I made, I can do those changes by myself. I'm now approving this PR (looks like there's no approve button for to click for now) :).
ezyang(2017-11-16 01:39:00):@linkerzhang Thanks for volunteering to finish the patch! I suggested a slight generalization to the proposed change, but it sounds good to me!
linkerzhang(2017-11-15 14:47:44):I think we need to have two global version numbers here, one for onnx, one for onnx-ml (we'll have two domains in onnx source tree, right?)

Let's also add comment to clarify a little bit more that this is the latest and biggest SinceVersion of an operator set and this "version" needs to be bumped accordingly when BC-breaking changes made on an operator set.
linkerzhang(2017-11-15 14:55:18):Change the API a little bit as "const OpSchema* LatestSchemaSinceVersion(const std::string& key, int version = MAX_INT)", so that backend will be able to call this API to get the lowest version it wants to support.
gramalingam(2017-11-15 18:14:16):A question: the extensibility scheme supports arbitrary domains; is the plan that this repository will provide special support for "onnx" and "onnx-ml"? Just trying to understand if it is okay to assume we have only these two domains in the implementation of the OpSchemaRegistry, or if we should design a more general mechanism.
ezyang(2017-11-16 01:18:06):This is a good idea! I wonder if we shouldn't also implement this for the "get all operators" function.
ezyang(2017-11-16 01:36:56):I didn't touch the domain mechanism because I knew you were going to be looking at it @linkerzhang :)

There are a bunch of ways to skin this cat, and I think the design should be related to how we do domains. IIRC, the plan for domains was to add a `Domain` field to OpSchema. In that case, it would probably make more sense for us to have map from domains to version numbers, to support however many version numbers we need to track.
CLAassistant(2017-11-15 17:39:51):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=264) <br/>All committers have signed the CLA.
gramalingam(2017-11-15 17:56:05):So, I guess this is just the API exposed for registering ops with a domain? I guess the registry implementation will also have to be extended (just like in Edward's PR 11 for version-number)? Is the plan to do that separately? Thanks.
linkerzhang(2017-11-15 18:24:38):@gramalingam , Thank you! I'll do the operator registry implementation change. I assume that you're talking about the Schema function in OpSchemaRegistry (say, calling with a op_type, version, domain, an op schema will be returned. I'll wait for Edward's change checked in firstly, otherwise, I'll have to resolve conflicts, you know. :)
bddppq(2017-11-16 01:25:30):LG
bddppq(2017-11-16 01:23:17):nit: since you don't use `domain` in this block, you can do `for _, _, op_type, schema ...`
CLAassistant(2017-11-15 19:57:21):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=265) <br/>All committers have signed the CLA.
linkerzhang(2017-11-15 21:04:13):I know few about python, looks you may want to use decoded = model.ParseFromString(obj.read()) :)
CLAassistant(2017-11-15 20:35:30):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=266) <br/>All committers have signed the CLA.
dzhulgakov(2017-11-15 22:30:36):Awesome!

Shall we also have a snippet on preferred way of adding tests, including model tests? Just code pointers are fine.

Also, maybe say that converter itself can be implemented in c++ and just have bindings?
marcelolr(2017-11-15 21:22:17):deligate->delegate
marcelolr(2017-11-15 21:22:43):an unified->a unified
marcelolr(2017-11-15 21:24:01):of varies hardwares->over varied hardware
CUDA gpus->GPUs (not sure it's worth clarifying CUDA-ness)
marcelolr(2017-11-15 21:24:45):whether -> whether
CLAassistant(2017-11-15 22:54:12):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=267) <br/>All committers have signed the CLA.
dzhulgakov(2017-11-16 00:01:09):nit: extract a common string?
dzhulgakov(2017-11-16 00:01:29):nit: you have lower/upper vs begin/end language inconsistency
gramalingam(2017-11-16 03:10:15):I don't understand this line (the ^T part). Is this some common notation? 
gramalingam(2017-11-16 03:11:12):Other things look good to me.
dzhulgakov(2017-11-16 04:10:49):you mean input tensors being int? or IR fields?
dzhulgakov(2017-11-16 04:14:40):It actually should be `X*W^T+B`. Might make sense to clarify the usual shapes of inputs (e.g. X being `batch X a` and W being `b X a`)
CLAassistant(2017-11-16 20:05:16):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=271) <br/>All committers have signed the CLA.
CLAassistant(2017-11-16 20:16:42):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=272) <br/>All committers have signed the CLA.
AppVeyorBot(2017-11-16 20:37:44)::x: [Build onnx 0.3.318 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.318) (commit https://github.com/onnx/onnx/commit/c897771a77 by @dzhulgakov)
bddppq(2017-11-16 20:39:21):@onnxbot retest this please
AppVeyorBot(2017-11-16 20:42:56)::x: [Build onnx 0.3.320 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.320) (commit https://github.com/onnx/onnx/commit/4c666328ca by @dzhulgakov)
linkerzhang(2017-11-16 23:19:04):Looks c++ test has not set up yet. I'll setup the test in a separate PR then.
linkerzhang(2017-11-17 03:31:48):@ezyang , I'm going to merge this change. But feel free to let me know if your have strong preference to not keep the lowest version there. It's anyway just used as a reference (no code reference), the same as the highest one. Thank you!
marcelolr(2017-11-16 22:42:48):I suggest renaming 'version' to 'maxInclusiveVersion' to clarify usage.
marcelolr(2017-11-16 22:44:01):This really needs a typedef at some point :)
linkerzhang(2017-11-16 23:14:49):make sense. fixed.
linkerzhang(2017-11-16 23:14:58):make sense. fixed.
ezyang(2017-11-16 23:24:34):It's good to explicitly say what each of these strings mean.
ezyang(2017-11-16 23:25:44):IIUC, the first element of this pair is not used by any code atm, is that right?
linkerzhang(2017-11-16 23:27:40):I thought the OpName_Domain_Version_Schema_Map name showed its meaning. I can add comments there too.
linkerzhang(2017-11-16 23:29:32):it's not. Per our discussion, we may want to remove too old histories form old.cc one day, right? in that way, this lowest version may be changed accordingly. Let me know if you think we don't need it. Thanks!
linkerzhang(2017-11-16 23:53:53):add some comments inline. Thank you!
ezyang(2017-11-20 06:50:09):Nope, I'm just making sure I understood the modification properly. Standard of review for code that is not exercised yet should, IMO, be higher, since you haven't actually tested the design, and future authors need to know that this thing that doesn't seem to be used indeed is not used.
ezyang(2017-11-29 20:14:56):I didn't catch this when I glanced through this patch originally, but having the map layer be operator name, and then domain, doesn't seem right. Operators with the same name in different domains are completely not related to each other, so I don't see why you should first case on op name, and then domain.
ezyang(2017-11-29 20:18:51):The consequence of getting the domain/name ordering wrong is that the semantics of this function are now very strange: it now *nondeterministically* picks one operator (when there are multiple references of it in multiple domains) and returns it. Not good! Latest registered schemas should instead now return `std::unordered_map<DomainAndName, OpSchema>` or something equivalent; that is an interpretation that actually makes sense.
gramalingam(2017-11-29 20:49:32):Good point ... 
linkerzhang(2017-11-29 21:28:47):hmmm, <op_name, domain> or <domain, op_name> both are fine, right?
linkerzhang(2017-11-29 21:29:14):this is a great catch! should have <op_name, domain, latest schema> returned.
bddppq(2017-11-17 10:51:18):superseded by #256 
CLAassistant(2017-11-16 23:20:56):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=281) <br/>All committers have signed the CLA.
bddppq(2017-11-17 00:14:38):Great! Thanks
Yangqing(2017-11-18 00:54:46):Now, if I understand it correctly, I should plead guilty for this one. It is an old behavior in Caffe that is very strongly discouraged nowadays, because it creates discrepancy between the way convolution and pooling deal with the last partial window. For more details, see the comment about history here:

https://github.com/caffe2/caffe2/blob/master/caffe2/proto/caffe2_legacy.proto#L23-L34

There are various discussions in the caffe repository history saying this is a bad choice, so I am in favor of not adding it for future looking purposes. Luckily, most of the frameworks nowadays no longer use this feature in default, and even Caffe moved on.

If we know the input size, this is a pretty much solvable during model conversion. Shall we consider correcting this discrepancy during conversion instead of adding this legacy argument (and drag history burden along)? :)
Yangqing(2017-11-18 01:09:41):(and to make myself less guilty, the Caffe behavior was to be consistent with the earlier cuda-convnet behavior IIRC)
ebarsoum(2017-11-18 01:44:30):@Yangqing  good to know who to blame...:) 

I would love to make this done in conversion and remove it from the API, I looked at that initially . However, there are 2 issues:
1. Won't work for unknown size image.
2. Even for known size input, some of the models you don't guaranty to have this information at the pool node input. In this case you will need to actually do inference during conversion.  

Yangqing(2017-11-18 08:11:25):If I recall correctly, most of the CoreML model actually has known sizes. CoreML's interface kind of derived from Caffe and it retains a lot of those assumptions, so 1 won't be an issue. For 2 - basically when there is a backend one can "play" the network to infer the size. It is done in both torch2caffe and caffe-to-caffe2 I believe. And you are right - one need to do inference during conversion. But IIRC this is better than dragging along an old issue :)
ebarsoum(2017-11-19 08:39:51):For 2, how that will work with a converter? Converter don't have backend.
dzhulgakov(2017-11-17 08:52:06):I think having this file checked in separately is fine (you can even try to make it include only non-standard-onnx ops, but it doesn't really matter). For ops themselves I'd defer to other folks for review.
gramalingam(2017-11-17 17:39:59):Having only the ML extensions in the new file sounds good to me. 
AppVeyorBot(2017-11-20 15:30:32)::white_check_mark: [Build onnx 0.3.417 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.417) (commit https://github.com/onnx/onnx/commit/c972d657d0 by @linkerzhang)
buehrer(2017-11-20 18:37:49):This looks good, thanks Ke and others for the corrections.
marcelolr(2017-11-17 18:57:09):no need for comma
marcelolr(2017-11-17 18:58:59):This reads more precisely without commas - either the format of ONNX or semantics of ONNX require an increment. The commas suggest make the format portion more vague.
marcelolr(2017-11-17 19:00:01):':' for a description reads better I think. If anything, break sentence after op_version an follow with. This tuple is written as ...
CLAassistant(2017-11-17 21:53:17):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=288) <br/>All committers have signed the CLA.
ezyang(2017-11-27 14:50:45):@ebarsoum For future reference, you should always bump the "current" operator version when you make a new BC breaking change, don't reuse the older operator version. This ensures that it is always possible to reference an operator set from a development version of ONNX without worrying that a BC-breaking change will get ninja-d on you (which is what would have happened if you had a model that used `Conv` and targeted operator version 2. Since we haven't officially released yet, it seems OK to just let this one slide.
ebarsoum(2017-11-27 18:59:03):@ezyang  I don't follow, I bumped the OP version to 2.  Which one did I reuse the old OP?
bddppq(2017-11-18 06:50:20):Backend test also needs to be updated: https://github.com/onnx/onnx/blob/master/onnx/backend/test/case/node/pad.py#L22
bddppq(2017-11-18 06:51:41):I think this is a also a breaking change.
bddppq(2017-11-18 06:51:59):breaking change
ebarsoum(2017-11-19 07:52:27):I will change it. @bddppq so why did it pass all tests?
ebarsoum(2017-11-19 07:53:22):Sure, will bump the version.
ebarsoum(2017-11-20 01:08:29):Done.
ebarsoum(2017-11-20 01:08:35):Done.
bddppq(2017-11-20 01:28:04):The test coverage is pretty low in onnx repo :(
Besides, "ONNX Backend Test" (i.e. those test cases in onnx/backend/test/) is not exercised in onnx repo since it needs a backend (onnx-caffe2, onnx-tensorflow ...) to run.
bddppq(2017-11-20 05:52:18):After updating the test cases python/numpy code, we need to run `backend-test-tools generate-data` to regenerate the test data. I have just done it for this PR.
ebarsoum(2017-11-20 07:01:44):Thanks @bddppq  do we have that written somewhere? 
bddppq(2017-11-20 07:05:46):Yep I added this doc recently: https://github.com/onnx/onnx/blob/master/docs/ONNX%20Backend%20Test.md
ebarsoum(2017-11-19 07:55:28):Add in the description that height_scale and width_scale need to be greater than 1.0.
AppVeyorBot(2017-11-18 19:31:34)::x: [Build onnx 0.3.372 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.372) (commit https://github.com/onnx/onnx/commit/2491a0c3c7 by @bddppq)
dzhulgakov(2017-11-19 04:28:28):Looks like windows build doesn't like contextlib.nested
AppVeyorBot(2017-11-19 05:24:19)::white_check_mark: [Build onnx 0.3.373 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.373) (commit https://github.com/onnx/onnx/commit/b05282f202 by @bddppq)
AppVeyorBot(2017-11-19 07:38:46)::x: [Build onnx 0.3.386 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.386) (commit https://github.com/onnx/onnx/commit/237478ba6f by @bddppq)
AppVeyorBot(2017-11-19 07:48:12)::x: [Build onnx 0.3.388 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.388) (commit https://github.com/onnx/onnx/commit/4b1f757b99 by @bddppq)
AppVeyorBot(2017-11-19 08:02:13)::x: [Build onnx 0.3.390 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.390) (commit https://github.com/onnx/onnx/commit/78e805e0f7 by @bddppq)
linkerzhang(2017-11-19 16:14:52):@bddppq,  appreciate for taking care of this! Sorry for making this issue (am really a newbie on these scripts and thought it worked after getting right bits in windows).
AppVeyorBot(2017-11-20 03:49:06)::x: [Build onnx 0.3.403 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.403) (commit https://github.com/onnx/onnx/commit/511c27c38b by @bddppq)
bddppq(2017-11-20 04:07:12):@linkerzhang This rabbit hole is deeper than I thought. After fixing the build scripts to correctly run onnx-ml build, the tests failed and the failures are legitimate. Our python code directly import "onnx_pb2" module in several places, which does not exist in onnx-ml build. I have now changed setup.py to autogenerate a "_pb.py" that a proxy imports the correct "_pb2.py", other python code should not directly import "_pb2" module any more.
AppVeyorBot(2017-11-20 04:07:40)::x: [Build onnx 0.3.404 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.404) (commit https://github.com/onnx/onnx/commit/c8c8f42441 by @bddppq)
linkerzhang(2017-11-20 05:37:54):Thank you so much! @bddppq .
dzhulgakov(2017-11-21 07:26:09):This is a more of top level list of links. Maybe have a separate section below "tools"?

We also have graphviz-based visualizer described here: https://github.com/onnx/tutorials/blob/master/tutorials/VisualizingAModel.md Link it too then?
lutzroeder(2017-11-21 14:16:22):I added 'Tools' and both links, pushed a new commit. Wondering if it makes sense to add it to http://onnx.ai/supported-tools, the link pointing there is mentioning frameworks & tools.
CLAassistant(2017-11-20 16:33:57):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=295) <br/>All committers have signed the CLA.
AppVeyorBot(2017-11-20 16:42:13)::x: [Build onnx 0.3.419 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.419) (commit https://github.com/onnx/onnx/commit/8f5bb0ef33 by @tjingrant)
AppVeyorBot(2017-11-20 16:53:24)::x: [Build onnx 0.3.420 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.420) (commit https://github.com/onnx/onnx/commit/313a9fed84 by @tjingrant)
bddppq(2017-11-20 17:30:13):#289 is doing this change
tjingrant(2017-11-20 17:47:30):Great! Didn't realize that by the title of the PR...
bddppq(2017-11-20 17:57:38):Sorry my bad, the title is indeed confusing.
AppVeyorBot(2017-11-20 19:22:55)::x: [Build onnx 0.3.423 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.423) (commit https://github.com/onnx/onnx/commit/40f6b4eeb8 by @prasanthpul)
AppVeyorBot(2017-11-21 02:32:48)::white_check_mark: [Build onnx 0.3.432 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.432) (commit https://github.com/onnx/onnx/commit/3c1579fde6 by @prasanthpul)
AppVeyorBot(2017-11-21 02:44:59)::white_check_mark: [Build onnx 0.3.434 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.434) (commit https://github.com/onnx/onnx/commit/fc0ba209b5 by @prasanthpul)
AppVeyorBot(2017-11-20 19:54:15)::white_check_mark: [Build onnx 0.3.425 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.425) (commit https://github.com/onnx/onnx/commit/977f60f72a by @linkerzhang)
AppVeyorBot(2017-11-20 21:52:59)::x: [Build onnx 0.3.426 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.426) (commit https://github.com/onnx/onnx/commit/01f6d95634 by @rwilliams58)
AppVeyorBot(2017-11-20 22:09:17)::x: [Build onnx 0.3.427 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.427) (commit https://github.com/onnx/onnx/commit/f9debf830d by @rwilliams58)
prasanthpul(2017-11-20 22:06:33):/availiable/available
prasanthpul(2017-11-21 02:54:39):Once the branch is cut, input and output name changes will be considered breaking changes. Until then, they are not
gramalingam(2017-11-21 19:19:17):Suggested changes:
(1) Change "a particular value can be either" to "a particular name can reference either a"
(2) Change "reassignment of values" to "reassignment of values to names"
bddppq(2017-11-21 07:55:21):nit: maybe change "usually" to "default"?

Another general question I have is whether we should mention axis can be negative?
bddppq(2017-11-21 07:55:43):missed "["
bddppq(2017-11-21 07:56:00):missed "["
AppVeyorBot(2017-11-22 04:58:50)::x: [Build onnx 0.3.452 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.452) (commit https://github.com/onnx/onnx/commit/e5bb532116 by @tjingrant)
tjingrant(2017-11-22 07:41:01):related to https://github.com/onnx/onnx/issues/261.
We've fixed the pads param but the test is wrong.
bddppq(2017-11-22 21:06:43):@onnxbot test this please
bddppq(2017-11-22 21:15:48):Thanks!
tjingrant(2017-11-22 22:31:44):My bad, wrong direction of the transposition. Thanks for fixing it!
bddppq(2017-11-22 21:03:27):This should be [0, 0, 1, 3, 0, 0, 2, 4]
AppVeyorBot(2017-11-22 21:43:14)::white_check_mark: [Build onnx 0.3.463 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.463) (commit https://github.com/onnx/onnx/commit/dad38afddc by @bddppq)
AppVeyorBot(2017-11-22 22:07:24)::white_check_mark: [Build onnx 0.3.468 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.468) (commit https://github.com/onnx/onnx/commit/73e9e2673a by @bddppq)
linkerzhang(2017-11-23 01:33:05):Dumb question please. autogen.sh will be used in other places, am I right? 
bddppq(2017-11-23 03:34:32):Yep, mostly for contributors. As we have more and more autogen stuffs, it’s hard for everyone to remember many commands :-)
linkerzhang(2017-11-23 19:00:10):Thanks! That's true. :)
ezyang(2017-12-01 19:10:33):I object to calling this script `autogen.sh` as it conflicts with the autotools convention
ebarsoum(2017-11-23 00:39:28):Wouldn't be better to put most  autogen in a python file? And then we can add an *.sh and *.bat or ps1 wrapper.
ezyang(2017-12-01 20:48:21):We are punting this PR for 1.0.
CLAassistant(2017-11-24 15:58:29):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=314) <br/>All committers have signed the CLA.
prasanthpul(2017-11-28 21:33:45):Is the conda-forge version updated automatically? Does it always reflect the latest master version or a specific branch?

djsutherland(2017-11-28 23:43:05):It's not updated automatically, it needs someone to go send a (very easy) PR to [the feedstock](https://github.com/conda-forge/onnx-feedstock/). Assuming no dependency changes, all it needs is an update to the `version` and `sha256` lines in [`meta.yaml`](https://github.com/conda-forge/onnx-feedstock/blob/master/recipe/meta.yaml).

@ezyang and I can both merge these PRs; if you want anyone else to be able to, just send a PR adding their github id to the bottom of `meta.yaml` and then they'll get merge privileges too.
ezyang(2017-11-29 02:51:14):It's far far easier to get the conda-forge package updated; it's no longer me (black box) popping out a binary when we release.
prasanthpul(2017-11-29 04:23:01):Perfect. Let's make it a part of our standard release process.
AppVeyorBot(2017-11-27 05:45:20)::x: [Build onnx 0.3.491 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.491) (commit https://github.com/onnx/onnx/commit/a63aa271ce by @dzhulgakov)
AppVeyorBot(2017-11-27 06:29:45)::x: [Build onnx 0.3.493 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.493) (commit https://github.com/onnx/onnx/commit/c59416e33f by @dzhulgakov)
AppVeyorBot(2017-11-27 06:51:47)::x: [Build onnx 0.3.497 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.497) (commit https://github.com/onnx/onnx/commit/29151598c8 by @bddppq)
AppVeyorBot(2017-11-27 08:15:49)::x: [Build onnx 0.3.500 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.500) (commit https://github.com/onnx/onnx/commit/9fed288464 by @bddppq)
AppVeyorBot(2017-11-27 20:27:26)::x: [Build onnx 0.3.505 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.505) (commit https://github.com/onnx/onnx/commit/feb0371ae8 by @bddppq)
AppVeyorBot(2017-11-27 20:38:47)::white_check_mark: [Build onnx 0.3.507 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.507) (commit https://github.com/onnx/onnx/commit/6db669e1b3 by @bddppq)
anderspapitto(2017-11-28 19:00:54):shouldn't this be an `except` block rather than a `finally` block? @dzhulgakov @bddppq 
bddppq(2017-11-28 19:14:49):`NamedTemporaryFile(delete=True)` (the original behavior) was going to delete the temp file no matter the download succeeded or not.
anderspapitto(2017-11-28 19:33:46):sure, but it wasn't going to lie about the test failing
bddppq(2017-11-28 19:50:12):Oh the print line, I was thinking about the `os.remove` line. Yes you are right, that should belong to `except` block
AppVeyorBot(2017-11-27 07:13:59)::white_check_mark: [Build onnx 0.3.498 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.498) (commit https://github.com/onnx/onnx/commit/c9c22f81a9 by @linkerzhang)
gramalingam(2017-11-27 18:52:54):Looks good to me (modulo the question about max_input and check on node.input_size above). I think it will be helpful to document the assumptions and restrictions: as I understand it,
(a) Only last parameter is allowed to be variadic.
(b) What is max_input supposed to be if we have k parameters and last is variadic? As you point out, it may be a good idea to eliminate NumInput(...) if it can be calculated. Then we don't have to worry about this.
gramalingam(2017-11-27 19:15:02):The question about outputs (2) is a good one. It makes sense to do the same. We also need to define what the behavior is for several of these cases for the output:
(a) Can the user specify an empty-string for an output that they do not care about?
(b) If the user specifies a name, say X, for an output that doesn't have a value (e.g., DropOut may not return one of the output values depending on test-mode), then what is X bound to?
buehrer(2017-11-28 21:18:46):I agree with the other comments, looks good.  Allowing only the last input to be variadic makes sense to me.
gramalingam(2017-11-27 18:35:07):I think that the check if node.input_size() > max_input should be made only if there is no variadic parameter. (Of course, this depends on what max_input is set to for an operator with a variadic parameter.)
gramalingam(2017-11-27 18:39:24):Do we want to permit only the last parameter to be variadic? If we allow a parameter other than the last one to be variadic (still restricting the number of variadic parameters to be one), then the association between the index of an actual parameter and the index of the corresponding formal parameter is more complex. For parameters before the optional one, it is a direct match. For parameters after the optional one, we need to count from the back ... that is, the k-th parameter from the end in the actual parameter list will correspond to the k-th parameter from the end in the formal parameter list.
linkerzhang(2017-11-27 21:28:11):Exactly on "Of course, this depends on what max_input is set to for an operator with a variadic parameter."

That's why I put a more thought "Remove all "NumInputs", "NumOutputs" API, which seem to be not needed now given we have "optional", "variadic" types specified clearly." with this PR.

I'm thinking of remove the min/max number of input setting. Because, for single/optional input types, it's easy to get to know min/max without an explict set. For variadic, it's always 0-n setting.

Make sense?

If we all agree on removing the "NumInputs, NumOutputs" APIs, I'll do that and change the logic here accordingly too.
gramalingam(2017-11-27 21:51:20):Yes. I guess others can chime in if they see a need for NumInputs for any other future extension. 
dzhulgakov(2017-11-28 05:17:16):we should change "First" here and in other places below
dzhulgakov(2017-11-28 05:17:32):nit: remove "optional" comment
dzhulgakov(2017-11-28 05:23:56):It might make sense to keep NumInputs just in case we'd need some very fancy operator versions (e.g. for training batchnorm there might be 1 or 3 arguments, but not 2; though it's a bit far-fetched).

But I agree with you that we can do Input() and Output() specs a default. E.g. if none of NumInputs() functions is invoked we just infer it from the Input() spec. Then we can first verify that all existing op specifications are valid before removing NumInput() specs.

But that can happen in a separate PR if your prefer.
dzhulgakov(2017-11-28 05:25:42):I'd say to allow variadic be at the end only, it makes it easier to reason (seems like you're already checking it below)

I also realized that this code had bug before, we need to check that `in_idx < inputs_.size()`
dzhulgakov(2017-11-28 05:27:40):this code should probably also look at indices from the end only. Original (my) implementation was not very correct
linkerzhang(2017-11-29 16:51:27):sure, will make that change in a separate PR.
linkerzhang(2017-11-29 18:23:10):@dzhulgakov Thank you very much for the great comments! For this one, I'll revisit this min/max number calculation in separate PR when trying to remove numinputs and numoutputs APIs. They should be able to be calculated now. Basically, min is the number of "Single"s. max is sum of number of singles and optionals. if there's variadic, max is INT_MAX. Sounds good?
linkerzhang(2017-11-29 18:35:09):Yep. FIxed and Thank you!
linkerzhang(2017-11-30 00:49:08):fixed. Thank you!
CLAassistant(2017-11-28 01:02:34):[![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/onnx/onnx?pullRequest=324) <br/>Thank you for your submission! We really appreciate it. Like many open source projects, we ask that you sign our [Contributor License Agreement](https://cla-assistant.io/onnx/onnx?pullRequest=324) before we can accept your contribution.<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/onnx/onnx?pullRequest=324) it.</sub>
bddppq(2017-11-28 20:59:57):@dzhulgakov could you add @anderspapitto to the onnx org?
anderspapitto(2017-11-29 15:47:09):looks good to me
AppVeyorBot(2017-11-28 23:30:38)::x: [Build onnx 0.3.524 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.524) (commit https://github.com/onnx/onnx/commit/3e534f84b6 by @SherlockNoMad)
ebarsoum(2017-11-29 00:16:18):This is a breaking change, you need to move the old one to old.def and bump the version of the new one. 

Also, this need an update: https://github.com/onnx/onnx/blob/df78798af405836d3bf7552857bc7c95a599b96b/docs/Implementing%20an%20ONNX%20backend.md
dzhulgakov(2017-11-29 17:24:57):Let's use this example to go through steps of bumping the version and filling out missing pieces listed in #211.

From what I understand we'd need to keep the old spec around and mark it deprecated and add translation code to rename the op
prasanthpul(2018-01-23 23:21:52):We decided to keep it LRN since that is the commonly used name
AppVeyorBot(2017-12-01 00:13:24)::white_check_mark: [Build onnx 0.3.564 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.564) (commit https://github.com/onnx/onnx/commit/a82cf94ff3 by @houseroad)
bddppq(2017-11-29 18:22:35):no need to compute test_name again
bddppq(2017-11-29 18:24:34):maybe set model_name to `test_name[len('test_'):]`?
bddppq(2017-11-29 18:28:54):I would prefer to extend ModelTestCase to have one more field `model_dir` (or `data_dir`)
AppVeyorBot(2017-11-29 22:06:04)::white_check_mark: [Build onnx 0.3.533 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.533) (commit https://github.com/onnx/onnx/commit/ef6d0324cc by @ezyang)
AppVeyorBot(2017-11-29 22:16:45)::white_check_mark: [Build onnx 0.3.535 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.535) (commit https://github.com/onnx/onnx/commit/3e4d69072b by @ezyang)
AppVeyorBot(2017-11-29 22:31:19)::white_check_mark: [Build onnx 0.3.539 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.539) (commit https://github.com/onnx/onnx/commit/a4b5b3b285 by @ezyang)
AppVeyorBot(2017-11-29 22:40:34)::white_check_mark: [Build onnx 0.3.541 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.541) (commit https://github.com/onnx/onnx/commit/861d36939b by @ezyang)
AppVeyorBot(2017-11-29 22:48:19)::white_check_mark: [Build onnx 0.3.543 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.543) (commit https://github.com/onnx/onnx/commit/2bbb666ce1 by @ezyang)
AppVeyorBot(2017-11-29 22:57:03)::white_check_mark: [Build onnx 0.3.545 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.545) (commit https://github.com/onnx/onnx/commit/655672d61e by @ezyang)
linkerzhang(2017-11-29 23:37:03):operator set 'ONNX may be better?
linkerzhang(2017-11-30 00:57:53):elif ONNX_ML and domain != 'ai.onnx.ml':
                continue

should also be added, I think.
linkerzhang(2017-11-30 01:01:29):missing some placeholder {} in this line? I mean otherwise, the domain is not displayed.
ezyang(2017-11-30 01:40:14):How about "the default ONNX operator set"? You don't want to say operator set 'onnx' because that's *not* what you're supposed to put in the opset_version.
ezyang(2017-11-30 01:44:03):Nice catch
gramalingam(2017-12-01 18:45:51):I am trying to understand various use-cases and scenarios: 
(1) the model decides to ignore one of the outputs, since it won't use it: here, it could use empty string; then, the operator implementation will not fill in that output
(2) the operator implementation may return one output conditionally: now, what do we do in the case where the model specifies an output variable X and the operator implementation does not return a value for this output? What will be the value bound to X in subsequent places where X is used? Or, does model evaluation stop with an error/exception in this case?
(3) The model specifies multiple outputs X, Y, and Z: so, I guess in this case it is the operator implementation's responsibility to fill in the right values for each of X, Y, and Z? What if it doesn't have enough values to fill in for X, Y, and Z? Also, does this complicate the shape-inference computation?

On a related note: is a tensor with zero elements a valid runtime value? (We could use such a tensor in some of these cases, but this will complicate shape-inference aspects potentially.)

gramalingam(2017-12-01 20:00:35):Thanks @linkerzhang for the explanation in the call. Just recording some key points here.

This proposal looks good to me. I think that a separate top-level clarification/documentation/discussion of the notion of erroneous models would be helpful (specifically, errors in the models not caught by the type-system), but that is separate from this PR.

My understanding is that case (1) in previous message is not allowed: if an output is single, the model should not use empty-string for that. (However, I think we can easily allow this semantically, since we can treat the empty-string as a wild-card, meaning a new name not used anywhere else. But this can be a future generalization, since the implementations need to be adapted to do this.)

linkerzhang(2017-12-01 22:28:45):OK, now, I'm stepping back a little bit and am convinced to take the semantics that "variadic" asks for >=1 input. "VariadicOptional" is not added as we're not using it. It could be added whenever it's needed.
@gramalingam @dzhulgakov .
linkerzhang(2017-12-02 03:23:37):It's fine to allow them in the middle, I'm just saying that reporting min_input in this case is invalid. Imagine you have
0: single
1: optional
2: single
We should either disallow this case or make min_input=max_input=3 for it. Current code min_input=2 and it doesn't make sense. Shall we disallow it for now?

This is a GOOD point!!! @gramalingam and I also discussed on this case. 

I personally think we should keep the min semantics as it is. In this case, min_input_ is still 2. max_input_ is 3. We should have other verification logic to ensure that node's inputs should be fed with 2 real inputs and one "" (empty string) in between. Otherwise, the min_input_ semantic is a little bit confusing, we say min number of this operator is 3, but the 2nd one is optional. It's weird. Thoughts?

linkerzhang(2017-12-02 03:24:38):I'm merging this PR and will for sure work on the last comment if strong preferences are there about my opinions. Thank you all!
gramalingam(2017-12-02 04:33:42):To expand on what @linkerzhang said: one straightforward way to do the check is to modify "verify" to add an extra check: if the number of actual inputs is less than the computed min-inputs, then we should verify that all the missing number of inputs at the end are "optional".

This is mostly an implementation detail (of how the check is done), so it doesn't matter much how it is done. @linkerzhang, if we let min_input be 3 in the above example, it is not unreasonable ... because it captures our rules for when an empty-string must be used. If we count the arguments specified, including empty string, we need to have a minimum of 3 to successfully pass verification (and not 2). Further, it is more efficient: instead of doing the "straightforward" check above at every node, we find how many of the trailing parameters are optional, and decrement min_input by it, once, when we compute min_input.

dzhulgakov(2017-12-01 06:34:45):so do we allow optional outputs now?
dzhulgakov(2017-12-01 06:35:59):nit: break
dzhulgakov(2017-12-01 06:36:42):this is the case only if the input appears in the end. If (for some reason) we have optional inputs in the middle, I wouldn't count them to the total
dzhulgakov(2017-12-01 06:38:29):so we need some annotation to signify that this variadic argument actually needs >=1 input. Shall we have Variadic and VariadicOptional?

basically your docs shouldn't change (it's a sign that the spec didn't change)
linkerzhang(2017-12-01 16:18:23):This is really a great question!!! In my current implementation, variadic argument needs >= 0 input.

I was thinking of using bitwise options. Say Single=1, Optional=2, Variadic=4 (Here variadic needs >=1 input). However, I realized that only Optional and Variadic could be merged. So I didn't pick up the design and change variadic needs >=0 input (which means VariadicOptional in your idea).

What do you think?
1) Keep current design to have Variadic needs >=0 input. or,
2) Make Variadic needs >=1 input and add VariadicOptional needs >=0 input.

linkerzhang(2017-12-01 16:19:59):yes, we do. In our current operators, BatchNormalization is the only one case to have optional outputs.
linkerzhang(2017-12-01 16:23:03):Not quite get your opinion here please. Are you saying that,
1. We should only allow optional inputs at the end please?
2. If there's optional input defined in the middle (there're Single inputs afterwards), we'll just not use it? in this case, the operator registration is invalid?
Thank you!
linkerzhang(2017-12-01 16:23:36):Thank you for the nice catch!
linkerzhang(2017-12-01 16:26:40):btw, @dzhulgakov , I put the comment in schema.h to show the meaning of each option as.
  // Formal parameter options.

  enum FormalParameterOption {

    // The input formal parameter is single and not optional.

    // Number of this input is 1.

    Single = 0,

    // The input formal parameter is single and optional.

    // Number of this input is 0 or 1.

    Optional = 1,

    // The input formal parameter is variadic.

    // Number of this input is [0, n].

    Variadic = 2,

  };

I can put it in *.md documents also. Let's make a decision on the two solutions above firstly.
linkerzhang(2017-12-01 19:51:47):I would keep the semantic of "optional" same for both input and output. This means, allowing "optional" input and output in the middle with empty string specified.
dzhulgakov(2017-12-02 03:08:42):It's fine to allow them in the middle, I'm just saying that reporting min_input in this case is invalid. Imagine you have

0: single
1: optional
2: single

We should either disallow this case or make min_input=max_input=3 for it. Current code min_input=2 and it doesn't make sense. Shall we disallow it for now?
ezyang(2017-12-01 05:33:25):LGTM (not that it's easy for me to look haha)
houseroad(2017-12-01 05:17:53):@ezyang @bddppq now, it's 0.3.
ezyang(2017-12-01 05:31:28):Yeah, we need a better story for getting PyTorch's version number to the ONNX exporter. Probably best to make it another arg so you don't have a compile time constant.
dzhulgakov(2017-12-02 03:18:51):Do you plan to commit also the scripts you've used for extraction of tests?
houseroad(2017-12-02 06:42:49):Sure, I will upload them. After one round, I will refactor the code a bit, and send it to onnx-fb-universe. Right now, some part of the script is hardcoded. :-)
AppVeyorBot(2017-12-04 02:19:31)::white_check_mark: [Build onnx 0.3.592 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.592) (commit https://github.com/onnx/onnx/commit/023759c94c by @linkerzhang)
dzhulgakov(2017-12-04 07:50:29):I think this code can be much more readable if you just change it to `min_input=max_input` here - basically min_input is the last non-optional input
linkerzhang(2017-12-04 16:37:09):Great comment!!! Fixed and thank you!
ezyang(2017-12-04 15:46:57):Most notably, this adds support for `compile_commands.json`, which means your IDE will start working when you work on ONNX.
ezyang(2017-12-12 20:16:33):Pushed with all CR handled.
linkerzhang(2017-12-04 16:54:53):I added the function below to return operator schema with newest but not greater than specified version. Is that what you want please?
linkerzhang(2017-12-04 16:58:04):As you commented here, I'm wondering why we not return const reference please :).
linkerzhang(2017-12-04 16:58:54):Given we have getter/setter for these fields, shall we use "class" and keep them as private please?
linkerzhang(2017-12-04 17:04:57):how about we add one const string "ONNX_DOMAIN", which may be a little bit more readable please?
linkerzhang(2017-12-04 17:07:11):Looks like we're not using these setters. We may expose a mutable_opset_imports API instead if you want to and make the two fields private :).
ezyang(2017-12-11 15:54:54):Yes it looks like they are equivalent.
ezyang(2017-12-12 20:06:55):It looks like pybind11 can handle it correctly, so reference it is.
ezyang(2017-12-12 20:07:48):OK
ezyang(2017-12-12 20:10:20):ok
tjingrant(2017-12-04 21:15:25):thanks!
nouiz(2017-12-04 22:50:54):Should onnx complain if it receive negative values for the pad in addition to removing the test?
dzhulgakov(2017-12-09 01:06:19):nit: `generated` -> `kind='Model'` in case we have more categories
dzhulgakov(2017-12-09 22:30:53):maybe generalize it a bit? we're going to have exported tests from various places and it might make sense to separate them down the road
houseroad(2017-12-11 21:16:15):Good idea. :-)
houseroad(2017-12-11 21:16:52):Yeah, this should be more general. Thanks!
CLAassistant(2017-12-05 00:35:09):[![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/onnx/onnx?pullRequest=352) <br/>Thank you for your submission! We really appreciate it. Like many open source projects, we ask that you sign our [Contributor License Agreement](https://cla-assistant.io/onnx/onnx?pullRequest=352) before we can accept your contribution.<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/onnx/onnx?pullRequest=352) it.</sub>
houseroad(2017-12-05 01:04:02):Please sign the CLA.
bddppq(2017-12-21 21:51:53):@kant Thanks for your contribution. Could you sign the CLA and rebase as there is merge conflict right now?
CLAassistant(2017-12-06 14:17:58):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=356) <br/>All committers have signed the CLA.
linkerzhang(2017-12-06 19:01:55):Please also sign the CLA agreement :).  Dumb question is, shall we add the attribute "order" to support the both please? Adding more folks. @ebarsoum @ezyang @gramalingam 
bddppq(2017-12-10 01:56:59):Good catch. ONNX uses NCHW everywhere. Support of other storage format will be left to the backends.
gramalingam(2017-12-06 22:20:13):topoligically => topologically
linkerzhang(2017-12-06 23:50:23):want to check no duplicate graph input name please? 
linkerzhang(2017-12-06 23:54:16):output name could also be empty when it's optional and you may skip the check when it is.
bddppq(2017-12-06 23:56:44):Added
bddppq(2017-12-06 23:56:53):Good catch, fixed.
ezyang(2017-12-07 10:08:10):Rereading this... don't constant nodes have no inputs? :)
bddppq(2017-12-07 16:14:23):Yep good catch
buehrer(2017-12-11 23:24:54):Looks good, thanks!
CLAassistant(2017-12-12 02:51:24):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=364) <br/>All committers have signed the CLA.
dzhulgakov(2017-12-13 01:28:52):Hm, the original op description was just relying on the number of outputs specified? Does it mean that we don't want to rely on the # outputs for ops behavior and instead make number of outputs fully inferable from the attributes?

I'm fine with it, just double-checking
bddppq(2017-12-13 02:21:49):This looks to me is really an unnecessary breaking change. The number of outputs is already there, just count the outputs. Adding another attributes for specifying it seems redundant. More importantly, we started with this, published v1, we really shouldn't frequently make breaking changes for no good reason.
yuanbyu(2017-12-13 17:23:21):@dzhulgakov @bddppq Thinking about it again, I think you are right. I got confused with how to do graph construction from CoreML.  I will cancel it.
yuanbyu(2017-12-14 05:02:14):Thinking about it again, I think that this PR actually addresses some use cases that the current Split can't. Suppose we want to split x evenly into 4 parts on axis=0 but x.dim(0) is unknown, without num_splits I don't see how we can accomplish it. I am reopening the PR.
yuanbyu(2017-12-14 05:03:18):@dzhulgakov @bddppq PTAL.
bddppq(2017-12-14 06:21:37):@yuanbyu The idea is to treat the number of outputs in this Operator as num_splits.
yuanbyu(2017-12-14 16:47:52):@bddppq Sorry I was confused again. This is not needed. Thanks to @linkerzhang for explaining it.
ebarsoum(2017-12-12 07:29:38):Shouldn't the type be AttrType::INT?
ebarsoum(2017-12-12 07:30:43):This is breaking change, you should bump the version number.
yuanbyu(2017-12-12 17:18:00):Thanks.
yuanbyu(2017-12-12 17:18:01):Yes. Done.
ezyang(2017-12-12 20:17:07):I'm a little confused; why wasn't this failing before?
linkerzhang(2017-12-12 21:48:29):previously we still have not had more than one versions in same .cc file for one operator. That will NOT introduce two static fields with same name in one .cc file. Now we're going to have it (Yuan is going to bump Split version to 3 so that we'll have two old versions available in old.cc). That introduces a compilation error. :(
linkerzhang(2017-12-12 21:49:34):I'm pushing in this PR to unblock Yuan. @ezyang , feel free to let me know if you have more comments please. Thank you!
ezyang(2017-12-12 21:57:14):Makes sense! Thanks for merging.
bddppq(2017-12-21 21:50:30):Are these test cases also auto-generated? If yes what's the difference between pytorch-operator and pytorch-converted?
houseroad(2017-12-21 23:03:10):PyTorch converted test are test cases converted from pytorch tests.

PyTorch operator test are the test cases generated from [test_operators.py](https://github.com/onnxbot/onnx-fb-universe/blob/master/test/test_operators.py).

They are quite different, focus on different aspects, and test_operators.py are much easier to control. So I would like to separate pytorch operator test out.
bddppq(2017-12-21 23:32:48):Ok separating them makes sense. However, I find the unittest class name "OnnxBackendPyTorchOperatorModelTest" looks a little bit weird :-)
houseroad(2017-12-22 07:20:16):I admit that. ;-) Any suggestion? I suppose we name tests as OnnxBackend{}ModelTest. 
bddppq(2018-01-10 21:37:58):How about OnnxBackendPyTorchSimpleModelTest? :-)
bddppq(2018-01-10 21:38:20):@onnxbot retest this please
bddppq(2018-01-23 05:37:00):@onnxbot retest this please
AppVeyorBot(2017-12-15 20:18:30)::x: [Build onnx 0.3.657 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.657) (commit https://github.com/onnx/onnx/commit/1ffa3560cd by @SherlockNoMad)
AppVeyorBot(2017-12-15 21:41:06)::x: [Build onnx 0.3.658 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.658) (commit https://github.com/onnx/onnx/commit/340137aa63 by @SherlockNoMad)
AppVeyorBot(2017-12-16 01:11:42)::x: [Build onnx 0.3.662 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.662) (commit https://github.com/onnx/onnx/commit/5c70cf9175 by @SherlockNoMad)
linkerzhang(2017-12-18 17:28:16):@onnxbot retest this please.
AppVeyorBot(2017-12-18 19:45:14)::x: [Build onnx 0.3.665 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.665) (commit https://github.com/onnx/onnx/commit/8c531ee9bd by @SherlockNoMad)
AppVeyorBot(2017-12-18 19:57:26)::white_check_mark: [Build onnx 0.3.666 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.666) (commit https://github.com/onnx/onnx/commit/413236f8ab by @SherlockNoMad)
bddppq(2017-12-16 21:28:12):@onnxbot retest this please
bddppq(2017-12-20 00:20:26):Hmm this change would implicitly change many attributes to be required. I agree this is better in engineering perspective. But for smoother transition I would prefer to just provide the option to declare default value and don't change the semantics of those undeclared ones for now, and then we can spin up tasks to go through all the operators to annotate all the attributes properly.
linkerzhang(2017-12-20 05:38:14):@bddppq agree with you on not changing the semantics for now, so I changed,
1. Attr API with "required=true" as default.
2. Set all existing Attr API call with "required=false" explicitly, which is the original semantics.
3. Call Attr with default value API for some attributes with clear default definition.
Make sense?

AppVeyorBot(2017-12-20 06:15:18)::white_check_mark: [Build onnx 0.3.678 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.678) (commit https://github.com/onnx/onnx/commit/d286584593 by @linkerzhang)
linkerzhang(2017-12-21 00:57:32):@bddppq Looks like using the Attr<AttributeProto::INT> way still can't resolve the compilation issue without cast. Below is what I tried, compiler still needs to have the cast for this call "Attr<AttributeProto::INT>("a", "desc", 1);". 

So, the last way to solve the problem is really doing renaming of each Attr API, which I personally think it looks a little bit ugly. I will go ahead with current solution then, of course will revisit this if you folks have better ideas (or you think renaming Attr APIs is a good idea). Thank you very much!

#define ATTR_SETTER_WITH_SINGLE_VALUE(type, field, attrtype)        \
    template<AttributeProto::AttributeType AttrType>                \
    OpSchema& Attr(const std::string& name,                         \
        const std::string& description,                             \
        const type& default_value);                                 \
                                                                    \
    template <>                                                     \
    OpSchema& Attr<attrtype>(const std::string& name,               \
        const std::string& description,                             \
        const type& default_value)                                  \
    {                                                               \
        AttributeProto a;                                           \
        a.set_name(name);                                           \
        a.set_##field(default_value);                               \
        a.set_type(attrtype);                                       \
        Attr(Attribute(name, description, a));                      \
        return *this;                                               \
    }                                                               \

    ATTR_SETTER_WITH_SINGLE_VALUE(int64_t, i, AttributeProto::INT);
    ATTR_SETTER_WITH_SINGLE_VALUE(float, f, AttributeProto::FLOAT);
linkerzhang(2017-12-21 04:13:38):Folks, Thank you all for the great review! I'm checking in the change now. Feel free to give me more comments if any later. We can keep iterate over it for sure.
linkerzhang(2017-12-18 19:57:09):default type: 1 means "float".
linkerzhang(2017-12-18 19:58:01):This default value needs to be confirmed.
linkerzhang(2017-12-19 01:32:11):this looks to me a "required" attribute, but there're test failure which asking it to be "optional". We should fix the test somehow, I think. Yes or no?
linkerzhang(2017-12-19 01:56:43):this needs to be confirmed. Should be "required"? (adding default value here is because there're test cases failing if setting it to be required).
linkerzhang(2017-12-19 01:57:30):as @dzhulgakov mentioned, this should be "required", however, there're test failures if setting it to "required".
linkerzhang(2017-12-19 02:15:35):test_backend_test.py failed with following error message:

onnx.onnx_cpp2py_export.checker.ValidationError: Required attribute 'axis' is missing.

==> Context: Bad node spec: input: "fire2/expand1x1_2" input: "fire2/expand3x3_2" output: "fire2/concat_1" name: "" op_type: "Concat"
houseroad(2017-12-19 06:21:10):This is not clear, we cannot easily figure out what is the element type.
houseroad(2017-12-19 06:22:41):This should be a required field.
houseroad(2017-12-19 06:25:34):Should be required
houseroad(2017-12-19 06:26:28):Confusing default value
houseroad(2017-12-19 06:26:40):Confusing default value
houseroad(2017-12-19 06:26:53):Confusing default value
houseroad(2017-12-19 06:27:03):Confusing default value
houseroad(2017-12-19 06:28:04):-1 means the invalid or the last dimension?
houseroad(2017-12-19 06:37:47):Why don't we have a function sig like this:
Attr(name, description, type)
Attr(name, description, type, default_value)

I think type is helpful for reading and it's easy for the compiler to figure out what is going on.
houseroad(2017-12-19 06:38:39):Mix int64_t with AttributeProto::INT is confusing. Similar to other types.
houseroad(2017-12-19 06:40:55):This should be an optional attribute, and default value is 1.0.
houseroad(2017-12-19 06:53:04):Yep, but it is really hard to decode this in our mind :-)
linkerzhang(2017-12-19 23:37:51):Good catch! Fixed.
linkerzhang(2017-12-19 23:38:35):changed to static_cast<int64_t>(TensorProto::FLOAT), which should be good enough.
linkerzhang(2017-12-19 23:39:04):changing it to "float" will break the test. Keep it and fix it in separate PR as well as fixing test failures. Sounds good?
linkerzhang(2017-12-19 23:41:45):These "int64_t", "I", "AttributeProto::INT" are actually redundant information for each other. Because, int64_t should be stored in field "I" and it must have the type as INT. However, using a macro with all these information here is to simplify the macro implementation. Make sense?
linkerzhang(2017-12-19 23:42:11):As discussed, changed to keep the type information and let's see whether there're other opinions. Thank you!
bddppq(2017-12-20 00:11:10):Does the compiler complain if you remove the cast here? It would be convenient to not need to explicitly cast in all the schemas.
linkerzhang(2017-12-20 00:16:10):Good catch!!! It was (there're another API name,desc,type, similar with name,type,int before. Now it should not complain. Let me fix it. I also hate it :).

I looked into it twice, compiler does complain for ambiguicity. :) I'm keeping it as it is now.

bddppq(2017-12-20 00:25:46):I would argue that in these schema declarations, readability is very important. (We can (and should) add tests for checking the cast if you have concern about the safely here).
linkerzhang(2017-12-20 05:39:39):Not get the point here. It's not because I have concern and I add cast here. It's because compiler can't decide which API it should call/use without explicit type setting here.
dzhulgakov(2017-12-20 22:36:17):nit - might be more readable if you proxy to an overloaded template function to do the setting of an actual value and make the Attr() method templated on default-value (I think that's what Lu originally suggested)
dzhulgakov(2017-12-20 22:39:00):It's a very confusing prototype and the way it looks in the declarations (`, false`) is very easy to confuse between required vs default value. Can we replace `bool required` with an enum OPTIONAL or REQUIRED? Then it'd be more obvious and readable
linkerzhang(2017-12-21 01:03:33)::). I'm assuming this (,false) calls should be temporary thing as well as we walking thru them and assign default values accordingly. However, you're right. It's not that readable and a little bit confusing, so let me create a "bool Optional = false;" and refer to this. Sounds good?

linkerzhang(2017-12-21 01:08:30):Don't quite get the point. Now the Attr APIs having type parameter there. like,
Attr(const std::string& name, const std::string& description, AttributeProto::AttributeType attr_type, const int64_t& default_value);

My original PR commit is Attr(const std::string& name, const std::string& description, const int64_t& default_value); because the attr_type in this case MUST be AttributeProto::INT. But Lu's right, adding the attr_type may be more readable although it's a dup information. That's why now I'm adding type check in-place to ensure that the attr_type and default_value's type are consistent.

AppVeyorBot(2018-01-05 21:44:10)::x: [Build onnx 0.3.709 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.709) (commit https://github.com/onnx/onnx/commit/1ea43945a0 by @yuanbyu)
yuanbyu(2018-01-05 22:25:28):OK. I made every output of LSTM optional. PTAL.
linkerzhang(2018-01-10 19:26:20):With latest clarification and discussion, checking #410 for more details please. This is not a breaking change, no need bumping version please. Thank you!
linkerzhang(2018-01-12 21:00:37):All the comments were resolved. It's now going to be checked-in.
linkerzhang(2018-01-02 18:48:03):When checking in this change, kindly remember to bump the global version in schema.h accordingly please. The version here may be updated too if the global version is changed by other users. For example, PR #390 .
dzhulgakov(2018-01-05 03:51:57):Yep, please do this
dzhulgakov(2018-01-05 03:54:10):I wonder whether we should make output Y_h optional too for symmetry purposes. You can probably sneak it in without bumping the version as it's just an optional output and optionals are not yet properly handled everywhere anyway.
yuanbyu(2018-01-05 18:58:22):I am fine with this. Should we go even further to make the outputs of every op that has >1 outputs optional?
bddppq(2018-01-08 20:03:10):This was optional.
linkerzhang(2018-01-12 19:33:22):Per latest discussion and decision, this change does not need to bump version, so revert all changes in old.cc now.
bddppq(2018-04-09 22:21:59):Fixed in #733 closing
linkerzhang(2017-12-22 17:20:22):Thank you for the change/fix! I think 3-dimensional input tensor of shape NCH (probably NCI) should also be allowed :).
dzhulgakov(2017-12-22 21:20:11):Yep, we can say that tensor is r-dimensional as normalization on all axises >=2 is applied
AppVeyorBot(2017-12-25 04:26:25)::x: [Build onnx 0.3.690 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.690) (commit https://github.com/onnx/onnx/commit/4a19918978 by @houseroad)
AppVeyorBot(2017-12-25 04:50:21)::x: [Build onnx 0.3.692 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.692) (commit https://github.com/onnx/onnx/commit/323a3be140 by @houseroad)
AppVeyorBot(2017-12-25 05:08:30)::x: [Build onnx 0.3.694 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.694) (commit https://github.com/onnx/onnx/commit/d8840742a6 by @houseroad)
linkerzhang(2018-01-02 18:42:50):Very interesting. When I tried to change it to "required", it broke several backend test cases (test_backend_test.py, cases: test_densenet121|test_inception_v1|test_inception_v2|test_shufflenet|test_squeezenet). I thought that these test models need to be updated accordingly. It's only my local issue? :)
houseroad(2018-01-03 17:50:23):Yeah, I think if we need to regenerate these models. The concats don't have axis in these models.
houseroad(2018-01-03 18:40:18):@anderspapitto could you take a look?
anderspapitto(2018-01-03 22:08:18):I've uploaded a new version of shufflenet, will do the others after I wait a bit to make sure I didn't break anything
bddppq(2018-01-03 23:37:55):@anderspapitto there is no need to “wait”, you can manually test the new model files
ezyang(2018-02-20 21:20:48):good to merge when CI passes
AppVeyorBot(2018-02-20 21:24:13)::x: [Build onnx 0.3.1173 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1173) (commit https://github.com/onnx/onnx/commit/4aad5fd910 by @bddppq)
linkerzhang(2018-01-02 18:39:54):"required" is the default value, so you may just remove "OPTIONAL" here. Ideally, there're only two APIs, one for required attribute, one for optional attribute with default value (and remove this flag finally).
linkerzhang(2018-01-02 18:44:17):This may not be needed as the default is "required".
CLAassistant(2018-01-03 20:22:03):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=397) <br/>All committers have signed the CLA.
AppVeyorBot(2018-01-03 20:29:49)::x: [Build onnx 0.3.700 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.700) (commit https://github.com/onnx/onnx/commit/7c29837b57 by @anderspapitto)
AppVeyorBot(2018-01-05 20:34:00)::x: [Build onnx 0.3.704 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.704) (commit https://github.com/onnx/onnx/commit/438fb06c0d by @anderspapitto)
AppVeyorBot(2018-01-05 20:41:33)::x: [Build onnx 0.3.705 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.705) (commit https://github.com/onnx/onnx/commit/4318e22340 by @anderspapitto)
AppVeyorBot(2018-01-05 20:52:19)::x: [Build onnx 0.3.706 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.706) (commit https://github.com/onnx/onnx/commit/2b591d1879 by @anderspapitto)
ezyang(2018-01-10 15:46:53):Sorry I am late to the party here.

@linkerzhang @anderspapitto  ~~I'm not convinced that BC version bump here was necessary. We're simply adding a new attribute, and its default value (0) matches the old behavior, right? So this is a backwards-compatible change and no version update is necessary. (Of course, it is also semantically valid to do a version update; it just means that you're forcing producers to rev their targeted operator version to take advantage of the new version. Best not to make their life hard.)~~ OK the spec text is unclear here. 

Also, technically, you were supposed to increase the max version number here. I'm going to write a bot that will lint these cases.
linkerzhang(2018-01-05 17:07:17):Please don't use this "OPTIONAL". Using the API to feed default value for optional attribute. Thanks!
linkerzhang(2018-01-05 17:10:38):Dumb question, any existing model applying this mechanism (apply linear transformation before multiplying) with better results please? Just want to understand more about the benefit of introducing this new attribute please. 
linkerzhang(2018-01-05 17:12:11):Version needs to be bumped in this case. You may refer to PR #390 for bumping the version.
dzhulgakov(2018-01-05 18:44:09):You'd need to add SinceVersion() here and add the 'old.cc' file similarly to #390
dzhulgakov(2018-01-05 18:46:44):@linkerzhang - I'm not sure whether it leads to better results, but the issue is that this is the version which CuDNN (and subsequently PyTorch) implements. I guess it's better to keep both versions representable (though not necessarily supported by all backends)
dzhulgakov(2018-01-05 18:50:37):why there are all these misspellings popping up?
anderspapitto(2018-01-05 20:04:33):this is a generated file - looks like someone spellchecked it manually, instead of fixing the source
linkerzhang(2018-01-06 17:08:12):@dzhulgakov Thank you very much for the clarification! Agree the point.
ezyang(2018-01-10 15:33:31):I looked at the source code history and I did not see any point in time where beginning was fixed manually. A more likely explanation is that the C++ code was not rebuilt prior to generating docs.
buehrer(2018-01-05 02:30:15):Looks good, thanks!
AppVeyorBot(2018-01-06 01:22:52)::x: [Build onnx 0.3.713 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.713) (commit https://github.com/onnx/onnx/commit/ef306ae5f2 by @houseroad)
linkerzhang(2018-01-06 03:44:16):Awesome to kick off this work. Have not reviewed each file in detail yet, just one high-level personal suggestion, shall we have graph/node/tensor interfaces/data structure etc move out to common folder please? btw, It will be very helpful to split the PR into common interfaces and optimization interface/implementation for folks' review and discussion, I personally think. :).
houseroad(2018-01-06 04:43:33):@linkerzhang Thanks for the comments, which make a lot sense. The PR is still in progress, so I will keep polishing it until we are all happy with it. :-)

BTW, I would give great credit to @anderspapitto for his awesome work on this optimizer. :-)
tjingrant(2018-01-07 21:15:50):Hi, great work on the optimizer! One minor question, do you plan to enable optimization by default? This could be a little bit tricky since different frameworks may like different patterns (i.e., TF does not prefer baking bias into the convolution) and by baking bias into convolution in ONNX one may lose other optimization opportunities.

Also, may I ask why is "split models into init and predict parts" considered optimization?
houseroad(2018-01-08 19:37:18):Right, I would like to keep the optimizer optional. I also plan to expose fine-grained APIs, so users can choose the optimization they are interested in.

There are several benefits to "split" the graph, for example, I can do the optimization without loading init data. And the splitting logic depends on the optimization infra. So I consider it as part of the optimization. But I will refactor the API, and it won't be enabled by default. :-)
anderspapitto(2018-01-08 23:52:46):oops, didn't mean to click the accept button quite yet
houseroad(2018-01-18 05:44:03):@linkerzhang, still mixed with the optimizer, but the optimizer part is relatively small, so I didn't split into two parts. I have scanned the code one round. Will keep polishing it, and it's ready for review. :-)
ezyang(2018-01-18 05:50:07):We should consider porting https://github.com/pytorch/pytorch/pull/4717 to this PR.
houseroad(2018-01-18 05:51:28):I have noticed that PR, agree.
houseroad(2018-01-18 05:55:16):@tjingrant we have redesigned the optimizer APIs.

Here is an example how to use: https://github.com/houseroad/onnx-caffe2/blob/035638e8498bd3a9cfe24e5282782d9d78873a7a/onnx_caffe2/backend.py#L762
AppVeyorBot(2018-01-18 18:45:37)::white_check_mark: [Build onnx 0.3.771 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.771) (commit https://github.com/onnx/onnx/commit/576bbe218e by @houseroad)
linkerzhang(2018-01-22 07:00:20):for ir.h, tensor.h etc common files, let's put them in onnx/common folder please.
ezyang(2018-01-22 17:14:28):I agree there are lots of things to fix up in the PR, but I think it might be better to merge this and then successively fix it up as we go along.
linkerzhang(2018-01-24 06:35:01):@houseroad Thank you so much for making the change! I'm OK to push forward although there're still many comments as I think we all agree the APIs may be tuned till they're all good.
houseroad(2018-01-24 06:57:07):@linkerzhang thanks for the thorough review! I agree there are several places we need to tune the PR. Since it is really a huge PR, and it's hard to continue the review work if I keep pushing new commits, let's land it first. I created issues in the repo for the remaining problems.
linkerzhang(2018-01-21 22:59:23):Do we want to use tensor structure defined in https://github.com/dmlc/dlpack/blob/master/include/dlpack/dlpack.h as proposed before please? @dzhulgakov 
linkerzhang(2018-01-22 00:21:19):destructor_ to conform to our current coding style?
linkerzhang(2018-01-22 00:23:56):how about having two constructors, 1 for "int", 1 for "string" (symbolic)
linkerzhang(2018-01-22 00:25:49):reuse AttributeType in AttributeProto please.
linkerzhang(2018-01-22 00:40:17):to be consistent of coding style, "name_" here. similar for other places please.
linkerzhang(2018-01-22 01:12:25):suggestion: change name "offset" as "index".
linkerzhang(2018-01-22 02:05:48):this is not needed, may be replaced with checking whether name is empty.
bddppq(2018-01-22 02:24:25):Should instead include "proto_utils.h" here
bddppq(2018-01-22 02:31:47):This interface should use proto instead of string for "content" (besides, let's change it this parameter to "model" to be more clear).
If user uses optimizer in c++, they should directly pass the protos (so no need to convert to bytes first and we recover here). If it's from python, then the bytes to proto conversion should happen in the pybind interface.
bddppq(2018-01-22 02:32:59):I feel throwing exception would be more appropriate in this case
bddppq(2018-01-22 02:43:34):If two neighboring passes are of the same api type, then there is no need to do the conversion in between.
bddppq(2018-01-22 02:47:21):Probably rename .cpp files to .cc? Just think it's better to use one unified suffix in a project.
linkerzhang(2018-01-22 03:48:00):Value should always have unique name in ONNX design, I think.
linkerzhang(2018-01-22 03:50:40):why we have both unique id and unique name please? only one is needed to identify node inputs/outputs per ONNX.
linkerzhang(2018-01-22 03:56:40):I think it's not always replacing all uses. Right? if we agree on this, then functions used to change the edge should be moved as the API of "Node"?
linkerzhang(2018-01-22 04:01:00):It's better to not contain parent-node reference in a Value.
linkerzhang(2018-01-22 04:01:39):It's better to not have APIs to access parent graph in this level.
linkerzhang(2018-01-22 04:02:22):same as above.
linkerzhang(2018-01-22 04:20:45):Vec --> vec, same for others, let's keep naming style consistent please. 
linkerzhang(2018-01-22 04:23:54):This is op_type, right? Let's keep the same name as it is in protobuf.
linkerzhang(2018-01-22 04:25:26):ArrayRef<Value*>& inputs()?
linkerzhang(2018-01-22 04:52:32):I understand that we have two level mutability here. 1) Add/Remove inputs/outputs. 2) Edit inputs/outputs inline. How about having 1) only, in this way, the interface will look like,

ArrayRef<const Value*>& inputs();
const ArrayRef<const Value*>& inputs() const;
bddppq(2018-01-22 05:04:13):uppercase
bddppq(2018-01-22 05:04:39):why share_ptr?
linkerzhang(2018-01-22 05:08:38):node --> node_output?

So topological order maintained by upper layer? there may be multiple ways of top sort, how to handle?
linkerzhang(2018-01-22 05:23:40):these insert/move APIs may be APIs in graph level. Move it to Graph?
linkerzhang(2018-01-22 05:29:31):const std::string& name
linkerzhang(2018-01-22 05:34:26):The point that we keep both APIs (proto and Ir in-memory) is that, the ir in-memory API design may not be good enough for optimization, right?

My opinion is that we should use the in-memory one as long as IR APIs are designed (the goal of IR APIs design is to be good enough for optimization and execution).
linkerzhang(2018-01-22 05:35:32):why "content"? should be either ModelProto or Graph following the pattern of optimizepass design below.
bddppq(2018-01-22 05:36:35):Initializers should be chopped off in this case.
bddppq(2018-01-22 05:44:39):What's the reason of using std::list here?
dzhulgakov(2018-01-22 06:10:15):'Value' in this case is more like an 'Output' of a node. Unlike LLVM this representation directly supports multi-output SSA and thus separate class of Value is required (instead of just using op). Effectively Value is a node+position pointer because of ssa.
dzhulgakov(2018-01-22 06:14:08):ArrayRef is an immutable object pretty much. It's more or less pointer + size pair. Thus copying it by value is fine. And there's no intention to edit the inputs this way (there's separate api below)
dzhulgakov(2018-01-22 06:16:55):nit: pull out addition to a helper function?

also - clients might want to register their own passes
dzhulgakov(2018-01-22 07:21:44):Definitely in-memory proto directly, not serialized

For a follow up discussion - we might need to build a separate API to abstract out initializers. Serializing them in memory sometimes is prohibitively expensive for big models
dzhulgakov(2018-01-22 07:22:37):throw on unknown passes?
dzhulgakov(2018-01-22 07:25:20):and also why non-const reference to a shared_ptr (it's usually very weird)
dzhulgakov(2018-01-22 07:26:20):we should probably rename 'nets'. And maybe later - rename them into more generic functionality of staging
dzhulgakov(2018-01-22 07:27:10):That struct is more for a interchange of data between APIs. Directly using it is annoying - you'd still need to allocate memory, do i/o, etc.
dzhulgakov(2018-01-22 07:27:49):do we actually support segmented/chunked tensors for serialization?

and I'd argue that for in-memory purposes even if we did - we should stitch them back together
dzhulgakov(2018-01-22 07:28:43):I wonder whether we should remove it from here and instead use a map<> from name to value in initializers
ezyang(2018-01-22 16:31:40):The optimizer might need to allocate new names in the process of optimization, and it is a bit more convenient to allocate new numbers than it is to allocate new strings. So that justifies unique id.

In the old design, `uniqueName` was simply the unique id plus an optional debug name (the unique id always uniquely identified.) The ability to set the unique name directly was added in https://github.com/pytorch/pytorch/commit/b97dfc8a92109fc4c311e0767a95ae75ee8ee4a5 to let you say, "please please please make the following node output as some name". 
ezyang(2018-01-22 16:37:05):I would agree with this if all the nodes were managed separately, but since every Node is memory managed by its owning Graph, I think having the pointer makes sense; you're never supposed to move a Node from one graph to another, and if you do, having owningGraph makes it easy to complain loudly in an assert.
ezyang(2018-01-22 17:06:45):No, it does replace all uses. The Value variant replaces all occurrences of `%3` with a new Value; the Node variant replaces all of the Value outputs of that node to point to the value outputs of a new Node.

(btw, there's a minor notational problem here, I posted about it here: https://github.com/pytorch/pytorch/issues/4782 )
ezyang(2018-01-22 17:10:27):I guess... that would work. But you'd have to maintain a canonical `inputs_array_ref_` in the struct (because the reference needs to point to something) and then remember to update it whenever you update `std::vector`, not to mention how easy it would be to accidentally make a copy of `ArrayRef` (because it is intended to be a pass-by-value type). Seems not worth it.
ezyang(2018-01-22 17:12:12):Yes, you're required to maintain topsort whenever you do operations, but the Graph always maintains a topological order (which is not uniquely determined). Sometimes it may be profitable to move nodes around to a different valid topological sort.
ezyang(2018-01-22 17:12:50):Every Node is uniquely owned by a Graph, so it's less typing to do it this way.
houseroad(2018-01-22 19:24:25):both list and vector should work. So, I can switch it to vector.
houseroad(2018-01-22 19:25:01):Actually, we don't have .cpp files any more. I can remove this part.
houseroad(2018-01-22 19:33:53):Let's keep track this topic here: https://github.com/onnx/onnx/issues/438.

If we decide to add support for dlpack, it's better to have another PR.
houseroad(2018-01-22 19:38:17):Hi @linkerzhang, this file is forked from llvm, shall we keep the coding style? Since later we may apply some updates on it (synced), just like what we did for the PyTorch IR. :-)
houseroad(2018-01-22 19:40:14):Sure. :-)
houseroad(2018-01-22 19:44:21):It's better to keep the ir independent from proto. It does not cost too much here. :-)
houseroad(2018-01-22 19:58:48):Good point!
houseroad(2018-01-22 19:59:28):Sure, I will scan the PR again.
ebarsoum(2018-01-22 20:46:25):How will you guaranty that they will stay in sync?
houseroad(2018-01-22 21:58:23):Another reason is we need to keep track the input and output names of GraphProto (ValueInfoProto has names). Otherwise after proto->ir->proto converting, we will lose the name information of ValueInfoProto.
houseroad(2018-01-22 22:36:53):At IR level, we use unique id in most of cases. This unique_name_ filed just stores names for ValueInfoProto. I think we already have restrictions on ValueInfoProto.
houseroad(2018-01-22 22:50:03):Since we allow the empty string as name of the input/output to resolve the optional parameter list issue, here we have to distinguish empty string and missing string.
houseroad(2018-01-22 22:50:21):Similar here, :-)
houseroad(2018-01-22 23:28:07):~~Actually this is API to bridge the python world and C++ world. If I remember correctly, passing bytes is the recommended solution. Unless we really want to bind python and C++ ModelProto.~~

I see the point here, yeah, I will change the code here, and move the converting code to the cpp2py_export.cc

houseroad(2018-01-22 23:32:05):@bddppq suggests to support ModelProto as input. So if someone wants to directly operate on Protobuf, just let them do.
houseroad(2018-01-23 00:05:40):I will move the string<==>proto conversion to the cpp2py_export.cc.
houseroad(2018-01-23 00:07:38):Our parser is not strong enough to parse all kinds of onnx models. If the model using an older version of ONNX IR, our parser may refuse it. So let's just return the original model here. @anderspapitto 
houseroad(2018-01-23 00:08:18):We have an optimization in our optimizer.
houseroad(2018-01-23 00:09:48):Sure.
houseroad(2018-01-23 00:10:15):Good catch, I will fix this.
houseroad(2018-01-23 00:18:24):Sure.
houseroad(2018-01-23 00:29:15):Sure, we can do that. (warning is probably too weak. We should make sure that users know how to call optimizer correctly.)
houseroad(2018-01-23 00:31:05):Will follow this issue here: https://github.com/onnx/onnx/issues/439
houseroad(2018-01-23 00:45:33):https://github.com/onnx/onnx/blob/master/onnx/onnx.proto#L341 shows that TensorProto supports name.
houseroad(2018-01-23 01:25:02):We can follow this issue here https://github.com/onnx/onnx/issues/438
houseroad(2018-01-23 21:31:43):There is no guarantee. I just manually sync them between different versions.
houseroad(2018-01-24 00:16:39):https://github.com/onnx/onnx/issues/450 will follow here.
houseroad(2018-01-24 00:26:46):Let's discuss more here. https://github.com/onnx/onnx/issues/451

linkerzhang(2018-01-24 05:44:53):this should be removed.
linkerzhang(2018-01-24 06:04:03):then we should reuse it directly, it's hard to keep them in sync, and they anyway are dependent each other (directly or manually).
linkerzhang(2018-01-24 06:12:22):the API wise is confusing callers, I think. Let's think more on this later.
linkerzhang(2018-01-24 06:16:53):Hmmm, not having these APIs in this level (Value) does not bring troubles. By contrast, giving the capability here make things too flexible, guys who doing optimization could now have 3 places to edit a graph, graph, node, and value. We should restrict this somehow.
linkerzhang(2018-01-24 06:17:44):Same as above, I'd still suggest to move these APIs out of "Value" class.
I'm not an optimization expert, I thought it's not always people want to replace one value's all uses, may be partial uses. no?

linkerzhang(2018-01-24 06:25:40):Read it again, and got that ArrayRef is an immutable. My personal point here is to not having two ways of editing inputs/outputs. 1) Add/Remove inputs/outputs. 2) Edit inputs/outputs inline. This is what current design is doing. We could simplify it by only offering the 1st mechanism with same capability. Make sense?

linkerzhang(2018-01-24 06:30:28):Hmmm, I do hold different opinion :). If someone wants to directly operate on Protobuf, it means our ir API is not good (even worse than protobuf :)), which should not be the case, and even if it's the case, community should keep tuning the API till it's good. :) Anyway, let's not be blocked on this for now.
houseroad(2018-01-24 06:38:32):Yeah, I think at least more docs should be helpful.
AppVeyorBot(2018-01-10 03:01:00)::x: [Build onnx 0.3.720 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.720) (commit https://github.com/onnx/onnx/commit/690c687ac4 by @houseroad)
AppVeyorBot(2018-01-10 06:44:21)::white_check_mark: [Build onnx 0.3.721 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.721) (commit https://github.com/onnx/onnx/commit/8d08ec1656 by @houseroad)
ezyang(2018-01-10 15:23:10):This is listed with a new operator version, but is this really a BC breaking change? If it's empty, the semantics should be same as the old version of ConvTranspose, no?
houseroad(2018-01-10 17:29:56):With clarification in https://github.com/onnx/onnx/pull/410, I think this PR is non BC change.
linkerzhang(2018-01-10 05:26:23):if it's optional, please try to use the API with default value set, Default value is an empty int vector?

linkerzhang(2018-01-10 05:43:16):for ONNX domain ops, the current max version is "2", so you should bump the version to “3” here. Check schema.h map_[ONNX_DOMAIN] = std::make_pair(1, 2); please.
linkerzhang(2018-01-10 05:44:05):feel like the name "output_padding" makes more sense, no?
linkerzhang(2018-01-10 05:48:46):To support both output_shape and adjs, ideally a specific attribute verification/parseing logic (If output_shape is set, this attribute will be ignored) should be put in-place. This is something I discussed with Edward on issue #362 . Any thoughts please?
houseroad(2018-01-10 06:09:43):The default value should be a list of 0, the length of the list is determined by the input dimension. This is quite similar to kernel_shape and strides, not easy to express the default value statically.

Regarding https://github.com/onnx/onnx/issues/362, I am fine with both solutions. My feeling is that extending existing defs/checker should be much easier to implement. I agree with Ed, we should be very clear in (English) description.
  
houseroad(2018-01-10 06:24:00):Both Caffe2 and MXNet use adjs, only PyTorch uses output_padding. I try to choose a more widely used name. I agree with that output_padding is more straightforward.
  
houseroad(2018-01-10 06:43:12):After thinking about it, I feel this change should not break anything... hmm, probably I should just add this change without bumping up the version number? If so, I should not add anything to old.cc. Any thought about this?
  
houseroad(2018-01-10 06:50:59):For the existing models, this change is fine. But for the new models with output_padding/adjs attribute, this may break the backend. If we think in this way, we can consider it as a breaking change.
ezyang(2018-01-10 15:31:17):@houseroad There will always be operators/attributes that backends don't support. A breaking change is when something that was *previously* supported now has a different meaning (true breaking change.)

I'll check and see if we've explained this adequately in VERSIONING
ezyang(2018-01-10 15:49:19):I reread VERSIONING and it looks like it supports your interpretation, so it looks like something got missed in translation.
ezyang(2018-01-10 16:59:24):I'm partial to `output_padding`, but I'm also a PyTorch developer :)
ezyang(2018-01-10 17:12:37):So, a convention in PyTorch is that if you have a single-element list where an n-element list is expected, we expand the list to cover the number of required elements, replicating the element. However, I'm also pretty OK with keeping things simple for people and NOT defining a default value here; you should just be expected to handle the case when the attribute is missing, which seems pretty reasonable to me. @linkerzhang, is there a reason to put a default value that I am not seeing?
linkerzhang(2018-01-10 17:41:54):I have been thinking of asking default value for all "optional" attributes in operator declaration. It's kind of like initialization when creating a variable. 
The op declaration here is just a standard (having some documentation functionality), so it's better for us to clarify it clearly.
But this case reminds me that we may have default value in the implementation without caller's specification, say if it's a "int" attribute and it's optional, its default value will be "0" if caller does not specify the default value explicitly. In this way, we can ensure that our documentation generated will always have a default value for optional attributes.

linkerzhang(2018-01-10 17:46:53):Hmmm, frankly speaking, I'm hesitating on this. 
With bumping version in this case, it's reasonable in terms of semantics, but it's kind of annoying as the version will be bumped frequently. 
Without bumping it, the case Lu mentioned will happen. A backend supports ONNX OP version=2 will NOT be able to load some models which use this new attribute. This looks bad, I think.
ezyang(2018-01-10 17:59:44):@linkerzhang Remember that the backend will always not be able to understand the new attribute, no matter what you do. So it doesn't seem like much of a benefit to bump the version, except maybe an earlier test that there's a problem.
ezyang(2018-01-10 19:27:18):OK, given that we're in the "non BC-breaking changes are OK to edit directly" regime, it would be nice to mention a date when new attributes are added to make it clear that they're less likely to be implemented by backends. Additionally, since this is an optional attribute without a default, it would be good to say what its semantics are when it's not set. (In this case, no padding; maybe this is reasonably clear and not necessary.)
houseroad(2018-01-10 17:22:20):I like examples. This is much clearer! 👍 
houseroad(2018-01-10 17:24:06):Shall we also update the version in VERSION_NUMBER file?
ezyang(2018-01-10 17:32:56):Nope, VERSION_NUMBER is the PR version number, we only update that when we are releasing onnx proper
houseroad(2018-01-10 17:34:54):I see, thanks.
AppVeyorBot(2018-01-10 17:46:09)::x: [Build onnx 0.3.725 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.725) (commit https://github.com/onnx/onnx/commit/51b2f31d20 by @ezyang)
linkerzhang(2018-01-10 19:22:23):There's also another kind of mistake. 

Developer bumps operator version without checking the domain max version here. Say, max version is 3 here, the specific op version is 1, developer may bump the version to 2 by mistake :).

Agree with you, we may need bot to do this bumping.
ezyang(2018-01-10 18:22:29):I don't really understand. What were the values of these variables in the bad case?
bddppq(2018-01-10 18:25:23):`python -c 'import onnx; print(onnx.helper.make_tensor_value_info("Z", onnx.TensorProto.FLOAT, ()))'`

before:
```
name: "Z"
type {
  tensor_type {
    elem_type: FLOAT
  }
}
```

after:
```
name: "Z"
type {
  tensor_type {
    elem_type: FLOAT
    shape {
    }
  }
}
```

houseroad(2018-01-10 18:31:00):Shall we set the shape as [0]?
tjingrant(2018-01-10 18:48:42):This might break the invariant num_dim == len(shape) since scalars are 0d tensors.
ezyang(2018-01-10 19:28:18):Yes, we definitely want to abide by this invariant.
ezyang(2018-01-10 19:37:24):OK, I think I understand what the code is doing now. Maybe the way I would have written the comment is something like:

```
# You might think this is a no-op (extending a normal Python list by []
# certainly is), but protobuf lists work a little differently; if a field is never
# set, it is omitted from the resulting protobuf; a list that is explicitly
# set to be empty will get an (empty) entry in the protobuf. This difference
# is visible to our consumers, so make sure we emit an empty shape!
```
bddppq(2018-01-10 19:45:42):@ezyang  Sure, updated the comment
CLAassistant(2018-01-11 01:33:54):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=414) <br/>All committers have signed the CLA.
CLAassistant(2018-01-11 06:00:58):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=415) <br/>All committers have signed the CLA.
bddppq(2018-01-17 10:27:23):Could there be cases that the shape of the output is dynamic?
linkerzhang(2018-01-17 19:14:54):if it's dynamic, we should use shape as the input, see RandomUniformLike.
CLAassistant(2018-01-15 07:30:35):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=422) <br/>All committers have signed the CLA.
linkerzhang(2018-01-15 19:21:09):Thanks! "conda_prefix" is only needed when "use_conda" is true. Move the warning message into "elif use_conda" block may be better.
guoyuhong(2018-01-15 20:25:50):use_conda = os.getenv('CONDA_PREFIX') and platform.system() == 'Windows'; So the env variable determines use_conda. Here is better?
linkerzhang(2018-01-16 03:54:39)::), Makes sense. 
houseroad(2018-01-16 22:12:28):Shall we error out the setup here?

Shall we make it explicitly that users should set PROTOBUF_INCDIR or CONDA_PREFIX?
CLAassistant(2018-01-16 08:18:59):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=423) <br/>All committers have signed the CLA.
AppVeyorBot(2018-01-16 08:24:04)::x: [Build onnx 0.3.751 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.751) (commit https://github.com/onnx/onnx/commit/330c41147b by @LucasMahieu)
AppVeyorBot(2018-01-16 09:46:26)::x: [Build onnx 0.3.752 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.752) (commit https://github.com/onnx/onnx/commit/dee2ba50d1 by @LucasMahieu)
AppVeyorBot(2018-01-16 10:26:03)::x: [Build onnx 0.3.753 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.753) (commit https://github.com/onnx/onnx/commit/d8eacefc4c by @LucasMahieu)
AppVeyorBot(2018-01-16 11:04:45)::x: [Build onnx 0.3.754 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.754) (commit https://github.com/onnx/onnx/commit/20dad57201 by @LucasMahieu)
LucasMahieu(2018-01-16 12:39:46):Test model that are used for test are not compatible with the changes of this commit.
Some attributes are not presents but they are required for this commit.
LucasMahieu(2018-01-16 12:41:04):This PR answer to https://github.com/onnx/onnx/issues/416
AppVeyorBot(2018-01-16 12:52:25)::x: [Build onnx 0.3.755 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.755) (commit https://github.com/onnx/onnx/commit/abf996cfb7 by @LucasMahieu)
AppVeyorBot(2018-01-17 09:39:00)::x: [Build onnx 0.3.757 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.757) (commit https://github.com/onnx/onnx/commit/e74122dc26 by @LucasMahieu)
AppVeyorBot(2018-01-17 09:46:54)::x: [Build onnx 0.3.758 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.758) (commit https://github.com/onnx/onnx/commit/66876bee5a by @LucasMahieu)
AppVeyorBot(2018-01-18 10:02:11)::x: [Build onnx 0.3.770 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.770) (commit https://github.com/onnx/onnx/commit/47583d379c by @LucasMahieu)
linkerzhang(2018-01-19 05:20:10):@bddppq @dzhulgakov @ezyang  please also help to review this, since it's changing some attributes to required, if we all agree on the change, please @bddppq help to update test models accordingly. Some tests are broken because of "required" attributes missing.
dzhulgakov(2018-01-19 07:54:34):Actually, why is it a breaking change? Replacing optional attribute with the attribute with default value still keeps it as optional. Old models should still be compatible. We don't really change the default value as it was assumed previously in op docs and presumably backends implemented it that way. If you keep strides/pads optional I think we could get rid of versioning change. Or am I missing something?
LucasMahieu(2018-01-19 08:36:35):There are different things that are changed in the PR:
- documentation update to notify the user of the default values (no BC)
- default values are used for some optional attr (no BC)
- some optional attributes are replaced by required attributes ( /!\ BC /!\)
For example, the kernel_shape should required for pooling nodes.
We can't know the kernel shape of pooling op from another manner.
All checks have passed, but it is because optional kernel shape for pooling is impossible, so tests are well written, but nn def was not good. kernel_shape has to be required, and this is BC ...
LucasMahieu(2018-01-19 10:25:50):PR Should be ready for the review and merged, according to me.
bddppq(2018-01-19 18:03:19):Actually I think even though `kernel_shape` has been changed from optional to required in the schema, this shouldn't be considered as BC. Because it was a mistake just in the spec, in fact there shouldn't be any existing "valid" models with these ops that have not specified `kernel_shape`. So I suggest let's don't bump the version of these pool ops.
Add our versioning expert @ezyang 
LucasMahieu(2018-01-19 18:18:47):I totally agree with you.
I will make the change ASAP.
ezyang(2018-01-19 19:12:33):Yes, I think `kernel_shape` being `OPTIONAL` is clearly a spec error. If someone complains we can revert the change and rev the version but I think this is unlikely to be necessary.
LucasMahieu(2018-01-22 07:51:03):Version revision has been changed to spec bugfix.
AppVeyorBot(2018-01-22 07:55:46)::white_check_mark: [Build onnx 0.3.793 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.793) (commit https://github.com/onnx/onnx/commit/a36bdd8fed by @LucasMahieu)
AppVeyorBot(2018-01-22 17:35:39)::x: [Build onnx 0.3.794 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.794) (commit https://github.com/onnx/onnx/commit/ffeeaac9d8 by @ezyang)
ezyang(2018-01-22 17:38:26):This is ready to merge as soon as the build passes.
bddppq(2018-01-22 17:42:31):@onnxbot test this please
AppVeyorBot(2018-01-22 17:47:44)::white_check_mark: [Build onnx 0.3.795 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.795) (commit https://github.com/onnx/onnx/commit/d5280f7229 by @ezyang)
linkerzhang(2018-01-17 05:57:57):The current max op set version of ONNX domain is 2, so bumping version to be "3" here. You may get to know the max op set version of ONNX domain in onnx\defs\schema.h as below.
"
DomainToVersionRange() {

      // Increase the highest version when you make BC-breaking changes to the

      // operator schema on specific domain. Update the lowest version when it's

      // determined to remove too old version history.

      map_[ONNX_DOMAIN] = std::make_pair(1, 2);

    }"
linkerzhang(2018-01-17 06:00:19):same comment as above, bumping version should be 3 now.
linkerzhang(2018-01-17 06:00:36):same comment as above.
linkerzhang(2018-01-17 06:07:54):this is optional in this case, right? it's even not needed. the kernel shape may be inferred from input "W".
linkerzhang(2018-01-17 06:11:53):Could be optional with "1" stride in all axes?
LucasMahieu(2018-01-17 08:57:25):ok, thanks

LucasMahieu(2018-01-17 08:58:05):Oh, you are right, I didn't think to this
LucasMahieu(2018-01-17 09:02:10):we could, but the problem is the len of the list of strides.
we can't know the default len of the list of stride values.
So, two solution : 
- To make this attributes required
- To make this attribute optional, without any default value, but in that case, modify the doc to say that back-ends have to use stride of 1 for each spacial axis of input
linkerzhang(2018-01-19 05:14:29):I mean it should be bumped to "3" here.
linkerzhang(2018-01-19 05:17:55):I'd pick up the option 2 here.
bddppq(2018-01-19 07:19:26):This should be optional (default to 0s). Similar to `strides`, we need to specify the default values in the doc.
LucasMahieu(2018-01-19 08:25:19):Ok, so i will make the change to fit option 2.
ezyang(2018-01-19 19:18:33):So, as we agreed, this is a spec bugfix, not a breaking change.
ezyang(2018-01-19 19:19:14):Also a spec bugfix, so not BC-breaking
ezyang(2018-01-19 19:19:45):Also a spec bugfix.
ezyang(2018-01-22 17:23:07):Nice :)
AppVeyorBot(2018-01-17 18:52:40)::white_check_mark: [Build onnx 0.3.759 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.759) (commit https://github.com/onnx/onnx/commit/d7241d6a34 by @houseroad)
bddppq(2018-01-17 18:57:08):I suggest to use "Call for contributions" as the headline & title.
houseroad(2018-01-17 19:35:05):Done :-)
AppVeyorBot(2018-01-19 07:54:03)::x: [Build onnx 0.3.773 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.773) (commit https://github.com/onnx/onnx/commit/59efd1a248 by @guoyuhong)
AppVeyorBot(2018-01-19 08:16:12)::x: [Build onnx 0.3.774 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.774) (commit https://github.com/onnx/onnx/commit/85c4676483 by @guoyuhong)
CLAassistant(2018-01-19 08:24:39):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=430) <br/>All committers have signed the CLA.
AppVeyorBot(2018-01-19 08:28:30)::x: [Build onnx 0.3.775 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.775) (commit https://github.com/onnx/onnx/commit/574a81f590 by @lupesko)
bddppq(2018-01-19 08:32:43):Oh you need to wrap the exception types with parenthesis
lupesko(2018-01-19 08:40:27):Yep... (python 3 always catches me off guard... J)
bddppq(2018-01-19 18:06:54):The data files need to be generated and committed to git as well
@guoyuhong @linkerzhang 
bddppq(2018-01-22 19:09:56):ping @linkerzhang and @guoyuhong for adding the missing data files. You can see the command of generating them in this doc: https://github.com/onnx/onnx/blob/master/docs/OnnxBackendTest.md
guoyuhong(2018-01-22 23:18:38):@bddppq Thanks for the reminding. I will try to add this data files. One more question: it looks like all the node tests are skipped. How to guarantee that our unit-test code is correct?
bddppq(2018-01-23 00:37:47):You will need to have a backend to actually run the tests.
guoyuhong(2018-01-23 05:22:59):@bddppq I write this test based on your previous node test examples. I will try to apply this unit test to a real backend to have a try. It may take some time.
bddppq(2018-01-23 05:39:12):@guoyuhong If it could save you some time: I have tested your new cases with onnx-caffe2 :-)
guoyuhong(2018-01-23 21:31:22):@bddppq. For a freshman, they may not know how to test it in a real back-end. Do we need some documentation on how to run a real back-end test to verify the code?
bddppq(2018-01-23 21:53:28):@guoyuhong Sure that would be helpful. Could you document it while you are exercising this process this time?
And besides, could you upload the generated data files soon? Right now new contributors need to manually filter out them before uploading the corresponding data files for their new test cases.
linkerzhang(2018-01-19 17:22:45):Thank you very much! Would you also help to add cases to cover,
1. broadcast not specified, and two inputs have different shape.
2. cases for different attribute values of "axis".
AppVeyorBot(2018-01-20 01:12:14)::x: [Build onnx 0.3.786 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.786) (commit https://github.com/onnx/onnx/commit/36ad7fcba1 by @jamesr66a)
jamesr66a(2018-01-20 02:22:59):Ref https://github.com/onnx/onnx/issues/140
AppVeyorBot(2018-01-20 02:42:17)::x: [Build onnx 0.3.788 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.788) (commit https://github.com/onnx/onnx/commit/4da75c71fd by @jamesr66a)
bddppq(2018-01-20 02:52:23):@onnxbot retrst this please
bddppq(2018-01-20 02:52:46):@onnxbot retest this please
AppVeyorBot(2018-01-20 03:34:25)::white_check_mark: [Build onnx 0.3.789 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.789) (commit https://github.com/onnx/onnx/commit/bd123e3493 by @jamesr66a)
jamesr66a(2018-01-22 19:56:26):TODO from me right now:
-onnx-caffe2 backend support
-Minimal unit tests for each op

From this hopefully we can get some conforming model protos out
AppVeyorBot(2018-01-23 18:33:51)::x: [Build onnx 0.3.816 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.816) (commit https://github.com/onnx/onnx/commit/8309555e74 by @jamesr66a)
AppVeyorBot(2018-01-23 18:47:32)::x: [Build onnx 0.3.817 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.817) (commit https://github.com/onnx/onnx/commit/7b088ec41a by @jamesr66a)
AppVeyorBot(2018-01-23 19:13:39)::x: [Build onnx 0.3.818 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.818) (commit https://github.com/onnx/onnx/commit/ceacd12929 by @jamesr66a)
AppVeyorBot(2018-01-23 20:44:51)::x: [Build onnx 0.3.819 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.819) (commit https://github.com/onnx/onnx/commit/46a9af6da6 by @jamesr66a)
jamesr66a(2018-01-27 00:00:33):Hi folks, I've addressed many of your comments. I've also made changes to the API based on internal feedback. The interface is now (max_trip_count, condition), with both inputs being optional. This covers a wide range of use cases, including while loops, do-while loops, for loops, and a generalized while loop with limited trip count. It also supports an infinite loop, but I'm not sure how useful this is going to be, since we don't have a `break` statement.

Please review the changes and let me know if there are more considerations we should make!

A TODO on my side: need to generalize the topological sort check to include values available via lexical scoping
AppVeyorBot(2018-01-27 00:06:19)::x: [Build onnx 0.3.909 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.909) (commit https://github.com/onnx/onnx/commit/035759e0e4 by @jamesr66a)
AppVeyorBot(2018-01-27 00:13:43)::white_check_mark: [Build onnx 0.3.910 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.910) (commit https://github.com/onnx/onnx/commit/ffd484e9c6 by @jamesr66a)
dzhulgakov(2018-01-27 01:40:11):Looks great. Few questions:
- passing `condition` to the `body` seems redundant here - it's always going to be true. I understand the rationale to make it seems like a loop carried dependency, but we already have the trip_count that doesn't follow this pattern, so it should be "fine"
- I'm also not sure about 'cond' being the input to the operator. it's effectively wrapping things into `if (cond) { }`, right? We can always use an external op for that if it's not that common.
- since you now pass loop_var as an explicit input treating it as a special var feels confusing. We can get rid of it and rely on the graph analysis for enforcing that only loop vars become inputs of `Index` for optimization purposes. It gets a bit clumsy though to incorporate affine transforms then.

@zdevito - what do you think?
jamesr66a(2018-01-29 18:31:16):Updated to include output specification for If and While ops. @linkerzhang let me know if there's any problems wrt hetergeneously-typed variadic outputs
AppVeyorBot(2018-01-29 18:37:17)::x: [Build onnx 0.3.926 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.926) (commit https://github.com/onnx/onnx/commit/6f2fd7b616 by @jamesr66a)
jamesr66a(2018-02-01 18:07:33):@onnxbot retest this please
dzhulgakov(2018-02-02 02:04:23):Comments from offline discussion:
- one thing that needs to be added to support "scan-like" pattern is the operator to extract Dimension of the input Tensor to be used as trip_count
- LoopIndexTensor in current form is a strict superset of Gather. If we implement 'performance helper' in ONNX that sanity-checks RNN patters we could get rid of special LoopIndexTensor one
gramalingam(2018-02-02 19:37:18):@dzhulgakov : in your second bullet, I guess you meant "LoopIndexTensor is a special-case (strict subset) of Gather"?

Is your goal to target "scan-like" pattern primarily? In which case, defining a "scan" operator may be worthwhile. I am slightly confused whether you want a general-purpose core/fundamental construct (like loop) or something in a specialized form ... separating these two goals may be useful. (E.g., may be go for simple general-purpose loops AND a complex specialized operator for a specific pattern.)
jamesr66a(2018-02-02 21:22:07):@gramalingam Our intent is not to target the "scan" operator pattern primarily, but rather to create a general-purpose primitive which can help us to support NLP use cases. Our new strategy document might be of interest to you: https://github.com/onnx/onnx/issues/501

I think creating an explicit "Scan" operator might be worthwhile, but that should be a separate discussion from this proposal, which targets the more general use case.
dzhulgakov(2018-01-22 08:28:03):is it optional as if it's a single loop?
dzhulgakov(2018-01-22 08:28:36):should it also have 'axis'?
dzhulgakov(2018-01-22 08:29:00):compile time -> statically in the model graph
dzhulgakov(2018-01-22 08:29:41):if -> while ?
dzhulgakov(2018-01-22 08:30:51):it needs to be marked as optional

Also it means that it's an always present in the graph description but might be 'empty string'
dzhulgakov(2018-01-22 08:31:08):if we treat input as optional - we could skip this attribute
dzhulgakov(2018-01-22 08:31:38):it should be required, no? or the body can be 'empty'?
dzhulgakov(2018-01-22 08:32:15):we should move it somewhere
dzhulgakov(2018-01-22 08:32:55):should we have Output spec too?

and add more details on requirements for then/else branches (e.g. outputs should match)
gramalingam(2018-01-23 03:37:14):I suggest not tying this operator so tightly with a loop. Indexing a tensor is a natural operation, independent of loops. Let's call this just "IndexTensor" and not restrict it to inside loops.
This requires rationalizing the use of "loop_var" attribute as well. I think "loop_var" is a variable representing a dynamic runtime value, and so it is best represented as an input instead of as an attribute. 
gramalingam(2018-01-23 04:03:21):What is the semantics of trip_count in a loop? Is it an additional condition along with loop-condition or an alternative for the loop-condition? 
jamesr66a(2018-01-23 18:15:41):@zdevito what do you think here?
jamesr66a(2018-01-23 18:16:16):It's an additional condition. The loop continues while both (just updated) of these conditions hold true.
ezyang(2018-01-24 05:09:26):This doesn't seem to have rendered correctly; surely there should be some outputs?
bddppq(2018-01-24 08:36:52):Rendering is correct, currently there is no outputs spec in the schema.
ezyang(2018-01-24 16:18:41):I guess we're going to need to write some custom validation code or generalize the checker, because if the checker rejects any If node that does not has zero outputs, this will not be very useful schema
linkerzhang(2018-01-24 18:28:44):no need specifying "true" here, as default is "true" :).
jamesr66a(2018-01-24 22:15:18):We don't have any way to specify variadic outputs at this time, right?
houseroad(2018-01-24 23:33:36):Create an issue?
bddppq(2018-01-25 00:48:25):@jamesr66a we "do": https://github.com/onnx/onnx/blob/65b4eb726592a0c847b811163763f248efb129f0/onnx/defs/experiments/defs.cc#L285 But IIUC it's restricted in the sense all outputs have to be of the same type. 

@linkerzhang
ezyang(2018-01-26 02:15:10):dependency
ezyang(2018-01-26 02:16:37):Would it be perhaps easier to parse the usage if we used one of the shorthand notations supported by the onnx? I guess the C code helps.
ezyang(2018-01-26 02:20:29):I'm not a fan of the read/write nomenclature, since that implies some sort of mutation is going on, but we're SSA, and we're definitely not in the business of having a memory model. As an alternate wording:

1) Values from the enclosing scope (i.e. variable a here) are in scope and can be referenced in the inputs of the loop
2) Any variables which you wish to make available in the enclosing scope (i.e. the variables b and keepgoing) must be declared in the loop-carried dependencies, both at the op input and the body net input and output. (Also, did you mean op output?)

As a meta point, graph attributes are a general trait of the ONNX protobuf representation, so it might make sense to standardize these properties across all of ONNX, and not just have them part of the statement here.
ezyang(2018-01-26 02:22:40):In other words, ONNX evaluation order is not your plain old strict evaluation order :) I guess we should emphasize this in the overall ONNX docs too.
ezyang(2018-01-26 02:23:32):It definitely doesn't make sense to restrict all outputs to be the same type.
ezyang(2018-01-26 02:30:35):I think we should be very careful about an operator like this. Previously, all of our operators were pure operations: it doesn't matter when you execute them, you can inline them into subgraphs unconditionally. `LoopIndexTensor`, on the other hand, is sensitive to the particular graph from which it is run; you can't push it into a subgraph; there is a kind of dynamic scoping going on. Maybe we will find this class of operators sufficiently useful that we will enshrine them as a class of their own but I would take a close look at it.
gramalingam(2018-01-26 08:10:29):I agree with @ezyang 's point, but would say the issue is not with the operator itself, but the use of "loop_var" ... one possibility is to view it as a mutable variable (which don't exist in the current ONNX) or as shorthand: the loop-body is implicitly a function (lambda) with the loop_var being an implicit parameter. 
ezyang(2018-01-26 16:40:51):It would be far better to model this as dynamic scoping (which now has the correct properties), but you can always desugar dynamic scoping into explicit parameter passing and that keeps ONNX simpler.
NiklasGustafsson(2018-01-26 17:34:21):This PR raises some interesting questions around scoping of names within subgraphs. It can certainly be made to work, in several ways, but we would need to very carefully specify the scoping rules within hierarchies of graphs so that we can avoid implementation divergence.

From a name scoping perspective, it may be easier to introduce function definitions and define control flow operators in terms of function attributes rather than graphs. It would require explicitly passing everything you need into the subgraph rather than capturing them from the parent graph(s).

Just a thought,

Niklas
jamesr66a(2018-01-26 19:04:26):So the motivation of me using the direct graph proto here is that it's a representative example that's simultaneously human-interpretable and machine-interpretable, allowing for backends (etc.) to use this as a test. The other options we have are

1) The graph printer format, which there isn't a parser for yet (iiuc) so we lose the machine-readable aspect
2) Use the helper functions from helper.py. This hurts human readability since you essentially have to execute this meta-program (either in Python or in your head)

Thoughts? I'd love to make this more readable
jamesr66a(2018-01-26 23:57:23):@linkerzhang  can you confirm if this is in fact the behavior? Are variadic inputs/outputs constrained to be all the same type?
linkerzhang(2018-01-31 17:14:15):branch graph nodes will have their own attribute data? I personally think using function in designing control flow ops is better. In that way, branch is a function (graph template), with parent node attributes/inputs/outputs, a graph could be instantiated.
jamesr66a(2018-01-31 18:18:15):I don't understand what you mean. Can you provide an example of what this would look like?
jamesr66a(2018-01-31 22:34:05):Hi @linkerzhang 

Do you think you could reproduce the example provided in the model definition using a function rather than a graph attribute?

    graph predict-net {
      %a = Constant[value = <Scalar Tensor [3]>]()
      %b = Constant[value = <Scalar Tensor [6]>]()
      %keepgoing = Constant[value = <Scalar Tensor [1]>]()
      %max_trip_count = Constant[value = <Scalar Tensor [10]>]()
      %keepgoing_out, %b_out, %user_defined_vals = Loop[body = <graph body-net>](%max_trip_count, %keepgoing, %b)
      return
    }

    graph body-net (
      %i[INT32, scalar]
      %keepgoing[BOOL, scalar]
      %b[INT32, scalar]
    ) {
      %my_local = Add(%a, %b)
      %b_out = Sub(%a, %b)
      %keepgoing_out = Greater(%my_local, %b_out)
      %user_defined_vals = Add(%b, %b)
      return %keepgoing_out, %b_out, %user_defined_vals
    }
gramalingam(2018-02-01 00:43:50):Regarding if-then-else: Like Dmytro mentioned earlier, it would be good to write down the conditions (about how the then-graph and else-graph should match). May be I missed this explanation, but I couldn't find it. Clearly both must have the same number of outputs. But are the outputs matched by name? (So, both branches have the same output-names?) I remember an earlier question about whether the corresponding outputs from the two branches must have the same type as well, but I can't find an answer to this.
jamesr66a(2018-02-01 06:30:56):>Clearly both must have the same number of outputs.

Yeah, that's mentioned in the description for the sub-graphs

>But are the outputs matched by name?

No, it's purely positional. For e.x., I have the If op with `outputs = ['foo', 'bar', 'baz']`. I can have the then_branch net have `outputs = ['x', 'y', 'z']`, and else_branch with `outputs = ['a', 'b', 'c']`. Note that all three of these outputs are in different scopes. The op outputs refer to values defined in the enclosing scope. The then_branch outputs define names within the then_branch graph's scope, and the else_branch likewise. Imagine the `If` op encapsulates phi-nodes that reconcile its outputs to a value from the taken branch, e.g. `foo = phi(x, a)` and so on.

> I remember an earlier question about whether the corresponding outputs from the two branches must have the same type as well, but I can't find an answer to this.

That sounds reasonable, but outside of MS I don't think we have any infrastructure to validate any such specification of types, so can I ask for this to be a contribution on your side?
gramalingam(2018-02-01 18:22:54):With the changes, this is no longer a special operator valid only inside a loop. I am not sure I understand the restrictions "MUST be ...". Some optimizations may be valid only under these conditions, but isn't that up to the compiler/runtime to figure out? I suggest changing the name to "IndexTensor", changing input-name "loop_idx" to "idx" and the documentation to say that the operation indexes the specified axis using the specified index-value to produce a sub-tensor.
gramalingam(2018-02-01 20:08:44):Why is the "condition" an input to the loop-body? 
jamesr66a(2018-02-01 20:11:58):EDIT: sorry misunderstood. `condition` is an input (though it'll always be true) since it can be treated as a loop-carried dependency like those inputs that follow, and thus having it there simplifies the backend implementation
gramalingam(2018-02-01 20:14:21):I personally feel that it will be better to have simpler primitives (like a simple while-loop) that can be composed to get this effect instead of having a single complex operator that combines multiple things (a for-loop and a while-loop together).
jamesr66a(2018-02-01 20:18:50):We take the philosophy of having higher-level operators, so that backends don't have to engage in complex pattern-matching algorithms to identify the semantics of the network and reason about which transformations are legal. Could you give an example of what primitives you would propose instead?
gramalingam(2018-02-01 22:30:11):(a) The function-proposal is one way to avoid this philosophical tradeoff between (1) A smaller set of core primitives and (2) A long list of higher-level operators. A common higher-level operator can be defined as a function, easily recognized and optimally implemented by the backend. 

(b) Are there realistic examples that combine trip-count with a while-condition? From an efficiency and higher-level operator perspective, I would think that a "scan" operator (like CNTK's "Recurrence"), without any trip-count, would be a common pattern worth capturing as an operator/function.

I am not saying that we shouldn't define a complex operator like this if there is a compelling motivation. But it is unclear to me whether the current motivation is to define the basic/core control-flow primitives or define a higher-level operator for a specific class of situations. If needed, we could do both.

(c) The primitive I would propose is a while-loop without any trip-counts (and/or a "scan" operator)
jamesr66a(2018-02-01 23:00:36):(a) Let me discuss this with the team. Will update later. But my instinctual response is that control flow operators are sufficiently changing the core semantics of the spec (in terms of analysis, optimization, etc) that it's okay to special-case this

(b) Beam search with a dynamic stopping condition as well as a max trip count is an example. This is a concrete use case we have that we want to support in ONNX

(c) The trip count gives semantic information to backends for optimization purposes. In the case where we have a trip count, backends can bulk allocate all required memory and/or do more fancy things like emit specific kernels for a given sequence length, etc. This would be harder to do if we have to pattern match the indexing machinery
AppVeyorBot(2018-01-23 17:46:41)::white_check_mark: [Build onnx 0.3.814 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.814) (commit https://github.com/onnx/onnx/commit/2abd665158 by @linkerzhang)
NiklasGustafsson(2018-01-23 18:50:25):While I like the removal of ONNX-ML and the addition of abstract types as an extension point, I think that removing the ability to use map and sequence in ML model interfaces seems to me that it will complicate good code generation  for statically typed languages. Map and sequence are as fundamental as tensors once you step outside the world of DNNs, and I'd rather see that we add those to ONNX than that they be removed. Based on the operator sets that a runtime can accept, any runtime would be free to reject models using types that are not supported.

It seems to me that going in this direction will, over time, lead to divergence rather than convergence in runtime semantics, which seems to me that it will lead away from portability rather than toward it.

I suggest that we add abstract type, as an extension point, but also agree on a small set of fundamental types (maps, sequence, images) that runtimes may or may not support, but that have fixed and well-documented semantics for those runtimes that choose to support them.

For what it's worth,

Niklas
ezyang(2018-01-24 03:08:16):Oh boy, I didn't realize you were going to delete the type grammar, this PR is going to bitrot if we don't merge it fast.

In my mind the big thing I am wondering about is the IR version bump. It seems unnecessary for onnx.proto, which has been extended purely in a BC way.
linkerzhang(2018-01-24 05:25:44):@ezyang Thank you for the great comments! 

Yep, as we're introducing abstract type, the type grammar is really not needed now. For bumping the version, yes, I know it's not breaking change for onnx, and it's only breaking change for onnx-ml. So, I'd ask for an exception for helping onnx-ml to handle this breaking change given bumping the version does not introduce much trouble for onnx. :)
ezyang(2018-01-24 05:29:35):The short answer is... bumping the IR version might cause trouble. Actually, I saw an extremely apropos comment not too long ago: https://github.com/pytorch/pytorch/issues/494#issuecomment-360016012
dzhulgakov(2018-01-24 20:45:04):Maybe split it into 2 parts? One removing ONNX-ML and another one deprecating the type inference. How hard would it be to do it? My worry is that we if some issue pops up this PR would be unrevertable. (not requesting, just asking)

As for IR version - how would the bump help you specifically? Do we have a way do differentiate old onnx-ml models? If so - then the IR version bump shouldn't be strictly necessary.
NiklasGustafsson(2018-01-25 00:03:37):A couple of more comments:

1. I agree that we should bump the IR version, since it does affect ONNX-ML.
2. Since the mitigation for map and sequence, which are both types that allow nesting, is to introduce Abstract, I would feel better if Abstract also allowed nesting by adding the following to the message declaration:

repeated TypeProto parameters = 3;

Niklas
houseroad(2018-01-25 18:40:48):@linkerzhang like @NiklasGustafsson mentioned, we need to support nested map/seq.
Without changing your solution, looks like two-level map can be expressed like name="Map<string, Map<string, string>>". Or you need to refer to another Abstract type in the name.
NiklasGustafsson(2018-01-25 18:44:31):@houseroad -- that is indeed semantically the same as adding the ability to parameterize the type, but it also means embedding a secondary syntax inside the protobuf syntax that expresses everything else. My opinion is that expressing everything, including nesting, in terms of protobuf structures is preferable.
linkerzhang(2018-01-26 05:28:45):I'm closing this PR and will bring it back with more thoughts soon. Thank you!
dzhulgakov(2018-01-22 08:34:15):can you run clang-format before landing to keep changes at bay?
dzhulgakov(2018-01-22 08:35:30):it's a bit verbose - maybe have a special wrapper for it?
dzhulgakov(2018-01-22 08:36:36):static vars in header files tend to blow up badly with python extensions (which get loaded with RLDT_LOCAL and thus ODR rule for templates in different shared objects breaks). Maybe do explicit template instantiation in .cc file?
dzhulgakov(2018-01-22 08:36:54):move to .cc
linkerzhang(2018-01-23 08:12:22):sure, and done.
linkerzhang(2018-01-23 08:12:30):sure and done.
linkerzhang(2018-01-23 08:12:42):thank you! fixed.
linkerzhang(2018-01-23 08:12:52):agree, and fixed.
ezyang(2018-01-24 03:02:24):...do you actually want to do it this way? If so, you probably want abstract type constructors, and not just abstract types...
ezyang(2018-01-24 03:06:53):So... this may have been a breaking change for onnx-ml, but it's not a breaking one for onnx, is it?
linkerzhang(2018-01-24 05:23:23):I didn't do it this way previously, and put the declaration codes in header file. All instantiations happen when using it in op declaration as needed. However, there's a good comment that static var should not be put in header file, so, I put all instantiation now in .cc globally.

It looks to me a little bit un-friendly, as it means the abstract type needs to be pre-defined here. However, the good thing in this way is, there will be no mistake in op registration to use (create) a new abstract type without attention (as it will has compilation error if an unknown abstract type introduced).
So, I'm OK with this way now.

gramalingam(2018-01-24 05:49:18):I too think that the notion of abstract types may need to be generalized if we want parametrized abstract types or abstract type constructors like map or sequence. The current solution works if we don't plan to exploit parametricity in abstract types. As Edward mentioned elsewhere, may be this needs to be committed/merged soon to avoid conflicts, and we can think about the generalization separately, but I think it would be good to get to that sooner rather than later. 
houseroad(2018-01-25 06:34:15):Do we also need to add DataType for "Seq"?
houseroad(2018-01-25 06:37:29):Can we support all kinds of key/value combination?
If user want to use map<string,string>, right now, we don't support. (Some macros may make code clean.)
houseroad(2018-01-25 06:38:54):Here, looks like the newline character is missing.
linkerzhang(2018-01-25 17:53:44):@houseroad no, abstract type now is behaving as an named opaque type. We're not moving in the direction of defining all static types.
Adding DataType for "Seq" means defining a static "Seq" type, which is what current onnx-ml is.
linkerzhang(2018-01-25 17:56:11):We can, but let's add them as needed.
houseroad(2018-01-25 18:28:49):I see. We don't have any operator which uses "Seq", yeah, no need to add it here.
houseroad(2018-01-25 18:29:19):Sounds good.
guoyuhong(2018-01-23 21:27:05):@bddppq After this change, I found "backend-test-tools generate-data" will not cause difference for the node test. So I don't need to add the data source now?
bddppq(2018-01-23 22:02:24):@guoyuhong You do need. Maybe your working copy is at the wrong branch/version?
houseroad(2018-01-23 16:57:11):This is quite similar to the node data generation part, we may consider to refactor it to a function.
houseroad(2018-01-23 17:18:29):We have load_model_tests, load_node_tests, collect_tests (in both case.model and case.node). Some comments can help us remember the purpose of the functions. 
bddppq(2018-01-23 20:31:14):@houseroad Yep but need to first make node testcases accept multiple sets of inputs&outputs.
bddppq(2018-01-23 20:38:05):Added
CLAassistant(2018-01-23 01:27:58):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=441) <br/>All committers have signed the CLA.
bddppq(2018-01-23 05:27:11):https://github.com/onnx/onnx/issues/424
houseroad(2018-01-23 08:08:49):@bddppq I just realized that turning an optional attribute to a required one should be considered as a breaking change. Because an old cast without 'to' won't work any more. Although I like the idea of asking for 'to'.
bddppq(2018-01-23 08:26:41):@houseroad IMO it should be treated as doc/schema fix, pretty much like the other `optional` to `required` attributes changes happened recently.
linkerzhang(2018-01-23 05:55:29):btw, should be AttributeProto::INT32? as it's enum. 
bddppq(2018-01-23 06:05:28):Yeah it's indeed better to use INT32. However, caffe2 supports the string version, so that means there could be "valid" model with string `to` attribute. Changing this to INT32 would be a breaking change.
linkerzhang(2018-01-23 18:06:00):@bddppq @houseroad I'd suggest we fix it if we all agree that it's better please, though it's a breaking change.
linkerzhang(2018-01-23 18:20:15):Sorry that I chime in a little bit late. I don't think we should add this named dimension conception. 
1. I personally feel keeping one order is simplifying the standard, and of course, tools may be offered to help transpose.
2. I'd rather to pick up defining attribute for each op to tell the order instead of putting this information in tensor if we want to support different orders.
tjingrant(2018-01-23 18:23:30):Hi @linkerzhang 
I think a significant portion of this patch is for ensuring correctness of execution. I used a bad example as introduction and please allow me to edit:

I plan to have support for using this feature to ensure that tensor operations are performed to the correct dimensions/axes. For instance, a "strict mode" can be introduced and disabled by default that ensures reasonable operation definition (for instance: issue warning on concat of differing standard denotation types; spatial pooling on channel dimension).

This also adds to our tensorflow users' confidence that they are feeding in tensors of the right order.  Whether or not to use this as a semantic changing attribute is up for debating (and apparently everyone but us is against it...) but we do not plan to do so in foreseeable future.
linkerzhang(2018-01-24 07:01:49):Hmmm, I personally believe, once we add this field, backend should/will use and check it to ensure that it's reading tensor with right order. In this way, the idea of introducing an attribute for each op is better and clearer (in this case, backend will use and check the attribute). @dzhulgakov @ezyang @ebarsoum may share thoughts please.
tjingrant(2018-01-24 07:25:19):@linkerzhang ideally I believe that's the case. If you read through the discussion mentioned in the PR msg, I hope you'd agree that per-op attribute is not the best idea. And btw, how would an attribute for each op be better..? Could you elaborate? An attribute for each op seems to imply that this attribute is semantic changing, which is probably not what everyone wants... I chose to place this attribute close to what it's describing (i.e., it's not telling what each op needs to do, it simply tells what a tensor stores).

Another motivation for doing so that I'd like to re-emphasize is that it **localizes** the assumptions made about tensor dimension order. For now, if a sequence of ops is going to be executed in NHWC order, we have something like:

```
[transpose -> op_1 -> transpose^{-1}] -> [transpose -> op_2 -> transpose^{-1}]...
```
Each op assumes that previous ops have handed them the tensor of the right order; this assumption is pretty wild and if one of the op fails to live up to its promise, we have a chain of disaster that is difficult to debug. 

If we have dimension denotation, the only assumption we are making is that denotation is correct, which is easy to check since denotation is contained in the proto object representing a tensor (keeping the attribute close to what it's describing principle); and in each op, we simply need to assert that the tensor we received was in NCHW format.

To summarize, I think we need an attribute somewhere that can be passed/propagated around ops and used to check for a reasonable graph definition. I'm open to alternatives.
dzhulgakov(2018-02-02 07:11:24):Sorry for getting late to it. So your idea is that we'd annotate these "symbolic dimensions" on input and pass them along in the shape inference of the graph (e.g. output channel dim of Conv is `DATA_CHANNEL`).

It sounds like a reasonable idea to thread through the shape inference and verify correctness. It might be also useful for changing the layout of the model if we want to translate the model to nchw.

I personally think it's not a bad idea to add it, but let's add comments in the protobuf describing the motivation and also highlighting that it's optional. cc @bddppq @ezyang for more perspectives

The standard definitions are better to be moved to .proto file so they can be used in backends too (just as string constants). Also, when you get to the implementing the verifier - it'd be nice if you do it in C++ as it might be useful to C++-based backends. @anderspapitto is going to start working on shape inference too, so it's quite related.
tjingrant(2018-02-02 22:54:05):@dzhulgakov thanks for your input, I'm not very familiar with protobuf, but searching around for protobuf constants lands me [here](https://github.com/google/protobuf/issues/3520). Doesn't look like constants are supported in protobuf 3. Can you point me to the correct way of defining constants in the proto file?
tjingrant(2018-03-08 16:22:51):Hi, @dzhulgakov, @bddppq, @anderspapitto, @ezyang  I've been traveling for a while, sorry for the absence. Is there any news about pushing forward with this patch? 
CLAassistant(2018-03-19 18:00:53):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=443) <br/>All committers have signed the CLA.
AppVeyorBot(2018-03-19 18:13:14)::x: [Build onnx 0.3.1646 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1646) (commit https://github.com/onnx/onnx/commit/e7d55ef606 by @)
AppVeyorBot(2018-03-19 18:57:53)::x: [Build onnx 0.3.1648 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1648) (commit https://github.com/onnx/onnx/commit/6fc315aad8 by @tjingrant)
AppVeyorBot(2018-03-19 19:08:41)::white_check_mark: [Build onnx 0.3.1649 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1649) (commit https://github.com/onnx/onnx/commit/ce1c2b228d by @tjingrant)
tjingrant(2018-03-19 21:40:39):@dzhulgakov hi, I've updated the patch as per your review.
tjingrant(2018-03-21 16:40:00):ping @dzhulgakov @bddppq 
AppVeyorBot(2018-03-22 04:45:53)::white_check_mark: [Build onnx 0.3.1753 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1753) (commit https://github.com/onnx/onnx/commit/4fd29b7835 by @tjingrant)
tjingrant(2018-03-23 01:22:04):@linkerzhang @dzhulgakov I've updated the patch. Please share your comments.
bddppq(2018-03-24 04:49:43):Do I understand correctly that in order to use this to help sanity checking, we also need to annotate each operator its expected inputs and outputs denotations? (kinda like adding shape inference functions for each op). 
tjingrant(2018-03-24 04:54:23):@bddppq yes indeed @dzhulgakov mentioned from very early on that this piece can work alongside shape inference. It's worth noting that standard denotations can **propagate** through ops (like transpose) and can also be **expected** by ops (like conv, pool). I'll work on another proposal integrating this into op definitions.
bddppq(2018-03-24 06:03:33):Ok this overall sounds like a good idea to me.
One small concern is using string to store this can be error-prone (e.g. misspelling). Also do we need to allow arbitrary denotations? Since we are going to annotate operator schemas, we can probably collect all the possible denotations onnx needs during this process and so make it an enum?
tjingrant(2018-03-24 06:11:15):@bddppq using string was suggested by @dzhulgakov (cf his comment on Jan 10 [here](https://github.com/onnx/onnx/issues/406)). If I remembered correctly that's for making SD easily extendable ;I'm not very familiar with Protobuf but I suppose a possible scenario of failure of using enum would be failure for old onnx backend to parse models annotated using new SD if we use enum.

Misspelling can be avoided if one sticks only to use pre-defined constants. And we do not allow arbitrary denotations.
bddppq(2018-03-24 06:41:11):@dzhulgakov why is using string more convenient?
AppVeyorBot(2018-03-28 21:42:18)::white_check_mark: [Build onnx 0.3.1930 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1930) (commit https://github.com/onnx/onnx/commit/c98e5d77bc by @tjingrant)
tjingrant(2018-03-28 21:51:19):@dzhulgakov I have put some more thought into the name change. And I think the problem with `semantic denotation` is that it would make it unreasonable for certain ops to **expect** an incoming set of denotations. Since `semantic denotation` implies that there may be multiple ways of describing a dimension of the same semantic meaning. Such implication impairs the verification power of this proposal. The name `standard denotation` also does not discourage customized extension, as long as the one-to-oneness of mapping from the standard denotations to axis semantic meanings are preserved. Thus I think we should stick to the original name.

Not to mention that, standardization is also a major initiative behind ONNX; there is no compelling case to deviate from the main theme here for easiness of ad-hoc instrumentation, I think. 
bddppq(2018-03-28 22:44:08):I would say just call the field `denotation` and the new message type `DimensionDenotationProto` :-)
bddppq(2018-03-28 22:45:35):@linkerzhang Could you take a look?
tjingrant(2018-03-28 23:09:07):@bddppq That works for me from a practical point of view, and we can use the original name for comment and documentation purpose.
tjingrant(2018-03-29 00:47:03):OK updated with programmatical short hand of `denotation`. Comments are kept unchanged.
bddppq(2018-03-29 03:20:23):@ezyang Does pytorch need to update the nanopb files once this is in?
linkerzhang(2018-03-29 05:24:34):Sorry that I missed this PR in last days. Dumb question please.

Does this mean all onnx kernels will be needed to support any annotated shape please? if yes, then it's definitely not a good idea. if not, then the interoperability of onnx model will be impacted much,  I may misunderstand something here, correct me if I'm wrong please. 
tjingrant(2018-03-29 05:49:25):@linkerzhang Let me walk you through some basic facts about standard denotations so that you can be assured that standard denotation is a self-contained and defensive feature that is implemented in a minimally invasive way; and thus it will be unlikely to present any harm to the interoperability of ONNX models.

1. Denotations are for shape verifications (or annotation for the sake of readability and expressiveness of ONNX models) only.
2. Denotations are optional and even non-empty standard denotations can be safely ignored by backends.

So to answer your question, NO, onnx operations **do not need to and shall not react** to standard denotations. Instead, many of them **shall expect** a fixed set of standard denotations according to their current definition (e.g., most convolution related operations shall expect the denotations equivalent of NCHW) and it is the job of the verifier to report a warning when the expectations are not met, thus providing desirable assurance to the user that operations are applied to the correct axis of the tensor.
linkerzhang(2018-03-29 11:33:04):Thank you so much @tjingrant for the detail walk thru and confirmation! 

So, ONNX as a standard will keep as it is to support only NCHW. This means all ONNX models should be in NCHW format, otherwise there'll be no ONNX backend (if a backend is only supporting ONNX standard without any functionality extension) being able to load and run them. This is why I don't understand shape annotation needed in onnx so far. :)

Verifier in the case you mentioned, in my mind, should be done by separate (converter) tools, for example, if there's a TF model, which is using NHWC, and we'd like to convert it into onnx, then the tool will realize some transpose will be done to output an onnx model (in NCHW).  Are you using ONNX protobuf as a temp storage of converting tool so that ONNX protobuf handling non-standard ONNX model is asked here please?




tjingrant(2018-03-29 16:54:20):@linkerzhang your comments raise a lot of interesting questions and allow me to further answer your concerns and I invite everyone to comment:

1. > ONNX as a standard will keep as it is to support only NCHW.

    We need to be careful about what you mean. There are no operations supporting NHWC format in ONNX; however, that does not preclude models from supporting NHWC format input. In fact, you cannot preclude such use case as long as you have a **transpose** in the operator set because you can always transpose the input once and enter the NCHW world thereafter. Thus, unless you explicitly pattern match and prohibit these cases in the checker, there is no way to prevent a user from creating  ONNX **models** that accept NHWC input but uses NCHW **operators**. Thus it is my opinion that 
    > This means all ONNX models should be in NCHW format,

    does not hold true as an implication from your previous statement.

2. Regarding your question of how I intend to use this PR, my answer is that I want to use this information to store the initial axis semantic of the input tensor. Regardless of whether 1 is true or not, ONNX is a generic neural network format and thus I do not see sufficient justification to assume a NCHW input annotations for all neural network models. Someone may be using an RNN or even maybe simply layers of activated matrix multiplication, in which case the semantic may be unknown and verifier should not kick in at all.

3. From a programming language point of view (neural networks are just more advanced programs), not having axis of tensors explicitly typed is a slippery slope to let go obvious and statically detectable programming (neural network construction) bugs. Let me walk you through a concrete example to put you in context. In the program below, we assume a NCHW model input:

    ```
    input -> Transpose(input, perm=[0, 2, 1, 3]) -> AveragePool(input, ...)
    ```

    In this neural network, a user mistakenly constructed a program that transposes an NCHW input to a weird NHCW format and pass through spatial pooling that assumes a NCHW input format. As clearly a mistake as it is, no existing infrastructure will report an error to the user. This is should be deeply unnerving to programmers who rely heavily on type checking as an integral part of program correctness guarantee.



gramalingam(2018-03-29 18:40:22):@tjingrant : the motivation here is good. There is a second part to this verification that seems currently missing: a formal typing/annotation language for the various operators to indicate the corresponding information for the inputs/outputs of those operators. Currently, this is only informally captured in the textual documentation of the operators. I think @linkerzhang 's concern stems (at least partly) from whether this proposal will be seen as generalizing the various operator specifications (to accept any reordered dimensions). 
tjingrant(2018-03-29 19:14:29):@gramalingam Indeed the second part is yet to come and operator schemas will eventually needs additional annotations similar to those used in shape inference. I plan to carry out this process of installing a type-checking mechanism in ONNX one step at a time to prevent time and resources wasted on our side. 
 
> whether this proposal will be seen as generalizing the various operator specifications (to accept any reordered dimensions)

I certainly do not see it this way and neither does anyone in this thread. And I will ensure that users and backend developers do not see it this way either. I will continue to annotate schemas programmatically and make operator documentations clear in the data format they expect (if it is not already so) wherever applicable.  Standard denotations are minimally exposed to the user since users are only allowed to annotate input tensors so it should be easy to enforce standard denotations not being seen and used as generalizing operator definitions.

In addition to that, with standard denotations, you can even **explicitly prohibit** NHWC formatted tensors from being used and provide helpful error message to the user, which is previously impossible and tends to create confusion.
linkerzhang(2018-03-30 15:38:11):Thank you @tjingrant for the detail clarification! as @gramalingam said, my personal concern here is this would impact the op spec somehow. 

So, besides adding this annotation, let's also update our op spec (the op description) to clarify the exact format that an op could accept in this PR before having an annotation language. btw, I personally don't think we need an annotation language so far.

say for conv, the spec now says, the input has size (N x C x H x W), let's change it to the input has size DATA_BATCH * DATA_CHANNEL * DATA_FEATURE * DATA_FEATURE.

Sounds good?
tjingrant(2018-03-30 16:07:27):@linkerzhang DATA_BATCH, DATA_CHANNEL, DATA_FEATURE are not numbers. In fact they are not quantities; they are descriptions. Think of them as strings (as they really are), then we shouldn't try to multiply them together. In addition, I think we should be even more explicit here because multiplication is commutative. Thus:
```
DATA_BATCH * DATA_CHANNEL * DATA_FEATURE * DATA_FEATURE = 
DATA_BATCH * DATA_FEATURE * DATA_FEATURE * DATA_CHANNEL
```

Instead we should probably simply say, i.e. for conv, "Convolution operation expects input tensor X to be in the format of (Batch, Channel, Feature [, Feature[, Feature]])." Let me know what you think and I can move forward with clarifying op descriptions.
linkerzhang(2018-04-02 18:45:29):DATA_BATCH * DATA_CHANNEL * DATA_FEATURE * DATA_FEATURE. is not showing they're multiplying together, like we're showing (N x C x H x W) in description and N C H W are also descriptive characters.

My personal point is to use the same predefined "string" in any operator spec if needed.
 (Batch, Channel, Feature [, Feature[, Feature]]) is not good as folks will need to align DATA_BATCH with Batch. Make sense please?
linkerzhang(2018-04-23 18:43:06):@tjingrant based on the agreements made during work shop, are you going to resolve the comments and merge this PR in a day or two please? @walrusmcd also has similar requirements and would like to expand your PR to cover more scenarios/cases. Thank you very much!
tjingrant(2018-04-24 19:28:42):ping @linkerzhang 
AppVeyorBot(2018-04-24 20:39:12)::white_check_mark: [Build onnx 0.3.2514 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2514) (commit https://github.com/onnx/onnx/commit/0d1696e15d by @tjingrant)
tjingrant(2018-04-24 20:47:13):ping @bddppq  @dzhulgakov , I think it's ready for merging.
AppVeyorBot(2018-04-26 05:03:59)::x: [Build onnx 0.3.2595 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2595) (commit https://github.com/onnx/onnx/commit/f23f7a24ed by @linkerzhang)
dzhulgakov(2018-03-09 10:21:09):please add a comment describing what it is
dzhulgakov(2018-03-21 21:02:23):did you manually add it to this file. These files are usually autogenerated (part of build process that you need to check in or just run `python onnx/gen_proto.py`, you might need to add ONNX_ML=1). Otherwise I don't get where the comment disappeared
dzhulgakov(2018-03-21 21:04:33):this needs to live in a separate message (as it's the fake message just for grouping purposes for constants). Otherwise those fields would be present in every TensorShapeProto instance.
tjingrant(2018-03-22 04:29:36):Simply running onnx/gen_proto.py did not update onnx-ml.proto3. I used the -m cmd line arg.
dzhulgakov(2018-03-27 07:26:42):actually if denotations go through shape inference  - shouldn't this be just FILTER_CHANNEL? otherwise it's weird that IN_ becomes OUT_ when passed to the next op

or is it only for the weights parameter?
dzhulgakov(2018-03-27 07:27:04):what do you think of calling it "semantic_denotation"?

potentially it might be useful also with non-standard names
tjingrant(2018-03-27 15:09:25):Yes, it's only for the weights parameter thus I do not expect this to be propagated. I sort of see the point of confusion. Now we are adding semantic description to the definition of a dimension thus the comment should be "identical in size" since they are not identical in meaning.
tjingrant(2018-03-27 15:12:08):OK, I see this might warrant the ad-hoc situation you mentioned above. Sounds reasonable.
bddppq(2018-03-29 03:13:44):change `shape_denotation` to `denotation` to match the protobuf field?
tjingrant(2018-03-29 03:25:57):Ah, I see shape_denotation as `shape.denotation`. Ideally it's `shape.dimension.denotation` but no one likes var name that long...
 
I think it's worth making the namespace clear since the denotation is a sub-attribute here (not describing a tensor directly).
bddppq(2018-03-29 03:29:38):ok makes sense
gramalingam(2018-03-29 05:46:20):I don't quite understand this ... is this meant to represent an "enum"? This is defined as a message, but is not used anywhere.
tjingrant(2018-03-29 05:53:00):Yes. (cf @dzhulgakov 's comment on Jan 10 [here](https://github.com/onnx/onnx/issues/406) and his comment in this thread regarding using string constants over enum).
gramalingam(2018-03-29 18:46:15):I understand the intention ... but was wondering about the Protobuf mechanism used for this purpose here. I see that default values are not supported in Proto3, which could be a problem.
tjingrant(2018-04-02 21:05:25):I think dzhulgakov said that ONNX is locked to protobuf 2. 
bddppq(2018-04-02 21:58:36):@gramalingam @tjingrant Yes we are locked to proto 2 syntax (note the syntax version is different from the protobuf library version).
codemzs(2018-05-29 05:44:17):@bddppq This is causing a regression in [MLdotNet](https://github.com/dotnet/machinelearning/) C# based machine learning toolkit which was relying on proto3 format for C# code gen from proto files. I'm now getting "onnx-ml.proto3: Explicit default values are not allowed in proto3." Is there a work around that I can use to unblock myself?  If ONNX is locked on proto2 format then can you please clarify why proto3 files are being produced? 

NiklasGustafsson(2018-05-29 14:32:03):The agreement we reached last year was to use the common subset of proto 2 and 3 semantics. Any protobuf feature that cannot be precisely represented in both v2 and v3 should not be used.
CLAassistant(2018-01-23 21:29:10):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=445) <br/>All committers have signed the CLA.
AppVeyorBot(2018-01-23 21:42:27)::x: [Build onnx 0.3.824 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.824) (commit https://github.com/onnx/onnx/commit/e175740032 by @spandantiwari)
bddppq(2018-01-23 22:05:19):@spandantiwari Thanks for contributing

Your branch is outdated, you need to merge with master.
spandantiwari(2018-01-23 22:08:32):OK, I will merge with latest master.
spandantiwari(2018-01-23 23:04:54):Abandoning this PR due to branch merge issues. Opened a new PR for this change. https://github.com/onnx/onnx/pull/448
bddppq(2018-01-23 22:26:36):Good. Please exclude "test_matmul_2d/output_0.pb" and "test_constant_pad/node.pb" and two "test_slice*.pb" files. (Data files can differs a little bit depends on platform, it's ok because when we actually run the tests we only assert the results are similar).
AppVeyorBot(2018-01-23 23:14:02)::x: [Build onnx 0.3.828 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.828) (commit https://github.com/onnx/onnx/commit/0bc2a0c310 by @guoyuhong)
bddppq(2018-01-24 01:27:44):Thanks!
AppVeyorBot(2018-01-23 23:25:14)::white_check_mark: [Build onnx 0.3.829 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.829) (commit https://github.com/onnx/onnx/commit/d92e0b5a11 by @spandantiwari)
bddppq(2018-01-24 08:41:48):@linkerzhang Hmm I was hoping that redundant line can be removed before this PR getting merged
spandantiwari(2018-01-24 18:37:26):@bddppq - Noted. I will remove the redundant line in one of my next changes. 
spandantiwari(2018-02-01 23:54:17):I am removing the redundant line in https://github.com/onnx/onnx/pull/468. 
bddppq(2018-01-24 03:27:01):This code is going to appear in the doc, I think it would be more clear to explicitly write down the `new_shape`.
spandantiwari(2018-01-24 06:18:05):I agree. Updated as suggested.
linkerzhang(2018-01-24 06:39:45):@onnxbot I thought it's a real bot, but looks like it's not? ahahahaha.
bddppq(2018-01-24 06:41:47):@linkerzhang lol it was me sorry. Reposing my comment:

Nice. I think the line `axis_default_value = 1` becomes redundant now.
AppVeyorBot(2018-01-24 01:43:39)::x: [Build onnx 0.3.834 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.834) (commit https://github.com/onnx/onnx/commit/6fb8f7a7d9 by @tianleiwu)
bddppq(2018-01-24 03:29:58):@tianleiwu Thanks. You need to sync with master
bddppq(2018-01-24 19:28:39):nit: change the producer name
bddppq(2018-01-24 19:31:36):Maybe do `check_model` after optimization?
bddppq(2018-01-24 19:32:45):I guess here you also want to assert the node left is Gemm
AppVeyorBot(2018-01-24 22:38:43)::x: [Build onnx 0.3.854 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.854) (commit https://github.com/onnx/onnx/commit/35935e1d09 by @bddppq)
tianleiwu(2018-01-24 22:50:48):Good catch of schema issue of Equal.

For Greater and Less, I think we can add support for integral types as well.
tianleiwu(2018-01-24 22:46:01):It is better to change the output node name from 'less' to 'z' or 'equal'.
ezyang(2018-01-25 04:15:51):I'm confused, doesn't this diff remove support for equality test on floating point?
bddppq(2018-01-25 04:37:10):Yeah I think it probably doesn't make too much sense to provide equality test on floats
houseroad(2018-01-26 21:31:07):One simple suggestion, because the case test also serves as sample cases, it's better to put some explicit tensors (instead of using random generator) for the input. Also we can also write the expected results (the content of the output tensors) explicitly in the comments.
houseroad(2018-01-26 21:32:57):something like:
```
def test_add:
    a = [1, 2, 3]
    b = [3, 4, 5]
    expected = np.add(a, b) # expected output: [4, 6, 8]
```
tianleiwu(2018-01-27 00:16:58):For document, I agree that explicit example could be helpful for understanding.  

Explicit tensors can be achieved by another way: during generating test data, the tool also dumps input and output tensors as example, instead of putting test code as example.

In this way, we can decouple test code and example.


tianleiwu(2018-01-27 02:27:32):Added manually crafted input tensors for test examples.
bddppq(2018-01-25 23:04:48):np.negative
bddppq(2018-01-25 23:04:57):np.negative
bddppq(2018-01-26 00:20:12):Probably `+ 2` to go away from 0?
tianleiwu(2018-01-26 01:49:49):randn is normal distribution so there is still chance x < -2. 
I added a line to avoid element with value 0:
    x[x==0] = 1.
bddppq(2018-01-26 03:30:21):Elements close to zero will have issues too. Use `np.random.rand(...) + 1`?
spandantiwari(2018-02-02 17:40:30):@houseroad - I have updated the change as per your feedback. Could you please take a look?
houseroad(2018-02-02 19:27:05):@spandantiwari All look good to me, expect one thing. Could you explicitly set the input and output type as np.float32 by calling .astype(). Otherwise, you will generate double tensors, and float32 is the most common case.
houseroad(2018-02-02 19:31:08):Just like what we did here: https://github.com/onnx/onnx/blob/master/onnx/backend/test/case/node/abs.py#L22

spandantiwari(2018-02-02 19:57:07):@houseroad - OK, done.
spandantiwari(2018-01-25 07:29:44):This was actually a vestige from my previous PR https://github.com/onnx/onnx/pull/448 . 
Removed as suggested by @bddppq.
houseroad(2018-01-25 22:46:08):I would suggest some cases where we have different width and height dimension. And also test different paddings on width and height dimension.
houseroad(2018-01-25 23:04:33):This should be 4-D tensor, please check the [spec](https://github.com/onnx/onnx/blob/master/docs/Operators.md#inputs-2---3)
spandantiwari(2018-02-02 00:48:58):I agree. I have modified the second test case export_conv_with_strides() to have input with different width and height. Also added a third test point in the same that has different padding for width and height dimensions, as suggested. 
spandantiwari(2018-02-02 00:49:06):Fixed.
bddppq(2018-01-25 19:45:13):Good. I think it makes senses to setup a cmake build system for building the c++ part in onnx, as we lean to have more stuffs in c++ to support more execution environments, and soon we want to provide  an onnx c++ library (without the python part).
On the other hand, having two ways (setup.py and cmake) to build onnx will eventually create divergence and it's hard to maintain. I would suggest let's use cmake as the source of truth to build onnx c++ part and change setup.py to use it.


linkerzhang(2018-01-30 01:14:51):@bddppq @Yangqing ,do you have more comments there please? for referring fixed version of protobuf, let me know if you have strong preferences please. For making setup.py using cmake, I agree on it and will make it (need to learn more about distutils :) ) in separate PR to move this forward so that some folks could get convenience by using the cmake.
bddppq(2018-01-30 01:53:59):Yeah for protobuf we need to use the shared one, otherwise it can easily get version/symbols conflict with other libs that link with the system protobuf. 
Changing setup.py in a separate diff makes senses to me. (You can look at caffe2’s setup.py to see how to delegate the c++ part to cmake).
linkerzhang(2018-01-30 04:30:57):@bddppq Thank you for the clarification! hmmm, are we saying we want to find and depend on the protobuf installed locally on each dev environment please? that means that people may use different version of protobuf. I think that's sth we don't want to lead to. No?

or we may introduce protobuf as a submodule into onnx and by default using "master" branch, and change to specific branch, say, https://github.com/google/protobuf/tree/3.4.x, when releasing onnx with a specific version. Make sense?

yinghai(2018-01-30 20:59:37):Any blockers? I really would like see this to be pulled so that we can make integration of ONNX easier. 
bddppq(2018-01-30 21:45:02):@linkerzhang We need to use the protobuf in user's environment. When users use onnx, it's very likely they will use it with frameworks/libs that depends on protobuf. Importing two libs linked with different copies of protobuf can easily get you symbols conflicts and version mismatch.
linkerzhang(2018-01-30 22:18:03):@bddppq OK, I'm now using local protobuf to push this in to unblock folks. For onnx-ml handling, I'll make it in a separate PR, btw.
onnxbot(2018-01-31 06:22:49):Build triggered. sha1 is merged.
onnxbot(2018-01-31 06:22:54):Build started sha1 is merged.

onnxbot(2018-01-31 06:25:05):Build finished. 

onnxbot(2018-01-31 17:38:24):Build triggered. sha1 is merged.
onnxbot(2018-01-31 17:38:28):Build started sha1 is merged.

onnxbot(2018-01-31 17:54:16):Build finished. 

onnxbot(2018-01-31 19:37:53):Build triggered. sha1 is merged.
onnxbot(2018-01-31 19:38:00):Build started sha1 is merged.

onnxbot(2018-01-31 19:57:32):Build finished. 

linkerzhang(2018-02-01 01:32:16):@bddppq what's onnx-fb-universe used for please? :)

ezyang(2018-02-01 04:03:11):It's our integration testing for onnx/onnx-caffe2/onnx-pytorch :)
linkerzhang(2018-02-01 06:01:38):Aha, is there a magic command to ask the bot (onnx-fb-universe) to re-trigger the build please?
houseroad(2018-02-01 06:03:19):@onnxbot retest this please?
onnxbot(2018-02-01 06:03:21):Build triggered. sha1 is merged.
onnxbot(2018-02-01 06:03:31):Build started sha1 is merged.

onnxbot(2018-02-01 06:05:44):Build finished. 

bddppq(2018-02-01 07:42:04):@houseroad the magic command needs to be done in the onnx-fb-universe repo :-)

@dzhulgakov @lupesko @prasanthpul Could you guys adjust the configuration to mark onnx-fb-universe CI as non-blocking for being able to merge a PR, or entirely remove the "Require status checks to pass before merging" mark. This is the second time CI outage being a blocker for merging PRs since we have enabled this check last week. I don't think any existing CI is robust enough (well, probably except the "license/cla" one).
linkerzhang(2018-02-06 20:58:59):Thank you all for the comments! Most of the comments are resolved except two,
1. change setup.py, and 2) change the output_size() return type. Which will be covered/revisited in separate PRs. I'm now checking into it.
Yangqing(2018-01-25 19:53:49):Might be a bit strict for build platforms like ubuntu 14.04.
Yangqing(2018-01-25 19:54:26):Do we... really need this high?

IIRC Android support for c++14 is still semi-complete so I think a safe choice is 11, unless we are already stuck with 14.
Yangqing(2018-01-25 19:56:12):This pretty much hard-codes protobuf to be the in-house one; do we want to do something like FindProtobuf.cmake so that when someone is building protobuf, they can use the system installed one?
Yangqing(2018-01-25 19:56:51):Do we need zlib? If I understand it correctly, this is only used as a downstream dependency for protobuf right?
Yangqing(2018-01-25 19:57:26):Um... I would refrain from patching a well known library; we should probably upstream this to google/protobuf.
houseroad(2018-01-25 19:58:43):Looks like no newline at the end.
Yangqing(2018-01-25 19:59:44):Actually, I would recommend adding /wd4800 to the compiler flag, instead of explicitly patching. Also, you will encounter way more than C4800 alone, see

https://github.com/google/protobuf/tree/master/cmake

(see "Notes on Compiler Warnings")
houseroad(2018-01-25 20:04:47):agree, 14 is probably too aggressive. Do we use any C++14 feature?
linkerzhang(2018-01-27 00:17:34):thank you! 11 is good.
linkerzhang(2018-01-27 00:18:53):I personally think it's good to use a hard code version and pull it from source. Let me know if you have strong preferences here.
linkerzhang(2018-01-27 00:19:06):thank you! Yangqing. We don't need it. Removed.
linkerzhang(2018-01-27 00:19:28):OK. changed to disable the warning in the project level.
linkerzhang(2018-01-30 22:18:52):onnx-ml will be handled in a separate PR, depending on my abstract type PR.
Yangqing(2018-01-31 01:28:38):Not sure if we need to include it - the following line should basically find protobuf already.
Yangqing(2018-01-31 01:30:34):Since pybind11 is a header only library, we probably don't need a binary dir for add_subdirectory I assume?
bddppq(2018-01-31 01:31:28):Why changing this?
Yangqing(2018-01-31 01:32:02):Just checking - since there are no find python command, PYTHON_INCLUDE_DIRS is automatically provided by pybind11, I assume?
Yangqing(2018-01-31 01:33:28):I am not sure if this is the convention, but usually files under cmake/ are utility files that cmake uses for custom builds. For the content of this file, it seems to be a better fit as a onnx/CMakeLists.txt file. Does that make sense?
Yangqing(2018-01-31 01:33:54):Do you mind adding a piece of comment about the need for the list removal?
Yangqing(2018-01-31 01:35:45):A friendly reminder that you might need more than these warnings, thanks to protobuf which is a huge pot of warning-generator. More specifically, this list might help:

https://github.com/caffe2/caffe2/blob/3eb636c0edc9337fb348ec554e366b1020604c68/cmake/MiscCheck.cmake#L110-L126
Yangqing(2018-01-31 01:37:45):Quick check if this is intentional. Seems that we might want to indeed to onnx/onnx.pb.h?

(If you made this change due to protobuf generated files, please ping back - I can help with that)
linkerzhang(2018-01-31 02:32:42):Aha, you're right, I made this change due to files generated by protoc were put directly in build folder. Didn't figure out a way here to tell PROTOBUF_GENERATE_CPP to put generated files in a specific folder :(
linkerzhang(2018-01-31 02:33:25):currently only the warnings that specified here need to be taken care of, as I can get a successful build now. We may add more when needed?
linkerzhang(2018-01-31 02:34:33):this is temporary thing, since setup.py is not changed to use cmake for now. The proto header/code files will also be generated by setup.py (pip install -e ./), so, I'm adding the temp logic here to remove them if they're generated. It should be removed later. Comments added :)
linkerzhang(2018-01-31 02:39:46):Aha, it's because that the header generated by protoc is put in /build folder directly. Yanqing asked the same question. 
linkerzhang(2018-01-31 02:41:51):you're right. it's not needed to add a binary dir.

Because I'm now putting cmakelists.txt in cmake folder instead of root folder, it creates out of source tree case. In this case, cmake/add_subdirectory asks to put a binary dir. I personally don't want to move the cmakelists.txt into root folder btw.


linkerzhang(2018-01-31 02:45:19):Hmmm, I think it in this way, 1) put all cmake related files into /cmake folder, instead of having them in different folders. 2) we may have more than one *.cmake later for different project separations, Sounds good?

linkerzhang(2018-01-31 02:48:47):you're right, it's not needed, removed. Thank you!
linkerzhang(2018-01-31 02:59:52):yes.
linkerzhang(2018-01-31 04:05:33):OK, now I merged things in one CmakeLists.txt and put it in root folder, which resolved several of the comments here.
linkerzhang(2018-01-31 04:05:59):Resolved this now by moving the CmakeLists.txt file in root folder now.
Yangqing(2018-01-31 04:33:35):Yeah, so protobuf's default command does not allow that, which is a big bummer. You can use a custom command with something like this:

https://github.com/caffe2/caffe2/blob/4f534fad1af9f77d4f0496ecd37dafb382330223/cmake/ProtoBuf.cmake#L137-L184

Note that if you want to use onnx as Windows dlls, you are going to need the dllexport_decl definition because otherwise protobuf does not work well with dlls.
Yangqing(2018-01-31 04:35:29):In theory, onnxir should also depend on the protobuf libraries (if we build it as libonnxir.so). This might only work for cases where BUILD_SHARED_LIBS=OFF.
ezyang(2018-01-31 05:03:03):Good, your cmake minimum required is 3.1 :)
linkerzhang(2018-01-31 05:33:03):Thank you Yangqing! Yep, that's my origin change to use a customized generation function. OK, changed to the way. ONNX so far is not used as a windows dlls for now, I think. :).
linkerzhang(2018-01-31 06:25:05):that should be added as a link dependency in application-layer cmake file, right?
ezyang(2018-01-31 15:44:23):We cribbed this function definition from somewhere, right? Let's record its provenance please.
ezyang(2018-01-31 15:45:13):You don't actually need a Glob here, right?
ezyang(2018-01-31 15:46:16):It's better if you declare it in the cmake. Then if someone else recursively includes this cmake, and they depend on onnxir, they will automatically see the protobuf dependency. If you omit it here they won't.
linkerzhang(2018-01-31 16:54:23):Aha, that's sth from ms code base, let me ask people whether they know where they copied from.
linkerzhang(2018-01-31 17:24:45):I think I don't need recursive here. Anyway, I'm changing to use "Remove" directly. This piece of codes will be removed anyway later.
linkerzhang(2018-01-31 17:31:03):looks like I can't figure out the code owner for now. I searched around and https://github.com/tensorflow/tensorflow/blob/d2c3b873c6f8ff999a2e4ee707a84ff00d9c15a5/tensorflow/contrib/cmake/tf_core_framework.cmake has the most similar one, so I'm going to put this link as a comment in place, sounds good?
bddppq(2018-01-31 17:35:25):After changing setup.py you will still need this. We will only use setup.py to manage the python packaging part, while cmake is going to be the one that does all the heavy lifting (including invoking protoc).
Yangqing(2018-02-02 20:21:22):Note that if you want to build dlls, you will want to handle that with dllexport symbols. This is how we do it in c2:

https://github.com/caffe2/caffe2/blob/b7d983f255ef5496474f1ea188edb5e0ac442761/cmake/ProtoBuf.cmake#L165-L184

Not sure if I pointed that out before, so I'll just put it up here again just in case.
Yangqing(2018-02-02 20:21:57):Naive question: do you want to remove headers?
linkerzhang(2018-02-02 23:17:15):yes, this copy should be removed. This copy is generated by current "setup.py" call. if not removed, there will be two header/cc files in cmake projects. :)
linkerzhang(2018-02-03 00:02:58):thank you! Will add it.
larroy(2018-02-06 16:31:09):Why not leave size_t as the type?
linkerzhang(2018-02-06 20:57:27):Good point. You mean the return type of node.output_size(), right? will revisit the API later. This is fixing the warnings as I'm turning on taking warnings as errors. :)
larroy(2018-02-08 22:44:09):yes, unintended narrowing casts can be a source of problems. I would leave them as size_t if possible.
CLAassistant(2018-01-25 19:59:03):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=470) <br/>All committers have signed the CLA.
AppVeyorBot(2018-01-25 23:08:17)::x: [Build onnx 0.3.875 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.875) (commit https://github.com/onnx/onnx/commit/9ac3fc9727 by @tianleiwu)
AppVeyorBot(2018-01-25 23:48:18)::x: [Build onnx 0.3.877 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.877) (commit https://github.com/onnx/onnx/commit/f0a064f82c by @tianleiwu)
tianleiwu(2018-01-26 01:21:36):Pow does not support broadcast. I propose to add broadcast in another pull request later.
AppVeyorBot(2018-01-26 20:20:34)::x: [Build onnx 0.3.904 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.904) (commit https://github.com/onnx/onnx/commit/da4aac8bec by @tianleiwu)
houseroad(2018-01-26 22:18:42):BTW, thanks a lot for the contribution.
tianleiwu(2018-01-27 02:26:39):Added manually crafted input tensors for test examples.
houseroad(2018-01-29 21:28:08):@onnxbot add to whitelist
AppVeyorBot(2018-01-25 23:40:35)::white_check_mark: [Build onnx 0.3.876 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.876) (commit https://github.com/onnx/onnx/commit/1bf260d022 by @anderspapitto)
AppVeyorBot(2018-01-26 00:13:09)::white_check_mark: [Build onnx 0.3.878 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.878) (commit https://github.com/onnx/onnx/commit/071d6a1b10 by @anderspapitto)
houseroad(2018-01-26 19:16:53):@onnxbot retest this please
AppVeyorBot(2018-01-26 20:13:46)::x: [Build onnx 0.3.903 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.903) (commit https://github.com/onnx/onnx/commit/fe5ad951fc by @houseroad)
houseroad(2018-01-26 20:23:17):@onnxbot retest this.
AppVeyorBot(2018-01-26 21:36:41)::white_check_mark: [Build onnx 0.3.905 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.905) (commit https://github.com/onnx/onnx/commit/7b40edb954 by @houseroad)
bddppq(2018-01-26 00:43:39):Thanks
CLAassistant(2018-01-26 11:38:04):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=482) <br/>All committers have signed the CLA.
bddppq(2018-01-26 17:31:06):Thanks for reporting. We have fixed this bug in master and will publish a patch release to pip today.
https://github.com/onnx/onnx/issues/444
AppVeyorBot(2018-01-26 23:27:26)::x: [Build onnx 0.3.907 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.907) (commit https://github.com/onnx/onnx/commit/e1f88daa32 by @jamesr66a)
ezyang(2018-01-30 00:51:46):One downside of putting the nested graphs out-of-line is that the lexical scoping relationship is no longer textually clear; furthermore, the use of `name` implies that the same graph may be referenceable multiple times from a single function, when in fact, the structural implementation of the proto does not demonstrate this.
onnxbot(2018-01-31 18:01:07):Build triggered. sha1 is merged.
onnxbot(2018-01-31 18:01:12):Build started sha1 is merged.

onnxbot(2018-01-31 18:17:31):Build finished. 

jamesr66a(2018-01-31 18:53:20):Going ahead and merging this, we can revisit the printout format later if people feel strongly about it
bddppq(2018-01-29 20:14:04):Is this just for backward compatibility or is there real cases that we don't want to print the subgraphs?
houseroad(2018-01-29 20:24:22):Do we even have any case which contain subgraph as attribute right now?
jamesr66a(2018-01-29 21:36:03):I basically just wanted to leave it up to the caller to handle the subgraphs. Notice that if the flag is true there's an extra return value and the caller can do whatever they want with it
bddppq(2018-01-29 22:37:45):I would say let's always print the subgraphs, and do not have different number of return values. Returning graph objects in a print function doesn't sound appropriate to me.
jamesr66a(2018-01-29 23:09:56):I think that makes sense in isolation, but in context this is used in a recursive descent algorithm. Printing the graph within the op doesn't look good, so what's happening now is we propagate all graphs up to the top-level (print_graph) and print them one after the other. This makes it look like a normal SSA organized into basic blocks
prasanthpul(2018-01-27 00:49:08):Will be useful to link PRs to issue(s) that originated it for better understanding.
dzhulgakov(2018-01-30 07:53:06):@prasanthpul  - we're looking into embedding ONNX lib into C2 directly (e.g. to reuse schemas and docs directly). In that case it's preferable to just link it statically to libcaffe2.so so prevent versioning problems - otherwise the releases have to always match. In that case one needs to rename the symbols to avoid hard-to-debug conflicts if the libonnx.so is loaded too.
yinghai(2018-02-05 23:11:45):> In that case one needs to rename the symbols to avoid hard-to-debug conflicts if the libonnx.so is loaded too.

I'm hitting exactly this problem right now. :) 
yinghai(2018-02-05 23:32:45):There are a couple of missed renaming in `defs/controlflow/defs.cc`. 
bddppq(2018-02-05 23:40:22):@yinghai good catch. maybe it's time to use a random namespace in the CI :-)
lutzroeder(2018-02-11 09:30:37):How can this be configured during build?

```
export ONNX_ML=1
export ONNX_NAMESPACE=onnx
python ./setup.py build --build-lib ./build/lib
```

The output files `./build/lib/onnx/onnx.proto` still contain `namespace ONNX_NAMESPACE`?


dzhulgakov(2018-01-27 01:56:30):oh wow - it works? is it because it gets substituted at the compilation stage? It's quite cool :)
bddppq(2018-01-27 03:42:24):Yep, protoc just treats it as normal name, the generated `pb.{h,cc}` files will have `namespace ONNX_NAMESPACE`. 
bddppq(2018-01-30 23:51:42):@linkerzhang Do you know how can I express this in windows? :-) I want to forbid directly hardcoding "namespace onnx" in all c++ source code.
houseroad(2018-02-01 20:25:53):Try findstr command?
dzhulgakov(2018-02-06 01:46:14):lol, it could have been just a big string ONNX_NAMESPACE_FOO_BAR_FOR_CI :)
AppVeyorBot(2018-01-29 19:00:20)::white_check_mark: [Build onnx 0.3.927 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.927) (commit https://github.com/onnx/onnx/commit/1c7566a661 by @jamesr66a)
bddppq(2018-01-27 04:37:23):`std::move`?
bddppq(2018-01-29 19:41:48):nit: add a comment saying no shadowing
bddppq(2018-01-29 19:44:14):nit: maybe just "names"?
bddppq(2018-01-29 19:45:57):nit: you can make it const or just create an anonymous one in the function call next line.
jamesr66a(2018-01-29 21:56:44):I named this output_names to be consistent with the code in the checker. maybe we should change both?
guoyuhong(2018-01-28 07:18:32):@bddppq @linkerzhang 
houseroad(2018-01-29 21:52:00):@onnxbot add to whitelist
CLAassistant(2018-01-30 00:58:35):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=490) <br/>All committers have signed the CLA.
houseroad(2018-01-30 01:00:29):@onnxbot add to whitelist
AppVeyorBot(2018-01-30 01:09:01)::x: [Build onnx 0.3.942 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.942) (commit https://github.com/onnx/onnx/commit/59619f0da3 by @bwasti)
AppVeyorBot(2018-01-30 01:19:13)::white_check_mark: [Build onnx 0.3.943 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.943) (commit https://github.com/onnx/onnx/commit/38bf535da0 by @bwasti)
linkerzhang(2018-01-30 01:24:01):Sorry that I probably missed sth, by looking thru the PR, I don't get the point of introducing the new type wrapper.
houseroad(2018-01-30 01:31:25):I think the purpose of this PR is to decouple the TensorProto.DataType from the IR, so IR will be more flexible. Users can register their own element type.
linkerzhang(2018-01-30 22:24:09):@houseroad Thank you very much for following up! I don't quite understand. How can they be decoupled please? TensorProto.DataType contains all types supported. There's no way for us to extend more types without touching this. Correct me if I'm wrong please.
bwasti(2018-02-01 04:24:48):holding off on attempting to merge this for now
bddppq(2018-01-31 09:08:08):For the record (IMO it doesn't add too much value to the discussions in this PR), I just checked protobuf, messages are movable starting from protobuf >= 3.4.0
houseroad(2018-01-31 23:34:52):If we change to non-const reference, no return value is needed. Right now, we replies on RVO to avoid one copy, which I think it's fine.
linkerzhang(2018-02-01 01:40:34):So we agree on using non-const reference so that further optimization (in-place change) could be done later without changing API again, right?
ezyang(2018-02-01 04:14:54):I am confused that, somehow, after all this discussion, we've concluded that non-const references should be used. You should basically never be using non-const references in C++ (since I don't have time to argue this from first principles, I refer you to the C++ style giude).
linkerzhang(2018-02-01 05:41:58):@ezyang I was asking whether that's the conclusion, as folks are telling in-place tuning could be done by removing const. Following c++ style guide, it's ok to not use non-const reference. then we'd keep the current API to use unique_ptr. right? otherwise, how can in-place tuning be done later? I missed some points?
houseroad(2018-02-01 05:51:28):@ezyang @linkerzhang 
I think the purpose of not using non-const reference is to distinguish the input and output parameters.

Actually, in cpp core guidelines, it suggests to use rvalue reference.
https://github.com/isocpp/CppCoreGuidelines/blob/master/CppCoreGuidelines.md#f18-for-consume-parameters-pass-by-x-and-stdmove-the-parameter

I don't have a strong preference.
linkerzhang(2018-02-01 05:55:27):There's kind of a principal that not using non-const reference but pointer if it's mutable. Though I'm not a fully follower of these principals, it's not a bad practice to follow it. My personal understanding here is we do want to have the capability to allow us to do in-place modification later to tune the memory usage, so const reference should not be good at least. Correct me if I'm wrong please.
bddppq(2018-02-01 07:30:16):About this PR:
The initial motivation is that currently optimizer is not modifying the input model at all so const reference is more appropriate than unique_ptr. 
Changing the optimizer to do in-place update involves two parts:
1. When converting the input ModelProto to a Graph, let the Graph create some internal pointers pointing to the underlying data (e.g. initializers) in the ModelProto. I think this largely deviates from the current design of the Graph class, and will need to rewrite a pretty large portion of the current IR classes.
2. Write the output ModelProto directly to the input ModelProto, instead of creating a new ModelProto as return value like right now. This part can be done without 1, the change should be small. And it will keep the possibility to do 1 in the future without changing the api interface.

@ezyang About the criticism to non-const reference:
I know there is this rule in Google's C++ style guide and never see it anywhere else. I don't agree with it and in fact the [C++ Core Guidelines](https://github.com/isocpp/CppCoreGuidelines/blob/master/CppCoreGuidelines.md) explicitly lists non-const reference as valid usage for in-out parameters.
Also I'm a little bit surprised by you saying this because I see there are many non-const references function parameters in PyTorch :-)

houseroad(2018-02-01 19:04:19):How about this: let's follow C++ Core Guidelines, use rvalue reference for the consumed input parameter. So the caller (user) should be happy, we are also able to do the in-place change, and we are following guidelines. @bddppq @linkerzhang @ezyang
linkerzhang(2018-02-01 19:19:39):@houseroad I'm OK as long as the API does not need to be changed again when doing in-place change/optimization later.
AppVeyorBot(2018-02-08 20:13:11)::x: [Build onnx 0.3.1068 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1068) (commit https://github.com/onnx/onnx/commit/48df3751b8 by @bddppq)
linkerzhang(2018-01-30 22:39:44):I'm hesitating.... The current way of using unique_ptr will make peak memory usage less, although it enforces callers to follow the programming API somehow.
yinghai(2018-01-30 22:54:52):Just curious, where does `unique_ptr` help reduce the peak memory usage? 
bddppq(2018-01-30 23:04:56):@linkerzhang It shouldn't. If the caller side doesn't need the original model anymore, it can move when passing the model proto into the optimizer.
yinghai(2018-01-31 00:12:26):I don't think we need this. `Optimize` is taking `const ModelProto&` which won't incur a copy. 
bddppq(2018-01-31 00:26:19):@yinghai It's not for avoiding copy when passing to optimizer, it's for freeing the `proto` once the `Optimize` call is done (previously it's won't be freed until the end of this pybind11 interface function.
yinghai(2018-01-31 00:58:58):I see. This will only work with newer version of protobuf, but it shoud be good. 
linkerzhang(2018-01-31 04:10:14):Yep. "it can move when passing the model proto into the optimizer." this is the usage of unique_ptr, I think :). Double check, does ModelProto have move constructor please? I thought it didn't have. 
bddppq(2018-01-31 04:25:48):No you missed my point. I mean the optimizer doesn't care about the ownership of the model proto that is passed in. It's should be the caller of the optimizer to decide whether the original model is still needed. If you use unique_ptr here, you are forcing the caller to either give up the ownership or make another useless copy.
linkerzhang(2018-01-31 05:12:38): Hmmm, so the caller code may be,
 ModelProto modelProto = ... ; // load model from a file.
 Optimizer optimizer;
 optimizer.optimize(modelProto, {"pass1", "pass2"});

The caller of optimizer has no good way to release the memory before optimizer to lower the peak memory usage, am I right? however, if there's a move constructor of ModelProto (needs double check), then it's still doable for caller of optimizer to write code like, optimizer.optimize(std::move(modelProto), {"pass1"});

This is not a strong point for now as we may only have offline tools calling optimizer codes (in that case, peak memory is probably not a concern)

bddppq(2018-01-31 06:10:06):The peak memory usage actually matters since I have seen OOM in onnx-caffe2's CI :-)
And I don't think this change will increase any memory footprint. Using your example:
```
ModelProto modelProto = ...;
```
If we use unique_ptr, then the user would need to make a second copy and wrap that with a unique_ptr to pass into the optimizer, it has two copies during the whole time optimizer is running. While with my change, no matter whether ModelProto has move ctor or not, since we are passing model proto as const ref, it only has one copy, so it actually uses less memory.

houseroad(2018-01-31 06:27:54):In the original implementation, we only pass one unique_ptr, so the caller (`cpp2py`) and the callee (`Optimize`) share the same instance. And in `Optimize` (after the std::move(proto)), the callee own the `proto`. When `Optimize` finish, the proto will be released.

In the proposed one, we pretty much do the same thing. The only difference is we transfer the ownership to the parameter of `Optimize`. So the first parameter of `Optimize` owns the proto. So the life scope of the proto ends at the end of `Optimize` function.

So I don't see big win here.
houseroad(2018-01-31 06:33:38):Actually, if we stick to unique_ptr, we can release the mp_in right after this conversion.

Just call mp_in.reset()
bddppq(2018-01-31 06:51:33):"In the proposed one, we pretty much do the same thing. The only difference is we transfer the ownership to the parameter of Optimize."
No, after this change Optimize doesn't have ownership on the ModelProto.

Note in the case you have a choice to either create an object or unique_ptr (like here in the pybind11 wrapper), memory usage won't make too much difference (although I can still argue it's more preferable to allocate on stack rather than doing unnecessary dynamic allocation). But as I have explained in the discussion in "optimize.h", in case the caller gets ModelProto from somewhere else, using the unique_ptr in the optimizer interface would force the caller to make another copy (and causes more memory usage).

But more importantly, the previous interface really confuses users: why does the optimizer need to own a copy of the ModelProto (does it imply it will actually do in-place modification?)
bddppq(2018-01-31 06:55:44):While with the new interface that uses const reference, we don't even have a copy from the beginning.
linkerzhang(2018-01-31 06:58:09):if using unique_ptr, then the code will look like,

std::unique_ptr<ModelProto> modelProto(new ModelProto);
....

peak memory usage should be lower down as there's always one ModelProto "data". 
bddppq(2018-01-31 07:05:48):@linkerzhang Sorry I really don't get how after this change it can become worse. Yes if the caller already has a unique_ptr, both interfaces will only have one copy (note the new interface is taking const reference, so it doesn't need the caller to make another copy). But if what the user has is an object, then using unique_ptr will cause a second copy while the new interface still only need one copy.
linkerzhang(2018-01-31 07:05:51):agree on this point "But more importantly, the previous interface really confuses users: why does the optimizer need to own a copy of the ModelProto (does it imply it will actually do in-place modification?)"

I'm going to propose that we should do in-place modification to avoid copy (especially now we're copying all weights into graph, which should be avoided, I think).
bddppq(2018-01-31 09:05:38):@linkerzhang I don't quite understand which copy are you trying to avoid:

There are these phases:
1. caller pass ModelProto to optimizer
2. optimizer converts ModelProto to Graph
3. optimizer passes Graph to each pass
4. optimizer converts Graph back to ModelProto
5. optimizer returns the new ModelProto

1 won't be doing copy anymore after this change, 3 is doing in-place modification already so no copy, 2 and 4 are by design doing copy so passes writer can more easily manipulate the underlying data (and to be fair not very related to this PR). 5 doesn't have copy thanks to RVO (and thanks to @yinghai pointed this out then leads to this PR).
houseroad(2018-01-31 15:36:39):I still cannot see how the original implementation will create more copies of ModelProto than the new one at step 1. Could you explain more?
houseroad(2018-01-31 15:37:58):At least, we have one copy here, right? (Although this ModelProto is a reference, the instance itself will live until the end of this function.)
yinghai(2018-01-31 17:03:26):Assuming we have protobuf <3.4.0 and we don't have move ctor for Message, it really depends on what caller of optimizer is using. If it uses 
auto optimized_model = std::make_unique<ModelProto>(optimize(old_model, {}));
Current PR will incur an copy. 
On the other hand, if the caller uses plain ModelProto and we have the old interface, it will need to copy the ModelProto in the unique_ptr, too. 

Either way it's not perfect. The interface in this PR is more natural IMO. If you limit your interface to `unique_ptr`, calls to this function will contain quite a bit of `std::move`. 
yinghai(2018-01-31 17:34:40):You have a point here. @bddppq, the point is `mp_in` is useless right after 
```
 std::shared_ptr<onnx::Graph> g(onnx::ImportModelProto(*mp_in));
```
and can thus be released to reduce memory. Therefore, it should not be `const`. 
linkerzhang(2018-01-31 17:46:38):OK, my bad that I didn't clarify clearly. You're right. With current implementation, there's no difference.

I'm thinking of whether we should really do in-place change. Like you mentioned, having a unique_ptr<ModelProto> sent from caller, means the caller gives the ownership to us, we could own it and change it in-place accordingly. Our current implementation is copying all data into graph object, right? (say, weights). This part could be avoided. By using const ModelProto&, our implementation inside will have to make a copy since it's not clear that we could own the ModelProto object sent in.
bddppq(2018-01-31 18:59:56):If we can change the optimizer to do in-place, then we should just change the const ref to ref in this PR, instead of passing in a copy through unique_ptr, delete it and then recreate another mp_out to return.
linkerzhang(2018-01-31 19:35:37):I meant the copy in 2 could be avoided with unique_ptr interface, but can't be avoided with const reference interface. @bddppq 
houseroad(2018-01-31 23:29:53):For the new implementation, we replies on RVO. Otherwise, we will have additional copy operation.
houseroad(2018-02-01 19:01:29):since you already turn this into a rvalue reference, why don't you change the Optimize() API to take a rvalue reference.
dzhulgakov(2018-02-01 19:36:57):btw, Google's style guide recommends to use pointers isntead of non-const reference for output params. Do we want to follow this style?
jamesr66a(2018-02-03 02:47:37):I *love* this style. It means that at callsites when you see `foo(param1, param2, &param3)`, that `param3` may possibly be mutated, and have the guarantee that `param1` and `param2` will not be (so long as they're not pointer types). Mutable references provide no such guarantee. 10/10 would recommend to a friend
AppVeyorBot(2018-01-31 00:25:40)::x: [Build onnx 0.3.960 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.960) (commit https://github.com/onnx/onnx/commit/19360ecef9 by @jamesr66a)
AppVeyorBot(2018-01-31 00:42:49)::white_check_mark: [Build onnx 0.3.961 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.961) (commit https://github.com/onnx/onnx/commit/14751cca80 by @jamesr66a)
spandantiwari(2018-02-02 18:51:34):Would it make sense to introduce an ``axis`` attribute to find the opt k elements along a user-specified dimension? Default value can be the last axis (-1). For instance, Numpy's ``argsort``, which is commonly used for getting top k elements, allows this as an input argument. 
jamesr66a(2018-02-02 22:48:22):@spandantiwari Yep, good suggestion. Updated
jamesr66a(2018-02-02 23:17:01):@onnxbot retest this please
bddppq(2018-01-31 22:23:24):The top_k_ref function code won't be extracted to the doc together (besides it's pretty long so might not be a good idea to put it into the doc either). Either put a hand-written simple inputs&outputs example here, or use np.sort with slice (you can select a stable sorting algorithm in np.sort).
bddppq(2018-01-31 22:38:53):I think this means the Input[2] is allowed to be consumed by Output[0], and that shouldn't be what you want :-)
spandantiwari(2018-02-04 22:17:09):My recommendation would be to name this attribute ``axis``, in line with other ONNX ops such as ``concat``, ``div``, ``flatten``, ``gather`` and others.
bddppq(2018-01-31 03:08:37):Please run `backend-test-tools generate-data` to generate the new test data and commit them as well.
CLAassistant(2018-01-31 17:46:42):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=498) <br/>All committers have signed the CLA.
EmergentOrder(2018-02-01 17:55:42):Accidentally introduced a merge commit, created a new PR instead, closing this.
bddppq(2018-02-01 18:08:17):@EmergentOrder Thanks for trying to create a Java binding!

Attributes are not supposed to default constructed. Regarding to your error, I left some comments in your issue in the JavaCPP repo.
houseroad(2018-02-01 21:04:34):@bddppq @linkerzhang Shall we explicitly forbid the default constructor?
EmergentOrder(2018-02-01 21:35:27):@bddppq thanks
tianleiwu(2018-02-02 21:38:02):@houseroad, the first case of each operator has explicit input and output. If needed, I can change the last case to be random generated.
CLAassistant(2018-02-05 07:38:41):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=506) <br/>All committers have signed the CLA.
AppVeyorBot(2018-02-05 07:45:52)::x: [Build onnx 0.3.994 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.994) (commit https://github.com/onnx/onnx/commit/f34c4178d5 by @pk-g)
AppVeyorBot(2018-02-05 09:24:32)::x: [Build onnx 0.3.995 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.995) (commit https://github.com/onnx/onnx/commit/bdc95c7a4f by @pk-g)
AppVeyorBot(2018-02-05 09:41:50)::white_check_mark: [Build onnx 0.3.996 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.996) (commit https://github.com/onnx/onnx/commit/664cc4c47d by @pk-g)
AppVeyorBot(2018-02-05 20:08:21)::x: [Build onnx 0.3.1001 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1001) (commit https://github.com/onnx/onnx/commit/7a08f6c3b1 by @pk-g)
bddppq(2018-02-16 17:54:09):@pk-g Could you address the minor review comments so we can add the test cases to master?
AppVeyorBot(2018-02-16 23:02:56)::x: [Build onnx 0.3.1149 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1149) (commit https://github.com/onnx/onnx/commit/1d160967d0 by @pk-g)
AppVeyorBot(2018-02-16 23:14:15)::white_check_mark: [Build onnx 0.3.1150 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1150) (commit https://github.com/onnx/onnx/commit/84ba34b487 by @pk-g)
pk-g(2018-02-16 23:36:37):@bddppq I have updated the PR as per the feedback. Can you please review and merge if it looks ok? Thanks
bddppq(2018-02-17 02:01:10):@pk-g looks good, thanks!
bddppq(2018-02-05 19:58:48):nit: space after comma
bddppq(2018-02-05 19:58:54):indent
AppVeyorBot(2018-02-05 20:16:44)::white_check_mark: [Build onnx 0.3.1002 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1002) (commit https://github.com/onnx/onnx/commit/4f9d7d298f by @pk-g)
AppVeyorBot(2018-02-05 18:21:20)::x: [Build onnx 0.3.998 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.998) (commit https://github.com/onnx/onnx/commit/ce924cbf00 by @pk-g)
AppVeyorBot(2018-02-05 18:39:01)::white_check_mark: [Build onnx 0.3.999 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.999) (commit https://github.com/onnx/onnx/commit/2c37942952 by @pk-g)
houseroad(2018-02-20 19:43:22):@pk-g could you update the PR to address the comments? So we can merge it.
AppVeyorBot(2018-02-21 00:05:59)::x: [Build onnx 0.3.1187 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1187) (commit https://github.com/onnx/onnx/commit/425665a15e by @pk-g)
bddppq(2018-02-21 00:54:01):@onnxbot test this please
pk-g(2018-02-21 01:06:42):@bddppq & @houseroad : I'll create another PR for this operation.

houseroad(2018-02-06 06:56:08):newline character is missing, and indent should be 4 space.
houseroad(2018-02-06 06:58:01):nit: one more comma
pk-g(2018-02-20 23:39:16):Sure, fixed.
pk-g(2018-02-20 23:39:29):Sure, fixed.
houseroad(2018-02-20 19:43:44):@pk-g could you update the PR to address the comments? So we can merge it.
pk-g(2018-02-21 20:39:58):@houseroad, please feel free to review the updated PR. the feedback is now applied.
houseroad(2018-02-06 07:07:30):Python 2 does not like this. Please fix it.
houseroad(2018-02-06 07:08:04):Also add a newline here, please.
pk-g(2018-02-21 02:12:57):Sure
pk-g(2018-02-21 02:13:29):Sure, added a fix
CLAassistant(2018-02-06 21:14:36):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=513) <br/>All committers have signed the CLA.
AppVeyorBot(2018-02-06 21:22:36)::x: [Build onnx 0.3.1025 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1025) (commit https://github.com/onnx/onnx/commit/2523a448c0 by @smessmer)
bddppq(2018-02-06 21:23:05):Looks good to me. python and numpy behave this way and I would like to follow the same semantic.
AppVeyorBot(2018-02-06 21:31:34)::x: [Build onnx 0.3.1026 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1026) (commit https://github.com/onnx/onnx/commit/d4e2649ea6 by @smessmer)
AppVeyorBot(2018-02-06 21:55:28)::x: [Build onnx 0.3.1027 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1027) (commit https://github.com/onnx/onnx/commit/bc3b465312 by @smessmer)
AppVeyorBot(2018-02-07 00:59:43)::x: [Build onnx 0.3.1039 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1039) (commit https://github.com/onnx/onnx/commit/7704fe088c by @smessmer)
AppVeyorBot(2018-02-07 01:06:51)::x: [Build onnx 0.3.1040 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1040) (commit https://github.com/onnx/onnx/commit/970e11711d by @smessmer)
dzhulgakov(2018-02-07 01:20:18):So, the desired behavior for (cut to the end) is INT_MAX, right? If so - shall we clarify it directly in the op description that it's what recommended? It might be useful in some backends to directly identify that the output is taken till the end. (I don't like INT_MAX but I don't see a better option)
houseroad(2018-02-07 01:24:13):@dzhulgakov We try to follow python/numpy style. Since the size can be arbitrary, it is safe to use INT_MAX. Although, any number larger than the size should work.
AppVeyorBot(2018-02-07 18:37:01)::white_check_mark: [Build onnx 0.3.1041 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1041) (commit https://github.com/onnx/onnx/commit/4838f69ad8 by @smessmer)
bddppq(2018-02-08 17:56:50):ping @linkerzhang for review
dzhulgakov(2018-02-08 22:09:55):I'm totally on board with it. What I'm saying - just add a comment in description that if you want to take towards the very end - using INT_MAX is fine.
AppVeyorBot(2018-02-16 01:04:13)::x: [Build onnx 0.3.1128 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1128) (commit https://github.com/onnx/onnx/commit/a933129105 by @smessmer)
AppVeyorBot(2018-02-16 01:26:50)::white_check_mark: [Build onnx 0.3.1129 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1129) (commit https://github.com/onnx/onnx/commit/79f98fd800 by @smessmer)
prasanthpul(2018-02-06 22:38:45):should we have CI for mac?
AppVeyorBot(2018-02-06 22:43:19)::white_check_mark: [Build onnx 0.3.1028 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1028) (commit https://github.com/onnx/onnx/commit/bd832ff83e by @dzhulgakov)
tianleiwu(2018-02-07 21:37:59):@houseroad, I updated doc to include description for softmax, logsoftmax and hardmax.
CLAassistant(2018-02-07 19:52:56):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=519) <br/>All committers have signed the CLA.
CLAassistant(2018-02-07 23:59:38):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=520) <br/>All committers have signed the CLA.
AppVeyorBot(2018-02-08 00:08:14)::x: [Build onnx 0.3.1053 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1053) (commit https://github.com/onnx/onnx/commit/a59fd70277 by @girifb)
houseroad(2018-02-08 19:18:14):@onnxbot add to whitelist
AppVeyorBot(2018-02-08 20:22:41)::white_check_mark: [Build onnx 0.3.1069 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1069) (commit https://github.com/onnx/onnx/commit/6e1a548e31 by @houseroad)
girifb(2018-02-09 01:46:42):Fixed the corner case issue in tests.
AppVeyorBot(2018-02-09 01:49:52)::x: [Build onnx 0.3.1080 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1080) (commit https://github.com/onnx/onnx/commit/31bcc095f9 by @girifb)
AppVeyorBot(2018-02-09 01:57:34)::x: [Build onnx 0.3.1081 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1081) (commit https://github.com/onnx/onnx/commit/c0d0a9e05b by @girifb)
AppVeyorBot(2018-02-09 02:09:06)::x: [Build onnx 0.3.1082 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1082) (commit https://github.com/onnx/onnx/commit/db01268d22 by @girifb)
AppVeyorBot(2018-02-09 02:26:56)::x: [Build onnx 0.3.1083 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1083) (commit https://github.com/onnx/onnx/commit/1bfee2413d by @girifb)
girifb(2018-02-09 02:29:56):Abandoning changes. They have been redone on https://github.com/onnx/onnx/pull/524
houseroad(2018-02-08 20:11:37):Since this code snippet also serves as docs, could you also add some case which contains manually created input and output. This is very helpful for users to understand the op spec.
houseroad(2018-02-08 20:12:34):https://github.com/onnx/onnx/blob/master/onnx/backend/test/case/node/ceil.py#L23

Just like this
girifb(2018-02-08 21:24:49):Done. Please review.
bddppq(2018-02-08 00:34:46):@yinghai 
bddppq(2018-02-08 22:50:40):We will need to do another PR after this one to separate `cpp2py_export.cc` out from "libonnx" (to avoid python dependency)
yinghai(2018-02-08 00:42:06):I think we need this one because onnx libraries will include proto headers. 
bddppq(2018-02-08 01:39:51):I'm apparently not knowledgeable on cmake :-) But it works in my osx and linux environments, so I guess `target_link_dependencies` implicitly adds the dependencies' include directories.
Maratyszcza(2018-02-08 22:05:32):Should be `target_link_libraries(onnx_proto PUBLIC ${PROTOBUF_LIBRARIES})`
linkerzhang(2018-02-08 22:36:14):It works because the ${ONNX_ROOT} is included when doing compilation and it's set as link dependency with target_link_dependencies.
bddppq(2018-02-08 22:43:44):thanks. fixed. Just curious, what's the default value?
linkerzhang(2018-02-08 22:45:52):${CMAKE_CURRENT_BINARY_DIR} is not needed, I think.:)
bddppq(2018-02-08 22:48:20):@linkerzhang I think it's needed :-) Say if another lib depends on onnx_proto, then it will need to be able to find "pb.h" file.
CLAassistant(2018-02-08 01:15:46):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=522) <br/>All committers have signed the CLA.
Maratyszcza(2018-02-08 17:12:07):Deprecating this PR in favor of #521, cc @bddppq 
AppVeyorBot(2018-02-09 02:37:19)::white_check_mark: [Build onnx 0.3.1084 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1084) (commit https://github.com/onnx/onnx/commit/1ca6d14f5d by @girifb)
houseroad(2018-02-12 21:08:18):@ebarsoum @linkerzhang ping for review :-)
yuanbyu(2018-02-15 16:25:23):It would be nice to add a test for non-zero axis.
houseroad(2018-02-16 19:27:40):Right, added.
AppVeyorBot(2018-02-09 20:05:47)::x: [Build onnx 0.3.1093 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1093) (commit https://github.com/onnx/onnx/commit/3dd637a061 by @bddppq)
bddppq(2018-02-09 20:25:37):There are two issues with this approach:

1. Our setup.py is doing incremental build, but we don't take environment variable as dependencies, so changing environment variable doesn't trigger a rebuild of the proto files. (This is the reason causing the CI failures right now).
2. We check in the generated proto files into the repo, if we build with a namespace value other than the default one "onnx" it will makes the proto files shown as modified in the working copy. (This one should be abled to solved by changing the gen_proto.py to genereate the proto files outside of the source directory.
AppVeyorBot(2018-02-09 20:33:12)::white_check_mark: [Build onnx 0.3.1095 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1095) (commit https://github.com/onnx/onnx/commit/f9b699bd7b by @bddppq)
AppVeyorBot(2018-02-12 18:19:48)::white_check_mark: [Build onnx 0.3.1102 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1102) (commit https://github.com/onnx/onnx/commit/762eeba971 by @bddppq)
ezyang(2018-02-12 20:03:25):In the new patchset, how do I go about updating the checked in generated files?
bddppq(2018-02-12 20:08:07):@ezyang You mean after updating the .in.proto file? As before by running the gen_proto.py. It defaults to use "onnx" as namespace and write to the source directory.

ezyang(2018-02-12 20:09:56):OK, LGTM!
bddppq(2018-02-12 20:13:06):@linkerzhang I'm merging this now since currently our proto files in the repo are in bad shape so it would be better to fix this issue as soon as possible. If you have other review comments, I can make another PR later.
yinghai(2018-02-09 20:06:36):Cool, this still happens in setup.py, right? We also need to call this in cmake. 
yinghai(2018-02-09 20:06:58):Why do we delete this? 
bddppq(2018-02-09 20:12:08):they are moved to individual libs (e.g. https://github.com/onnx/onnx/blob/a6f7955392ed7f4518c655edd17573ba01bf055a/CMakeLists.txt#L87)
Maratyszcza(2018-02-16 00:03:58):Can we support int32/uint32 output tensors? int64 is pretty demanding, even GPUs don't natively support int64 operations, and I'm concerned accelerators may not support it at all.
ebarsoum(2018-02-15 17:35:56):Why do we need `Size`? Isn't size can be implemented on the top of Shape?
jamesr66a(2018-02-15 20:26:12):This might be unclear, maybe say `containing the shape of the input tensor` and give an example?
jamesr66a(2018-02-15 20:26:39):Might be good to have an example here, but this one seems more clear
bddppq(2018-02-15 21:02:32):I don't really like to put the example in the doc string because they render poorly. The short code snippets (from the test cases) serve much better as example.
bddppq(2018-02-15 21:02:41):Same as above.
bddppq(2018-02-15 21:04:46):Wording updated
houseroad(2018-02-15 22:12:09):Could you add some cases only containing manually assigned inputs/outputs, something like the case here: https://github.com/onnx/onnx/blob/master/onnx/backend/test/case/node/sqrt.py#L23

That is easier for users to understand.
houseroad(2018-02-15 22:12:24):Could you add some cases only containing manually assigned inputs/outputs, something like the case here: https://github.com/onnx/onnx/blob/master/onnx/backend/test/case/node/sqrt.py#L23

That is easier for users to understand.
bddppq(2018-02-15 22:32:22):Added
bddppq(2018-02-15 22:32:27):Added
Maratyszcza(2018-02-16 00:00:46):Why 1D tensor? I thought ONNX supports scalars as 0-dimensional tensors
houseroad(2018-02-16 07:24:16):Shape + ~~ReduceSum~~ ReduceProd?
bddppq(2018-02-16 07:33:05):@ebarsoum I thought about that, but 1) Right now onnx doesn't have Prod op (there is ReduceProd, but it's a little bit overkill to use in this case IMO) 2) we in general want to follow the numpy api and it has both `shape` and `size` (pytorch and caffe2 as well).
ebarsoum(2018-02-16 19:34:18):@bddppq we can add Prod OP, the problem is that how generic `size` is? For example, for batch of variable length sequences do I want to get the total number of sample per each sequence multiply by the batch size? In some cases, you would want the number of element per sample and in others you want across the batch axis. Regarding the availability of size in numpy, Caffe2 and PyTorch. This is API level not IR level, so that shouldn't impact the inclusion in the ONNX, especially if we can represented in ONNX.

How `size` in PyTorch and Caffe 2 work with variable length sequences?
bddppq(2018-02-17 09:00:47):@ebarsoum 
1. I agree we should add Prod, it's a fundamental math operation.
2. What do you mean by "variable" length sequences? Are you saying that each sequence could have different length?
3. IMO size is a pretty bare bone op that shouldn't take batching into consideration. numpy, caffe2, pytorch, mxnet and tensorflow return the total number.

Considering we can easily add an optimization pass right now, I don't have strong preference between size or shape + prod (although slightly prefer the former which you can tell from my arguments).

@dzhulgakov @jamesr66a thoughts?
bddppq(2018-02-17 09:27:20):Original reply:
Updated to return a scalar

Update:
@Maratyszcza I initially thought you were saying Size. The output of shape op should be a list of numbers, right?
ebarsoum(2018-02-18 23:10:08):(2) yes.
Shape and Size usually used to compute some form of normalization, Shape is generic enough and we need it.   
Maratyszcza(2018-02-19 01:05:30):@bddppq Right, I meant the `Size` op.
bddppq(2018-02-20 21:42:12):@ebarsoum I don't think we can express variable length sequences without introducing more advanced data structure, and as I said before, size is supposed to be a very fundamental operator that should just simply return the total number of elements.
bddppq(2018-02-16 02:25:14):Hmm, it's under the `Development` section, which has already explicitly mentioned you should run the install command again after c++ files changes.
Besides, `python setup.py install` is not always correct here, because if you have installed in dev mode, you should run the dev mode install command again.
smessmer(2018-02-16 03:53:55):makes sense. I reformulated it.
AppVeyorBot(2018-02-20 23:26:49)::white_check_mark: [Build onnx 0.3.1182 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1182) (commit https://github.com/onnx/onnx/commit/1a2f5b7a69 by @anderspapitto)
AppVeyorBot(2018-02-21 19:03:11)::white_check_mark: [Build onnx 0.3.1214 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1214) (commit https://github.com/onnx/onnx/commit/fd829e8be5 by @anderspapitto)
AppVeyorBot(2018-02-21 19:29:48)::white_check_mark: [Build onnx 0.3.1217 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1217) (commit https://github.com/onnx/onnx/commit/f332863ab2 by @anderspapitto)
AppVeyorBot(2018-02-22 19:50:03)::white_check_mark: [Build onnx 0.3.1229 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1229) (commit https://github.com/onnx/onnx/commit/9e07627ebc by @anderspapitto)
AppVeyorBot(2018-02-22 20:08:12)::white_check_mark: [Build onnx 0.3.1232 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1232) (commit https://github.com/onnx/onnx/commit/82f1941801 by @anderspapitto)
AppVeyorBot(2018-02-23 01:06:44)::white_check_mark: [Build onnx 0.3.1294 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1294) (commit https://github.com/onnx/onnx/commit/e45f121413 by @anderspapitto)
CLAassistant(2018-02-17 03:04:47):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=537) <br/>All committers have signed the CLA.
AppVeyorBot(2018-02-17 03:16:40)::white_check_mark: [Build onnx 0.3.1154 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1154) (commit https://github.com/onnx/onnx/commit/4a4624de85 by @huitseeker)
bddppq(2018-02-17 18:19:57):@onnxbot test this please
AppVeyorBot(2018-02-18 21:24:45)::x: [Build onnx 0.3.1163 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1163) (commit https://github.com/onnx/onnx/commit/88b7575d2c by @huitseeker)
AppVeyorBot(2018-02-18 21:32:28)::x: [Build onnx 0.3.1164 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1164) (commit https://github.com/onnx/onnx/commit/a19cda7561 by @huitseeker)
bddppq(2018-02-19 03:29:14):@huitseeker Everything looks good now except you need to also checkin the new test case's code (i.e. the file node/gather.py).
AppVeyorBot(2018-02-19 04:57:23)::white_check_mark: [Build onnx 0.3.1165 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1165) (commit https://github.com/onnx/onnx/commit/47186d24d9 by @huitseeker)
huitseeker(2018-02-19 04:59:19):@bddppq Yup, sorry, got a bit messed up in the included set for the PR while rewriting the commit. Thanks!
bddppq(2018-02-19 05:02:46):Nice Thank you!
bddppq(2018-02-17 18:25:28):Should here be axis=1?
yinghai(2018-02-18 06:03:00):Comments addressed. 
Maratyszcza(2018-02-17 23:49:19):This part may fail. It relies on `python` being available as a command, which is not true with recent HomeBrew installation (it adds `python2` and `python3` symlinks, but not `python`, unless you add `/usr/local/opt/python/libexec/bin` to `$PATH`). I think a better solution would be 
```cmake
find_package(PythonInterp REQUIRED)
add_custom_command(...
  COMMAND ${PYTHON_EXECUTABLE} ${GEN_PROTO_PY}
  ...)
```
Maratyszcza(2018-02-18 00:05:10):Dependencies are not properly tracked here. In particular, changes in `*.proto.in` files would not cause a rebuild. Also, IIUC, you are calling `gen_proto.py` for every `.proto.in` file, but `gen_proto.py` generates the same set of output files. Multiple execs of `gen_proto.py` writing to the same output files would break parallel builds.

It is better to split this `add_custom_command` into two parts: running `gen_proto.py` to generate `*.proto` from `*.proto.in`, and running `protoc` on produced `*.proto` files to get `*.pb.cc` and `*.pb.h`.  `add_custom_command` invocations need both `OUTPUT` and `DEPENDS` arguments to properly track the deps.
Maratyszcza(2018-02-18 00:08:51):I think we can remove this line
yinghai(2018-02-18 05:49:15):Yes, `gen_proto.py` actually takes an argument for specific `in.proto`. I'll add that. 
AppVeyorBot(2018-02-19 20:29:53)::x: [Build onnx 0.3.1169 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1169) (commit https://github.com/onnx/onnx/commit/b4b0d4cfec by @yinghai)
AppVeyorBot(2018-02-19 20:40:14)::white_check_mark: [Build onnx 0.3.1170 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1170) (commit https://github.com/onnx/onnx/commit/b1e56af55e by @yinghai)
yinghai(2018-02-21 00:22:19):Addressed review comments. Also fixed https://github.com/onnx/onnx/issues/504. 
AppVeyorBot(2018-02-21 00:36:10)::x: [Build onnx 0.3.1190 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1190) (commit https://github.com/onnx/onnx/commit/f194015198 by @yinghai)
AppVeyorBot(2018-02-21 00:43:16)::x: [Build onnx 0.3.1191 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1191) (commit https://github.com/onnx/onnx/commit/a7fdafd327 by @yinghai)
AppVeyorBot(2018-02-21 00:50:42)::x: [Build onnx 0.3.1192 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1192) (commit https://github.com/onnx/onnx/commit/6eec0a9ec1 by @yinghai)
AppVeyorBot(2018-02-21 00:57:43)::x: [Build onnx 0.3.1193 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1193) (commit https://github.com/onnx/onnx/commit/2f68fdb099 by @yinghai)
AppVeyorBot(2018-02-21 01:21:45)::x: [Build onnx 0.3.1196 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1196) (commit https://github.com/onnx/onnx/commit/9de641e268 by @yinghai)
yinghai(2018-02-21 02:15:46):@bddppq onnx-fb-universe seems to have picked up the old commits for testing. How to retrigger it? 
bddppq(2018-02-21 04:42:15):@yinghai There was some failure in our bot, should be back to normal soon.
bddppq(2018-02-20 17:48:17):find_executable
bddppq(2018-02-20 17:50:47):Some systems don't use ".." as parent dir. Python has `os.pardir`.
bddppq(2018-02-20 17:55:37):Why is `link_args` handled differently with `libraries` and `include_dirs`?
bddppq(2018-02-20 17:56:30):ditto
bddppq(2018-02-20 17:59:32):This is not always correct. e.g. on ubuntu14.04, the protobuf that comes from the system package manager put the lib in `/usr/lib/x86_64-linux-gnu/libprotobuf.so.8`.
Also, see here https://github.com/onnx/onnx/issues/504
bddppq(2018-02-20 18:06:09):1. `libprotobuf` has been added to `libs` already, why add `-lprotobuf` again here?
2. `-L` + `-l` is not enough. This only makes libprotobuf can be found at link time, but if libprotobuf is in some custom path, it won't be found by the loader. 
yinghai(2018-02-20 18:13:08):I was trying to hide the data member. But yes. it's unnecessary... 
yinghai(2018-02-20 18:16:23):The trick is to find `protoc`. For a system with multiple protobuf versions installed, user decides which one to use by setting the `PATH` to pickup the right protoc, and library would be under `../lib`. This is picked up from Caffe2 cmake file and the goal is to be consistent with Caffe2's behavior so that they end up picking the same protobuf.  
yinghai(2018-02-20 18:18:43):Yeah, actually, I don't know what dependency libs here does. when I print out the actual `gcc` compilation command, it doesn't seem to affect anything. That's why I added `-L` and `-l` which worked for me (checked by ldd). Any suggestions? Say adding `-Wl,-R`? 
yinghai(2018-02-20 18:22:44):I think https://github.com/onnx/onnx/issues/504 explains a lot on the messed up situation. I can fix that and it will avoid quite a bit customized logic. 
bddppq(2018-02-21 01:16:01):Is there a way in find_program to set this as required?
yinghai(2018-02-21 01:52:33):There is not. But if it fails to find `PROTOBUF_PROTOC_EXECUTABLE`, `PROTOBUF_PROTOC_EXECUTABLE` will be set to `PROTOBUF_PROTOC_EXECUTABLE-NOFOUND` and the downstream operation will fail with pretty clear message. 
AppVeyorBot(2018-02-20 22:25:22)::x: [Build onnx 0.3.1177 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1177) (commit https://github.com/onnx/onnx/commit/8237a4324b by @houseroad)
anderspapitto(2018-02-20 23:50:38):lgtm as long as it doesn't break anything
AppVeyorBot(2018-02-20 23:51:19)::x: [Build onnx 0.3.1185 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1185) (commit https://github.com/onnx/onnx/commit/487f82e847 by @houseroad)
AppVeyorBot(2018-02-20 23:57:24)::x: [Build onnx 0.3.1186 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1186) (commit https://github.com/onnx/onnx/commit/b3c9e681f5 by @houseroad)
AppVeyorBot(2018-02-21 00:30:58)::x: [Build onnx 0.3.1189 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1189) (commit https://github.com/onnx/onnx/commit/9e38fc4582 by @houseroad)
CLAassistant(2018-02-21 00:15:44):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=542) <br/>All committers have signed the CLA.
brettkoonce(2018-02-21 00:17:38):p.s. should this be forward, not foward?     https://github.com/onnx/onnx/blob/535c8c1ded151c772051dd8f7a0887752c2d4fd5/onnx/defs/rnn/defs.cc#L16 
AppVeyorBot(2018-02-21 00:23:45)::x: [Build onnx 0.3.1188 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1188) (commit https://github.com/onnx/onnx/commit/827665c4e7 by @brettkoonce)
AppVeyorBot(2018-02-21 03:51:46)::x: [Build onnx 0.3.1203 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1203) (commit https://github.com/onnx/onnx/commit/499504c65e by @brettkoonce)
houseroad(2018-02-21 03:54:38):Please fix the typos in def.cc files, then compile onnx project, and run gen_doc.py to update the docs.
AppVeyorBot(2018-02-21 03:58:53)::x: [Build onnx 0.3.1204 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1204) (commit https://github.com/onnx/onnx/commit/98509356f6 by @houseroad)
houseroad(2018-02-21 20:25:31):@brettkoonce yes, it should be forward.
AppVeyorBot(2018-02-27 21:01:26)::x: [Build onnx 0.3.1328 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1328) (commit https://github.com/onnx/onnx/commit/a207809926 by @houseroad)
AppVeyorBot(2018-02-27 21:13:25)::white_check_mark: [Build onnx 0.3.1329 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1329) (commit https://github.com/onnx/onnx/commit/db7177df92 by @houseroad)
brettkoonce(2018-03-02 00:39:41):@houseroad Thanks for doing the update steps, my linux install is acting up!  Will get the workflow going before doing this next time!
houseroad(2018-03-02 00:41:58):@brettkoonce No problem, thanks for your contribution. :-) 
pk-g(2018-02-21 20:41:07):@bddppq , @houseroad : please feel free to review this PR instead of earlier PR on cast operator tests.
AppVeyorBot(2018-02-23 00:36:01)::white_check_mark: [Build onnx 0.3.1290 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1290) (commit https://github.com/onnx/onnx/commit/75cb369816 by @houseroad)
bddppq(2018-02-27 01:08:06):@onnxbot test this please
houseroad(2018-02-27 00:15:02):In the generated doc, user may be confused about the type_map. If we put TENSOR_TYPE_TO_NP_TYPE here directly, it will be clear to users.
pk-g(2018-02-27 01:17:57):Sure, pushed an update.
bddppq(2018-02-22 04:14:42):Better to not delete user's files, move it to say `~/.onnx/models/squeezenet.old.1` (or `.2` if `.1` already exists)
houseroad(2018-02-22 20:12:22):Now move it as .old.0, old.1, etc...
CLAassistant(2018-02-22 17:28:36):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=547) <br/>All committers have signed the CLA.
AppVeyorBot(2018-02-24 00:23:48)::white_check_mark: [Build onnx 0.3.1306 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1306) (commit https://github.com/onnx/onnx/commit/bdb6832ce1 by @linkerzhang)
CLAassistant(2018-02-22 18:01:49):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=548) <br/>All committers have signed the CLA.
AppVeyorBot(2018-02-22 18:10:49)::x: [Build onnx 0.3.1226 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1226) (commit https://github.com/onnx/onnx/commit/3a293963ac by @fumihwh)
houseroad(2018-02-22 18:23:30):@fumihwh Thanks for the contribution. You should also need to run https://github.com/onnx/onnx/blob/master/onnx/defs/gen_doc.py to update the md files, and https://github.com/onnx/onnx/blob/master/onnx/backend/test/cmd_tools.py to regenerate the backend test data.
fumihwh(2018-02-22 18:47:57):@houseroad Sorry for the connivence. I will add a commit for doc updating.
houseroad(2018-02-22 18:50:15):No worries, also backend test data should be updated. :-)
bddppq(2018-02-23 18:00:51):@onnxbot test this please
Maratyszcza(2018-02-22 19:21:27):onnx's CMakeLists also needs other changes to make it work well as a submodule. In particular, changing global compilation/linking options is unacceptable:
```
# Set C++11 as standard for the whole project
set(CMAKE_CXX_STANDARD 11)

...

if (WIN32)
    # parallel build
    add_compile_options(/MP)
    add_compile_options(/WX)
    set(CMAKE_EXE_LINKER_FLAGS "${CMAKE_EXE_LINKER_FLAGS} /WX")
    set(CMAKE_STATIC_LINKER_FLAGS "${CMAKE_STATIC_LINKER_FLAGS} /WX")
endif()
```
If needed, this flags should be set for individual targets via `target_compile_options`, etc
AppVeyorBot(2018-02-22 19:23:51)::white_check_mark: [Build onnx 0.3.1227 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1227) (commit https://github.com/onnx/onnx/commit/592bf83217 by @yinghai)
yinghai(2018-02-22 22:45:28):@bddppq I think @Maratyszcza's comment is legit. We should create an issue for that? 
bddppq(2018-02-22 23:22:15):@yinghai @Maratyszcza  yep definitely
AppVeyorBot(2018-02-23 06:13:59)::white_check_mark: [Build onnx 0.3.1300 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1300) (commit https://github.com/onnx/onnx/commit/80187689a7 by @Maratyszcza)
VincentLin78(2018-03-09 03:33:31):For further verification on the API, will be any open-sourced BE implementation on certain hardware platform ?
Maratyszcza(2018-03-09 04:19:59):@VincentLin78: @yinghai is working on a backend for a certain hardware platform behind this interface, so we know the API can be implemented in practice. Not sure if we would be able to open-source @yinghai's work, though, as it depends on non-public components.
VincentLin78(2018-03-09 06:28:36):Would you consider to add more features(Utilities) on it.
1. LRU cache helper function for BE to cache private graph IR to disk and restore to RAM when necessary.
2. ONNX Graph validation helper function
3. Graph identity(checksum or something) which BE won't have to compile the same model(can just restore the compiled one from disk cache or return the existed handle)
That helps this standard more attractive to BE vendors.
Maratyszcza(2018-03-09 19:31:28):@VincentLin78 1 is out of scope of ONNX, but there are LRU cache implementations in general-purpose C++ libraries, e.g. [boost](http://www.boost.org/doc/libs/1_62_0/libs/compute/doc/html/boost/compute/program_cache.html). 2 is already implemented in ONNX core library, see [onnx/checker.h](https://github.com/onnx/onnx/blob/master/onnx/checker.h). If vendors request 3, we are open to consider it for ONNX core library (not Backend API, which is only an interface). However, AFAIK major vendors are already working on ONNX support (as a file format, at least), and would prefer to use their own code.
AppVeyorBot(2018-04-04 00:33:37)::x: [Build onnx 0.3.2048 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2048) (commit https://github.com/onnx/onnx/commit/8f9d2894c0 by @Maratyszcza)
AppVeyorBot(2018-04-23 06:52:15)::white_check_mark: [Build onnx 0.3.2478 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2478) (commit https://github.com/onnx/onnx/commit/c84c26fb4e by @Maratyszcza)
AppVeyorBot(2018-04-25 20:42:36)::white_check_mark: [Build onnx 0.3.2565 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2565) (commit https://github.com/onnx/onnx/commit/05da517be8 by @Maratyszcza)
jspisak(2018-04-25 21:28:06):Merging. Great work everyone!
hlu1(2018-02-22 23:06:59):"ONNX_MEMORY_TYPE_CPU (always supported)" is one flag. Any other flags that need to be supported? If not, you may want to rephrase the last sentence

Do we have plan to support device specific memory?

hlu1(2018-02-22 23:12:24):Is step 1 missing?
hlu1(2018-02-22 23:46:48):I don't really understand how auxPropertiesList can be used. There is no documentation on what and how properties could be set
Maratyszcza(2018-02-23 00:52:34):Vendors can introduce extensions and use high 32 bits of this enum/bitfield for vendor-specific memory types. When there is enough interest, we will standardize these new types.
Maratyszcza(2018-02-23 00:52:47):Good catch!
Maratyszcza(2018-02-23 00:55:06):```c++
// Example: passing CUDA stream for use by a backend
// Note: ONNX_BACKEND_PARAMETER_CUDA_STREAM is not in current proposal.
const uint64_t auxProperties[] = {
    ONNX_BACKEND_PARAMETER_CUDA_STREAM, reinretpret_cast<uint64_t>(myCudaStream),
    ONNX_BACKEND_PARAMETER_NONE,
};
onnxInitBackend(backendIndex, auxProperties, &backend);
```
smessmer(2018-02-23 00:58:17):This means we won't be able to dynamically load backends at runtime. We have to know about them at compile time. I'm not sure if C ABIs offer a way to have dynamic plugins loaded (wasn't there COM or something doing that some time in the past?), but if we can find a way to do this, it would give us more flexibility.

Passing function pointers from the backend to the caller when the backend is initialized might be one way of doing it. The backend could still implement this very same static interface, but instead of directly publishing the functions, it would be wrapped in dynamic loader code, which is the same for all backends.
smessmer(2018-02-23 01:07:38):What about

    typedef struct onnxGraph {
        void* handle;
    } onnxGraph;

? Would be more typesafe for the caller.

I was also thinking about just forward declaring it, i.e.

    struct onnxGraph;

and letting the backend decide how to implement this struct, but that might clash with having multiple backends loaded at the same time. Or at least I'm not sure if that would work.
smessmer(2018-02-23 01:09:04):We might also want to consider wrapping other types (onnxStatus, onnxEnum, onnxBitfield, onnxPointer) into a struct for type safety. Not sure if it makes sense for all of them (e.g. code for computing AND or OR on bitfields would be more verbose), but it has the potential of reducing number of programmer mistakes.
smessmer(2018-02-23 01:12:38):Nit: mention that these strings are null-terminated. Probably self-explanatory though.
smessmer(2018-02-23 01:13:50):Do we want to allocate `ONNX_BACKEND_NAME_MAX+1` to not confuse someone who tries to set it to a name with exactly the maximum length and wondering why it crashes with a segfault?
smessmer(2018-02-23 01:17:45):Nit: Is there a reason the numbers are out of order? If we're specifying these new (i.e. they're not used anywhere else yet), we can also order them somehow reasonably.
smessmer(2018-02-23 01:19:18):What is the reasoning behind storing strings in onnxBackendCoreInfo inline and here as a pointer?
salexspb(2018-02-23 18:52:30):What if data preparation already happens on the backend memory? Like on a case of OpenGL. Requesting it to go through CPU would be not efficient. 
salexspb(2018-02-23 18:55:09):do you suggest here that a vendor would have several backends? Otherwise, why do we need onnxGetNumBackendsGAMMA ? 
smessmer(2018-02-23 21:29:41):I'd rather put these into two different functions which are more strictly typed. One returns the `onnxBackendCoreInfo`, the other function returns the `onnxBackendPerformanceInfo`. This way, we also get rid of the `backendInfoType` flag and it's safer to user. Also, the parameter can then be more strictly typed instead of just `void*`.
smessmer(2018-02-23 21:38:37):If I want to (1) check backend compatibility for my whole model and then (2) run it, I have to send the protobuf once to `onnxGetBackendCompatibility`, and then to `onnxInitGraph`, forcing the backend to parse it twice. Can we combine this? For example by having a `onnxLoadGraph` which is called first, and creates  a handle that can be passed in to `onnxGetBackendCompatibility` or `onnxInitGraph`?
smessmer(2018-02-23 21:44:43):Do we want to use something like `warn_unused_results` for these functions to notice call sites that forget to check the status?
smessmer(2018-02-23 21:46:12):What happens if `inputsCount` or `outputCount` don't match the number of required inputs/outputs?
Maratyszcza(2018-02-24 05:54:23):Suffix is needed to allow loading backends with `RTLD_GLOBAL` (which is itself needed to let high-level frameworks, e.g. Caffe2 and MXNet, implement this interface too).
Maratyszcza(2018-02-24 07:35:04):The interface has provisions to support other types of memory, but the MVP intends to be as simple as possible. Vendors can add custom memory types by using the high 32 bits of `memoryType` enum/bitfield and indicating a vendor-specific extension through `ONNX_BACKEND_EXTENSIONS` query in `onnxGetBackendInfo`. When there is enough interest, and at least one implementation, we will standardize these additional memory types.
Maratyszcza(2018-02-24 07:45:12):An ONNX backend is a combination of a software library and hardware device, so for sure there can be multiple backends from the same vendor. But importantly, there also can be multiple backend libraries, either from the same vendor, or from different vendors. We will provide a wrapper library that would load multiple vendor-provided libraries and expose them under a single interface (this library is coming in another PR). To make it all work with `RTLD_GLOBAL`, the functions in different backend libraries must have distinctive names, which we achieve by using vendor-specific suffix.
Maratyszcza(2018-02-24 07:48:32):Oops, I copied old branch. These structure should no longer exist, we decided in favor of OpenCL-like interface.
Maratyszcza(2018-02-24 07:52:34):I would prefer to not depend on ABI specifics for struct passing, it is a bit fragile, and does change at times.
Maratyszcza(2018-02-24 08:07:17):We will be able to dynamically load backends at runtime. E.g. on Linux we will search for backends that match `/libonnxbe\-([a-z0-9]+)\.so/`, and then e.g. `dlsym(library, "onnxGetBackendInfo" + match.group(1).upper())`. The directory where vendors would put ONNX backend libraries is still TBD.
Maratyszcza(2018-02-24 08:08:56):Yes, the values match ONNX TensorProto.DataType enum
Maratyszcza(2018-02-24 08:09:32):No good reason, so in the updated interface `onnxBackendCoreInfo` struct is gone
Maratyszcza(2018-02-24 08:10:15):Undefined behavior
Maratyszcza(2018-02-24 08:11:07):Yeah, thats a good idea
benbarsdell(2018-02-28 00:41:32):Any reason not to use enums for these (and all the other enumerations in this header)?
smessmer(2018-02-28 02:13:26):Hm string matching doesn't seem like the best approach to me, but I'm not sure there is a better one. Is there some way to use function pointers instead? Maybe have only one `initBACKEND()` method that needs to be found using string matching and that returns a set of function pointers to the other API methods?
smessmer(2018-02-28 02:13:47):Agree. I wasn't aware struct passing is instable in ABIs.
smessmer(2018-02-28 02:15:45):So actually there is no reason to pass these in at all because the values are predetermined anyhow by the model? Hm. I'm unsure, it probably still makes sense to keep it as-is (i.e. keep the arguments there) to allow backends to do sanity checking.
Maratyszcza(2018-02-28 05:19:50):enums do not have a portable ABI, they can be 8-bit wide on some platforms/compilers/compiler flags (e.g. `-fshort-enums` in gcc).
Maratyszcza(2018-02-28 05:23:37):Now I get that you asked a different question. If `inputsCounts`/`outputsCount` doesn't match the number of GraphProto.inputs (excluding static weights) and GraphProto.outputs, the function fails with `ONNX_STATUS_UNIDENTIFIED_NAME`
Maratyszcza(2018-02-28 05:41:47):Current mechanism is extensible: vendors can implement additional functions, and users can find them with `dlsym(dlopen("libonnxbe-myvendor.so", RTLD_NOLOAD), "onnxVendorSpecificFunctionMYVENDOR")` or `GetProcAddress(GetModuleHandleW(L"onnxbe-myvendor.dll"), "onnxVendorSpecificFunctionMYVENDOR")`. `initBACKEND` approach wouldn't have such flexibility.
smessmer(2018-02-28 19:10:00):Maybe we should clear up the documentation for this error code. Currently, it only seems to catch the case when ModelProto.graph.input > inputDescriptors. What do we do when ModelProto.graph.input < inputDescriptors?
gramalingam(2018-02-28 19:48:04):Is this supposed to be OnnxGraph and not "OnnxGraph*" like elsewhere?
smessmer(2018-02-28 19:51:11):I think this is an output parameter.
gramalingam(2018-02-28 20:01:54):Why not make this a pointer, instead of limiting the number of dimensions?
gramalingam(2018-02-28 20:05:35):Why not have a separate function to check whether a single node in a given graph is compatible? Constructing a ModelProto for this case (which is what you'd want to do to check for partial compatibility) seems unnecessary.
gramalingam(2018-02-28 20:10:26):I think it will be more efficient to make parameter-passing positional instead of name-based. This is going to be invoked for every inference. In particular, it may be worth eliminating the name from onnxTensorDescriptor.
lupesko(2018-02-28 20:31:23):Why limit to NCHW? There's frameworks that have limited support for that. Why not allow users to specify the tensor layout, and then the underlying implementation can update layout as needed? 
lupesko(2018-02-28 20:34:50):This looks good, but what about allocating/deallocation the actual input/output tensors?
lupesko(2018-02-28 20:35:46):We probably want to offer control over backend-specific params like memory, threads, and other details that are backend specific?
lupesko(2018-02-28 20:41:24):One thing that we may be missing here is error handling.
A good library would return meaningful errors rather than seg-faulting on users.
It is probably wise to have a common set of error codes defined for backends.
smessmer(2018-02-28 22:04:57):I read up a bit on dlopen/dlsym. Seems there's actually no reason for the MYVENDOR suffix. Multiple libraries with the same symbol names don't clash. With same names everywhere, my points here are actually mute. Is it feasible to get rid of the suffixes?
Also, did we consider using glib or libtooling? They seem be somewhat cross platform whereas dlopen/dlsym isn't. 
linkerzhang(2018-02-28 22:25:22):This should be the API for hardware vendors, right? Each hardware vendor should be one backend.
linkerzhang(2018-02-28 22:27:39):What's the onnxModel? It should be a sub-graph, I think.
linkerzhang(2018-02-28 22:28:40):How do we split the whole graph into sub-graphs and dispatch to different backends please? a sub-graph partition API might be designed and provided.
dzhulgakov(2018-03-01 01:06:20):I think it's just from the consideration to make # of API functions smaller.

We could add the function for a single node - but then it needs to have access also to input/output shapes (as some backends might support only some sizes). GraphProto already conveniently has a place to store shape information

Also, at least from the use cases we're aware of, if the backend can support a graph then it can support any sub-part of the graph too (not sure whether it's true in the more general case).
dzhulgakov(2018-03-01 01:15:43):SetGraphIO is actually intended to be called once at setup and you can invoke RunGraph multiple times reusing buffers if you want to. It matches well devices with costly memory allocation.

But I agree that it's not perfect solution for frameworks that do dynamic memory allocation (if it's on cpu especially). @Maratyszcza - thoughts on positional arguments?
dzhulgakov(2018-03-01 01:17:35):Memory is allocated by the caller always. Even for dynamic output sizes we propose the caller to allocate maximally possible buffer beforehand. It's a tradeoff but allows to cut off extra level of complexity with managing memory
dzhulgakov(2018-03-01 01:18:51):Mostly for simplicity and since current ONNX ops assume NCHW. From our experience - we usually offload the entire subgraph to backend and it can change the layout of tensors in the bigger chunk internally.
Maratyszcza(2018-03-01 04:54:47):`auxProperties` argument to `onnxInitBackend` lets users pass arbitrary parameters to the backend. Currently, no such parameters are standardized, but vendors can implement any auxiliary parameter as an extension.
Maratyszcza(2018-03-01 04:57:11):We do define common error codes and document which error code are returned in which situation. See `ONNX_STATUS_*` constants in `onnx_backend.h`
Maratyszcza(2018-03-01 05:01:21):Hardware vendors would implement this API, and high-level applications would use this API. Deep learning frameworks could both use and implement this API.

An ONNX backend is a combination of software layer (`onnx-myvendor.dll` library) **and** a hardware device (e.g. `"NVIDIA Tesla P100"`). Thus, software library implementing this interface can expose multiple ONNX backends.
Maratyszcza(2018-03-01 05:02:40):Yes, this is an output parameter, thus the pointer.
Maratyszcza(2018-03-01 05:28:28):Note that de-facto it affects only input and output tensors, because within the graph the backend library is free to use any representation. Frameworks which favor different tensor layouts can still use the Backend API by inserting ONNX **Transpose** op before all inputs and after all outputs.
Maratyszcza(2018-03-01 12:12:24):We considered three options: `NodeProto`, `GraphProto`, and `ModelProto`. However, when we tried to map existing vendor libraries to the backend API with the first two options, we realized that they lack important context details:

- `NodeProto` doesn't internally contain information about input and output shapes, and this information is important to decide is an operator is supported. Many of today's vendor libraries have a limit on maximum shapes that can handle, and we know one vendor library that imposes divisibility restrictions on number of input and output channels (i.e. number of channels must be a multiple of 8, otherwise operator is unsupported), thus input/output shapes are crucial for `onnxGetBackendCompatibility` operation.

- `GraphProto` is a better candidate as it provides input/output shapes and data types, but `GraphProto` still lacks crucial information to validate the graph: `opset_import` and `ir_version`.

Thus, we settled on `ModelProto` for the proposal. It has the additional advantage that much code implementing and using `onnxInitGraph` and `onnxGetBackendCompatibility` can be shared.
Maratyszcza(2018-03-01 12:19:56):`onnxModel` is a serialized `ModelProto` message representing a subgraph. Please see above on the reasons for choosing `ModelProto` instead of `GraphProto`.
Maratyszcza(2018-03-01 12:53:20):If `ModelProto.graph.input < inputDescriptors`, then either some input descriptors are duplicate, or they contain a name that is not in `ModelProto.graph.input`. Either way `onnxSetGraphIO` would fail with `ONNX_STATUS_INVALID_NAME`.
Maratyszcza(2018-03-01 13:36:15):Sub-graph partitioning is not part of the proposal: its implementation it left to high-level frameworks which use the Backend API. The Backend API, however, provides information queries to help the high-level level frameworks with this task:  `ONNX_BACKEND_MACS_FP32`/`ONNX_BACKEND_MACS_FP16` and `ONNX_BACKEND_MEMORY_BANDWIDTH` information queries get information about the target backend needed to build a roofline model of performance for any known subgraph. Combined with `ONNX_BACKEND_CPU_MEMORY_READ_BANDWIDTH` and `ONNX_BACKEND_CPU_MEMORY_WRITE_BANDWIDTH`, the model would provide an estimate for the time to offload inputs to a subgraph to the backend, execute the subgraph on the backend, and get the results back.
Maratyszcza(2018-03-01 13:54:47):Good question! We started with more rigid HAL-like API, but with current more flexible proposal this limitation doesn't make sense. I'll change to a pointer.
linkerzhang(2018-03-01 18:46:56):Correct me if I'm wrong please. Are we expecting one backend (hardware accelerator or software vendor) wrapping against another bunch of backends?

I'm a little bit confusing here. For example, let's say there's hardware vendor A which implements this API to be able to run a subgraph of onnx (covering part of onnx operators). How does a higher layer runtime (say caffe2, cntk) call it please? Call the getnumofbackends to get to know there're 2 "backends" and then call the "load", "run" APIs to run the subgraph? so the "load", "run" API will need to have an argument to specify which "backend" (as there're 2 backends) should be called?

linkerzhang(2018-03-01 19:40:51):dumb question, are we expecting each backend will have full capability to run any onnx model please? so that the partitioning in high-level framework will only need to understand the resource bandwidth of each backend.
dzhulgakov(2018-03-02 09:55:55):No, backend can implement only subset of ops. That's why `onnxGetBackendCompatibility` is here - to allow hosting backend (e.g. caffe2) to ask whether particular chunk is supported or not (usually calling node by node). That's usually the approach e.g. TensorRT is integrated in TF
Maratyszcza(2018-03-02 13:41:25):It is possible that a high-level framework (e.g. Caffe2, CNTK, or MXNet) implements that Backend API. Internally, the high-level framework can use also use Backend API to leverage hardware-specific backends exposed by vendors. Applications can distinguish between backends using a specific device and backends internally managing execution between multiple devices by doing `onnxGetBackendInfo` for `ONNX_BACKEND_DEVICE_TYPE`. The return value for such high-level backends spanning multiple hardware devices is `ONNX_DEVICE_TYPE_FRAMEWORK`, as opposed to `ONNX_DEVICE_TYPE_NPU`, `ONNX_DEVICE_TYPE_DSP`, `ONNX_DEVICE_TYPE_GPU`, `ONNX_DEVICE_TYPE_CPU`, or `ONNX_DEVICE_TYPE_FPGA` for backends corresponding to real hardware devices.
Maratyszcza(2018-03-02 13:44:11):A typical sequence of ONNX Backend API function calls on application side is documented in https://github.com/Maratyszcza/onnx/blob/ad150df9ef67843f9ae5649834f748d0691fd94c/docs/BackendAPI.md#how-to-use-onnx-backend-interface
Maratyszcza(2018-03-02 13:46:36):Symbol names from different libraries do clash if library is loaded with `DYLD_GLOBAL` flag, and `DYLD_GLOBAL` is required for proper functioning of high-level frameworks with dynamic operator registration, such as Caffe2 and MXNet, if they choose to implement Backend API themselves.
Maratyszcza(2018-03-02 21:06:30):Passing parameters by name is convenient, and I expect that the time spent on string matching would be unnoticeable, despite its contribution is dependent on the use-case:
- In graphs with static tensor shapes, `onnxSetGraphIO` would be called only once, followed by many calls to `onnxRunGraph`. The cost of `onnxSetGraphIO`, including the cost of string matching, will be amortized across many inference invocations, and thus negligible.
- In graph with dynamic tensor shapes, `onnxSetGraphIO` would be called for every, or nearly every, call to `onnxRunGraph`. In each call `onnxSetGraphIO` would have to do shape inference for all internal tensors, memory allocation for internal blobs, and potentially code generation. String matching doesn't add much time on top of the above issues.

In the current models, it is typical to have very few (< 10) input and output tensors. If in the future we'd have use-cases where `onnxSetGraphIO` is called for every inference invocation, and the number of input and output tensors is so big that name matching becomes an issue, backend libraries could implement perfect hashing to tensor names, as the names of all inputs and outputs are available when graph is constructed.
gramalingam(2018-03-06 22:18:35):May be we should have a single call where we can pass in a GraphProto and get back a list of all the nodes in the Graph that can be supported. This is useful if we want the partition the graph into subgraphs that can be run on the backend.
VincentLin78(2018-03-07 07:09:47):Why this is optional ?
BE implementation can be built with BE API in the same lib. ?
VincentLin78(2018-03-07 07:13:50):What if only part it can run on BE ?
Who takes care of the graph partition ? 
The runnable part will still be dispatches to BE in the latter call ?
Maratyszcza(2018-03-07 13:01:08):To clarify: BE implementation is a software library that implements the 9 functions defined in the backend API. A typical user of a BE API is a deep learning library.

This step is optional for two reasons:
1. BE loader library (which provides `onnx_backend_load`) is not a part of the spec, it is provided only for convenience. Deep learning frameworks can use their own loader code instead.
2. In embedded environments, where the Backend API library is always available, users can use dynamic linking instead of dynamic loading (i.e. pass `-lonnxbe` argument at link time instead of using `dlopen`).
Maratyszcza(2018-03-07 13:15:45):If a backend supports only some operators of the graph, the user of the API (a deep learning framework) would have to do the partitioning. In the general case, the process is outlined below:
1. The user iterates operators in a model graph one-by-one, convert them to ONNX, and use `onnxGetBackendCompatibility` to check which of the operators can be offloaded to the backend.
2. The user constructs connected subgraphs of operators that can be offloaded to the backend.
3. The user queries the backend about it high-level performance characteristics using `ONNX_BACKEND_MACS_*` and `ONNX_BACKEND_MEMORY_BANDWIDTH` information queries. These data let the user build a simple roofline model of backend performance.
4. For every subgraph the user estimates time to do inference using the roofline model. The user additionally estimates time to transfer subgraph inputs to the backend using `ONNX_BACKEND_CPU_MEMORY_READ_BANDWIDTH` information query and to transfer subgraph outputs from the backend using `ONNX_BACKEND_CPU_MEMORY_WRITE_BANDWIDTH`.
5. If predicted time to transfer inputs to the backend, do inference, and transfer outputs from the backend is less than predicted time to do the inference on default engine (e.g. CPU), the user offloads the subgraph execution to the ONNX backend by calling `onnxSetGraphIO` and `onnxRunGraph`
linkerzhang(2018-03-07 18:13:52):Complex types are missed here.
linkerzhang(2018-03-07 19:16:22):Thank you for clarification! We're saying high-level callers will construct temp Model (only one node) one by one per main model to get to know the capability. Hmmm, @gramalingam is also suggesting that we should have a way to check the capability with one call without consisting of temp one-node models. 
Maratyszcza(2018-03-07 19:43:03):Sure, I'll add the complex types from ONNX's `TensorProto.DataType`
Maratyszcza(2018-03-07 19:59:53):We certainly should consider this option. It would reduce the time to discover the set of supported operations, but I have two concerns:

- The current `onnxGetBackendCompatibility` API can be implemented on top of all existing vendor libraries, e.g. by feeding the `ModelProto` to the library and checking if it fails. The suggested change would require new functionality in vendor libraries, and we need to check if vendors would be willing to implement it.
- With the current API, deep learning frameworks don't need to pay much attention to serialization of a model graph into ONNX format, and it is typically hidden behind a high-level function call. The proposed change would require deep learning frameworks to maintain mapping between the order of operators in serialized ONNX message and the same operators in their internal IR.

I think we'd need to do some experiments, and in particular quantify the win from this change before deciding on it.
linkerzhang(2018-03-07 21:29:49):@Maratyszcza , this is really a great detail clarification. I understood and agreed on these points since the similar questions are also there in our side. However, I personally still think that "Node" is a better granularity to check the capability and do execution. Probably we should fix the issue of "Node" in onnx now. Thoughts? although fixing node may introduce breaking IR changes :).

The design that Node does not contain shape/type info for input/output introduces many issues and looks like https://github.com/onnx/onnx/pull/588 is also fixing one of them. @bddppq  @houseroad 
ke1337(2018-03-07 21:43:16):It would be nice if Backend API allows vendor to store additional info along with the model, preferably in one file. Some backend may have optimization process before running the model for the first time, and caching the computation might be valuable.
Maratyszcza(2018-03-09 04:04:37):@linkerzhang It seems that your comment is for another thread, about the `onnxGetBackendCompatibility` function. We don't have any use-cases where `ModelProto` passed to `onnxGetBackendCompatibility` need to contain more than one Operator. I suggest we change the documentation to require that a user always pass `ModelProto` with an only Operator in the graph.
Maratyszcza(2018-03-09 04:14:27):Vendors can add functions to store and load internal representation of a graph that is specific to the backend. However, such functions are not standardized in the proposal.
lupesko(2018-03-14 04:44:21):Note that running transpose has a perf toll that would be great to avoid if possible.
However, this is not a one-way-door, and we can always revisit and add support for NHWC in the future.
I'm OK with this approach for now.
lupesko(2018-03-14 04:48:57):OK.
lupesko(2018-03-14 04:49:25):OK.
lupesko(2018-03-14 04:49:41):OK
dzhulgakov(2018-04-02 23:00:43):maybe we should put basic CNN here - but it's a separate discussion. @houseroad 
dzhulgakov(2018-04-02 23:02:05):maybe expand it a bit: "expose additional functions in the C API (for example separate calls to setup device-specific async execution)". Otherwise it's not clear which functions we're talking about
dzhulgakov(2018-04-02 23:02:46):so why some things have `onnx` prefix and some `onnxifi`. Is it intentional?
Maratyszcza(2018-04-02 23:36:30):Macros have `ONNXIFI` prefix. Functions and types have `onnx` prefix. This is because macros live in a global namespace and clash with macros in ONNX core library, but functions and types in core library are defined in their own C++ namespace, so there is no possibility for collision.
Maratyszcza(2018-04-02 23:38:36):I can make all ONNXIFI names start with `onnxifi` instead, albeit IMO this prefix is too long, often comparable to the name itself (e.g. `onnxifiStatus`)
dzhulgakov(2018-04-02 23:41:05):I guess it's fine, was just curious about the reasoning. Makes sense!
Maratyszcza(2018-04-02 23:41:16):I'll leave it to Compliance Working Group.
houseroad(2018-04-03 05:42:26):@dzhulgakov it's a great topic for the test and compliance working group. I have recorded it.
larroy(2018-04-25 19:30:55):backed -> backend
CLAassistant(2018-02-23 01:59:44):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=552) <br/>All committers have signed the CLA.
AppVeyorBot(2018-02-23 02:07:20)::x: [Build onnx 0.3.1299 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1299) (commit https://github.com/onnx/onnx/commit/4bba249344 by @spidyDev)
linkerzhang(2018-02-24 00:12:51):Please also run https://github.com/onnx/onnx/blob/master/onnx/defs/gen_doc.py to update the documents accordingly.
AppVeyorBot(2018-02-27 20:41:04)::x: [Build onnx 0.3.1326 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1326) (commit https://github.com/onnx/onnx/commit/e7f2680b56 by @houseroad)
AppVeyorBot(2018-02-27 20:52:09)::white_check_mark: [Build onnx 0.3.1327 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1327) (commit https://github.com/onnx/onnx/commit/739ffc0e6c by @houseroad)
AppVeyorBot(2018-02-23 23:59:31)::x: [Build onnx 0.3.1305 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1305) (commit https://github.com/onnx/onnx/commit/4df2670f11 by @yinghai)
ezyang(2018-02-27 03:23:34):I don't want to block people from getting things done, can we confirm with upstream that (1) it is an intended design that you cannot load two protobuf files with the same name, and (2) renaming the protobuf in this way is the correct workaround? A GitHub issue on the protobuf upstream repo would be enough; I did a cursory search but didn't see anything.
yinghai(2018-02-27 04:04:22):@ezyang The restriction is in the code: https://github.com/google/protobuf/blob/master/src/google/protobuf/descriptor_database.cc#L53-L60. And if you take a look of our generated `onnx.pb.cc`, the file name is used as a string as a key to the DescriptionProto, basically file_name. 

`protoc` also doesn't support change on package name like macros. There is a discussion here: https://github.com/google/protobuf/issues/2283

bddppq(2018-02-27 04:09:01):We have found upstream Github issues that have seen the same error as we do (search for "error: file already exists in database"), they all confirm normally you are not supposed to link the same pb.cc files into multiple shared libs, besides we have looked into the generated source files and seen they literally use the filename as key in the registry. The suggested workaround is to either avoid doing this, or statically link libprotobuf.a (so that you end up having multiple registries).
ezyang(2018-02-27 04:12:57):OK, so then the obvious question is, "Why aren't we statically linking protobuf?"
bddppq(2018-02-27 04:15:46):because then you need to hide all the protobuf symbols (otherwise multiple definitions error), and it's not doable for shared libs that need to do RTLD_GLOBAL
ezyang(2018-02-27 04:30:23):Aight, carry on folks.

(mumble mumble clearly someone needs to reimplement protobuf runtime without these silly constraints mumble mumble)
AppVeyorBot(2018-02-27 23:07:47)::x: [Build onnx 0.3.1335 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1335) (commit https://github.com/onnx/onnx/commit/5d42154a1c by @yinghai)
AppVeyorBot(2018-02-28 01:25:57)::x: [Build onnx 0.3.1338 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1338) (commit https://github.com/onnx/onnx/commit/87e26abe75 by @yinghai)
AppVeyorBot(2018-02-28 02:54:57)::white_check_mark: [Build onnx 0.3.1339 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1339) (commit https://github.com/onnx/onnx/commit/bbebdfdb9c by @yinghai)
bddppq(2018-02-26 19:38:49):nit: maybe use onnx-ml_PACKAGENAME.proto to avoid having different processing logic here?
bddppq(2018-02-26 19:39:21):nit: `#pragma once`?
bddppq(2018-02-26 19:42:10):In case of ONNX_NAMESPACE is the default value (onnx), I would like to preserve the output .proto files name as `onnx.proto`, otherwise development git checkout of the onnx repo will almost always have some random .proto files showing as untracked. It's also easier to see whether we need to update the generated (and checked in) onnx.proto files.
bddppq(2018-02-26 19:45:38):Maybe also move the generation of "pb_py" to gen_proto.py? As it's similar to the proxy "pb.h" file.
yinghai(2018-02-27 03:09:32):Hmm, why do we have `onnx.proto` in our src file in the first place? It is supposed to be generated. 
bddppq(2018-02-27 04:14:19):it's easier for users to just grab the onnx.proto file and use it for their own purpose. Not all use cases will need to have their own namespace.
bddppq(2018-02-28 18:00:51):This is needed for both cases. files can be deleted or outdated.
bddppq(2018-02-28 18:01:30):this should be `_pb.h`?
bddppq(2018-02-28 18:02:43):Use onnx-ml_PACKAGENAME to avoid have different processing logic here?
bddppq(2018-02-28 18:04:02):nit: use '--ml' here for clarity
yinghai(2018-02-28 18:13:44):I'm a bit reluctant to do this because simplifying the logic here will require changes to use `onnx-ml_PACKAGENAME` at many other places. 
yinghai(2018-02-28 18:15:42):`_pb.h` is not in the same directory. It's included in L154 I think. 
yinghai(2018-02-28 18:16:53):Both cases? 
bddppq(2018-02-28 18:29:17):lol does it mean we have a proxy to proxy now?
yinghai(2018-02-28 18:32:29):Yes! 
yinghai(2018-02-28 18:32:49):Since we can't change `_pb.h`. 
Maratyszcza(2018-05-29 04:16:58):@onnxbot retest this please
yinghai(2018-06-06 22:55:29):@prasanthpul Any objection to merging this? 
prasanthpul(2018-06-07 12:35:22):@yinghai it was being reviewed by @shschaefer. will check status
yinghai(2018-06-07 16:56:54):@bddppq Is the failure on appveyor expected? Doesn't seem to be related to this PR to me. 
bddppq(2018-06-07 17:37:35):@yinghai doesn't look related to me either
bddppq(2018-05-24 21:01:11):Could you create a macro for clearing error state? I think it will be useful for onnx backend implementations as well. 
Maratyszcza(2018-05-25 02:11:43):Not sure it will be helpful, as this sequence is specific to dynamic loading. For other functionality, it is customary to use `SetLastError`/`errno` combination.
linkerzhang(2018-06-17 23:58:19):why suffix is needed please? looks like a "back door" :)
linkerzhang(2018-06-17 23:58:36):out parameter may be put at the end?
Maratyszcza(2018-06-18 17:25:19):On Linux/Android, onnxifi is loaded with `RTLD_GLOBAL` (this is needed to support backends relying on operator auto-registration). With `RTLD_GLOBAL`, different libraries must have different symbol names to avoid name conflict, and the convention is that the library named `libonnxifi-something.so` would expose symbols with `SOMETHING` suffix, e.g. `onnxInitGraphSOMETHING`.
Maratyszcza(2018-06-18 17:46:08):Moved to the end of argument list. Attn @yinghai for change in the interface.
AppVeyorBot(2018-02-27 02:08:47)::x: [Build onnx 0.3.1317 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1317) (commit https://github.com/onnx/onnx/commit/7c40db094b by @pk-g)
houseroad(2018-03-01 17:37:37):@pk-g friendly ping, could you address the comments, and update the PR?
Thanks.
houseroad(2018-03-08 22:56:18):@pk-g re-ping, could you update the PR?
pk-g(2018-03-09 01:22:55):@houseroad Thanks for the review, addressed the feedback and updated the PR.
AppVeyorBot(2018-03-09 01:30:28)::x: [Build onnx 0.3.1521 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1521) (commit https://github.com/onnx/onnx/commit/d117a209d9 by @pk-g)
AppVeyorBot(2018-03-09 22:16:45)::x: [Build onnx 0.3.1542 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1542) (commit https://github.com/onnx/onnx/commit/ed90b6811e by @pk-g)
AppVeyorBot(2018-03-09 22:34:25)::x: [Build onnx 0.3.1544 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1544) (commit https://github.com/onnx/onnx/commit/4e40fc35a2 by @pk-g)
AppVeyorBot(2018-03-09 23:04:28)::x: [Build onnx 0.3.1548 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1548) (commit https://github.com/onnx/onnx/commit/fbd168eee7 by @pk-g)
AppVeyorBot(2018-03-13 21:16:21)::x: [Build onnx 0.3.1575 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1575) (commit https://github.com/onnx/onnx/commit/47ad86460b by @pk-g)
AppVeyorBot(2018-03-13 21:55:08)::white_check_mark: [Build onnx 0.3.1579 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1579) (commit https://github.com/onnx/onnx/commit/e5cb167f66 by @pk-g)
pk-g(2018-03-16 17:58:48):@houseroad can you please review the latest changes? thanks
bddppq(2018-03-23 19:00:36):@pk-g Any progress on addressing the review comments?
pk-g(2018-03-23 20:13:21):@bddppq : we are gathering more opinion on this op to evaluate if a change of behavior/def is needed. I still believe that it's a better idea to be consistent with numpy/tf on this op, i.e.,  not using output shape for inferring split size and instead use split attribute to define split size directly.
pk-g(2018-03-23 20:55:28):@houseroad can you please clarify what is exactly preventing us from specifying the split attribute in equal size chunks scenario? i.e., is there any fundamental difference between equal/not equal size splits that would require us to NOT specify split attribute in equal size scenario?
houseroad(2018-03-23 22:35:44):@pk-g if you want to equally split the input, you should not have split attribute. Totally depends on the output numbers...
houseroad(2018-03-27 22:32:23):@pk-g could you address my comments? Or you need more information about my suggest?
AppVeyorBot(2018-03-27 22:39:13)::x: [Build onnx 0.3.1901 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1901) (commit https://github.com/onnx/onnx/commit/01cde79ecf by @houseroad)
pk-g(2018-03-28 23:57:22):@houseroad For the purpose of this PR, we can proceed with your suggestion. However, I feel it's worthwhile to get more clarity and opinion from ONNX community on expected behavior of this op. Let's track that design discussion as a separate item and close on this PR. I will update the branch with your suggestion shortly.
AppVeyorBot(2018-03-28 23:57:46)::x: [Build onnx 0.3.1932 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1932) (commit https://github.com/onnx/onnx/commit/172049d8b0 by @pk-g)
houseroad(2018-03-28 23:58:47):@pk-g sure, thanks for the update. :-)
AppVeyorBot(2018-03-29 21:19:07)::x: [Build onnx 0.3.1948 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1948) (commit https://github.com/onnx/onnx/commit/2edc8e725b by @pk-g)
AppVeyorBot(2018-03-29 21:59:05)::white_check_mark: [Build onnx 0.3.1949 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1949) (commit https://github.com/onnx/onnx/commit/c4338737f1 by @pk-g)
AppVeyorBot(2018-03-30 00:34:05)::x: [Build onnx 0.3.1952 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1952) (commit https://github.com/onnx/onnx/commit/5f15222cb5 by @pk-g)
AppVeyorBot(2018-03-30 01:08:31)::white_check_mark: [Build onnx 0.3.1953 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1953) (commit https://github.com/onnx/onnx/commit/f7a6fec2cb by @pk-g)
pk-g(2018-03-30 01:35:31):@houseroad Please feel free to take a look at the updated PR that incorporates your proposed changes. Thanks for the review.
pk-g(2018-04-02 21:49:14):@houseroad Friendly Ping, I'd appreciate it if you could review the updated PR. 
houseroad(2018-04-02 21:50:23):@pk-g I will do it later today. Thanks for reminding me. :-)
AppVeyorBot(2018-04-03 20:24:46)::x: [Build onnx 0.3.2027 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2027) (commit https://github.com/onnx/onnx/commit/5fdb55c652 by @pk-g)
pk-g(2018-04-03 22:38:32):@houseroad  can you elaborate more on what you mean by "add the default value 0 for axis" ? we already have two cases for axis = 0 in 1d test cases. 
pk-g(2018-04-05 17:40:47):@houseroad ping ... can you please provide more details on previous question so we can move forward with completing this PR? thanks
houseroad(2018-04-05 18:06:36):I mean, you should some cases which do not have axis attribute. So you can check the behavior when default value is used.
pk-g(2018-04-06 00:28:52):@houseroad please feel free to review the updated PR with additional test cases for default axis.
houseroad(2018-04-06 05:13:59):@pk-g thanks for the update, I will check it soon. :-)
houseroad(2018-04-06 23:16:48):I mean change the description of attribute axis and regenerate the doc. Thanks! @pk-g 
pk-g(2018-04-06 23:56:53):@houseroad I've updated the PR with latest suggestion on default axis clarification. Please feel free to review so we can close on this PR. Thanks!
AppVeyorBot(2018-04-09 17:13:19)::white_check_mark: [Build onnx 0.3.2201 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2201) (commit https://github.com/onnx/onnx/commit/5885931d4f by @houseroad)
pk-g(2018-04-10 16:56:46):@houseroad shall we merge this PR? thanks!
houseroad(2018-04-10 16:58:44):@pk-g I will do it soon.
AppVeyorBot(2018-04-10 17:48:28)::white_check_mark: [Build onnx 0.3.2231 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2231) (commit https://github.com/onnx/onnx/commit/5c768c75db by @houseroad)
houseroad(2018-02-27 23:12:01):No need to check here. Looks like all s are lists.
houseroad(2018-02-28 01:02:05):Similar here, only list
houseroad(2018-03-16 18:48:22):We should not have the split attribute here, because the split length is determined by outputs length.
houseroad(2018-03-16 18:48:50):We should not have the split here if we use equal split.
pk-g(2018-03-16 18:56:23):Are you suggesting that the following part from defs.cc is not accurate too? 

"If the 'split' argument contains only one element, and that element divides the length of split axis evenly, the tensor is split into equal sized parts with given length."

Shouldn't it be the case that outputs length is determined by 'split' attribute ? 
pk-g(2018-03-16 18:56:43):Are you suggesting that the following part from defs.cc is not accurate too? 

"If the 'split' argument contains only one element, and that element divides the length of split axis evenly, the tensor is split into equal sized parts with given length."

Shouldn't it be the case that outputs length is determined by 'split' attribute ? 
houseroad(2018-03-16 19:09:53):The original doc:
Split a tensor into a list of tensors, along the specified 'axis'. Lengths of the parts can be specified using argument 'split'. Otherwise, the tensor is split to equal sized parts.

If you change the behavior, determine the length according to the split, then it will be a breaking changes. I would like to keep the original behavior. But we need to be more clear on the corner case, i.e., if dim[axis] % length[outputs] != 0, probably tell users dim[axis] % length[outputs] must be 0.
pk-g(2018-03-16 20:32:57):I am not sure why 'split' attributed should not be used in this case even as per the original doc. 'split' attribute is not an optional attribute in original doc and it's defined as:

(From the original doc)
'split' : list of ints
length of each output

So , one interpretation of this definition is that if 'split' is a list only containing one element, that element becomes length of each output.  My proposed change in definition is intended to add more clarity into this interpretation.

Also, a left-to-right logic may define the flow an op more naturally, i.e.,  it may be more natural to use 'input' characteristics  and 'attribute' (in this case 'split') to generate the 'output',  as compared to using 'output' characteristic to define op behavior or possibily interpolate intended 'attribute'.

houseroad(2018-03-16 21:22:19):https://github.com/onnx/onnx/blob/master/onnx/defs/tensor/defs.cc#L154 shows that split is an optional attribute, also if it is required, you will find 'required' in the description of this attribute.

The reasons I don't want to use split in this case:
1) It is a breaking change, which requires backend changes...
2) if we use split to specify the length, this is redundant with the length of the output (also static information), which can also help us figure the length.
linkerzhang(2018-03-17 00:28:44):Thank you very much! Lu.

I agree with Lu here. Sorry that I missed this point. "Split" should be either not available or containing same number of elements as number of outputs, and the sum of split elements should be equal to the dimension value along the axis.

So in this case, it means, there're two outputs specified, either there's no split specified, the input will be split into 2 (specified by number of outputs) evenly, or the split could be a 2-element array specifying each output's length separately, say {2, 4} or even {3, 3}
houseroad(2018-04-03 03:57:19):Nit: remove trailing space?
houseroad(2018-04-03 03:57:22):Also here
houseroad(2018-04-03 03:57:28):Also here
houseroad(2018-04-03 05:01:08):Please explicitly specify the dtype=float here.
houseroad(2018-04-03 05:01:30):Also dtype here.
houseroad(2018-04-03 05:02:30):dtype please
houseroad(2018-04-03 05:02:38):dtype please
houseroad(2018-04-03 05:02:52):dtype please
houseroad(2018-04-03 05:03:22):dtype please
pk-g(2018-04-03 17:47:29):fixed
pk-g(2018-04-03 17:47:34):fixed
pk-g(2018-04-03 17:48:03):fixed
pk-g(2018-04-03 17:52:00):fixed
pk-g(2018-04-03 17:52:11):fixed
pk-g(2018-04-03 17:52:21):fixed
pk-g(2018-04-03 17:52:31):fixed
pk-g(2018-04-03 17:52:42):fixed
pk-g(2018-04-03 17:52:51):fixed
houseroad(2018-04-06 23:12:18):Nit: remove one space here
houseroad(2018-04-06 23:15:45):Nit: add comment, tell users our default axis is 0
pk-g(2018-04-06 23:34:49):Not sure why we need to clarify that? The axis of type INT, and it's clear that INT has a default value of 0. 
houseroad(2018-04-06 23:37:33):https://github.com/onnx/onnx/blob/master/docs/Operators.md#attributes-56, in this case, it's 1. We have to explicit say it.

Even in numpy, the doc says default axis is 0. https://docs.scipy.org/doc/numpy/reference/generated/numpy.split.html
pk-g(2018-04-06 23:49:52):removed
pk-g(2018-04-06 23:53:18):Sure
houseroad(2018-04-09 16:43:57):OPTIONAL ==> static_cast<int64_t>(0)
https://github.com/onnx/onnx/blob/master/onnx/defs/tensor/defs.cc#L327
pk-g(2018-04-09 17:34:15):Sure, thanks! updated the PR.
AppVeyorBot(2018-02-28 00:49:51)::white_check_mark: [Build onnx 0.3.1336 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1336) (commit https://github.com/onnx/onnx/commit/b9d3d88f5d by @anderspapitto)
bddppq(2018-02-28 01:22:45):unordered_set
bddppq(2018-02-28 01:26:39):why add a `list` call here?
anderspapitto(2018-02-28 04:07:00):++ not needed, will remove
anderspapitto(2018-02-28 04:07:08):++ will switch
houseroad(2018-02-28 04:46:40):Nit: we should have an Identity pass... and if pass is empty, we can just simply return the original proto.

Existing solution looks a little bit weird to me. ++
houseroad(2018-02-28 05:00:10):Call it graph_outputs?
houseroad(2018-02-28 05:07:00):To be more clear here, we should say, the output information (including shape/type) is already stored in graph's output.
houseroad(2018-02-28 05:14:09):You may also want to check value_info's name, type and shape. 
anderspapitto(2018-02-28 17:14:51):sure ++
anderspapitto(2018-02-28 17:15:21):++
anderspapitto(2018-02-28 17:15:39):sure ++
yinghai(2018-03-01 17:12:09):Ping. 
AppVeyorBot(2018-03-02 19:54:04)::x: [Build onnx 0.3.1402 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1402) (commit https://github.com/onnx/onnx/commit/8d51222359 by @yinghai)
linkerzhang(2018-02-28 19:28:37):We should define shape inference function for each op (optional), and have a shape inference pass based on all those shape inference functions. We may provide some common shape inference functions for sharing of course.
anderspapitto(2018-03-01 19:49:46):@dzhulgakov @bddppq  biggest comment was addressed - moving to registration-style. Are you happy with that aspect of it?
AppVeyorBot(2018-03-01 19:54:51)::white_check_mark: [Build onnx 0.3.1382 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1382) (commit https://github.com/onnx/onnx/commit/7af1ac4e8b by @anderspapitto)
AppVeyorBot(2018-03-02 20:21:07)::white_check_mark: [Build onnx 0.3.1405 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1405) (commit https://github.com/onnx/onnx/commit/015b5f75b3 by @anderspapitto)
anderspapitto(2018-03-02 20:27:59):what is everyone's thoughts on calling shape inference an optimization pass? should we/is it worth renaming "optimization" to "graph_transformation" or something, on the grounds that this is more of an analysis/annotation pass than an optimization pass?

I slightly lean towards just leaving it, even though it's technically not an optimization.

@bddppq @ezyang @dzhulgakov 
anderspapitto(2018-03-02 23:07:05):@onnxbot retest this please
bddppq(2018-03-05 10:04:35):@anderspapitto Regarding to separate the optimizer and share inference, I'm ok with not doing it in this diff, but IMO it needs to be done eventually.
linkerzhang(2018-03-05 17:44:26):+  OpSchema& TensorInferenceFunction(std::function<void(Node*)> inferenceFunction); 

This is coupling op definition with (experimental) IR implementation, which we indeed should not do. I mean we should still keep ONNX (model format and op definitions) somehow standalone so that different partners could still have their own upper layer implementation. Make sense?
bddppq(2018-03-07 05:32:12):In terms of testing, @houseroad is changing the backend tests to pass output value info to the backend in #588, this can be used to test the inference functions (create a dummy backend like here https://github.com/onnx/onnx/blob/e1d3155ffa270ebff64716ee2db7cb5b33949193/onnx/test/test_backend_test.py#L24, in `run_node` do shape inference and assert they match the output value info and then skip the real test).
anderspapitto(2018-03-08 19:42:30):@linkerzhang @dzhulgakov 

I've prototyped removing the use of the IR representation. Here's an example of a shape inference function for Transpose

- without IR https://gist.github.com/anderspapitto/5dd869c13eeaa9732f4bc0d34db6c068
- with IR https://gist.github.com/anderspapitto/01af001e5a6af3e424ee172a77a7f7e0

I don't think it's reasonable to write shape inference functions in the first style - it is incredibly unwieldy. We need to have some auxiliary data structures/extra processing, and we need to expose it to any third party who wants to add new shape inference functions. And since we need something for this role, I think we might as well use the graph IR which we already have.

Note that Tensorflow has an analogous construct, InferenceContext (https://www.tensorflow.org/extend/adding_an_op#shape_functions_in_c). Any third party who wants to add a shape inference function must use it.


So, to summarize some of the conflicting desires

- @linkerzhang wants the Schemas to not expose the IR
- @dzhulgakov wants shape inference to be part of the Schemas
- I want to use the IR to implement shape inference functions

and it's possible to satisfy any two of those constraints, but not all three
dzhulgakov(2018-03-09 09:59:11):Good point, the first code (with pure protobuf) looks incredibly ugly and useless.

Let's try to figure out a compromise. Some options:

1) we give up 'shape inference to be part of the Schemas' and throw it in the gigantic file (i'd still recommend registry instead of if statements for extensibility)

2) we refactor IR a bit to separate the Graph-related part (connection between nodes and such) from pure attributes part. Note, that for shape inference purposes one doesn't need next/prev-like fields in the Node, just the Node itself. This refactoring would be tedious but one can hope to get a clean subset of simple in-memory single Node wrapper

3) we write a separate wrapper for easy access of attributes and shapes similar to InferenceContext. It pretty much needs to provide input shapes and easy access to attributes

From requirements - I think we should target the ability to invoke shape inference from the dependent framework directly, at least we should definitely do it from Caffe2 to reuse code. But I'd say if one wants to use it - it's fine to use the ir.h stuff.

Thus it seems that 1) is probably the way to go - i.e. giving up on my original suggestion :-\
linkerzhang(2018-03-09 16:56:38):@anderspapitto Thank you so much for the two versions of shape inference codes. I totally agree with you that the first version showing there is NOT good.

I'm now starting to believe more and more that Node protobuf (NodeProto) design should somehow be changed by moving type/shape info from graph level to node level. The current NodeProto design now makes #588 #551 and also this shape inference function design in troubles. Say, for #551 , the reason that we need to put Model containing one node only as parameter when checking backend's capability is because that NodeProto now does not contain enough information, which it should contain. Same for #588 .

As @dzhulgakov also agreed (correct me if my understanding is wrong), the point of not coupling shape inference function with onnx IR too much is because shape inference function will be part of the standard (in op schema), while onnx IR should not (otherwise, the standard will be very heavy to partners/vendors). We may even want to create another repo named as "ONNX runtime” to have ONNX IR and others (optimization, runtime, etc).

So, my personal suggestions:
1. Change NodeProto design to fix the NodeProto missing type/shape issue, or,
2. Have a very light weight version of Node design, which contains NodeProto info + shape/type info.

I personally prefer the 1st one, although it will introduce IR version change, but if it's the right thing to do, let's do it as early as possible.

Thoughts? @dzhulgakov @ezyang @gramalingam 

btw, I'm more than happy to see shape inference function added into ONNX as part of op schema.
bddppq(2018-03-09 18:27:02):@dzhulgakov @anderspapitto @linkerzhang This is not a fair comparison, over half of the code in the protobuf version is extracting inputs' shape info from the graph and at the end put the outputs value info back to the graph, and this is the in-place style I explicitly commented before as not preferred. These code that interact with the graph can be written once in the shape inference engine, and individual inference function will just take the NodeProto and a list of ValueInfoProto as parameters, and return a list of ValueInfoProto.
IMO for our needs the only thing we need to fix in the protobuf api is to provide a dict-lick interface for accessing attributes, and this can be done by adding some util functions like how caffe2 does https://github.com/caffe2/caffe2/blob/afe8f0997210694124fb349e2bf5b9878a987c35/caffe2/utils/proto_utils.h#L281.
anderspapitto(2018-03-09 19:47:44):ok, plenty of things to say

@bddppq I partially agree. Yes, we can write some helper functions to find input shapes/output shapes corresponding to a node, and bundle it together in some sort of struct. Yes, we can also bundle in attributes so that they are easy to access. Yes, we can even write some code to perform a topological sort (in fact we will have to).

But once we do that, we have just reimplemented the IR with a different name.

Also note that if we care about performance (maybe we don't), repeatedly looping over all of these repeated protobuf fields is asymptotically slower than it needs to be. We could fix it by doing all the work up front and sharing -> again, reimplementing the IR.

The IR is basically built for exactly this type of manipulation. I'm open to cleaning it up a little bit, but we need something in this role, and it's really not a huge heavyweight thing, it's basically the same as what you're proposing.

@linkerzhang I'm potentially open to changing the Proto definition. However, currently the Proto definition has the advantage of being normalized (the shapes for each value are only stored in one place). If we inline them into the NodeProto, they will need to be stored with multiple shapes (because we need to access them from any node that takes them as inputs), and now we have complexity from keeping them consistent. The solution to this is an in-memory graph representation using pointers - i.e. the IR.

Regarding a separate onnx-runtime repository - in fact, the optimizer originally was in such a separate repo, see here https://github.com/onnx/onnx-caffe2/commit/cbc4c1727c85d5e5284ed7bafa0670fbfd96ad61.
We moved it into onnx/onnx for ease of maintenance. We could potentially move it out again.

@dzhulgakov I agree with all your points.

bddppq(2018-03-09 19:56:05):@anderspapitto No I'm not suggesting creating another structs for bundling these things, NodeProto and ValueInfoProto both exists already. Having an IR is great for more complex graph manipulation operations, but in shape inference you mostly just need to access the Attributes and ValueInfo.
Why do we need to perform a topological sort? We already explicitly check all nodes are in topological sorted order in checker.
bddppq(2018-03-09 20:02:37):And I don't think we should decouple shape inference and schemas, reason being shape inference (although named "inference") is actually part of the spec that describes the expected behavior of an operator. if you look at the doc strings in existing schemas, there are many places saying "output is 1D", "output and input have same shape", we should use shape "inference" to codify these.
anderspapitto(2018-03-09 20:17:55):@bddppq regarding why you need topological sort, imagine you have an identity op ID that just passes through its input, and a graph that looks like

input_x -> ID_1 -> intermediate_y -> ID_2 -> output_z

Assume that only the shape of input_x is provided initially.

shape inference must be applied to ID_1 before it's applied to ID_2, so that shape inference for ID_2 knows the shape of intermediate_y.
anderspapitto(2018-03-09 20:28:29):discussed a bit offline with @bddppq, I think I can make everyone happy
anderspapitto(2018-03-15 19:05:56):addressed a bunch of concerns
AppVeyorBot(2018-03-15 19:15:10)::white_check_mark: [Build onnx 0.3.1610 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1610) (commit https://github.com/onnx/onnx/commit/c8d592572c by @anderspapitto)
AppVeyorBot(2018-03-19 18:40:24)::white_check_mark: [Build onnx 0.3.1647 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1647) (commit https://github.com/onnx/onnx/commit/0792e08378 by @anderspapitto)
AppVeyorBot(2018-03-19 20:50:47)::white_check_mark: [Build onnx 0.3.1653 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1653) (commit https://github.com/onnx/onnx/commit/f49dd0d4a6 by @anderspapitto)
anderspapitto(2018-03-19 22:29:22):If everyone's pretty satisfied, I'd love to merge this and start adding in implementations for all the operators.

@linkerzhang @dzhulgakov 
AppVeyorBot(2018-03-19 23:12:09)::white_check_mark: [Build onnx 0.3.1667 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1667) (commit https://github.com/onnx/onnx/commit/23c33fcefd by @anderspapitto)
AppVeyorBot(2018-03-21 18:49:46)::x: [Build onnx 0.3.1719 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1719) (commit https://github.com/onnx/onnx/commit/519687d8e6 by @anderspapitto)
AppVeyorBot(2018-03-21 20:53:28)::x: [Build onnx 0.3.1726 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1726) (commit https://github.com/onnx/onnx/commit/c2d1d62376 by @anderspapitto)
anderspapitto(2018-03-21 21:13:04):@dzhulgakov each of your comments was either addressed here or added to the follow-up task https://github.com/onnx/onnx/issues/632

@bddppq I added a shape-inference version of the backend tests, as you suggested
AppVeyorBot(2018-03-21 21:32:49)::x: [Build onnx 0.3.1728 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1728) (commit https://github.com/onnx/onnx/commit/7e6ebd1c67 by @anderspapitto)
AppVeyorBot(2018-03-21 23:33:51)::x: [Build onnx 0.3.1738 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1738) (commit https://github.com/onnx/onnx/commit/b8dc41d63c by @anderspapitto)
AppVeyorBot(2018-03-22 00:24:36)::white_check_mark: [Build onnx 0.3.1743 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1743) (commit https://github.com/onnx/onnx/commit/a0d0361fff by @anderspapitto)
AppVeyorBot(2018-03-22 16:52:55)::x: [Build onnx 0.3.1757 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1757) (commit https://github.com/onnx/onnx/commit/83af550251 by @anderspapitto)
anderspapitto(2018-03-23 00:09:07):oops, accidentally closed
anderspapitto(2018-03-23 00:25:18):@onnxbot retest this please
anderspapitto(2018-03-23 18:23:07):@yinghai i'll address your comments in my next diff
dzhulgakov(2018-04-03 16:39:56):@anirudhacharya @lupesko FYI
houseroad(2018-02-28 18:58:00):Shall we move the real logic into a separate method for each op? Otherwise, this function may have too many lines...
houseroad(2018-02-28 19:00:20):Nit: trans1 == trans
dzhulgakov(2018-02-28 19:21:19):it's not very scalable. Shall we have a nice registration-style interface for each op? You can take inspiration from C2 or TF code - it's probably best to put it next to the op declaration and also provide helpers for common patterns.

Alternative is having a small DSL - but it might be an overkill
houseroad(2018-02-28 19:37:55):Here is how TF handles shape inference: https://www.tensorflow.org/extend/adding_an_op#shape_functions_in_c

C2 and TF both register their inference functions in the schema.
anderspapitto(2018-02-28 19:43:06):++
anderspapitto(2018-02-28 19:43:51):I would call it more of a style issue than a scalability issue - there's still N chunks of logic, it's just a matter of where they live.

That said, yeah I'm fine with a registration-type thing, I just wanted to keep the first diff simple.

Also, having a DSL or not is an orthogonal issue to refactoring into a registration style - and also we can start with direct code like this and introduce a DSL a little down the line if we like.
bddppq(2018-03-01 21:15:51):Here it's better to just check whether an Operator has inference function and yes then apply it (or make the default inference function nop so you can blindly apply it). Otherwise, every time we add a inference function to an operator we need to also remember to change here.
bddppq(2018-03-01 21:30:43):nit: Maybe call it `ValueInfoInferenceFunction`?
bddppq(2018-03-01 22:08:24):Hmm such in-place interface looks to (maybe just) me another styling issue, e.g. you can easily forget to check `out->sizes().empty()`. What would be better is a more functional interface that takes the operator and inputs' shapes and return output's shapes.
anderspapitto(2018-03-01 23:38:25):++ sure, I'll go with whatever name. This is what Caffe2 calls it
anderspapitto(2018-03-01 23:43:04):yeah, I can probably do that. It does require exposing a little more of ir.h to schema.h though
anderspapitto(2018-03-02 00:59:52):++ yeah I just have add some machinery to map from kTranspose -> OpSchema::Transpose
linkerzhang(2018-03-02 04:35:50):Dumb question: The Node does not contain input tensor data, right? if yes, input tensor data may be needed in some cases to calculate the outputs' shape. 
linkerzhang(2018-03-02 04:39:05):why not ShapeInferenceFunction?
dzhulgakov(2018-03-02 10:01:48):one question with using Node - does it support standalone creation (without owning Graph?)

I also wonder whether Node can be refactored a bit to have only minimal set of helper functions for accessing attributed, not all graph-associated machinery in this case. The concern is of keeping the schemas part very light-weight. But maybe it's ok
dzhulgakov(2018-03-02 10:03:07):yes, there can be data dependent ops. But the point of shape inference is to be able to operate on GraphProto only - i.e. with shapes but without actual data. Thus it might produce "undefined" shapes along the way.

@anderspapitto - let's document it somewhere in comments
dzhulgakov(2018-03-02 10:05:02):yeah, we should have more functional interface I'd say. Also, there are probably some easy helper functions we can do to help writing shape inference functions
dzhulgakov(2018-03-02 10:05:55):I think we can assume that dispatch mechanism does the right thing. There might be some ops without associated constant type in ir.h and we should be able to still handle those
dzhulgakov(2018-03-02 10:11:24):so `kind()` is of `Symbol` type -> it has `toString()` method to get the string back. And note that it supports even statically unknown types. So then you can lookup schema (please do proper unknown handling) and do dispatch in generic terms.
anderspapitto(2018-03-02 18:32:05):sure, I'll do ShapeInferenceFunction
anderspapitto(2018-03-02 18:32:10):++
anderspapitto(2018-03-02 18:33:23):yes, all standard abstraction/code reuse strategies apply, since they're just functions
anderspapitto(2018-03-02 18:39:20):@linkerzhang in general, inputs are not available, but those that are initializers to the graph are (see addInitializer in ir.h)

@dzhulgakov "undefined" shapes isn't anything special, it's basically the default. But sure, I can add a comment that shape inference is best-effort and not guaranteed to figure out everything
anderspapitto(2018-03-02 18:42:26):no, it must live in a Graph. that's not an obstacle though, since we're always operating on a full Graph and all the machinery is already in place to create it.

If we do registration-style, then schemas need to understand the bits relevant to shape inference. If we separate shape-inference away from the Schema code, then it doesn't. That's a fundamental tradeoff here.
bddppq(2018-03-05 08:47:15):Should also pass `domain` to do the schema lookup.
bddppq(2018-03-05 08:53:14):Better to sanity check the size of `out_types` and `out_dims` are <= `n->outputs().size()`. 
bddppq(2018-03-05 08:55:28):We should give a warning here if the inferred one doesn't match the one stored in NodeProto.
bddppq(2018-03-05 09:23:35):@linkerzhang @anderspapitto Note this is not only doing shape inference but also types.
linkerzhang(2018-03-05 17:46:41):Decouple the shape inference function (which is part of op definition) with IR please.
smessmer(2018-03-07 00:32:09):This breaks encapsulation. If you need write access, it makes more sense to keep it more strict, for example have a 

    setSize(index, Dimension)

or a

    setSizes(vector)
smessmer(2018-03-07 00:34:46):-`typedef` +`using`
smessmer(2018-03-07 00:37:26):nit: `++i` instead of `i++` can lead to better perf on old compilers
smessmer(2018-03-07 00:39:11):`emplace_back` is never worse and sometimes better for perf than `push_back`, depending on how the `Dimension` type is implemented.
smessmer(2018-03-07 00:41:06):Can we use `make_unique`? It's better for exception-safety.
smessmer(2018-03-07 00:42:41):nit: Making classes and structs `final` makes sure your class doesn't fall in one of the many inheritance traps (say not having virtual destructors) because someone inherited from it and you didn't anticipated it.
Also, making it `final` allows the compiler to better optimize some code like type casts.

In this particular case, it doesn't apply because the destructor of `OptimizePass` is already virtual, but it's still a good general C++ rule, and the compiler optimization point still applies.
bddppq(2018-03-07 00:51:15):`make_unique` is >=c++14, but sure you can have a fallback implementation. But IMO here doesn't need unique_ptr, the value type of `passes` should be `const OptimizePass`.
linkerzhang(2018-03-17 00:03:13):Great! Personal suggestion: also put the output typeproto as an interface here?
linkerzhang(2018-03-17 00:04:22):if putting the output type also as an interface of InferenceContext, then the signature here could be changed to 

typedef bool (*InferenceFunction)(InferenceContext&); return value indicates whether shape inference done successfully or not.
linkerzhang(2018-03-17 00:05:00):minor suggestion: Just return the function and let the caller to call the function?
anderspapitto(2018-03-17 01:14:35):what types of failure would you like to signal? I think it should not be required that all output shapes are fully inferred.
anderspapitto(2018-03-17 01:15:01):yes, that will be better
anderspapitto(2018-03-17 01:15:18):ok, will do
linkerzhang(2018-03-19 16:00:12):there may be cases that output shapes can't be inferred I think. In those cases, we may want to return "false" to callers to indicate that. Ideally, it would be better to have a "Status" class representing both flag (true|false) and some error message (std::string) if any. Make sense please?
anderspapitto(2018-03-19 18:21:34):@bddppq are you ok with this? Since you initially asked for the return type to be the vector of output types itself.
jamesr66a(2018-03-19 19:54:22):enum class
jamesr66a(2018-03-19 19:56:23):Why the braces here?
anderspapitto(2018-03-19 20:31:39):++
anderspapitto(2018-03-19 20:33:39):it's just organizational, and in some similar places (though not this one) it makes it clear that any temporary variables inside are in fact temporary.

I could take it out if you want me to
dzhulgakov(2018-03-20 06:51:19):why status - we can use exceptions for error propagation (as the rest of schema checking does)
dzhulgakov(2018-03-20 06:53:23):I wonder whether we can have a simple wrapper that typecasts the attribute values to the right type. It'd make implementing functions much easier. Those helpers might be more isolated that ir.h and be just `getAttr<vector<int>>("perm", optional_default)` or something
dzhulgakov(2018-03-20 06:54:43):shall we have some helper function for creating TensorProto_Tensor too? otherwise this mechanics of protobuf are too cumbersome to operate.
dzhulgakov(2018-03-20 06:56:52):it can be const pointers to AttributeProto too
dzhulgakov(2018-03-20 06:57:51):this is wrong - you can't skip inputs - the size of allInputTypes_ should be the same as n->input_size()
dzhulgakov(2018-03-20 06:58:05):check that it's non null?
dzhulgakov(2018-03-20 06:59:46):should it be merging of data? what if only type but not shape is specified in the valueInfoProto? in this case we should merge. Also after inference details might be more specified that they were before. Ideally we should check compatibility and override if necessary
dzhulgakov(2018-03-20 07:00:44):more tests? e.g. for corner cases when value_info is already populated with something
bddppq(2018-03-20 08:01:31):@dzhulgakov Rip off `GetSingleArgument`/`GetRepeatedArgument` from c2? :-)
bddppq(2018-03-20 08:06:27):it should be fairly easy to hook it up with the backend tests (see my comment above re. `DummyBackend`)
anderspapitto(2018-03-20 16:17:14):I have no preference, @linkerzhang requested it. @linkerzhang are you ok with switching back to void return and using exceptions for errors?
anderspapitto(2018-03-20 16:18:25):sure, this is just the initial example though. I'll start adding reusable helpers in the next diff when I start to add more supported operators.
anderspapitto(2018-03-20 16:19:46):sure, but same as above - this is a small thing, we can iterate on it.
anderspapitto(2018-03-20 16:19:50):++
anderspapitto(2018-03-20 16:20:01):++ will look at it again
anderspapitto(2018-03-20 16:21:01):it's never null, it defaults to a no-op lambda
anderspapitto(2018-03-20 16:21:49):++
anderspapitto(2018-03-20 16:22:32):++ yes, I'd like to add this too but it's not necessary to get the first version landed
jamesr66a(2018-03-20 17:01:48):Yeah it's confusing because there are actually no temporary variables in this immediate block and anonymous blocks are typically used for RAII guards
dzhulgakov(2018-03-21 06:51:29):Rest of ONNX code uses exceptions (e.g. checker) - so unless there are strong concerns I'd prefer that, for consistency
dzhulgakov(2018-03-21 21:20:26):some follow ups are better to just include in code as TODO. E.g. in this place - regarding merging of partially specified shapes
linkerzhang(2018-03-21 21:27:20):personal suggestion:
change virtual void setAllOutputTypes(std::vector<TypeProto_Tensor>&& allOutputTypes) = 0; to 
virtual std::vector<const TypeProto_Tensor *>& getAllOutputTypes() = 0; or
virtual std::vector<const TypeProto_Tensor *>* getAllOutputTypes() = 0; if folks have concern on using non-const reference :)
linkerzhang(2018-03-21 21:30:19):I was planning to introduce a "status" class into ONNX to avoid exceptions, but haven't gotten chance to do that. I'd suggest to move to the direction, although it takes time to change all others to return status, I can do that later.
anderspapitto(2018-03-21 22:51:32):to be clear, setAllOutputTypes is used by a shape inference function to tell the InferenceContext what the inferred types are.

the problem with the first suggestion (`virtual std::vector<const TypeProto_Tensor *>& getAllOutputTypes() = 0;`) is that the `TypeProto_Tensor`s have no owner - they probably lived on the stack inside the shape inference function, which is now returning.

the problem with the second suggestion is that only allows reading the values, not setting them.

I might have a neater solution though
anderspapitto(2018-03-21 22:51:49):++ I'll add it here as well
houseroad(2018-03-21 23:32:30):Probably more text, where to store the information, what will happen if the original information is incorrect?
houseroad(2018-03-22 05:00:25):If not matching with pre-existing value_info, we can provide two options: 1) error out, 2) override. We can use the former one as the default, which is safer.
houseroad(2018-03-22 05:01:24):test_transpose_preeisting_incorrect_shape?
yinghai(2018-03-23 05:43:19):Why do we have this `Impl` instead of just having `InferenceContext`? 
yinghai(2018-03-23 05:43:40):nit

`const AttributeProto* getAttribute(const std::string& name) const override {`
yinghai(2018-03-23 05:44:09):nit, we don't need `ONNX_NAMESPACE` here? 
yinghai(2018-03-23 05:44:56):`const auto&` to avoid copy
yinghai(2018-03-23 05:47:08):`resize`? 
http://en.cppreference.com/w/cpp/container/vector/resize
anderspapitto(2018-03-23 18:21:39):InferenceContext is just an interface - we need some concrete implementation of it
anderspapitto(2018-03-23 18:22:18):++
anderspapitto(2018-03-23 18:22:26):++
anderspapitto(2018-03-23 18:22:44):++
anderspapitto(2018-03-23 18:22:47):++
gramalingam(2018-04-03 18:43:42):Thanks for initiating the shape inference work.

One suggestion: I think it will be useful to extend “InferenceContext” by adding a method that allows the shape-inference function to generate a new symbol (name) representing a new unknown dimension. e.g.:
	virtual string GenerateNewDimParam() = 0;

Explanation: TensorShapeProto::Dimension allows the use of “string dim_param” to denote a dimension of unknown size. If some op OP1 returns a one-dimensional tensor whose size will be known only at runtime, it shape-inference function can return (“N”) as the output shape. However, different invocations of OP1 can return tensors of different sizes. If we have two occurrences of OP1 in a graph, we would like shape-inference to return different dimensions for the two cases: say (“N1”) and (“N2”). Only the caller has the necessary context to ensure uniqueness of these names.

anderspapitto(2018-04-03 19:44:21):Yes, that could be useful. An alternative is being discussed here https://github.com/onnx/onnx/pull/681#discussion_r178886494 - reserve the empty string as a special value that is treated as a unique name
gramalingam(2018-04-03 20:22:56):While an empty string may be convenient in many situations, it is not sufficient for all situations. E.g., if an operator takes an input with shape [N] and returns an output with shape [N,N]. Here, is the input shape is [""], then we need to create a name to represent the output shape. If we use ["", ""], we will lose the correlation between the two dimensions. (Even in the context of the other PR: I would rather aim for simplicity in the protobuf file format ... introducing special exemptions like this is more likely to introduce some problem somewhere because someone didn't read this special condition.)
anderspapitto(2018-04-03 20:33:16):That's true. I'll probably add it at some point then - if you have a more urgent need feel free to also to send a PR with GenerateNewDimParam.
AppVeyorBot(2018-02-28 23:58:59)::white_check_mark: [Build onnx 0.3.1364 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1364) (commit https://github.com/onnx/onnx/commit/928ec73132 by @anderspapitto)
anderspapitto(2018-03-01 21:56:19):@bddppq approve? the failure is in jenkins, has nothing to do with this diff
anderspapitto(2018-03-01 18:30:31):open question: how many of these configurations do we actually want to run? I've added a macos version of everything, which doubles the number of configurations from 8 to 16. Do we want to add just one or two instead?
bddppq(2018-03-01 18:53:33):let's remove all the ninja ones, it's for devs and will go away once we switched to call cmake in settup.py 
bddppq(2018-03-01 18:56:17):Both are not needed.
@smessmer Actually I don't think onnx-coreml needs these either.
bddppq(2018-03-01 19:01:58):nit: it's (in general) better to resolve symlink before going to parent dir. Here is the cross-platform way of doing it (thanks to Python and @dzhulgakov):
`script_path=$(python -c "import os; import sys; print(os.path.realpath(sys.argv[1]))" "${BASH_SOURCE[0]}")`
bddppq(2018-03-01 19:02:53):We need this. travis builds use environment variables to separate build caches.
bddppq(2018-03-01 19:04:32):Also do brew install python in case of python2 to make sure it's really using the brew python2 (note for brew python2 you need to do `export PATH=/usr/local/opt/python/libexec/bin:$PATH"`)
bddppq(2018-03-01 19:33:21):python3.6 recommends using `python3.6 -m venv` to create virtualenv, but I'm not sure what's the subtle differences between these couple ways. I think the current way should be fine as onnx doesn't have crazily dependencies, probably better to just add a comment here.
smessmer(2018-03-01 19:35:30):Below you're installing onnx, which is already happening in install.sh. No need to do it again.
bddppq(2018-03-01 19:35:32):nit: why setting such a timeout?
smessmer(2018-03-01 19:38:13):Basically we want to add as many as we can without causing too much of a Travis CI backlog. My guess is that more than 5 configurations will cause jams, but that's more a feeling than something experimentally verified ;)
smessmer(2018-03-01 19:39:33):This way works both for python 2 and 3. Using venv, we'd have to branch based on python version.
smessmer(2018-03-01 19:40:36):Uploading the cache with the default timeout fails too often
anderspapitto(2018-03-01 19:58:26):++
anderspapitto(2018-03-01 19:58:31):++
anderspapitto(2018-03-01 19:58:37):++
anderspapitto(2018-03-01 20:01:22):++ should we be also setting it in onnx-coreml then?
anderspapitto(2018-03-01 20:03:09):++
anderspapitto(2018-03-01 20:03:13):++
anderspapitto(2018-03-01 20:03:30):++
smessmer(2018-03-01 21:59:37):I'm on it
smessmer(2018-03-01 22:02:43):Not sure if this is what you want. You probably rather want to change the `PYTHON_DIR` above.
bddppq(2018-02-28 21:28:04):I'm also changing the "Indices" output of TopK to be of type int64.
Previous schema didn't provide a way to specify the "Indices" output's type so it's ambiguous.
bddppq(2018-03-02 19:56:01):ping @jamesr66a @linkerzhang @ebarsoum for schema change
bddppq(2018-03-07 00:09:01):re-ping @jamesr66a @linkerzhang @ebarsoum for schema change
AppVeyorBot(2018-03-27 23:50:53)::white_check_mark: [Build onnx 0.3.1902 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1902) (commit https://github.com/onnx/onnx/commit/9a2f1689ad by @bddppq)
ezyang(2018-03-01 16:07:39):Hi @linkerzhang.

>  In this PR, I removed the attribute reference, however, I just realize that it should be added back since it's needed even for 1st motivation. We need to take function similar as operator definition, which contains attribute declaration but no data.

Yeah, this is a good reason to not take on too many motivations, because you might discover that you shouldn't actually solve them via the same mechanism.

Remember that for motivation 1 (defining composite operators in the ONNX spec), the actual function definitions never occur in model files; they occur in the ONNX operator database. So there is no reason to *necessarily* force user-defined functions and composite operator functions to coincide, and indeed, in the way that ONNX is setup right now, if you want this particular technical proposal (add a FunctionDef accessible from ModelDef) to work for motivation 1, there's a lot of other work you have to do (how are you getting the composite operator defs to the consumers? What if a consumer is operating on an older version of the ONNX library?)

Let me offer a counter-proposal, which was communicated to me from @dzhulgakov. For motivation 1, we should defer the question of designing a protobuf format for backends (where composite operator definitions would live.) Instead, we should treat this as a source transformation problem: an operator unfolding is defined via a *program* which translates from a composite operator into its constituent elements. There are a few good properties of this scheme:

1. You side-step the problem of how to get operator definitions to backends. Instead, the onus is on the *user* to run the program to desugar the operators, and then pass it on to the backends. This decoupling means that we can easily update the desugarer independently of the backend (which might be something like Windows OS and cannot really get upgraded easily.)

2. You side-step the problem of defining an expressive enough language to represent function operators. As I mentioned in our call, once you have attribute references, you'll have people wanting to compute on the attribute references, and anything you do you'll have to implement in your interpreter.

The only problem is that this doesn't let you solve motivation 2 (indicating shared subgraphs upon export), but that's why we are still going to take this technical proposal (adding FunctionDef to model proto), but without attribute references. And since I know solving motivation 2 is a priority for MS, I'd suggest deferring motivation 1 for now.

It will be helpful for people reading the conversation if the wiki page gets updated.

P.S. I do want to bikeshed the details of the protobuf but let's get this part established first.
linkerzhang(2018-03-01 17:36:04):Thank you @ezyang for the comments! Let's discuss more on motivation 1 later and push this PR forward (without attribute reference) to solve motivation 2.
ezyang(2018-03-02 23:50:44):OK, protobuf bikeshedding time. I wonder why we need FunctionDefProto, as opposed to reusing GraphProto. They are literally identical once you remove the things necessary for attribute proto references (in the PR as submitted right now, FuncAttrDeclProto hasn't been removed, and ParameterDeclProto is a new thing; if you revert those changes you have GraphProto).

I think if we use GraphProto instead, this will be a really simple and really awesome change :)
AppVeyorBot(2018-03-03 01:45:56)::white_check_mark: [Build onnx 0.3.1420 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1420) (commit https://github.com/onnx/onnx/commit/e0dc3a3910 by @linkerzhang)
ezyang(2018-03-06 16:34:44):OK, I'm liking the protobuf structure. Now for details:

1. Are subgraphs of a graph lexically scoped? That is to say, can I refer to inputs of the parent graph inside the graph? They probably should be, for consistency with graph attributes.
2. Can we have a reference implementation of graph inlining, just to validate the idea that you can desugar away function graphs if a backend does not support them directly?
bddppq(2018-05-09 06:46:17):I'm closing this since proto changes for adding functions have already been done in another PR #802
gramalingam(2018-03-01 20:06:45):Are we removing attribute-references inside functions? If so, shouldn't we remove this field also? It can't really be used, right?
Yangqing(2018-03-22 00:34:30):One thing I'd like to point out is that, we might want to put a reserved word prefix to denote functions, if we want to refer to functions by name. This is for extensibility purposes:

(1) say we do not have an operator "Foo" defined now.
(2) one user defines a model that has a function called Foo.
(3) some time later, we add an operator "Foo" to the onnx operator set.

This would cause the model definition somewhat ambiguous. We should probably explicitly say

(1) functions should have an explicit prefix that shall not be used by operator names, or
(2) functions take precedence when resolving operator types.
linkerzhang(2018-03-26 03:06:50):Thank you very much @Yangqing  for the great comments!

In my personal thinking, there're two kinds of functions, one is common function in ONNX domain, which will be part of ONNX spec, same as all current primitive ops, another is customized function which will have its own domain and also its body defined in a model, like an "operator" declaration defined in a model with body (subgraph). In this way, there should be no name conflict based on different namespace (domain). Conflicts should be detected when registering an common function or adding a customized function into a model. Make sense please?

btw, @ezyang helped me a lot here and kindly suggested me having motivation 2 resolved firstly (https://github.com/onnx/onnx/wiki/Proposal----Adding-Function-into-ONNX). That's why in this PR, only part of changes https://github.com/linkerzhang/onnx/blob/kezhan/add_function_private/onnx/onnx.in.proto is contained. However, after more thinking this thru, I realize that we might still take motivation 1 together into consideration since same mechanism should be designed for the 2 motivation in my opinion, instead of having different ones for each.

Let's sync and understand more on it please.

AppVeyorBot(2018-03-01 06:29:29)::x: [Build onnx 0.3.1373 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1373) (commit https://github.com/onnx/onnx/commit/7a554a1733 by @fumihwh)
AppVeyorBot(2018-03-01 06:39:57)::white_check_mark: [Build onnx 0.3.1374 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1374) (commit https://github.com/onnx/onnx/commit/7f57c3be3f by @fumihwh)
houseroad(2018-03-01 22:52:48):@onnxbot test this please
houseroad(2018-03-01 22:53:26):@onnxbot add to whitelist
AppVeyorBot(2018-03-02 02:46:47)::x: [Build onnx 0.3.1394 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1394) (commit https://github.com/onnx/onnx/commit/05210a9641 by @fumihwh)
houseroad(2018-03-05 21:04:40):@onnxbot test this please
AppVeyorBot(2018-03-09 02:46:00)::white_check_mark: [Build onnx 0.3.1525 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1525) (commit https://github.com/onnx/onnx/commit/f60d17dce1 by @fumihwh)
houseroad(2018-03-16 18:49:41):@onnxbot retest this please
AppVeyorBot(2018-03-17 03:38:31)::white_check_mark: [Build onnx 0.3.1641 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1641) (commit https://github.com/onnx/onnx/commit/83605f7202 by @houseroad)
houseroad(2018-03-17 06:15:55):@fumihwh sorry for the late review. Overall, the code looks good. Only one thing you probably should add to description of the AveragePool op: when computing the average of one window, the pad is not counted into the divisor. (I think most of the existing models implement this way, also your code does the same thing.)
fumihwh(2018-03-17 13:40:39):@houseroad AveragePool and MaxPool are using same base doc.
If I want to add description for AveragePool, I have two choices:
1. Copy and paste to make individual AveragePool and MaxPool ONNX_OPERATOR_SCHEMA
2. Add arg to PoolOpSchemaGenerator called `additional description`

I prefer 2. What do you think?
houseroad(2018-03-19 17:05:50):@fumihwh sure, feel free to do it, as long as it is easy to use and maintain. 
houseroad(2018-03-08 18:16:27):Please notice that: auto_pad is a deprecated attribute, we encourage users to use explicit padding.

Please also add how we calculate the output_shape, when we use explicit padding:
Some reference is here:
http://pytorch.org/docs/master/nn.html#torch.nn.AvgPool2d
I think most frameworks compute the output shape using this formula. (Checked Caffe2, PyTorch, MXNet)
houseroad(2018-03-08 18:17:44):One comment here: input_shape (1, 1, 5, 5)
houseroad(2018-03-08 18:29:19):Because auto_pad is deprecated, please provide (additional) examples with explicit padding.
fumihwh(2018-03-09 01:44:42):@houseroad Actually, two formulas are same. ( I just ignored I should add pad_spatial to input_spatial_shape in formula.)
Anyway, I will fix it.

AppVeyorBot(2018-03-02 02:55:05)::white_check_mark: [Build onnx 0.3.1395 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1395) (commit https://github.com/onnx/onnx/commit/8d08fa6ac0 by @fumihwh)
houseroad(2018-03-05 21:04:13):@onnxbot test this please
fumihwh(2018-03-08 01:29:48):@houseroad 
About oop and reusing function same in `averagepool.py` and `maxpool.py`,
I have no idea where should I put them (`_get_pad_shape`, `_get_output_shape`, `_pool`) into.
Should I put them into `utils.py`? or a new file? I am not sure.
Any advice?
houseroad(2018-03-08 01:34:17):You can create another file called pool_op_common.py in folder https://github.com/onnx/onnx/tree/master/onnx/backend/test/case/node, since these functions are only used in our test cases.
fumihwh(2018-03-22 03:40:53):@houseroad I've updated this pr.
houseroad(2018-03-23 05:54:43):@onnxbot retest this please
houseroad(2018-03-23 06:13:18):@fumihwh all looks good. Nit: we should also mention that the max value does not consider the pad. 
houseroad(2018-03-05 18:41:40):@onnxbot test this please
AppVeyorBot(2018-03-05 19:01:31)::white_check_mark: [Build onnx 0.3.1425 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1425) (commit https://github.com/onnx/onnx/commit/3eb6b7038d by @houseroad)
AppVeyorBot(2018-03-05 22:47:12)::white_check_mark: [Build onnx 0.3.1448 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1448) (commit https://github.com/onnx/onnx/commit/8ce5af1c28 by @houseroad)
AppVeyorBot(2018-03-09 01:38:28)::white_check_mark: [Build onnx 0.3.1522 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1522) (commit https://github.com/onnx/onnx/commit/4e8950a5ce by @fumihwh)
CLAassistant(2018-03-01 19:07:45):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=575) <br/>All committers have signed the CLA.
AppVeyorBot(2018-03-01 19:08:28)::x: [Build onnx 0.3.1381 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1381) (commit https://github.com/onnx/onnx/commit/50e175451c by @jendrikjoe)
houseroad(2018-03-05 18:40:42):@onnxbot test this please
AppVeyorBot(2018-03-05 18:51:30)::x: [Build onnx 0.3.1424 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1424) (commit https://github.com/onnx/onnx/commit/08b5e91c13 by @houseroad)
AppVeyorBot(2018-03-05 22:36:49)::x: [Build onnx 0.3.1447 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1447) (commit https://github.com/onnx/onnx/commit/9762144456 by @houseroad)
AppVeyorBot(2018-03-06 20:07:37)::x: [Build onnx 0.3.1467 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1467) (commit https://github.com/onnx/onnx/commit/312f36b386 by @jendrikjoe)
AppVeyorBot(2018-03-06 20:37:54)::x: [Build onnx 0.3.1470 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1470) (commit https://github.com/onnx/onnx/commit/e4939b65e6 by @jendrikjoe)
AppVeyorBot(2018-03-06 21:40:38)::x: [Build onnx 0.3.1471 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1471) (commit https://github.com/onnx/onnx/commit/5f192524e1 by @jendrikjoe)
AppVeyorBot(2018-03-06 22:00:22)::x: [Build onnx 0.3.1472 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1472) (commit https://github.com/onnx/onnx/commit/b274e96df6 by @jendrikjoe)
AppVeyorBot(2018-03-06 22:11:01)::white_check_mark: [Build onnx 0.3.1473 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1473) (commit https://github.com/onnx/onnx/commit/7c098a7963 by @jendrikjoe)
jendrikjoe(2018-03-08 17:12:45):@houseroad great thanks for the feedback :) 
houseroad(2018-03-23 18:58:53):@jendrikjoe we have landed the PRs on averagepool and maxpool. You can adjust your pr based on the existing functions. I would suggest provide several representative cases. :-)
houseroad(2018-03-27 22:31:24):@jendrikjoe If you could update the PR according to existing test, it will be awesome!
jendrikjoe(2018-03-28 00:08:39):@houseroad working on it :)
AppVeyorBot(2018-03-28 06:54:51)::x: [Build onnx 0.3.1915 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1915) (commit https://github.com/onnx/onnx/commit/2ac5c85768 by @jendrikjoe)
AppVeyorBot(2018-03-28 07:45:16)::x: [Build onnx 0.3.1921 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1921) (commit https://github.com/onnx/onnx/commit/ac5253d480 by @jendrikjoe)
AppVeyorBot(2018-03-29 00:10:20)::white_check_mark: [Build onnx 0.3.1933 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1933) (commit https://github.com/onnx/onnx/commit/a4f061f21b by @jendrikjoe)
bddppq(2018-05-23 06:06:14):Closing since these have been added by @fumihwh in #572 and #573 
AppVeyorBot(2018-03-02 20:00:57)::x: [Build onnx 0.3.1404 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1404) (commit https://github.com/onnx/onnx/commit/2752597c8e by @bddppq)
AppVeyorBot(2018-03-03 00:25:44)::x: [Build onnx 0.3.1413 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1413) (commit https://github.com/onnx/onnx/commit/326750581c by @bddppq)
AppVeyorBot(2018-03-03 00:41:59)::x: [Build onnx 0.3.1415 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1415) (commit https://github.com/onnx/onnx/commit/7ed3fedffd by @bddppq)
AppVeyorBot(2018-03-03 01:00:26)::x: [Build onnx 0.3.1419 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1419) (commit https://github.com/onnx/onnx/commit/651775ca83 by @bddppq)
AppVeyorBot(2018-03-06 01:16:53)::x: [Build onnx 0.3.1455 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1455) (commit https://github.com/onnx/onnx/commit/c9a4a9fe06 by @bddppq)
AppVeyorBot(2018-03-06 04:44:27)::x: [Build onnx 0.3.1457 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1457) (commit https://github.com/onnx/onnx/commit/7b8519f24f by @bddppq)
AppVeyorBot(2018-03-06 04:51:28)::x: [Build onnx 0.3.1459 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1459) (commit https://github.com/onnx/onnx/commit/10c964610c by @bddppq)
AppVeyorBot(2018-03-07 07:24:13)::x: [Build onnx 0.3.1479 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1479) (commit https://github.com/onnx/onnx/commit/9a6995ba99 by @bddppq)
yinghai(2018-03-08 00:45:21):Issue on Windows resolved? 
CLAassistant(2018-03-03 11:51:37):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=580) <br/>All committers have signed the CLA.
houseroad(2018-03-05 18:40:20):@onnxbot test this please
houseroad(2018-03-05 20:12:02):I just checked our specification of PRelu, it's problematic. Since it's element-wise operator, we should follow the same broadcasting rules as other element-wise operator, such as Add, Sub, Pow, etc.
@tfujiwar could you also fix the specification and update the test cases?
@ebarsoum @yuanbyu @linkerzhang does this sound good to you?
tfujiwar(2018-03-05 23:08:40):@houseroad Thanks for checking. Sure, I will.
AppVeyorBot(2018-03-06 14:25:16)::white_check_mark: [Build onnx 0.3.1460 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1460) (commit https://github.com/onnx/onnx/commit/d9a86a1cf6 by @tfujiwar)
AppVeyorBot(2018-03-07 13:15:03)::white_check_mark: [Build onnx 0.3.1480 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1480) (commit https://github.com/onnx/onnx/commit/6cf095aa4a by @tfujiwar)
houseroad(2018-03-27 22:35:02):I will re-evaluate the changes in this PR, and give further comments. :-)
AppVeyorBot(2018-03-28 12:13:57)::x: [Build onnx 0.3.1922 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1922) (commit https://github.com/onnx/onnx/commit/4f1f667bae by @tfujiwar)
AppVeyorBot(2018-03-28 12:35:04)::white_check_mark: [Build onnx 0.3.1923 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1923) (commit https://github.com/onnx/onnx/commit/4f1f667bae by @tfujiwar)
tfujiwar(2018-04-05 04:33:09):@jamesr66a You are right. That case is similar with the following example and test case.
- https://github.com/onnx/onnx/pull/580/files#diff-def508ad6a02fca823d0a2debbbf65c2R5221
- https://github.com/onnx/onnx/pull/580/files#diff-a9ea2f3ab1d90865a1aa7acf36bd8f88R31
jspisak(2018-04-11 03:37:10):@ebarsoum @linkerzhang @yuanbyu @lupesko Guys, this is a pretty easy one. Take a look, otherwise we'll just merge given this is just adding test cases.
bddppq(2018-05-13 05:12:22):@houseroad @tfujiwar the broadcast support added here seems to be our old-style (i.e. with axis instead of following the one from numpy) version. Maybe let's don't add the broadcast in this PR but after #907 has settled?
houseroad(2018-05-13 23:29:02):@bddppq yeah, let's wait until the we align to the numpy one. After that, we can keep polish this one.
bddppq(2018-05-23 23:11:30):@tfujiwar @houseroad @linkerzhang I have updated this PR to follow #907
houseroad(2018-05-23 23:26:12):@bddppq delete the stale cases generated by the old cases?

bddppq(2018-05-23 23:28:04):@houseroad Good catch. I swear I had deleted them locally :-)
AppVeyorBot(2018-03-06 20:26:40)::white_check_mark: [Build onnx 0.3.1468 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1468) (commit https://github.com/onnx/onnx/commit/ec5e1b2ecd by @anderspapitto)
anderspapitto(2018-03-06 18:49:18):I'll clean this up before pushing (and below), but I don't want to retrigger all the tests right now
bddppq(2018-03-06 18:49:30):remove this
bddppq(2018-03-06 18:52:23):remove this
bddppq(2018-03-06 18:53:19):Move this to right after virtualenv activation
bddppq(2018-03-06 18:54:30):In travis linux, if language is set to python, then it has already set up a virtualenv.
bddppq(2018-03-06 18:55:05):nit: wrap this protobuf installation into a function
bddppq(2018-03-06 18:56:00):What's the reason of doing upgrade?
anderspapitto(2018-03-06 19:07:47):I took it from https://github.com/onnx/onnx-coreml/commit/f8af8753e266d0d02e3301e070333416e9942a5c, I believe it's what actually upgrades from python2 to python3, will confirm with @smessmer 
anderspapitto(2018-03-06 19:08:02):ok, sure
anderspapitto(2018-03-06 19:08:22):ok, I'll try putting this under an IF
anderspapitto(2018-03-06 19:08:38):++
anderspapitto(2018-03-06 19:08:42):++
anderspapitto(2018-03-06 19:25:51):confirmed
bddppq(2018-03-06 23:49:39):nit: `-z`
bddppq(2018-03-06 23:50:06):nit: There is no need to do `export` here.
bddppq(2018-03-06 23:50:29):nit: local
bddppq(2018-03-06 19:01:35):Test cases should be updated as well (ok to do it in a separate diff): https://github.com/onnx/onnx/blob/176e3575ea15db994a84702470da90ee90a0dbf6/onnx/backend/test/case/node/cast.py#L21


linkerzhang(2018-03-06 17:22:41):This is the only change in this file, all the other changes are due to format changes after running clang-format against this file, and I kept the format changes as they're. Thanks!
houseroad(2018-03-07 01:23:44):This will break caffe2-onnx backend, created this PR to fix it. https://github.com/caffe2/caffe2/pull/2161
bddppq(2018-03-07 05:12:40):We should inspect whether the backend's `run_node` function has `outputs_info` keyword argument. https://docs.python.org/3/library/inspect.html#inspect.getargspec to keep backward compatibility. Backends can opt in to receive these information.
dzhulgakov(2018-03-07 21:13:51):Shall we just turn all run_node functions into run_model? :) Graph proto already has input/output info
houseroad(2018-03-07 21:28:19):@dzhulgakov that is doable. Without additional information, everything will be considered as inputs. Shall we add some hints about the weights into schema for each operator?
bddppq(2018-03-07 21:31:00):@dzhulgakov that has been discussed and is indeed the most ideal solution, however that needs some more work (basically to annotate which inputs are weights and which are real inputs), I propose let's do this simple change to unblock integrations of frameworks that needs these shape information.
yinghai(2018-03-08 00:55:37):> Shall we just turn all run_node functions into run_model? :) Graph proto already has input/output info

I support this! This will make the code more unified. 
bddppq(2018-03-07 19:32:15):this variable doee not seem to be needed
bddppq(2018-03-07 19:33:38):Doesn't `getargspec` also work in python3?
houseroad(2018-03-07 19:38:01):According to https://docs.python.org/3/library/inspect.html#inspect.getargspec, it should work, but it is deprecated since 3.0.
bddppq(2018-03-09 20:05:01):@linkerzhang @yuanbyu @ebarsoum @lupesko please review
jamesr66a(2018-03-15 18:45:26):@linkerzhang @yuanbyu @ebarsoum @lupesko ping for review
AppVeyorBot(2018-03-15 18:54:09)::x: [Build onnx 0.3.1608 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1608) (commit https://github.com/onnx/onnx/commit/ed84fb8051 by @jamesr66a)
houseroad(2018-03-27 22:35:52):@jamesr66a do we implement Unique using ATen or still need to introduce unique operator?
houseroad(2018-03-27 22:40:12):After offline sync, Unique op should be included in ONNX op set.

Would you mind to take a look? @ebarsoum @linkerzhang @yuanbyu 
jspisak(2018-04-11 03:35:00):@ebarsoum @linkerzhang @yuanbyu @lupesko Guys - can you review this? It's been approved on our side for 14 days.

AppVeyorBot(2018-05-08 18:19:16)::x: [Build onnx 0.3.2935 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2935) (commit https://github.com/onnx/onnx/commit/c0862b54e5 by @jamesr66a)
AppVeyorBot(2018-05-08 18:44:06)::x: [Build onnx 0.3.2937 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2937) (commit https://github.com/onnx/onnx/commit/73052d60f6 by @jamesr66a)
bddppq(2018-05-23 06:07:43):@jamesr66a Could you resolve the merge conflicts and fix the CI failures?
jamesr66a(2018-05-29 18:04:46):We can resurrect this later if needed
bddppq(2018-03-08 00:33:46):nit: mention "along axis"
smessmer(2018-03-12 18:12:50):Why do we define bool attributes as `int`? Are there language bindings that don't support bool directly?
smessmer(2018-03-12 18:14:42):We should either specify the order or explicitly state that the order of the elements is undefined.
ebarsoum(2018-04-18 20:10:18):Order need to be specified, how we will match the output between frameworks.
ebarsoum(2018-04-18 20:13:10):The name of `inverse_indices` look weird, from the outside it tell me that it inverses the result and not returning the indices as an extra output. Shouldn't we call it `return_indices`?
ebarsoum(2018-04-18 20:15:23):Here TF implementation: https://www.tensorflow.org/api_docs/python/tf/unique

From the above: "This operation returns a tensor y containing all of the unique elements of x sorted in the same order that they occur in x.", Why don't we do the same?
ebarsoum(2018-04-18 20:16:12):I assume X is N dimension and not 1D?
ebarsoum(2018-04-20 05:33:38):@jamesr66a  @bddppq  any update on this?
jspisak(2018-05-07 03:19:09):@jamesr66a  Can you take a look?
jamesr66a(2018-05-08 17:56:17):@gramalingam specifying this as unsorted with an auxiliary `Sort` operator is the more general case. Concretely:

- Front-end with sorted Unique operator: emit a Unique and a Sort operator to ONNX
- Front-end without a sorted Unique operator: emit a Unique
- Back-end with a sorted Unique operator: Fuse `Sort` into `Unique` with a simple peephole pass and emit the sorted operator. In the case where we find an unsorted `Unique` operator, we're still fine since the ordering semantics were unspecified and the network description must be valid regardless of the order. The backend is free to emit the sorted Unique op in place of the unsorted one
- Back-end without a sorted Unique operator: Can still accept Unique without a corresponding `Sort` operator. Fault on `Sort`

In the case where we specify the ordering in this schema

- Front-end with sorted Unique operator: emit Unique
- Front-end without a sorted Unique operator: emit a Unique *changing ordering semantics*
- Back-end with a sorted Unique operator: emit sorted Unique
- Back-end without a sorted Unique operator: *cannot accept Unique at all, unless you lower to separate Unique + Sort ops, as is specified here*

Both front-ends and back-ends may choose to implement `Unique` to be unsorted, for example for efficiency reasons. An example is caffe2.

Having a separate `Sort` is more general anyway and will likely find uses other than in tandem with `Unique`
jamesr66a(2018-05-08 17:58:01):sure
ebarsoum(2018-05-11 22:13:37):@jamesr66a that mean the Unique would match between frameworks? Would the test only verify that the elements are in or not?
Maratyszcza(2018-03-08 01:11:46):`if (TARGET protobuf::protoc)` check would be more appropriate
yinghai(2018-03-08 01:12:41):They come hand-in-hand. But yes! 
Maratyszcza(2018-03-08 17:35:57):Maybe use `if(DEFINED ONNX_CUSTOM_PROTOC_EXECUTABLE)` instead? If `-DONNX_CUSTOM_PROTOC_EXECUTABLE` is specified, but doesn't exist (due to user error), the outcome would be very inintuitive: CMake will try to build and use the `protoc` for a different platform
yinghai(2018-03-08 18:28:03):For this usecase, we would like to use `ONNX_CUSTOM_PROTOC_EXECUTABLE` as a truth. Actually, if we fallback to let cmake build its own protoc, it can fail and error message will not be as intuitive (Check Caffe2 CI for Android and iOS). 
houseroad(2018-03-08 22:15:50):@onnxbot merge after CI pass
houseroad(2018-03-08 22:16:08):Is this feature available already/
yinghai(2018-03-08 22:23:21):> Is this feature available already/

What do you mean? 
houseroad(2018-03-08 22:25:36):I remember that @bddppq mentioned onnxbot may support more commands, something like this.
yinghai(2018-03-09 03:14:50):@onnxbot retest this please 
AppVeyorBot(2018-03-09 02:00:02)::x: [Build onnx 0.3.1524 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1524) (commit https://github.com/onnx/onnx/commit/f2d444bcf2 by @smessmer)
AppVeyorBot(2018-03-09 03:06:40)::x: [Build onnx 0.3.1526 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1526) (commit https://github.com/onnx/onnx/commit/28d39bb699 by @smessmer)
AppVeyorBot(2018-03-09 03:11:51)::x: [Build onnx 0.3.1527 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1527) (commit https://github.com/onnx/onnx/commit/cc1abcec2f by @smessmer)
AppVeyorBot(2018-03-09 20:14:56)::x: [Build onnx 0.3.1540 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1540) (commit https://github.com/onnx/onnx/commit/bb8dc0afc7 by @smessmer)
AppVeyorBot(2018-03-09 21:43:32)::x: [Build onnx 0.3.1541 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1541) (commit https://github.com/onnx/onnx/commit/f9632e142f by @smessmer)
AppVeyorBot(2018-03-09 22:52:48)::x: [Build onnx 0.3.1546 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1546) (commit https://github.com/onnx/onnx/commit/9014e8bf23 by @smessmer)
AppVeyorBot(2018-03-09 22:56:24)::x: [Build onnx 0.3.1547 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1547) (commit https://github.com/onnx/onnx/commit/b3bcb20591 by @smessmer)
AppVeyorBot(2018-03-09 23:20:46)::x: [Build onnx 0.3.1549 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1549) (commit https://github.com/onnx/onnx/commit/a78eaa3a5a by @smessmer)
AppVeyorBot(2018-03-12 20:48:18)::x: [Build onnx 0.3.1562 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1562) (commit https://github.com/onnx/onnx/commit/51029ca56c by @smessmer)
AppVeyorBot(2018-03-12 21:10:44)::x: [Build onnx 0.3.1564 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1564) (commit https://github.com/onnx/onnx/commit/beb36c5dbe by @smessmer)
AppVeyorBot(2018-03-12 22:07:57)::x: [Build onnx 0.3.1565 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1565) (commit https://github.com/onnx/onnx/commit/5f838f31fa by @smessmer)
AppVeyorBot(2018-03-12 22:12:44)::x: [Build onnx 0.3.1566 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1566) (commit https://github.com/onnx/onnx/commit/7c0af900fc by @smessmer)
AppVeyorBot(2018-03-12 22:24:38)::x: [Build onnx 0.3.1567 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1567) (commit https://github.com/onnx/onnx/commit/44d4b3893c by @smessmer)
AppVeyorBot(2018-03-12 22:33:38)::white_check_mark: [Build onnx 0.3.1568 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1568) (commit https://github.com/onnx/onnx/commit/5a434a86a1 by @smessmer)
smessmer(2018-03-12 23:00:55):Superseded by https://github.com/onnx/onnx/pull/607
dzhulgakov(2018-03-09 10:13:18):caffe2 shouldn't be here probably
smessmer(2018-03-09 18:51:46):Yes, this is still WIP. I need to create this PR to trigger travis builds because I can't push branches directly to the repo.
smessmer(2018-03-09 18:55:01):Note: removed caffe2. Numpy has too complex type relations for correct and comprehensive type stubs. I added some good-enough ones to the onnx-coreml repo and will add them here too later, but I want this PR to just be the minimal set of code changes needed to make mypy pass.
yinghai(2018-03-09 18:33:40):Maybe we should define `ONNX_BUILD_MAIN_LIB` if it's a dynamic lib, otherwise we don't define it. 
Maratyszcza(2018-03-09 18:37:33):That would be right to do, I did just a minimal fix
CLAassistant(2018-03-09 22:10:17):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=602) <br/>All committers have signed the CLA.
bddppq(2018-03-09 22:19:16):cc @anderspapitto 
AppVeyorBot(2018-03-09 22:24:51)::white_check_mark: [Build onnx 0.3.1543 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1543) (commit https://github.com/onnx/onnx/commit/6fd0b9ffd1 by @)
AppVeyorBot(2018-03-09 22:43:25)::white_check_mark: [Build onnx 0.3.1545 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1545) (commit https://github.com/onnx/onnx/commit/3083eb3c8a by @)
AppVeyorBot(2018-03-10 00:24:58)::white_check_mark: [Build onnx 0.3.1550 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1550) (commit https://github.com/onnx/onnx/commit/67c042c278 by @yinghai)
yinghai(2018-03-09 22:12:20):We don't need to define `ONNX_API` manually as it's handled here: https://github.com/onnx/onnx/blob/b184dd3cb80f9f0ab47719ed261d46cdec0f697b/onnx/onnx_pb.h#L23-L42

The correct way is to set `ONNX_BUILD_SHARED_LIBS` and `ONNX_BUILD_MAIN_LIB` during CMake. 
Maratyszcza(2018-03-09 23:30:23):This is not right. Whether you link the runtime library statically or dynamically is independent of whether you build your library as static or dynamic. You can have all four combinations.
yinghai(2018-03-10 00:17:35):What's what I meant minimal fix. LOL. I addressed it in the next diff. 
houseroad(2018-03-13 02:49:11):@linkerzhang proposes composite op/macro/function (the name is not determined yet) in https://github.com/onnx/onnx/pull/570. So this case be easily represented as sqrt and reciprocal ops. 
fumihwh(2018-03-13 14:38:46):@houseroad sounds great!
lygstate(2018-08-24 18:50:00):@hchandola How to implement rsqrt in composite op/macro/function 
lygstate(2018-08-24 18:50:40):@linkerzhang Also asked. How to implement rsqrt in composite op/macro/function
AppVeyorBot(2018-03-10 05:54:22)::x: [Build onnx 0.3.1555 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1555) (commit https://github.com/onnx/onnx/commit/8da3d50a86 by @yinghai)
anderspapitto(2018-03-12 00:53:34):do we have any tests that load multiple copies of ONNX? I guess it would be kind of annoying because it can't use the standard flow of the CI setup
yinghai(2018-03-22 16:26:44):> do we have any tests that load multiple copies of ONNX? I guess it would be kind of annoying because it can't use the standard flow of the CI setup

The onnx-caffe2 tests are kind of testing this already. This is one onnx statically linked in caffe2.so and this the other in the onnx Python .so file. 
AppVeyorBot(2018-03-12 18:07:56)::x: [Build onnx 0.3.1559 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1559) (commit https://github.com/onnx/onnx/commit/20de5e8fa5 by @anderspapitto)
AppVeyorBot(2018-03-12 18:12:42)::x: [Build onnx 0.3.1560 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1560) (commit https://github.com/onnx/onnx/commit/5b97e1caa5 by @anderspapitto)
AppVeyorBot(2018-03-12 18:17:19)::x: [Build onnx 0.3.1561 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1561) (commit https://github.com/onnx/onnx/commit/9a38849fca by @anderspapitto)
AppVeyorBot(2018-03-12 20:49:04)::x: [Build onnx 0.3.1563 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1563) (commit https://github.com/onnx/onnx/commit/275afac3c9 by @anderspapitto)
AppVeyorBot(2018-03-13 21:17:32)::x: [Build onnx 0.3.1576 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1576) (commit https://github.com/onnx/onnx/commit/da9163141e by @anderspapitto)
AppVeyorBot(2018-03-13 21:18:35)::x: [Build onnx 0.3.1577 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1577) (commit https://github.com/onnx/onnx/commit/155fff9722 by @anderspapitto)
AppVeyorBot(2018-03-13 21:34:24)::x: [Build onnx 0.3.1578 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1578) (commit https://github.com/onnx/onnx/commit/e0b2f2cfac by @anderspapitto)
AppVeyorBot(2018-03-13 22:00:53)::x: [Build onnx 0.3.1580 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1580) (commit https://github.com/onnx/onnx/commit/a77abfc581 by @anderspapitto)
anderspapitto(2018-03-14 17:05:52):@onnxbot retest this please
AppVeyorBot(2018-03-14 18:16:40)::x: [Build onnx 0.3.1589 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1589) (commit https://github.com/onnx/onnx/commit/a492ea55c1 by @anderspapitto)
AppVeyorBot(2018-03-14 21:42:19)::x: [Build onnx 0.3.1595 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1595) (commit https://github.com/onnx/onnx/commit/b1004136b7 by @anderspapitto)
AppVeyorBot(2018-03-16 21:16:44)::x: [Build onnx 0.3.1629 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1629) (commit https://github.com/onnx/onnx/commit/62de111ece by @anderspapitto)
AppVeyorBot(2018-03-16 22:26:13)::x: [Build onnx 0.3.1630 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1630) (commit https://github.com/onnx/onnx/commit/3e369e8bff by @anderspapitto)
AppVeyorBot(2018-03-16 22:42:05)::x: [Build onnx 0.3.1631 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1631) (commit https://github.com/onnx/onnx/commit/52251a01f3 by @anderspapitto)
AppVeyorBot(2018-03-16 22:51:23)::x: [Build onnx 0.3.1632 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1632) (commit https://github.com/onnx/onnx/commit/548f103986 by @anderspapitto)
AppVeyorBot(2018-03-17 01:04:55)::x: [Build onnx 0.3.1636 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1636) (commit https://github.com/onnx/onnx/commit/bc98e8dcaf by @anderspapitto)
AppVeyorBot(2018-03-19 20:33:35)::x: [Build onnx 0.3.1652 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1652) (commit https://github.com/onnx/onnx/commit/074d387713 by @anderspapitto)
AppVeyorBot(2018-03-19 20:59:05)::x: [Build onnx 0.3.1654 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1654) (commit https://github.com/onnx/onnx/commit/1d68e1bc9c by @anderspapitto)
AppVeyorBot(2018-03-19 21:12:32)::x: [Build onnx 0.3.1655 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1655) (commit https://github.com/onnx/onnx/commit/b7ea8aed7b by @anderspapitto)
AppVeyorBot(2018-03-19 21:24:44)::x: [Build onnx 0.3.1656 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1656) (commit https://github.com/onnx/onnx/commit/2d4302eb3f by @anderspapitto)
AppVeyorBot(2018-03-19 23:02:50)::x: [Build onnx 0.3.1666 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1666) (commit https://github.com/onnx/onnx/commit/89de3e3c5c by @anderspapitto)
AppVeyorBot(2018-03-19 23:20:14)::x: [Build onnx 0.3.1668 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1668) (commit https://github.com/onnx/onnx/commit/aa8f3bea7a by @anderspapitto)
AppVeyorBot(2018-03-19 23:28:31)::x: [Build onnx 0.3.1669 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1669) (commit https://github.com/onnx/onnx/commit/4cd706d4a0 by @anderspapitto)
AppVeyorBot(2018-03-20 22:59:20)::x: [Build onnx 0.3.1696 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1696) (commit https://github.com/onnx/onnx/commit/65df54e59a by @anderspapitto)
AppVeyorBot(2018-03-20 23:28:33)::x: [Build onnx 0.3.1698 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1698) (commit https://github.com/onnx/onnx/commit/e7b7ee052f by @anderspapitto)
AppVeyorBot(2018-03-21 18:41:50)::x: [Build onnx 0.3.1718 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1718) (commit https://github.com/onnx/onnx/commit/af2504ab9d by @anderspapitto)
AppVeyorBot(2018-03-21 20:22:30)::x: [Build onnx 0.3.1723 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1723) (commit https://github.com/onnx/onnx/commit/d81a9e39e9 by @anderspapitto)
AppVeyorBot(2018-03-21 21:01:42)::x: [Build onnx 0.3.1727 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1727) (commit https://github.com/onnx/onnx/commit/c61e663ab9 by @anderspapitto)
AppVeyorBot(2018-03-21 22:18:27)::x: [Build onnx 0.3.1731 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1731) (commit https://github.com/onnx/onnx/commit/3f660f8ade by @anderspapitto)
AppVeyorBot(2018-03-21 22:25:52)::x: [Build onnx 0.3.1732 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1732) (commit https://github.com/onnx/onnx/commit/13c8ad11bf by @anderspapitto)
AppVeyorBot(2018-03-21 23:05:58)::x: [Build onnx 0.3.1736 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1736) (commit https://github.com/onnx/onnx/commit/86b35b2aed by @anderspapitto)
AppVeyorBot(2018-03-22 01:00:38)::x: [Build onnx 0.3.1747 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1747) (commit https://github.com/onnx/onnx/commit/174456c888 by @anderspapitto)
AppVeyorBot(2018-03-22 18:57:36)::x: [Build onnx 0.3.1762 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1762) (commit https://github.com/onnx/onnx/commit/e49168b96d by @anderspapitto)
AppVeyorBot(2018-03-22 19:29:50)::x: [Build onnx 0.3.1765 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1765) (commit https://github.com/onnx/onnx/commit/1e7914d867 by @anderspapitto)
AppVeyorBot(2018-03-22 19:49:01)::x: [Build onnx 0.3.1767 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1767) (commit https://github.com/onnx/onnx/commit/71951dbcbe by @anderspapitto)
AppVeyorBot(2018-03-22 20:16:39)::x: [Build onnx 0.3.1770 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1770) (commit https://github.com/onnx/onnx/commit/3b5b53cf85 by @anderspapitto)
AppVeyorBot(2018-03-22 20:22:49)::x: [Build onnx 0.3.1771 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1771) (commit https://github.com/onnx/onnx/commit/8bcd028bb3 by @anderspapitto)
AppVeyorBot(2018-03-22 21:02:25)::x: [Build onnx 0.3.1773 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1773) (commit https://github.com/onnx/onnx/commit/d22129809b by @anderspapitto)
AppVeyorBot(2018-03-23 20:45:25)::x: [Build onnx 0.3.1819 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1819) (commit https://github.com/onnx/onnx/commit/a500532e27 by @anderspapitto)
AppVeyorBot(2018-03-23 21:19:35)::x: [Build onnx 0.3.1824 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1824) (commit https://github.com/onnx/onnx/commit/c4b68747bd by @anderspapitto)
AppVeyorBot(2018-03-23 21:26:41)::x: [Build onnx 0.3.1825 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1825) (commit https://github.com/onnx/onnx/commit/5094c53c64 by @houseroad)
AppVeyorBot(2018-03-26 20:53:44)::x: [Build onnx 0.3.1852 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1852) (commit https://github.com/onnx/onnx/commit/ca950d7e74 by @anderspapitto)
AppVeyorBot(2018-03-26 22:53:40)::x: [Build onnx 0.3.1853 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1853) (commit https://github.com/onnx/onnx/commit/e37191443b by @anderspapitto)
AppVeyorBot(2018-03-27 02:48:10)::x: [Build onnx 0.3.1867 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1867) (commit https://github.com/onnx/onnx/commit/1bab042ce8 by @anderspapitto)
AppVeyorBot(2018-03-27 02:55:19)::x: [Build onnx 0.3.1868 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1868) (commit https://github.com/onnx/onnx/commit/0f6df40720 by @anderspapitto)
AppVeyorBot(2018-03-27 04:21:28)::x: [Build onnx 0.3.1881 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1881) (commit https://github.com/onnx/onnx/commit/5cd97c4f88 by @anderspapitto)
AppVeyorBot(2018-03-27 21:38:18)::x: [Build onnx 0.3.1895 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1895) (commit https://github.com/onnx/onnx/commit/77ac58eaa3 by @anderspapitto)
AppVeyorBot(2018-03-28 21:11:06)::x: [Build onnx 0.3.1929 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1929) (commit https://github.com/onnx/onnx/commit/c5d409885f by @anderspapitto)
AppVeyorBot(2018-03-30 23:44:13)::x: [Build onnx 0.3.1957 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1957) (commit https://github.com/onnx/onnx/commit/23e45c03b5 by @anderspapitto)
yinghai(2018-03-30 23:59:31):Folks, once you make changes, please make new commit instead of squash and `git push -f` or it's difficult to track the progress of this PR. :) 
AppVeyorBot(2018-03-31 00:03:30)::white_check_mark: [Build onnx 0.3.1958 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1958) (commit https://github.com/onnx/onnx/commit/cf52b70b99 by @raymondxyang)
AppVeyorBot(2018-04-02 18:20:16)::white_check_mark: [Build onnx 0.3.1980 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1980) (commit https://github.com/onnx/onnx/commit/35e61700bc by @raymondxyang)
AppVeyorBot(2018-04-02 19:10:47)::x: [Build onnx 0.3.1982 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1982) (commit https://github.com/onnx/onnx/commit/1c939e746b by @anderspapitto)
AppVeyorBot(2018-04-02 19:52:19)::x: [Build onnx 0.3.1984 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1984) (commit https://github.com/onnx/onnx/commit/7cb5d7a17a by @anderspapitto)
AppVeyorBot(2018-04-02 20:20:27)::white_check_mark: [Build onnx 0.3.1985 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1985) (commit https://github.com/onnx/onnx/commit/f99ac3a6c1 by @anderspapitto)
AppVeyorBot(2018-04-03 21:31:54)::white_check_mark: [Build onnx 0.3.2031 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2031) (commit https://github.com/onnx/onnx/commit/172007b4ce by @anderspapitto)
yinghai(2018-03-12 20:56:30):`MODULE` is the recommended type as it's a Python plugin. https://cmake.org/cmake/help/v3.0/command/add_library.html
yinghai(2018-03-12 20:57:56):Depending on the compiler we might need different flags. This is `GCC` specific.  Check
https://github.com/caffe2/caffe2/blob/53cc7dfe10fa1edd5d731654bea21d317375ffbf/cmake/public/utils.cmake#L10-L24
yinghai(2018-03-12 22:43:37):Can we have a name without `lib` just to indicate that it is a python module? 
anderspapitto(2018-03-12 23:19:33):++ meant to change it back
anderspapitto(2018-03-12 23:19:39):++ will do
anderspapitto(2018-03-12 23:22:36):maybe. building with cmake via `add_library(foo)` forces it to be called 'libfoo' as far as I can tell - do you know how to disable that?
yinghai(2018-03-12 23:47:21):Try this? `set_target_properties(onnx_cpp2py_export PROPERTIES PREFIX "")`
yinghai(2018-03-23 05:11:16):If we define `ONNX_ML`, when calling `gen_proto.py` we need `--ml` flag. 
yinghai(2018-03-23 05:12:03):Since you hve `pycmd` now, you can use that on L92 too. 
yinghai(2018-03-23 05:13:38):Why do we have this line again? 
yinghai(2018-03-23 05:14:29):Why do we remove this? We should leave this as optional and update the CI if necessary. 
bddppq(2018-03-30 23:35:12):I prefer to no support building python extension without going through setup.py. All these PY_EXT_SUFFIX, PYTHON_EXECUTABLE cmake variables should be directly passed from setup.py (passing via cmake flags).
bddppq(2018-03-30 23:37:34):we can remove all these NINJA stuffs, basically delegate the ninja generation work to cmake and we only check whether `ninja` command is available.
bddppq(2018-03-30 23:45:08):We don't need this logic here, because the extensions list is fixed in onnx (aka. we are always building ['onnx_cpp2py_export']).
bddppq(2018-04-03 20:06:04):where is this used for?
bddppq(2018-04-03 20:07:00):this is not used anymore after this change.
anderspapitto(2018-04-03 20:31:02):looks like it's not, I'll try removing it
anderspapitto(2018-04-03 20:31:12):conda_prefix is added to PATH below
AppVeyorBot(2018-03-13 23:07:51)::white_check_mark: [Build onnx 0.3.1581 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1581) (commit https://github.com/onnx/onnx/commit/02b80320a1 by @smessmer)
AppVeyorBot(2018-03-15 22:40:59)::white_check_mark: [Build onnx 0.3.1617 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1617) (commit https://github.com/onnx/onnx/commit/149459c7b4 by @smessmer)
AppVeyorBot(2018-03-23 22:17:57)::white_check_mark: [Build onnx 0.3.1830 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1830) (commit https://github.com/onnx/onnx/commit/af5075df6b by @smessmer)
AppVeyorBot(2018-03-26 23:03:03)::white_check_mark: [Build onnx 0.3.1854 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1854) (commit https://github.com/onnx/onnx/commit/216b24b560 by @smessmer)
AppVeyorBot(2018-03-26 23:48:59)::white_check_mark: [Build onnx 0.3.1860 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1860) (commit https://github.com/onnx/onnx/commit/5fe19d2398 by @smessmer)
AppVeyorBot(2018-03-27 03:04:32)::white_check_mark: [Build onnx 0.3.1870 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1870) (commit https://github.com/onnx/onnx/commit/2c44893ed7 by @bddppq)
bddppq(2018-03-21 07:09:29):Is there a way to avoid this? Need to mark every numpy import is pretty cumbersome.
bddppq(2018-03-21 07:11:28):I prefer to not change this part (because we want to test whether the "standard" installation works), and then after all the tests have passed, we install mypy and do type checking (like what we do for using flake8 to lint).
smessmer(2018-03-21 22:21:58):I could set `ignore_missing_imports = True` in the config file. However, that would mean that mypy won't report any invalid imports anymore (i.e. not just missing typing information, but it'll also not show any warning if the whole module is missing). And if for some reason a module exists but mypy doesn't find it, it'll also silently ignore it and not typecheck anything related to that module, hiding a lot of errors. Not sure if that is a good idea.
smessmer(2018-03-21 22:23:49):fixed
smessmer(2018-03-21 22:45:57):An alternative might be to have a local onnx.numpy module that just does

    from numpy import *  # type: ignore

Then, other modules could do

    from onnx import numpy

without having to add `type: ignore`
smessmer(2018-03-23 21:59:23):@bddppq Opinions?
bddppq(2018-03-26 19:11:10):I'm a little scared of playing with python import mechanism :-), let's not add a local numpy import proxy.
bddppq(2018-03-26 19:18:52):What does this line do?
bddppq(2018-03-26 19:22:46):nit: don't use `file` as variable name, which is a builtin
use `SRC_DIR` instead of "onnx" (former is an absolute path)
smessmer(2018-03-26 19:38:22):It allows mypy to find typing information (`.pyi` files or `.py` files) in the `stubs/` directory and in the `third_party/pybind11` directory (`pybind11` for the corresponding module and `stubs` for some general local overrides)
fumihwh(2018-03-27 01:04:08):@smessmer 
BTW, what does it ignore? Imported but not used?
fumihwh(2018-03-13 00:23:29):Just a question, why do we need attribute `shape` and input `output_shape` both for defining the shape of output tensor?
linkerzhang(2018-03-13 00:35:19):@fumihwh we don't need both definitely. that's why I'm adding "output_shape" as optional for backward compatibility.  but as @bddppq said, if we all agree to make a breaking change here, it's a cleaner way. Let me do that and see whether there're any other concerns from other folks.
AppVeyorBot(2018-03-13 00:52:18)::x: [Build onnx 0.3.1571 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1571) (commit https://github.com/onnx/onnx/commit/4ba549eb7f by @linkerzhang)
bddppq(2018-03-13 01:25:39):This change will pretty much break all models since Reshape is so widely used.
One idea we have discussed before is for each breaking change we also provide a conversion/migration script to upgrade the existing models. Shall we use this as an opportunity to figure out such a mechanism? :-) cc @dzhulgakov @ezyang
AppVeyorBot(2018-03-16 05:29:21)::x: [Build onnx 0.3.1623 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1623) (commit https://github.com/onnx/onnx/commit/f46bb00e1f by @bddppq)
AppVeyorBot(2018-03-16 05:37:54)::white_check_mark: [Build onnx 0.3.1624 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1624) (commit https://github.com/onnx/onnx/commit/56a9cf4d03 by @bddppq)
linkerzhang(2018-03-16 05:55:44):Looks good! Thank you so much! @bddppq 
AppVeyorBot(2018-03-13 04:18:59)::white_check_mark: [Build onnx 0.3.1572 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1572) (commit https://github.com/onnx/onnx/commit/aba7599d35 by @Maratyszcza)
yinghai(2018-03-14 04:48:52):@onnxbot retest this please
yinghai(2018-03-14 04:54:43):@onnxbot retest this please
AppVeyorBot(2018-03-14 23:36:06)::white_check_mark: [Build onnx 0.3.1597 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1597) (commit https://github.com/onnx/onnx/commit/1e40a99994 by @Maratyszcza)
yinghai(2018-03-14 05:00:47):Please change `onnx::` to `ONNX_NAMESPACE::`. Or use `using`. 
Maratyszcza(2018-03-14 05:16:02):Fixed
linkerzhang(2018-03-14 23:09:32):let's put this into \tools folder please. Sounds good?
Maratyszcza(2018-03-14 23:26:13):Done! Clear to merge?
AppVeyorBot(2018-03-27 00:05:58)::white_check_mark: [Build onnx 0.3.1862 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1862) (commit https://github.com/onnx/onnx/commit/6974c8ffa4 by @smessmer)
AppVeyorBot(2018-04-11 15:15:14)::white_check_mark: [Build onnx 0.3.2256 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2256) (commit https://github.com/onnx/onnx/commit/3074ac7f36 by @smessmer)
bddppq(2018-04-12 22:44:48):@onnxbot retest this please
AppVeyorBot(2018-03-15 22:31:54)::x: [Build onnx 0.3.1616 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1616) (commit https://github.com/onnx/onnx/commit/28a8a3e0b1 by @smessmer)
AppVeyorBot(2018-03-15 23:00:24)::x: [Build onnx 0.3.1619 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1619) (commit https://github.com/onnx/onnx/commit/4977da1b59 by @smessmer)
AppVeyorBot(2018-03-15 23:10:06)::x: [Build onnx 0.3.1620 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1620) (commit https://github.com/onnx/onnx/commit/b1a2474afe by @smessmer)
AppVeyorBot(2018-03-16 00:35:53)::x: [Build onnx 0.3.1622 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1622) (commit https://github.com/onnx/onnx/commit/7732208f25 by @smessmer)
AppVeyorBot(2018-03-19 22:45:26)::x: [Build onnx 0.3.1664 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1664) (commit https://github.com/onnx/onnx/commit/cf0b5ddf68 by @smessmer)
AppVeyorBot(2018-03-21 23:13:09)::x: [Build onnx 0.3.1737 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1737) (commit https://github.com/onnx/onnx/commit/150c29318b by @smessmer)
AppVeyorBot(2018-03-22 20:10:18)::x: [Build onnx 0.3.1769 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1769) (commit https://github.com/onnx/onnx/commit/e65d43a847 by @smessmer)
AppVeyorBot(2018-03-26 23:18:58)::x: [Build onnx 0.3.1856 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1856) (commit https://github.com/onnx/onnx/commit/0ceeb2dee3 by @smessmer)
AppVeyorBot(2018-03-26 23:55:53)::x: [Build onnx 0.3.1861 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1861) (commit https://github.com/onnx/onnx/commit/566c52b5c6 by @smessmer)
AppVeyorBot(2018-04-11 10:29:55)::x: [Build onnx 0.3.2254 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2254) (commit https://github.com/onnx/onnx/commit/9b19e3a8b2 by @smessmer)
AppVeyorBot(2018-04-11 15:39:29)::x: [Build onnx 0.3.2257 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2257) (commit https://github.com/onnx/onnx/commit/ec0646463b by @smessmer)
AppVeyorBot(2018-04-11 19:56:41)::x: [Build onnx 0.3.2262 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2262) (commit https://github.com/onnx/onnx/commit/bf0e6891f1 by @smessmer)
AppVeyorBot(2018-04-17 21:01:17)::x: [Build onnx 0.3.2341 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2341) (commit https://github.com/onnx/onnx/commit/69836569b1 by @smessmer)
AppVeyorBot(2018-04-18 01:28:18)::x: [Build onnx 0.3.2352 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2352) (commit https://github.com/onnx/onnx/commit/6686357c06 by @smessmer)
AppVeyorBot(2018-04-19 01:17:52)::x: [Build onnx 0.3.2382 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2382) (commit https://github.com/onnx/onnx/commit/8675f5c055 by @smessmer)
AppVeyorBot(2018-04-20 07:08:54)::x: [Build onnx 0.3.2431 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2431) (commit https://github.com/onnx/onnx/commit/b1fca4ce56 by @smessmer)
AppVeyorBot(2018-04-20 07:45:54)::x: [Build onnx 0.3.2432 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2432) (commit https://github.com/onnx/onnx/commit/4432ef2374 by @smessmer)
AppVeyorBot(2018-04-28 00:13:29)::x: [Build onnx 0.3.2659 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2659) (commit https://github.com/onnx/onnx/commit/58f9567448 by @smessmer)
AppVeyorBot(2018-04-28 00:31:09)::x: [Build onnx 0.3.2661 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2661) (commit https://github.com/onnx/onnx/commit/7528335b97 by @smessmer)
AppVeyorBot(2018-04-28 00:38:38)::x: [Build onnx 0.3.2662 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2662) (commit https://github.com/onnx/onnx/commit/e6b816ed5a by @smessmer)
AppVeyorBot(2018-04-28 01:27:22)::x: [Build onnx 0.3.2665 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2665) (commit https://github.com/onnx/onnx/commit/f4e59cd99f by @smessmer)
AppVeyorBot(2018-05-08 20:25:50)::x: [Build onnx 0.3.2940 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2940) (commit https://github.com/onnx/onnx/commit/d1f987656a by @smessmer)
smessmer(2018-05-08 21:18:18):I'm replacing this with a stack of PRs:

https://github.com/onnx/onnx/pull/910
https://github.com/onnx/onnx/pull/911
https://github.com/onnx/onnx/pull/912
https://github.com/onnx/onnx/pull/913
https://github.com/onnx/onnx/pull/914
smessmer(2018-03-19 21:44:03):@houseroad It has advantages like exception safety (i.e. it's much harder to use it wrongly), and I don't see any disadvantages. Is there something I'm missing?
houseroad(2018-03-19 23:30:55):I prefer not introducing make_unique, since it won't bring many benefits in our case, and it may mislead developers by leaving them impression that they can use C++14 feature here. 
smessmer(2018-03-20 00:48:47):I don't think we should avoid language features we can make work in c++11 just because they look like c++14. The reason we're sticking to c++11 is not that we like it better, but that we have to support older compilers. Whatever features bring us advantages and we can make work in our set of supported compilers (i.e. in c++11), we should. It's the job of CI to make sure nobody accidentally uses c++14 features we didn't backport to c++11 (yet).
AppVeyorBot(2018-03-21 23:47:44)::x: [Build onnx 0.3.1739 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1739) (commit https://github.com/onnx/onnx/commit/e0d71437d6 by @smessmer)
AppVeyorBot(2018-03-22 01:11:02)::white_check_mark: [Build onnx 0.3.1748 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1748) (commit https://github.com/onnx/onnx/commit/9700e83b7e by @smessmer)
AppVeyorBot(2018-03-27 03:18:26)::x: [Build onnx 0.3.1872 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1872) (commit https://github.com/onnx/onnx/commit/c6f2be7dd6 by @bddppq)
AppVeyorBot(2018-03-27 04:44:33)::x: [Build onnx 0.3.1884 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1884) (commit https://github.com/onnx/onnx/commit/01b7beed3a by @smessmer)
AppVeyorBot(2018-03-27 04:48:47)::x: [Build onnx 0.3.1885 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1885) (commit https://github.com/onnx/onnx/commit/fa435aea11 by @smessmer)
AppVeyorBot(2018-03-29 08:24:16)::x: [Build onnx 0.3.1945 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1945) (commit https://github.com/onnx/onnx/commit/4c9b15dce9 by @bddppq)
houseroad(2018-03-19 21:45:59):If user want, they can have their own optimizer, which is a sub-class of our Optimizer class.
smessmer(2018-03-20 00:45:08):Ok makes sense. I removed `final` and added a virtual destructor to avoid partial destruction when people subclass it.
rdzhabarov(2018-03-20 02:20:03):curious, why there is a dup above
```
_registerOptimizer<FuseConsecutiveTransposes>();
_registerOptimizer<FuseConsecutiveTransposes>();
```
any specific reason?

bddppq(2018-03-20 08:43:24):It's probably copied from caffe2? folly later added an additional android check.
Besides, I recommend don't flip the if-else from the original file, especially when there are several conditional checks.

smessmer(2018-03-21 22:57:46):It was a slightly modified copy of the caffe2 implementation. I now switched to the one from folly.
smessmer(2018-03-21 23:29:55):nope. Thanks for catching.
bddppq(2018-03-26 20:46:59):Let's put it inside `ONNX_NAMESPACE` (In case of either our detection macros is not robust enough or there is another library does the same trick to pollute std namespace, there will be multiple definitions of std::make_unique)
yinghai(2018-03-14 04:40:05):> That's technically a C++14 feature. Do all our compilers support this? 

We now include ONNX as part of Caffe2 and Caffe2 only supports up to C++11... 
smessmer(2018-03-14 19:27:22):What guarantees do we give exactly? Is it "Caffe2 works on all compilers that support C++11"? Or is it "Caffe2 works on GCC since version x and Clang since version y"? In the latter case, we might be able to use some C++14 features since most old compiler versions support a subset of it.
bddppq(2018-03-14 20:38:30):ONNX has the same requirement/guarantee of using c++11
smessmer(2018-03-15 22:48:20):ok I re-established c++11 compatibility
AppVeyorBot(2018-03-15 23:18:47)::white_check_mark: [Build onnx 0.3.1621 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1621) (commit https://github.com/onnx/onnx/commit/256ffd7d88 by @smessmer)
AppVeyorBot(2018-03-21 20:38:27)::x: [Build onnx 0.3.1724 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1724) (commit https://github.com/onnx/onnx/commit/c4f44bece8 by @smessmer)
AppVeyorBot(2018-03-22 20:01:59)::white_check_mark: [Build onnx 0.3.1768 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1768) (commit https://github.com/onnx/onnx/commit/d1eaa51111 by @smessmer)
AppVeyorBot(2018-03-14 20:43:37)::white_check_mark: [Build onnx 0.3.1590 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1590) (commit https://github.com/onnx/onnx/commit/c461579e63 by @smessmer)
AppVeyorBot(2018-03-15 21:23:32)::white_check_mark: [Build onnx 0.3.1612 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1612) (commit https://github.com/onnx/onnx/commit/1eaf4e9cb9 by @smessmer)
AppVeyorBot(2018-03-14 22:36:00)::x: [Build onnx 0.3.1596 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1596) (commit https://github.com/onnx/onnx/commit/e31f5ee319 by @anderspapitto)
houseroad(2018-03-15 16:47:37):@anderspapitto could you also provide node test for ReshapeDynamic?
anderspapitto(2018-03-15 18:46:57):added tests
AppVeyorBot(2018-03-15 19:04:41)::x: [Build onnx 0.3.1609 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1609) (commit https://github.com/onnx/onnx/commit/70dd6fdd60 by @anderspapitto)
linkerzhang(2018-03-15 21:05:44):I'd suggest that we make a breaking change against existing Reshape OP (Versioning was designed for this purpose :) ).
CLAassistant(2018-03-15 00:37:00):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=616) <br/>All committers have signed the CLA.
Maratyszcza(2018-03-15 01:25:48):cc @ajtulloch, @jspark1105
linkerzhang(2018-03-15 01:47:13):Thank you very much @hlu1 for the PR. let's discuss and understand more about quantization cases here. https://gitter.im/onnx/quantization?utm_source=share-link&utm_medium=link&utm_campaign=share-link 
AppVeyorBot(2018-03-15 03:05:27)::x: [Build onnx 0.3.1599 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1599) (commit https://github.com/onnx/onnx/commit/8a7ef0391e by @linkerzhang)
AppVeyorBot(2018-03-15 03:19:34)::white_check_mark: [Build onnx 0.3.1600 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1600) (commit https://github.com/onnx/onnx/commit/26926f53d5 by @linkerzhang)
bddppq(2018-03-15 17:58:03):Here also needs to be removed:
 https://github.com/linkerzhang/onnx/blob/e0ffbea9d46007ea75cba1371c03a5715207d9f1/onnx/defs/schema.cc#L161
bddppq(2018-03-15 18:00:22):@houseroad Models in model zoo have `consumed_inputs` attributes
linkerzhang(2018-03-15 20:59:54):@bddppq Thank you for the comments! Removed the piece of codes. As you said, models in model zoo are using the consumed_inputs attribute, which is being broken.
AppVeyorBot(2018-03-15 21:09:54)::x: [Build onnx 0.3.1611 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1611) (commit https://github.com/onnx/onnx/commit/24a976b4e1 by @linkerzhang)
houseroad(2018-03-16 00:18:53):Let's fix the model zoo first, will do it soon :-)

Tracking here: https://github.com/onnx/models/issues/26
AppVeyorBot(2018-03-19 00:57:18)::x: [Build onnx 0.3.1645 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1645) (commit https://github.com/onnx/onnx/commit/d5a672d5a3 by @linkerzhang)
AppVeyorBot(2018-03-21 20:14:10)::x: [Build onnx 0.3.1722 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1722) (commit https://github.com/onnx/onnx/commit/4cf895eab8 by @linkerzhang)
AppVeyorBot(2018-03-21 20:44:33)::x: [Build onnx 0.3.1725 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1725) (commit https://github.com/onnx/onnx/commit/b51a82736d by @linkerzhang)
AppVeyorBot(2018-03-23 17:38:23)::x: [Build onnx 0.3.1801 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1801) (commit https://github.com/onnx/onnx/commit/56580e86b2 by @linkerzhang)
AppVeyorBot(2018-03-23 19:34:20)::x: [Build onnx 0.3.1813 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1813) (commit https://github.com/onnx/onnx/commit/ffbc03ae1b by @linkerzhang)
AppVeyorBot(2018-03-23 20:57:06)::white_check_mark: [Build onnx 0.3.1820 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1820) (commit https://github.com/onnx/onnx/commit/e591e2ef08 by @linkerzhang)
linkerzhang(2018-03-24 00:59:57):onnx-fb-universe error:
E   AssertionError: 'ir_version: 2\nproducer_name: "pytorch"\nproducer_version: "0.3"\ngraph {\n  no [truncated]... != 'ir_version: 3\nproducer_name: "pytorch"\nproducer_version: "0.3"\ngraph {\n  no [truncated]...
00:43:49 E   - ir_version: 2
00:43:49 E   ?             ^
00:43:49 E   + ir_version: 3
00:43:49 E   ?  

@houseroad I think the error is not related with this change, right?
bddppq(2018-03-24 06:59:47):@onnxbot retest this please
AppVeyorBot(2018-03-25 23:27:10)::white_check_mark: [Build onnx 0.3.1846 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1846) (commit https://github.com/onnx/onnx/commit/627c8f2451 by @linkerzhang)
linkerzhang(2018-03-25 23:50:54):Thank you very much @bddppq  and @houseroad  a lot for reviewing the PR and helping on make it happen. All comments are resolved and I'm merging it. Let me know if there're more comments please.
bddppq(2018-03-24 07:05:24):It has been bumped from 5 to 6 for this purpose https://github.com/onnx/onnx/commit/5f69c37628002efa8c03d70d652db03c9d5ffca7
houseroad(2018-03-24 07:07:44):Wait, as we discussed, we just keep using 6 right?
houseroad(2018-03-24 07:10:18):As we discussed, this should be .SinceVersion(6), (because in my previous PR, we already bump up the opset version, no need to do it again, otherwise, I have to prepare opset_7 model zoos, which does not exist yet... why our test case does not fail on this?
linkerzhang(2018-03-25 23:19:40):technically, it's risky to reuse 6. as in the time in-between, there could be models using the other operators with consumed_input with version 6. :) I'm changing to use version 6 though.

CLAassistant(2018-03-15 17:43:06):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=619) <br/>All committers have signed the CLA.
houseroad(2018-03-20 18:42:05):@onnxbot test this please
bddppq(2018-03-20 18:48:16):@onnxbot retest this please
AppVeyorBot(2018-03-21 21:58:57)::x: [Build onnx 0.3.1729 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1729) (commit https://github.com/onnx/onnx/commit/e0331d0e97 by @houseroad)
ruimashita(2018-03-22 03:20:28):@houseroad 
Thanks for review.
OK. I will split the two test cases into two functions.
ruimashita(2018-03-22 11:10:26):I did it 😄 
AppVeyorBot(2018-03-22 19:40:53)::white_check_mark: [Build onnx 0.3.1766 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1766) (commit https://github.com/onnx/onnx/commit/b354fafb0c by @ruimashita)
CLAassistant(2018-03-15 17:49:25):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=620) <br/>All committers have signed the CLA.
AppVeyorBot(2018-03-17 01:14:07)::white_check_mark: [Build onnx 0.3.1637 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1637) (commit https://github.com/onnx/onnx/commit/bd207538a0 by @raymondxyang)
linkerzhang(2018-03-16 17:00:51):I think that ONNX_ML = 1 covers ONNX_ML=0, so let's remove ONNX_ML=0 part.
linkerzhang(2018-03-16 17:08:00):I think this can be removed in this way.
AppVeyorBot(2018-03-17 03:30:16)::x: [Build onnx 0.3.1640 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1640) (commit https://github.com/onnx/onnx/commit/bcac359d19 by @jamesr66a)
Maratyszcza(2018-03-17 04:37:08):@hlu1 did the size measurements on FB use-cases
smessmer(2018-03-19 21:23:54):I think it would be better if the macro doesn't have to be used on each operator definition. It could be hidden in the DSL instead.
bddppq(2018-03-19 22:01:54):I would prefer not to add these ifdefs since from our analysis doc strings only account for very small portion to the code size. @hlu1 has done a code size analysis for onnx:

```
                                           FILE SIZE
                                      --------------
 onnx/onnx/common/ir_pb_converter.cc   1.17Mi   6.3%
 onnx/onnx/defs/math/defs.cc           1.14Mi   6.1%
 onnx/onnx/common/interned_strings.cc  1.04Mi   5.6%
 onnx/onnx/checker.cc                  1.01Mi   5.4%
 [section .debug_loc]                   813Ki   4.3%
 onnx/onnx/defs/nn/defs.cc              797Ki   4.2%
 onnx/onnx/optimizer/optimize.cc        735Ki   3.9%
 onnx/onnx/defs/experiments/defs.cc     718Ki   3.8%
 onnx/onnx/defs/schema.cc               682Ki   3.6%
 onnx/onnx/defs/tensor/defs.cc          645Ki   3.4%
...
```

Considering we have <150 ops and assuming each op has <1000 chars doc strings, total number of them would be <150k, looks to me striping them won't help too much.

Maratyszcza(2018-03-20 00:48:40):@bddppq For mobile, 150K is a lot!
bddppq(2018-03-20 00:51:48):@Maratyszcza 150k is uncompressed, compare it with other entries in the size table
Maratyszcza(2018-03-20 00:57:25):Something isn't right with the table, it probably includes debug symbols
dzhulgakov(2018-03-21 21:00:20):I wonder if we can move setDoc as an inline function to the header file and then C++ would be smart enough to eliminate string literals even without flto
smessmer(2018-03-21 22:48:32):@dzhulgakov Yes, that's what I meant above with "hiding it in the DSL". The C++ stripper should be smart enough to do that, especially if we make the strings `constexpr` and keep them declare-only (i.e. don't provide a definition). Might also work if they're just inline-defined strings in the `.setDoc()` call.
bddppq(2018-05-08 18:02:39):@jamesr66a @Maratyszcza Is this PR still needed?
jamesr66a(2018-05-08 21:10:20):@Maratyszcza @hlu1 if you all need this I can keep this up, otherwise I'll probably close
Maratyszcza(2018-05-08 21:12:11):@bddppq @jamesr66a We certainly need it for OSS mobile build. Any reasons to not merge it?
bddppq(2018-05-08 21:41:36):@Maratyszcza For Caffe2 it has been configured to only link with onnx pb.{h,cc} files in mobile builds

And the reason I'm against adding these ifdefs is this requires manually wrapping string literals in couple places, so it will make the onnx schema files harder to maintain. On the other hand, I don't see proofs of nonnegligible (compressed) size saving in mobile builds.
Maratyszcza(2018-05-08 22:16:06):Ok, if schema is not included in mobile builds, it is safe do drop this PR
smessmer(2018-03-19 21:59:28):Will add a PR to onnx-coreml shortly. The general syntax is:

    backend_test = onnx.backend.test.BackendTest(CoreMLBackend, __name__)
    backend_test.include('test_densenet121')
    backend_test.tolerance('test_densenet121', rtol=1e-3, atol=1e-7)
houseroad(2018-03-19 22:01:57):Could you add the information in data.json for each model? such as
https://github.com/onnx/onnx/blob/master/onnx/backend/test/data/model/real/test_bvlc_alexnet/data.json

And also the logic to parse this info
smessmer(2018-03-19 22:04:19):You mean the model should specify the tolerance? I thought it might be different for different backends, that's why I added it to the test definition instead of the model definition. i.e., this way, backends can choose their own tolerance settings if they need it for a model.
houseroad(2018-03-19 22:07:18):Yeah, I mean each model can provide a default tolerance. Of course, we can change that in ONNX backend test, just like your code did.
AppVeyorBot(2018-03-19 22:55:26)::white_check_mark: [Build onnx 0.3.1665 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1665) (commit https://github.com/onnx/onnx/commit/7dfd036173 by @smessmer)
smessmer(2018-03-19 22:58:35):Having a default value per model makes sense. It's probably a separate stream of work though and can be done independently from this PR.
pk-g(2018-03-19 23:08:32):Thanks for the PR! this would be really helpful. LG in general, with a minor suggestion. Let's consider the example you provided for setting tolerance on densenet:

"backend_test = onnx.backend.test.BackendTest(CoreMLBackend, __name__)
 backend_test.include('test_densenet121')
 backend_test.tolerance('test_densenet121', rtol=1e-3, atol=1e-7)"

Assuming that we include a batch of tests including above, wouldn't this setup override the global tolerance for other tests as well? would it be possible to add the tolerance upon inclusion only for the given case? (i.e. something like, backend_test.include('text_densenet121', rtol=1e-3, atol= 1e-7)? 

AppVeyorBot(2018-03-19 23:36:56)::white_check_mark: [Build onnx 0.3.1670 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1670) (commit https://github.com/onnx/onnx/commit/90ce9a5101 by @houseroad)
smessmer(2018-03-20 00:51:19):@pk-g : Calling this `tolerance()` function will only set the tolerance for the one test you specify as its parameter.
bddppq(2018-03-20 17:53:33):The current diff only allows configuring these tolerances for model tests, not node tests.
houseroad(2018-03-20 18:13:07):@smessmer like @bddppq and I mentioned, check line 296, you don't provide the rtol and atol for node_test. Please fix it. Here is the log: https://ci.pytorch.org/jenkins/job/caffe2-builds/job/py2-cuda8.0-cudnn6-ubuntu16.04-test/1701/console
dzhulgakov(2018-03-21 20:50:59):Didn't we want to do all node tests behave as model tests (doesn't really make sense to have this difference). And we certainly want to specify tolerance on test level in the onnx repo instead of making each backend copy-paste the same regex + tolerance override. If it's blocking - let's land this one but please follow up.
AppVeyorBot(2018-03-21 22:10:23)::white_check_mark: [Build onnx 0.3.1730 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1730) (commit https://github.com/onnx/onnx/commit/8bf11ccf3e by @smessmer)
bddppq(2018-05-21 18:59:50):@smessmer Are you still working on this one?
houseroad(2018-09-05 17:46:20):@Ac2zoom would you like to pick up this one?
houseroad(2018-03-19 22:12:24):you probably want to set the default values, in case you have a lot node tests, do you really want to manually set them one by one. Check line 296
smessmer(2018-03-19 22:57:38):The default values are still there, they just moved to the `_tolerance` function.
bddppq(2018-03-20 07:44:07):make these two constants configurable (and expose them in the class level).
bddppq(2018-03-20 07:48:07):Maybe do `rtol=None` and `atol=None` and set them to the default value if not specified? So people don't have to override both of them.
bddppq(2018-03-20 07:54:18):You can finalize these configurations in `tests_cases`, `test_suites` and `tests` functions, to avoid doing this search repeatedly inside the real tests (extend `Testitem` store the corresponding `rtol` and `atol` as metadata).
smessmer(2018-03-21 21:14:15):This method is private and only used inside this class. I'll add default parameters to the `tolerance()` method though, which is the one called from outside.
bddppq(2018-03-21 22:06:06):I tag the wrong line, I meant in `def tolerance`, not `def _assert_similar_outputs`. But looks like you have done the right change :-)
CLAassistant(2018-03-20 21:44:38):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=633) <br/>All committers have signed the CLA.
anirudhacharya(2018-03-20 21:47:47):I see that all the previous releases, have a separate remote branch on the main repo with the release code. Should I similarly create a rel-1.1.0 branch on the main repo
bddppq(2018-03-20 23:58:19):https://github.com/onnx/onnx/tree/rel-1.1.0
AppVeyorBot(2018-03-21 00:18:06)::white_check_mark: [Build onnx 0.3.1699 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1699) (commit https://github.com/onnx/onnx/commit/cce2f95c61 by @hlu1)
CLAassistant(2018-03-21 06:54:56):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=637) <br/>All committers have signed the CLA.
bddppq(2018-03-21 07:00:01):This is not right, all the pybind11 includes in our code have the "pybind/" part.
Matrixsun(2018-03-21 07:15:16):after   "third_party/pybind11/include" changing  to "third_party/pybind11/include/pybind11"  ,  then can find the real include file  in line 200 of CMakeLists.txt
bddppq(2018-03-21 17:16:25):@Matrixsun could you elaborate? which include in our c++ files needs to look for pybind11 header file from "third_party/pybind11/include/pybind11" instead of "third_party/pybind11/include"?
AppVeyorBot(2018-03-22 00:15:42)::x: [Build onnx 0.3.1742 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1742) (commit https://github.com/onnx/onnx/commit/72cae5d7e4 by @houseroad)
bddppq(2018-03-23 18:29:20):closing. feel free to reopen if you have pybind11 includes issue @Matrixsun 
AppVeyorBot(2018-03-21 19:07:50)::x: [Build onnx 0.3.1720 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1720) (commit https://github.com/onnx/onnx/commit/402a12a71a by @houseroad)
houseroad(2018-03-21 19:39:27):@onnxbot retest this please
AppVeyorBot(2018-03-21 19:55:48)::x: [Build onnx 0.3.1721 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1721) (commit https://github.com/onnx/onnx/commit/ccb721c516 by @houseroad)
AppVeyorBot(2018-03-21 23:57:11)::white_check_mark: [Build onnx 0.3.1740 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1740) (commit https://github.com/onnx/onnx/commit/62e2961bb7 by @houseroad)
tjingrant(2018-03-22 00:09:02):~~EnforceConsumed seems to be in the pip release as well.~~ EnforceConsumed is causing CI troubles for us : https://travis-ci.org/onnx/onnx-tensorflow/jobs/356599866. 

edit: it's just the github master branch that's causing errors for us. please ignore the pip part.
houseroad(2018-03-22 00:20:48):@tjingrant I think you should be good now.
linkerzhang(2018-03-22 04:04:07):so this is good to be merged now? :)
AppVeyorBot(2018-03-22 04:37:22)::x: [Build onnx 0.3.1752 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1752) (commit https://github.com/onnx/onnx/commit/08e54b8882 by @houseroad)
AppVeyorBot(2018-03-22 04:59:53)::x: [Build onnx 0.3.1754 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1754) (commit https://github.com/onnx/onnx/commit/f3518dbcb8 by @houseroad)
AppVeyorBot(2018-03-22 05:12:14)::white_check_mark: [Build onnx 0.3.1755 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1755) (commit https://github.com/onnx/onnx/commit/3a07a49260 by @houseroad)
AppVeyorBot(2018-03-22 18:16:52)::x: [Build onnx 0.3.1758 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1758) (commit https://github.com/onnx/onnx/commit/9ab75dffa8 by @houseroad)
AppVeyorBot(2018-03-22 19:09:48)::white_check_mark: [Build onnx 0.3.1763 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1763) (commit https://github.com/onnx/onnx/commit/52dc348661 by @houseroad)
CLAassistant(2018-03-22 20:16:04):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=644) <br/>All committers have signed the CLA.
AppVeyorBot(2018-03-22 20:34:05)::white_check_mark: [Build onnx 0.3.1772 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1772) (commit https://github.com/onnx/onnx/commit/f7880d9bd8 by @liqunfu)
AppVeyorBot(2018-03-26 17:19:16)::x: [Build onnx 0.3.1849 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1849) (commit https://github.com/onnx/onnx/commit/67ab6736e0 by @liqunfu)
bddppq(2018-03-27 23:27:42):closing #668 
AppVeyorBot(2018-03-22 23:56:27)::white_check_mark: [Build onnx 0.3.1774 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1774) (commit https://github.com/onnx/onnx/commit/340fd32652 by @jamesr66a)
yinghai(2018-03-23 20:27:24):@anderspapitto It's used when you want to compile `onnx` into a library. For example: 
https://github.com/caffe2/caffe2/blob/c7ea012b844aa45acfc77a5a91370614c6b27ff0/cmake/Dependencies.cmake#L568
yinghai(2018-03-24 04:08:21):CI stuck in Mac build which is not related to this change. Merging. 
AppVeyorBot(2018-03-23 05:34:42)::x: [Build onnx 0.3.1787 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1787) (commit https://github.com/onnx/onnx/commit/cc57cf7ef7 by @jamesr66a)
AppVeyorBot(2018-03-23 05:43:28)::white_check_mark: [Build onnx 0.3.1788 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1788) (commit https://github.com/onnx/onnx/commit/a82e2a2e6f by @jamesr66a)
houseroad(2018-03-23 05:28:12):yeah, we need it :-)
bddppq(2018-03-23 09:31:15):this is based on https://github.com/onnx/onnx/pull/650
bddppq(2018-03-23 18:14:28):this needs https://github.com/onnx/onnx/pull/652
AppVeyorBot(2018-03-23 18:47:22)::x: [Build onnx 0.3.1807 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1807) (commit https://github.com/onnx/onnx/commit/7da35eddb2 by @bddppq)
AppVeyorBot(2018-03-23 20:02:12)::white_check_mark: [Build onnx 0.3.1815 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1815) (commit https://github.com/onnx/onnx/commit/3a7dec320e by @bddppq)
bddppq(2018-03-26 19:09:46):@houseroad Do you want to wait for other open PRs that are related to backend tests to be merged first?
houseroad(2018-03-26 19:11:55):@bddppq I think it's ok to merge this first. I will review later... Let me first unblock you :-)
bddppq(2018-03-26 20:39:33):@houseroad This one is not blocking me, feel free to make your choice :-)
AppVeyorBot(2018-03-26 23:39:45)::x: [Build onnx 0.3.1859 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1859) (commit https://github.com/onnx/onnx/commit/4e09bec913 by @houseroad)
AppVeyorBot(2018-03-27 03:55:30)::x: [Build onnx 0.3.1877 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1877) (commit https://github.com/onnx/onnx/commit/0cd0f9ffd6 by @bddppq)
AppVeyorBot(2018-03-27 04:57:48)::x: [Build onnx 0.3.1886 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1886) (commit https://github.com/onnx/onnx/commit/bdc3e9bf29 by @bddppq)
AppVeyorBot(2018-03-27 05:07:18)::white_check_mark: [Build onnx 0.3.1887 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1887) (commit https://github.com/onnx/onnx/commit/bdc3e9bf29 by @bddppq)
houseroad(2018-03-27 18:03:35):Probably add another two fields: atol, rtol for future use?

Also some comments to explain what the purpose of each field is?
bddppq(2018-03-27 18:36:15):atol and rtol should be dealt in this PR https://github.com/onnx/onnx/pull/628
houseroad(2018-03-23 18:18:46):Check output == 1?
AppVeyorBot(2018-03-23 19:48:46)::x: [Build onnx 0.3.1814 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1814) (commit https://github.com/onnx/onnx/commit/10b672266c by @anderspapitto)
anderspapitto(2018-03-23 21:02:37):closing temporarily so that my other PR runs sooner in appveyor
AppVeyorBot(2018-03-23 21:45:12)::white_check_mark: [Build onnx 0.3.1827 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1827) (commit https://github.com/onnx/onnx/commit/8a4e6f81ba by @anderspapitto)
yinghai(2018-03-23 20:23:12):Once you use `override`, you can remove `virtual`. 
anderspapitto(2018-03-23 21:02:58):closing temporarily so that my other PR runs sooner in appveyor
AppVeyorBot(2018-03-23 21:32:33)::x: [Build onnx 0.3.1826 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1826) (commit https://github.com/onnx/onnx/commit/10fd968a0b by @anderspapitto)
AppVeyorBot(2018-03-23 22:06:38)::x: [Build onnx 0.3.1829 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1829) (commit https://github.com/onnx/onnx/commit/f15b721c7b by @anderspapitto)
AppVeyorBot(2018-03-23 22:55:14)::white_check_mark: [Build onnx 0.3.1833 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1833) (commit https://github.com/onnx/onnx/commit/6ad9c0ca43 by @anderspapitto)
AppVeyorBot(2018-03-27 22:13:56)::x: [Build onnx 0.3.1899 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1899) (commit https://github.com/onnx/onnx/commit/e33c3df062 by @anderspapitto)
AppVeyorBot(2018-04-03 23:33:41)::x: [Build onnx 0.3.2041 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2041) (commit https://github.com/onnx/onnx/commit/dd489bbcdd by @anderspapitto)
AppVeyorBot(2018-04-03 23:52:30)::x: [Build onnx 0.3.2044 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2044) (commit https://github.com/onnx/onnx/commit/bf1f4be218 by @anderspapitto)
anderspapitto(2018-04-05 19:34:14):@dzhulgakov review again? test failure is spurious CI issue
anderspapitto(2018-04-06 21:52:20):@linkerzhang are you ok with me adding the additional utility methods to InferenceContext as requested by @dzhulgakov, or would you prefer for me to keep them separate?
AppVeyorBot(2018-04-10 04:10:49)::white_check_mark: [Build onnx 0.3.2213 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2213) (commit https://github.com/onnx/onnx/commit/1d9e301306 by @anderspapitto)
AppVeyorBot(2018-04-10 07:39:48)::white_check_mark: [Build onnx 0.3.2215 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2215) (commit https://github.com/onnx/onnx/commit/56ef3f6d4d by @anderspapitto)
AppVeyorBot(2018-04-10 23:53:33)::white_check_mark: [Build onnx 0.3.2244 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2244) (commit https://github.com/onnx/onnx/commit/5ab4db94e4 by @anderspapitto)
AppVeyorBot(2018-04-11 00:41:52)::white_check_mark: [Build onnx 0.3.2246 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2246) (commit https://github.com/onnx/onnx/commit/6d6a2f758c by @anderspapitto)
dzhulgakov(2018-03-27 07:47:26):this is just getNumInputs?
dzhulgakov(2018-03-27 07:48:51):put it into InferenceContext itself?

and maybe something like 'typeSameAsInput' or 'propagateTypeFromInputToOutput'
dzhulgakov(2018-03-27 07:51:32):so what would be the shape if we don't even know the rank of some tensor? would you expect that shape won't be set at all in this case? Then we need to test for it somehow
dzhulgakov(2018-03-27 07:52:31):it's not obvious that it appends to the shape tensor at all
dzhulgakov(2018-03-27 07:53:15):should we just enforce that it has only 1 input?
dzhulgakov(2018-03-27 07:54:18):you need to check that shape() is not set. Size zero shape basically means scalar
dzhulgakov(2018-03-27 07:55:03):hm, I'm curious whether we should throw in this case
dzhulgakov(2018-03-27 07:55:47):so elem_type actually defaults to FLOAT in protobuf. We should verify that the field is set
dzhulgakov(2018-03-27 07:56:38):you don't handle dim_param here
anderspapitto(2018-03-27 20:26:15):@linkerzhang are you ok with adding helpers as methods of InferenceContext, or do you prefer that I leave them separate?
anderspapitto(2018-03-27 20:28:00):this is implementing "By default, reverse the dimensions" from the ONNX spec for transpose. I can add a comment
anderspapitto(2018-03-27 20:29:10):++
anderspapitto(2018-03-27 20:31:32):why/where does it default to FLOAT?
anderspapitto(2018-03-27 20:32:39):yes
anderspapitto(2018-03-27 20:33:51):seems like more the role of the checker
anderspapitto(2018-03-27 20:34:02):sure, can rename
anderspapitto(2018-03-27 20:36:26):yeah, I guess we might as well throw here, and convert it to a warning message in the main loop
anderspapitto(2018-03-27 20:46:55):yes, I intentionally left that out for now. If I infer a shape of `(3, x, 10)`, it's not quite clear what to do with the `x`, so I just leave the second dimension alone
anderspapitto(2018-03-27 21:36:57):ah I see what you mean. I guess there are two cases

- there is no type information at all (handled at the top of the function). This represents unknown rank

- dim_size() == 0. This would represent a scalar tensor, and this code should handle that fine (it puts nothing in the output type dimensions). I'll add a test for this case
linkerzhang(2018-04-03 00:36:39):can we rename this file please? I'm seeing two files with name "shape_inference.h".
dzhulgakov(2018-04-06 18:56:19):so can it be just getNumInputs? with "types" it reads weird
bddppq(2018-04-06 19:57:48):shouldn't this be verified by the checker?
bddppq(2018-04-06 19:58:53):nit: --i
bddppq(2018-04-06 19:59:19):++i
bddppq(2018-04-06 20:01:04):nit: Could you change it to "if (!schema) continue;" to avoid one extra level of indentation?
bddppq(2018-04-06 20:02:38):Doesn't look like we need this extra tmp variable.
bddppq(2018-04-06 20:08:11):From line 51 to here is to construct a context from a graph, maybe put these into the constructor InferenceContext?
bddppq(2018-04-06 20:24:12):scalar's dim_size is 0, so we can not use it as the null check
bddppq(2018-04-06 20:26:05):How about let's change the code starts from here to the end into:
```
auto iter = valueTypesByName.find(output);
if (iter != valueTypesByName.end()) {
   assert_equal_value_info(iter->second, inferredType);
} else {
   iter->second = inferredType;
}
```
(of course will need to add a separate function `assert_equal_value_info` dedicated for doing the equality check).
bddppq(2018-04-06 20:27:24):nit: change this to absolute include
bddppq(2018-04-06 20:27:51):Do we still need this typedef?
anderspapitto(2018-04-06 20:37:22):even if it has 1 input, that input might not have a ValueInfo
anderspapitto(2018-04-06 20:37:34):sure
anderspapitto(2018-04-06 20:37:44):++
anderspapitto(2018-04-06 20:38:29):I already have these changes in a followup diff https://github.com/onnx/onnx/pull/702/files

no reason to pull them into this one
anderspapitto(2018-04-06 20:39:06):++
anderspapitto(2018-04-06 20:41:42):actually this is also in https://github.com/onnx/onnx/pull/702/files, don't want to pull it in here
anderspapitto(2018-04-06 20:43:48):++
anderspapitto(2018-04-06 20:44:34):nope, will remove
anderspapitto(2018-04-06 20:45:13):actually, yeah can take it out
anderspapitto(2018-04-06 20:47:23):++
anderspapitto(2018-04-06 21:01:20):it's not an equality check though. There can be an existing value info with partial information, which needs to be merged in (e.g. one of the dims is "x3", and we infer it to be 5
dzhulgakov(2018-04-06 21:21:33):maybe make it part of the InferenceContext for consistency?
dzhulgakov(2018-04-06 21:21:42):same?
dzhulgakov(2018-04-06 21:23:03):actually rename it to numInputs/getNumInputs? "Types" part is confusing
dzhulgakov(2018-04-06 21:24:35):more details in the error message?
dzhulgakov(2018-04-06 21:26:06):hm, I think you actually need to error out if the sizes are different. I.e. if ranks conflict
dzhulgakov(2018-04-06 21:26:22):maybe extract all this comparison semantics into a separate function?
anderspapitto(2018-04-06 21:33:20):I will do this iff @linkerzhang is ok with it
anderspapitto(2018-04-06 21:33:38):sure
anderspapitto(2018-04-06 21:33:47):++
anderspapitto(2018-04-06 21:34:06):hmm
anderspapitto(2018-04-06 21:37:41):yeah, i guess I am working around some bugs in make_tensor_value_info() on the python side - it still uses empty dimensions to represent a missing shape. I'll fix that code and then I can just use has_shape() here
bddppq(2018-04-07 18:06:05):oh right, then how about
```
auto iter = valueTypesByName.find(output);
if (iter != valueTypesByName.end()) {
   assert_loose_equal_value_info(iter->second, inferredType);
}
iter->second = inferredType;
```

I guess my point is to split out the checking and updating logic :-)
anderspapitto(2018-04-09 18:06:21):well that also doesn't quite work - the inferredType may also lack information which the existing ValueInfo already contains.

I'll pull the logic out in some fashion though
bddppq(2018-04-09 20:13:03):const AttributeProto&
bddppq(2018-04-09 20:15:12):`self.assertRaises`
bddppq(2018-04-09 20:25:29):const
bddppq(2018-04-09 20:31:23):this doesn't seem to be needed
bddppq(2018-04-09 20:32:31):You need to pass the opset_version to get the correct schema.
bddppq(2018-04-09 20:34:29):++i
bddppq(2018-04-09 20:34:36):++j
bddppq(2018-04-09 20:37:05):Here should error out if `existingType` is existing ValueInfo in the graph
bddppq(2018-04-09 20:39:36):remove this clause
linkerzhang(2018-04-09 21:48:39):it seems to me that they're helper functions, let's keep them outside of the interface for now. Thank you very much!
bddppq(2018-04-10 19:03:22):These should be moved to proto_utils.{h,cc}. (Can be done is separate PR).
bddppq(2018-04-10 19:03:36):nit: absolute include
bddppq(2018-04-10 19:08:04):Why not putting these into the InferenceContext class? They are tightly tied to InferenceContext and there is no way to use them.
@linkerzhang 
bddppq(2018-04-10 19:16:28):So in case `perm` attribute is provided, and shape is not set, this will cause segfault.
bddppq(2018-04-10 19:21:42):These should be removed.
anderspapitto(2018-04-10 19:31:19):++
anderspapitto(2018-04-10 19:31:23):++
anderspapitto(2018-04-10 19:33:12):I think either way is acceptable. @linkerzhang has asked that we keep them out of InferenceContext though (I guess so the API exposed to 3rd parties is smaller), so I am keeping it separate
anderspapitto(2018-04-10 19:35:35):++
anderspapitto(2018-04-10 19:35:58):++ I'll extract the relevant bits of CheckerContext
bddppq(2018-04-10 21:02:34):Upper case the function name so it's consistent with other functions in this file.
bddppq(2018-04-10 21:04:39):nit: const
bddppq(2018-04-10 21:12:47):If existingType has multiple dims but the inferredType has zero dim, then this for loop will be skipped and so we won't be able to detect the mismatch.
bddppq(2018-04-10 21:14:15):This doesn't need to call "mutable"_shape(), in general we should be careful about calling it because it can change the semantics of whether the shape is not set or it's a scalar.
CLAassistant(2018-03-24 02:19:39):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=657) <br/>All committers have signed the CLA.
bddppq(2018-03-24 04:36:25):@anirudhacharya please sign the CLA
AppVeyorBot(2018-03-24 04:51:27)::white_check_mark: [Build onnx 0.3.1841 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1841) (commit https://github.com/onnx/onnx/commit/041d63b4af by @bddppq)
AppVeyorBot(2018-03-24 23:26:28)::x: [Build onnx 0.3.1845 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1845) (commit https://github.com/onnx/onnx/commit/44bf341c25 by @dzhulgakov)
AppVeyorBot(2018-03-26 17:58:35)::white_check_mark: [Build onnx 0.3.1850 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1850) (commit https://github.com/onnx/onnx/commit/cb174378bd by @liqunfu)
bddppq(2018-03-27 23:27:35):closing #668 
dzhulgakov(2018-03-27 03:53:30):Maybe instead of having a flag we can have a `dim_param` with empty string? Basically meaning "not filled in"
anderspapitto(2018-03-27 21:06:13):@linkerzhang would you be ok with using the empty string name as @dzhulgakov suggests?

All unique names works, but it is an extra burden on anyone generating onnx models (have to have a supply of unique names), and also might make them less readable (you have to look at the whole model to find if a name is unique, instead of just seeing that it is UNSPECIFIED or "")
houseroad(2018-03-27 23:35:30):I agree with @linkerzhang and @dzhulgakov, adding another flag to the Dimension is too much. And we should avoid changing the proto files if not necessary. We should use a special symbolic variable (name) to represent the unspecified dimension, "" sounds good to me, or you can choose another one.
linkerzhang(2018-03-27 00:27:04):I'd keep current design to use unique symbolic name in this case as implied here, other than having another flag (enum). 
AppVeyorBot(2018-03-27 04:31:05)::white_check_mark: [Build onnx 0.3.1882 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1882) (commit https://github.com/onnx/onnx/commit/1492766b93 by @liqunfu)
bddppq(2018-03-27 23:26:53):closing #668 
bddppq(2018-03-27 23:26:22):closing #668 
bddppq(2018-03-27 23:27:07):closing #668 
linkerzhang(2018-03-27 20:47:25):@bddppq In this PR, I changed "axes" for Reduce ops to be required. It seems that it breaks some model verification in onnx-fb-universe. What's the default value in caffe2 given it's not a required attribute please? Thank you!
bddppq(2018-03-27 21:20:36):@linkerzhang it's not about default values but our pytorch exporter doesn't add the `axes` attribute when exporting `prod`, and this is because this PR has changed the schema but not bumped the version.
houseroad(2018-03-27 21:21:22):In Caffe2, if axes is empty, we reduce along all the dimensions. This behavior is consist with other ops in onnx spec, such as slice.

I also checked numpy and tensorflow, they have the same definition on empty axes  list. So we should allow the empty axes, and provide the same semantics (full dimension reduction).
linkerzhang(2018-04-02 04:32:10):@houseroad / @bddppq , ok, I'm keeping axes as OPTIONAL as it was before.
AppVeyorBot(2018-04-03 05:41:35)::white_check_mark: [Build onnx 0.3.2009 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2009) (commit https://github.com/onnx/onnx/commit/523e291729 by @houseroad)
AppVeyorBot(2018-04-03 15:22:07)::x: [Build onnx 0.3.2013 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2013) (commit https://github.com/onnx/onnx/commit/80016a1b53 by @houseroad)
AppVeyorBot(2018-04-03 16:22:59)::x: [Build onnx 0.3.2015 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2015) (commit https://github.com/onnx/onnx/commit/239803bf95 by @linkerzhang)
AppVeyorBot(2018-04-03 21:53:11)::white_check_mark: [Build onnx 0.3.2033 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2033) (commit https://github.com/onnx/onnx/commit/0fba15ca38 by @linkerzhang)
AppVeyorBot(2018-04-04 03:54:28)::x: [Build onnx 0.3.2060 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2060) (commit https://github.com/onnx/onnx/commit/7ceaee1e76 by @linkerzhang)
bddppq(2018-04-07 04:23:29):@ezyang @linkerzhang Do you want to apply the new opset_version bump mechanism (aka. tie to release) starting from now? 
linkerzhang(2018-04-07 15:02:10):@bddppq ,I'd suggest doing that. @ebarsoum will talk with folks in next week's sync meeting on this proposal.
bddppq(2018-04-02 04:54:28):nit: maybe sort this list in semantics (like uint*, int*, float*, double)? Right now they are out of order (e.g float is the first one and double is the last one).
houseroad(2018-04-03 17:29:23):shall we use int64 here to be consistent with topk?
linkerzhang(2018-04-03 18:26:46):I think, we should change topK's indices (2nd output) with type "uint64" too, since there's no negative index. :)
linkerzhang(2018-04-03 18:32:18):sure.
bddppq(2018-04-04 02:13:10):I had some offline discussions with @dzhulgakov, we think it's better to use int64 to represent indices:

1. some system doesn't support uint64 (e.g. Java)
2. in case error happened, say -1 is returned, it's easier to discover when it's -1 instead of wraparound
houseroad(2018-04-04 15:52:01):Another data point is from tensorflow, they also only support signed int for the indices.

For example, in their argmax/argmin, https://www.tensorflow.org/api_docs/python/tf/argmax, the return values (indices) are either int or int64. I think signed int is more expressive when we need to return some special values/errors.
linkerzhang(2018-04-06 18:20:08):Thank you guys! Changed to int64 now.
bddppq(2018-04-07 04:16:55):nit: sort these types :-)
houseroad(2018-03-27 22:20:59):@liqunfu could you keep only one pr for the same content?
AppVeyorBot(2018-03-27 22:25:41)::white_check_mark: [Build onnx 0.3.1900 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1900) (commit https://github.com/onnx/onnx/commit/8461e749c6 by @liqunfu)
bddppq(2018-03-28 00:50:33):cc @linkerzhang 
bddppq(2018-03-28 00:51:05):@onnxbot retest this please
AppVeyorBot(2018-03-28 06:35:58)::x: [Build onnx 0.3.1914 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1914) (commit https://github.com/onnx/onnx/commit/1d7c446016 by @bddppq)
bddppq(2018-03-28 06:40:36):@onnxbot  retest this please
AppVeyorBot(2018-03-28 19:59:01)::x: [Build onnx 0.3.1926 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1926) (commit https://github.com/onnx/onnx/commit/12adc13359 by @liqunfu)
AppVeyorBot(2018-03-28 20:50:45)::white_check_mark: [Build onnx 0.3.1927 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1927) (commit https://github.com/onnx/onnx/commit/3dae84b640 by @liqunfu)
houseroad(2018-03-29 17:24:30):@onnxbot test this please
houseroad(2018-04-06 22:28:43):@liqunfu friendly pint, could you address the comments?
houseroad(2018-04-13 21:34:32):@liqunfu friendly pint, could you address the comments?
liqunfu(2018-04-13 22:46:21):Thanks @houseroad. You comments are very valid. Sorry I just need to complete some work before coming back addressing it.  Hope I do not block anything. 

houseroad(2018-04-13 22:48:27):@liqunfu no worries, take your time, it's not blocking me.
AppVeyorBot(2018-04-26 03:32:24)::x: [Build onnx 0.3.2594 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2594) (commit https://github.com/onnx/onnx/commit/9289ced63b by @linkerzhang)
AppVeyorBot(2018-05-02 18:22:23)::x: [Build onnx 0.3.2761 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2761) (commit https://github.com/onnx/onnx/commit/fd0e2403e2 by @liqunfu)
AppVeyorBot(2018-05-02 18:42:47)::white_check_mark: [Build onnx 0.3.2762 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2762) (commit https://github.com/onnx/onnx/commit/5a562d17ce by @liqunfu)
AppVeyorBot(2018-05-02 22:58:57)::white_check_mark: [Build onnx 0.3.2774 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2774) (commit https://github.com/onnx/onnx/commit/570ba536f0 by @liqunfu)
liqunfu(2018-05-03 14:29:17):@houseroad, please when you get time check if this current PR is ok. just let me if further change is needed. I just got a break after our April sprint end to revisit this. thanks.
AppVeyorBot(2018-05-04 20:09:01)::white_check_mark: [Build onnx 0.3.2855 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2855) (commit https://github.com/onnx/onnx/commit/6e4559a3e4 by @liqunfu)
liqunfu(2018-05-09 17:39:36):@houseroad @bddppq @linkerzhang Is this PR ready for merge into the master? 
houseroad(2018-05-10 00:13:44):@liqunfu I will take a look this week.
AppVeyorBot(2018-05-10 01:02:33)::x: [Build onnx 0.3.2992 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2992) (commit https://github.com/onnx/onnx/commit/8f34501de3 by @houseroad)
AppVeyorBot(2018-05-10 05:46:08)::x: [Build onnx 0.3.3005 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.3005) (commit https://github.com/onnx/onnx/commit/576ae2f99e by @liqunfu)
AppVeyorBot(2018-05-10 17:03:50)::x: [Build onnx 0.3.3016 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.3016) (commit https://github.com/onnx/onnx/commit/2a5efcaa37 by @liqunfu)
liqunfu(2018-05-10 17:43:25):@houseroad I am working on the reduce_sum test failure. However, after I have done: git clean/reinstall/generate doc/pytest, the test_data_set for reduce_sum did not get updated at my local and I am still getting the same test failure as from ONNX build system. 
AppVeyorBot(2018-05-10 18:06:37)::x: [Build onnx 0.3.3018 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.3018) (commit https://github.com/onnx/onnx/commit/8f5118cabb by @liqunfu)
houseroad(2018-05-10 21:35:57):The shape inference rants on the new models, could you take a look? 
liqunfu(2018-05-11 03:06:33):Thanks @houseroad , doing it.
AppVeyorBot(2018-05-11 20:42:04)::white_check_mark: [Build onnx 0.3.3037 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.3037) (commit https://github.com/onnx/onnx/commit/5003ab985d by @liqunfu)
AppVeyorBot(2018-05-11 23:49:23)::white_check_mark: [Build onnx 0.3.3046 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.3046) (commit https://github.com/onnx/onnx/commit/6e2089525b by @liqunfu)
liqunfu(2018-05-12 04:54:22):@houseroad , hope this PR can get in before 5/15. Thanks
AppVeyorBot(2018-05-14 17:18:36)::white_check_mark: [Build onnx 0.3.3097 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.3097) (commit https://github.com/onnx/onnx/commit/18e096aec8 by @liqunfu)
houseroad(2018-05-15 00:20:55):@liqunfu thanks for addressing the comments, could you also run `flake8` to fix the lint issue?
And use the gen_doc to update the docs.
liqunfu(2018-05-15 00:40:37):Thanks @houseroad! good to know there is a flake8 tool. It would be great to add its use instruction for contributors. 
I updated already operator.md and changelog.md according to travis build error. Probably because of this, flake8 did not produce any output and has no effect at my local. I am hoping this time my commit get pass travis build.     
AppVeyorBot(2018-05-15 02:27:48)::white_check_mark: [Build onnx 0.3.3144 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.3144) (commit https://github.com/onnx/onnx/commit/f971637fb8 by @liqunfu)
AppVeyorBot(2018-05-15 05:12:36)::white_check_mark: [Build onnx 0.3.3171 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.3171) (commit https://github.com/onnx/onnx/commit/cc7b6c636e by @liqunfu)
houseroad(2018-05-15 09:35:23):According to CI report https://travis-ci.org/onnx/onnx/jobs/379047734, you still need to update the doc with env ONNX_ML=0
AppVeyorBot(2018-05-15 16:30:23)::white_check_mark: [Build onnx 0.3.3174 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.3174) (commit https://github.com/onnx/onnx/commit/526721de8f by @liqunfu)
liqunfu(2018-05-15 16:32:12):@houseroad, sorry for keeping failing at Travis. This is what I did the latest:
1. remove Changelog.md and Operators.md just to make sure they are updated.
2. git clean
3. pip install -e .
4. flake8 .
5. python onnx/defs/gen_doc.py

at this stage I am getting Operators.md and Changelog.md with the same lines that are cause Travis to fail. Do I miss anything?  
liqunfu(2018-05-16 20:05:23):@houseroad , I ran the last steps the second time and all passed. Probably my mistake before. Please see if everything is ok and if so, please help merging to master. Thanks.
AppVeyorBot(2018-05-18 01:28:15)::white_check_mark: [Build onnx 0.3.3332 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.3332) (commit https://github.com/onnx/onnx/commit/07bf3cda55 by @linkerzhang)
linkerzhang(2018-05-19 02:06:03):@houseroad does this look good to you now? Thank you!
houseroad(2018-05-19 02:12:54):I will review this PR tonight. :-)
AppVeyorBot(2018-05-19 02:35:56)::x: [Build onnx 0.3.3389 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.3389) (commit https://github.com/onnx/onnx/commit/3561d47b28 by @linkerzhang)
AppVeyorBot(2018-05-19 14:58:30)::x: [Build onnx 0.3.3394 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.3394) (commit https://github.com/onnx/onnx/commit/048fc1fe8e by @linkerzhang)
houseroad(2018-05-20 11:30:22):~~Some windows CI keeps failing. @liqunfu @raymondxyang could you take a look before merging the pr?~~ conda website is unstable.
AppVeyorBot(2018-05-20 11:47:06)::x: [Build onnx 0.3.3400 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.3400) (commit https://github.com/onnx/onnx/commit/3a9a062260 by @linkerzhang)
houseroad(2018-04-03 23:28:01):Since the output precision matters in this case, could you use the numpy function to calculate the results. You can add the output value in the comment. The users can still quickly check them. :-)

BTW, I think sqrt(5) is closer to 2.2360679775, 2.23606777 is probably not precise enough.
houseroad(2018-05-14 00:16:44):nit: test_reduce_l1_keep_dims2 ==> test_reduce_l1_keep_dims_random
houseroad(2018-05-14 00:16:46):nit: test_reduce_l1_keep_dims1 ==> test_reduce_l1_keep_dims_example
houseroad(2018-05-14 00:19:28):Update the following number. Use more accurate numbers listed in the previous example.
Also we enable keepdims, the dimension in the following comments looks wrong.
houseroad(2018-05-14 00:20:53):Nit: comma after each number.
houseroad(2018-05-14 00:26:32):Nit: comma here
liqunfu(2018-05-14 17:22:06):@houseroad I updated according to your comments. I was trying to keep python output format (without comma, with extra spaces). Maybe we shall standardize a way to show python example so that there is less editing and closer to python output formatting?  
AppVeyorBot(2018-03-29 01:05:09)::x: [Build onnx 0.3.1937 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1937) (commit https://github.com/onnx/onnx/commit/c18f2e296b by @smessmer)
AppVeyorBot(2018-03-29 01:13:52)::x: [Build onnx 0.3.1938 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1938) (commit https://github.com/onnx/onnx/commit/da1ef0b87e by @smessmer)
AppVeyorBot(2018-03-29 01:24:05)::white_check_mark: [Build onnx 0.3.1939 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1939) (commit https://github.com/onnx/onnx/commit/25288b95ed by @smessmer)
bddppq(2018-04-05 03:39:27):Could you put it in the BUILD_PYTHON block below? Also the output files should be put in `${ONNX_DLLEXPORT_STR}${OUTPUT_PROTO_DIR}` like other autogenerated files and then setup.py copy them to the corresponding path.
bddppq(2018-03-29 00:38:55):Do you have a script to generate these files? Maybe also put it into this repo?
houseroad(2018-03-29 00:40:55):It's here: https://github.com/onnxbot/onnx-fb-universe/blob/master/test/convert_pytorch_test.py

Since we have dependencies on pytorch and caffe2... probably not appropriate to add to onnx repo directly. :-)
houseroad(2018-03-29 05:10:03):@spidyDev I just updated the files, could you check again?
rajanksin(2018-03-29 14:29:00):@houseroad  AvgPool3d tests LGTM.
For: AvgPool1d (onnx/backend/test/data/pytorch-converted/test_AvgPool1d/model.onnx)
This test seems to be testing avgpool2d instead of avgpool1d.
I  think we dont need  (unsqueeze-squeeze) , also the avgpool op can have kernel_shape , pad and stride corresponding to 1d tensor i.e. (2) , (0,0) , (2) respectively.

houseroad(2018-03-29 17:14:32):@spidyDev thanks for the check. Actually those tests are automatically converted from PyTorch tests. In PyTorch, the AvgPool1d is implemented as unsqueeze + avgpool2d + squeeze. That's why we have such models. I think such models are useful since they also cover other ops, such as Unsqueeze and Squeeze. We should also include them in the test data. :-)
rajanksin(2018-03-29 17:20:45):@houseroad  Thanks for clarifying. PR LGTM.
AppVeyorBot(2018-03-29 15:10:11)::x: [Build onnx 0.3.1946 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1946) (commit https://github.com/onnx/onnx/commit/c0628f384e by @postrational)
jamesr66a(2018-03-29 22:56:28):Hi @postrational! 

Thanks for the PR and proposal. We've actually been working on a similar proposal this week that treats the `zip` file format as a first-class archive format for ONNX models: https://github.com/onnx/onnx/wiki/%5Bdraft%5D-Proposal-for-ONNX-archive-file-format. I think we can incorporate the ideas from both of our proposals. Let me read over them and see what we should do
postrational(2018-03-30 23:00:37):Sounds good, we can work together to combine these ideas. 
NiklasGustafsson(2018-04-03 17:44:22):Looking at both proposals, it seems to me that some advancement on this front will be a great plus.

Here are some questions about the proposals:

1. If the tensors are stored in a Zip file in the proposal by @jamesr66a, but you want to be able to memory-map the data directly, I presume you would not be able to compress anything. Is that right?

2. The ONNX spec specifically forbids strings from the 'raw_data' field of TensorProto. How will string tensors be serialized in the Zip-based proposal?

3. There's another use case for serializing tensors outside the model file -- incremental updates to a model after retraining. This can be useful in situations where weights are adjusted on a frequent basis and there are bandwidth concerns. The Zip-based proposal would complicate this, it seems to me, while the proposal from @postrational would not.

Thanks,

Niklas
Microsoft
lutzroeder(2018-04-03 23:15:11):Could something similar to the ZIP approach be accomplished in a less invasive way? For example, building on the `external_data` field? Attach the tensor data at the end of a regular ONNX protobuf file and use a special URI scheme to index into that data? This might even be possible without having to change existing ONNX protobuf loaders sans the URI support.

Taking a dependency on ZIP means all tools need to take a dependency on ZIP. Might make sense as a transport optimization but should the standard enforce a ZIP dependency or is that limiting options down the road? I'm thinking about this in terms of how opening those file in a browser would require extra hoops but decompression time vs. memory mapping might cause issues for mobile as well.
jamesr66a(2018-04-06 17:34:57):@NiklasGustafsson  great questions!

1) Yes, that is correct. To be honest, zip compression probably will not buy you much with serialized flaoating-point tensor values (except in pathological cases such as serializing zero-initialized tensors) so for the common case we're simply using zip as a well-defined and well-supported container format that we can index into. This trade-off might be different for e.g. integrally-typed tensors. Zip allows you to compress on a per-file basis, so mmap v.s. direct zipfile read is simply a single `if` check in the backend on whether the data is compressed.
2) I'm not quite familiar with the context behind string tensor serialization. Is this something you would be interested in being a part of the zip proposal? I'd imagine we would have to standardize encoding and delimiting for how the strings would appear in a binary blob.
3) This is an interesting use case and I'd like to learn more. Does the model need to remain in the ONNX form throughout these updates or can there be an explicit packaging step at the end that wraps everything up in the zip file for distribution?

@lutzroeder 

> Attach the tensor data at the end of a regular ONNX protobuf file and use a special URI scheme to index into that data?

Through discussions at FB we've considered custom serialization formats, but decided that it's better to stick with a tried-and-tested container format, especially for something that is meant to be a solid standard. One thing we want to avoid is rolling our own index for the container format -- we'd like to get O(1) access to weight files in a way that is likely bug-free.

We've taken inspiration from APK (mobile) and Office (Windows) and `zip` seemed to check the boxes in terms of cross-platform support more than the other formats we considered. You have an interesting point about handling in the browser, could you elaborate on the options there (I'm not familiar with the space)?

As far as mobile, our mobile guys at FB seem to be okay with this and support the `mmap` approach, which is similar to what is done with the APK format. Let me know if there are specific concerns there
jamesr66a(2018-04-06 17:45:35):@NiklasGustafsson just spoke with @Yangqing and he pointed out the constraints on raw strings in the `raw_data` field from his experience:

>The tricky thing is that if you specify a string delimiter, one has to make sure that binary strings do not contain that delimiter. That makes two constraints, either (1) we only allow certain encodings, or (2) we need to do a base64-type encoding to explicitly reserve some delimiter

These issues are probably not insurmountable, but I'd like to discover 1) How important is this for you and 2) What approach is the best for the common use cases in ONNX?
NiklasGustafsson(2018-04-06 20:37:39):I share the view on compression not being particularly likely for floats, given the typical bit patterns. My question was about the ability to memory-map, which seems to preclude compression, and we're on the same page there.

I have no particular interest in string tensors, I just wondered whether it wasn't the case that the Zip-based design would exclude string tensors. The multi-file design also seems to have the same limitation, by the way. I don't know how common it will be to have string tensors as anything except graph inputs and outputs, but it's a limitation we should be cognizant of.

Beyond the ability to have really large models, one thing that is <b>really</b> important for some of our scenarios is the ability to incrementally update models, especially those really large models that this is being designed for. Therefore, I want to make sure that if we consider a multi-part model design, that this incremental-update ability be part of any solution.
linkerzhang(2018-11-13 18:57:05):I had a concern about having one file to store one tensor, which will create too many files potentially. Offline discussed Michal, we will extend the external_data to be able to carry index information so that one file can store more than one tensor as needed. The external_data will look like <file_name>?offset=<>&length=<>.

shinh(2018-11-28 10:50:05):@postrational As locally chatted at ONNX workshop, it'd be worth considering to have some recommendation and/or restriction on alignments of `offset` field to allow backends to use mmap efficiently (e.g., SIMD alignment). I'm guessing just enforcing 4k boundary would be the simplest choice, but I'm not sure.
houseroad(2018-12-03 22:29:10):Some update from PyTorch: we just switched from our own inline container to zip format: https://github.com/pytorch/pytorch/blob/master/caffe2/serialize/inline_container.h
postrational(2018-12-20 16:14:10):I added updates to the PR to reflect changes based on discussions from the last ONNX workshop.
CLAassistant(2019-01-07 17:02:45):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=678) <br/>All committers have signed the CLA.
yufenglee(2019-01-09 02:29:28):@houseroad I just added the checker. Please help to take a look.
yufenglee(2019-01-11 00:20:33):@houseroad I added the checker. Could you help to take a look at it? 
houseroad(2019-01-11 00:59:10):@yufenglee I will find some time tomorrow and push it forward, no worries.
linkerzhang(2019-01-16 18:38:23):@houseroad @postrational  guys, thanks a lot for pushing this forward. I'm seeing that we all agreed the proto changes but having some different opinions on the helper functions. 

I'm suggesting to merge this PR for 1.4 release, as the standard itself cares most about proto changes, while helper functions are nice to have (no impact on the standard).  Thoughts?
houseroad(2019-01-16 18:44:48):Yes, we should push it in. Let's polish more on the API and merge it.
lutzroeder(2018-12-20 21:29:36):`UNKNOWN = 0` or `MESSAGE_OR_RAW = 0` or `LEGACY = 0`? Some protobuf implementations could return `0` as default if "field exists" check is missing... or  `RAW = 0` as it is easier to double check with `len(raw_data) == 0`.
postrational(2018-12-21 11:13:30):I would think we should default to `UNKNOWN = 0` or `NOTSET = 0` and treat that as the legacy value.
houseroad(2019-01-14 19:12:03):please add some check on tensor, to make sure we only create ExternalDataInfo when tensor contains valid `external_data` 
houseroad(2019-01-14 19:17:57):Please add a warning here, because here we will deserialize the ModelProto, very likely we can write the external tensor data before we convert the in-memory model to bytes.
houseroad(2019-01-14 19:42:51):We should also have two functions called `convert_model_to_external_data`, `convert_model_from_external_data`. Think about how users would like to save and load their models. Usually, they don't want to manipulate the storage location for each tensor. One switch for all the tensor will be quite useful.
houseroad(2019-01-14 19:44:12):Let's make it more clear here: it's POSIX file system relative path.
houseroad(2019-01-14 20:10:48):for `load_model` we should have another parameter `load_external_data` (default should be true) to let user decide whether loading the external data or not. 

Also we should have another helper function `load_external_data_for_model(model, base_dir)`. I think we need to make it clear that it's users' responsibility to keep the base dir info. It's kind of ugly to save extra info and strip it out after loading.
houseroad(2019-01-14 20:11:23):I think it's to ask user to provide the base_dir.
houseroad(2019-01-14 20:12:07):Let's don't add basepath in the external_data, instead, ask user to provide it as an argument.
houseroad(2019-01-14 20:17:56):We can add a parameter in save_model, all_data_as_external=False, so all tensors' data will be save to external_data. We probably need to modify the proto, and revert it back after saving.
yufenglee(2019-01-15 19:41:54):I agree. By default, we assume the external data under the same directory of the model and load the external data. If it fails (that is to say, external data is not in the same directory), it's the user's responsibility to call load_external_data_for_model to load the external data. @postrational , any thoughts? 
yufenglee(2019-01-15 19:53:08):And we also need an argument external_data_file that allows user to save all the external data to one specific file. 
yufenglee(2019-01-15 19:55:54):@houseroad i'm wondering what these 2 functions work for. Could you explain a little bit more? 
houseroad(2019-01-15 23:12:41):how about having three options for users: 
1) embedded weights (default)
2) one file for all external data
3) separate files for different external data
houseroad(2019-01-15 23:24:09):convert_model_to_external_data: convert your model to use external data, assume the original model uses embedded data.

convert_model_from_external_data: embed the data into your model, assume the originla model uses external data.
postrational(2019-01-16 13:51:58):Agreed. We can make the API more flexible by adding more methods and parameters.

We should just make sure we keep the user interface simple and backwards compatible. A simple call to `onnx.load_model(path)` and `onnx.save_model(proto, path)` should work whether data is internally or externally stored.
postrational(2019-01-16 13:52:19):Done.
postrational(2019-01-16 14:05:01):`ExternalDataInfo` acts as just a struct for holding information about the external data. I don't think we should prevent the construction of this object if the `location` field is not set.

postrational(2019-01-16 14:05:36):What should be warning message in this case?
houseroad(2019-01-16 18:28:50):something like: "We deserialize the serialized proto again before writing. To improve the performance, please pass the proto before serialization."
linkerzhang(2019-01-16 18:36:17):That's not good. It means the model file itself is not self-contained.
houseroad(2019-01-16 18:38:14):this is not a valid key in our spec.
yufenglee(2019-01-17 06:00:07):Added.
yufenglee(2019-01-17 06:01:55):Added a helper convert_model_to_external_data. Users can call this function first and then save tensor data externally.
houseroad(2019-01-18 19:29:16):we need remove the file created in testing
houseroad(2019-01-18 19:29:51):never mind, you already delete the whole folder, which is fine

AppVeyorBot(2018-03-30 19:15:20)::x: [Build onnx 0.3.1954 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1954) (commit https://github.com/onnx/onnx/commit/71e096953e by @anderspapitto)
AppVeyorBot(2018-03-30 19:25:52)::x: [Build onnx 0.3.1955 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1955) (commit https://github.com/onnx/onnx/commit/054734ab0b by @anderspapitto)
AppVeyorBot(2018-04-03 05:50:22)::x: [Build onnx 0.3.2010 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2010) (commit https://github.com/onnx/onnx/commit/bf62ca604b by @houseroad)
AppVeyorBot(2018-04-03 20:48:59)::x: [Build onnx 0.3.2029 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2029) (commit https://github.com/onnx/onnx/commit/fb5c278f7c by @anderspapitto)
AppVeyorBot(2018-04-03 21:01:47)::x: [Build onnx 0.3.2030 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2030) (commit https://github.com/onnx/onnx/commit/48678da27d by @anderspapitto)
AppVeyorBot(2018-04-03 23:42:54)::white_check_mark: [Build onnx 0.3.2042 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2042) (commit https://github.com/onnx/onnx/commit/497492305e by @anderspapitto)
anderspapitto(2018-04-13 18:25:06):actually, I feel this is not needed, because a `oneof` is actually a one-or-zero-of. That means that we can use a dim which is present but which is neither a dim_value nor a dim_param to represent uncorrelated, unknown dimensions.

(Open to discussing further, but I'm satisfied with that approach, which already works, and so I'll close this PR)
linkerzhang(2018-04-03 00:30:49):operator name should be unique in its domain.
houseroad(2018-04-03 05:31:36):Let's also provide the meaning of the empty string here.
houseroad(2018-04-03 05:31:52):Also here, the meaning of the empty string.
houseroad(2018-04-03 05:35:31):We should add scope for each type of names. For example, Node name must be unique in the whole model.
houseroad(2018-04-03 05:35:59):Model scope unique.
houseroad(2018-04-03 05:36:14):Node scope unique?
houseroad(2018-04-03 05:36:25):Domain scope unique.
dzhulgakov(2018-04-03 16:33:46):Actually - do we require graph names to be present? If that's an inlined for loop - it probably doesn't matter that much
dzhulgakov(2018-04-03 16:34:53):Yep, we should discuss it - or maybe make it graph-level unique? @jamesr66a probably has a view on it given the control flow considerations

Btw, shall it be "optional"?
dzhulgakov(2018-04-03 16:35:26):Yeah, operators don't fit it as they're more part of the spec than the model
dzhulgakov(2018-04-03 16:35:43):Global scope?
gramalingam(2018-04-10 18:22:21):It would also be useful to have a mechanism to indicate an arbitrary (unique) sequence of dimensions. How about using a special string value such as "..." to indicate this? So, a shape ["..."] would mean any possible shape, while ["N", "..."] would mean a shape with at least one dimension, etc.
gramalingam(2018-04-10 18:23:09):This could be tricky if some existing code assumes that the length of the tensor-shape list indicates its rank.
anderspapitto(2018-04-10 20:21:59):yes, at the moment we are making this assumption. Probably having rank-polymorphism is a big enough change that it would warrant it's own proposal
gramalingam(2018-04-10 20:28:52):So, how should we handle worst-case scenarios where shape-inference cannot determine anything about the output shape? Is the plan to use a missing shape in a TypeProto::Tensor to indicate a completely unknown shape? (That seems the obvious choice, but want to double-check.)
anderspapitto(2018-04-10 21:34:24):yes. has_shape() will return false for a completely unknown shape. has_shape() = true and dim_size() = 0 indicates a scalar
anderspapitto(2018-04-10 21:35:22):note that you can also just construct your protobuf in this state directly, so those semantics are already set even before introducing shape inference
AppVeyorBot(2018-03-30 20:01:26)::white_check_mark: [Build onnx 0.3.1956 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1956) (commit https://github.com/onnx/onnx/commit/7849823a01 by @houseroad)
AppVeyorBot(2018-03-31 00:13:36)::x: [Build onnx 0.3.1959 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1959) (commit https://github.com/onnx/onnx/commit/5b71739730 by @houseroad)
AppVeyorBot(2018-03-31 00:22:42)::x: [Build onnx 0.3.1960 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1960) (commit https://github.com/onnx/onnx/commit/eb3a01136e by @houseroad)
AppVeyorBot(2018-03-31 00:32:29)::x: [Build onnx 0.3.1961 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1961) (commit https://github.com/onnx/onnx/commit/b8d0a5c4fc by @houseroad)
AppVeyorBot(2018-03-31 00:46:22)::x: [Build onnx 0.3.1962 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1962) (commit https://github.com/onnx/onnx/commit/61bd9de357 by @houseroad)
AppVeyorBot(2018-03-31 01:03:23)::x: [Build onnx 0.3.1963 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1963) (commit https://github.com/onnx/onnx/commit/b58287fd31 by @houseroad)
AppVeyorBot(2018-03-31 04:11:37)::x: [Build onnx 0.3.1965 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1965) (commit https://github.com/onnx/onnx/commit/a026ffd7c9 by @houseroad)
AppVeyorBot(2018-03-31 04:36:46)::white_check_mark: [Build onnx 0.3.1966 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1966) (commit https://github.com/onnx/onnx/commit/b31efad79d by @houseroad)
bddppq(2018-04-01 06:34:55):Actually, since you have setup notebooks, should we simply not put any python code in the markdown files (since they are not verified by tests)?
houseroad(2018-04-02 05:57:58):After some thoughts, I think keeping short(er) version in the markdown file makes sense. The summarized version is easy to check, and users can quickly grab the usage from just one page. :-)
AppVeyorBot(2018-04-04 00:24:09)::x: [Build onnx 0.3.2047 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2047) (commit https://github.com/onnx/onnx/commit/981a0a644c by @houseroad)
AppVeyorBot(2018-04-04 02:54:13)::x: [Build onnx 0.3.2057 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2057) (commit https://github.com/onnx/onnx/commit/e6acd5a353 by @houseroad)
AppVeyorBot(2018-04-04 03:15:34)::white_check_mark: [Build onnx 0.3.2058 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2058) (commit https://github.com/onnx/onnx/commit/a60ac71c95 by @houseroad)
houseroad(2018-04-04 03:37:31):@onnxbot retest this please
AppVeyorBot(2018-04-04 04:14:21)::white_check_mark: [Build onnx 0.3.2061 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2061) (commit https://github.com/onnx/onnx/commit/56010cdb7f by @houseroad)
bddppq(2018-04-01 06:27:45):`from_array` takes an optional argument `name`.
bddppq(2018-04-01 06:30:15):Can be done in a separate diff: We should wrap this into a helper function (say `save_model`), especially soon we will have support for storing tensor values in separate files.
bddppq(2018-04-01 06:33:12):There should be typo here.
houseroad(2018-04-02 05:44:31):Sure, will cover this case. :-)
houseroad(2018-04-02 05:53:01):Good point. Working on it, will rebase this diff on that one.
houseroad(2018-04-02 05:58:10):Will fix it.
houseroad(2018-04-02 20:19:43):https://github.com/onnx/onnx/pull/692
dzhulgakov(2018-04-02 20:41:23):typo in the name X -> Y
dzhulgakov(2018-04-02 20:41:52):unfinished line? is this code runnable? :)
dzhulgakov(2018-04-02 20:42:31):should we have a wrapper function that hides to/from string? and also there should be a "no-brainer" pass list as default
dzhulgakov(2018-04-02 20:42:56):same here - hide serialization inside the library
houseroad(2018-04-02 21:05:26):Good catch!
houseroad(2018-04-02 21:06:16):Yep, forgot to delete something.
houseroad(2018-04-02 21:07:39):Yep, I will do that in a different PR. And then rebase the introduction doc.
CLAassistant(2018-04-01 16:15:21):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=684) <br/>All committers have signed the CLA.
bddppq(2018-04-02 16:56:00):Thanks for the fix
AppVeyorBot(2018-04-02 17:39:49)::x: [Build onnx 0.3.1979 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1979) (commit https://github.com/onnx/onnx/commit/680e469bb6 by @bddppq)
linkerzhang(2018-04-03 00:17:39):Should we allow "GraphProto.input includes both weights and graph input," please? I feel graphproto.input should only contain graph input. make sense please?
bddppq(2018-04-03 00:46:14):@linkerzhang the purpose of putting initializer into graph.input is if (someday) onnx supports training,  then existing models can simply drop the initializers and its graph is still a valid graph. TBH I don't feel there are too much differences between including or not including initilaizers' ValueInfoProto in graph.input :-) But since we have started this way, it's probably not worth the effort of introducing a breaking change?
AppVeyorBot(2018-04-03 03:37:34)::white_check_mark: [Build onnx 0.3.2004 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2004) (commit https://github.com/onnx/onnx/commit/8d83efb6af by @houseroad)
gramalingam(2018-04-03 17:12:16):@bddppq : but the change in the documentation seems to have dropped the lines that say that the caller-specified value overrides the weight specified in the model ... so, this is a bit confusing. If we want to retain the original behavior, it would be nice to have a line in the documentation indicating this.
houseroad(2018-04-02 17:09:03):delete the first `constants` in the parenthesis?
houseroad(2018-04-02 17:11:22):I think this paragraph says that even the graph contains some values in the initializer, we can still ignore it by passing the value in the input list. 
bddppq(2018-04-02 17:19:38):@houseroad Yeah that was in the old days we were trying to provide the convenience to users to override the constant values in the `run_model`, `run_node` python interface, but it doesn't work anymore as we have more and more stuffs relying on the static graph analysis.
houseroad(2018-04-02 17:31:34):Also shall we explicitly say these are weights of the models. :-)
CLAassistant(2018-04-02 19:06:58):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=691) <br/>All committers have signed the CLA.
AppVeyorBot(2018-04-02 19:24:57)::white_check_mark: [Build onnx 0.3.1983 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.1983) (commit https://github.com/onnx/onnx/commit/2ef535fdf5 by @NiklasGustafsson)
bddppq(2018-04-02 20:29:49):this should belong to helper_test.py

Update:
nvm these functions are not in helper.py
dzhulgakov(2018-04-02 20:39:20):nit: put it into try/finally?
houseroad(2018-04-02 20:54:14):Good point!

AppVeyorBot(2018-04-03 03:07:32)::x: [Build onnx 0.3.2003 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2003) (commit https://github.com/onnx/onnx/commit/f3bf043496 by @houseroad)
AppVeyorBot(2018-04-03 14:15:46)::white_check_mark: [Build onnx 0.3.2011 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2011) (commit https://github.com/onnx/onnx/commit/3e15ed3b85 by @fumihwh)
AppVeyorBot(2018-04-03 17:01:37)::white_check_mark: [Build onnx 0.3.2016 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2016) (commit https://github.com/onnx/onnx/commit/a4ad988559 by @houseroad)
bddppq(2018-04-03 20:22:52):CI failure is just some travis infra flakiness
raymondxyang(2018-04-03 20:41:54):Abandon this to work with @houseroad to find a better resolve
raymondxyang(2018-04-03 20:40:16):<https://github.com/onnx/onnx/blob/master/onnx/backend/test/runner/__init__.py#L192> CUDA is manually set as a default value.. probably we can pass only CPU into it 
houseroad(2018-04-03 20:42:47):@raymondxyang that is also intended. Because we don't really invoke any CUDA related code, it's fine to enable it in ONNX repo. The correct implementation of backend will override supports_device anyway, so no worries.

I think the root cause is OOM due to python inefficient memory management. :-)
raymondxyang(2018-04-03 21:04:42):Got it @houseroad so shall we disable the alexnet case for CI?
houseroad(2018-04-03 21:10:02):@raymondxyang I think no need to disable it. Let's see the Appveyor's result first.
AppVeyorBot(2018-04-03 21:41:04)::x: [Build onnx 0.3.2032 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2032) (commit https://github.com/onnx/onnx/commit/520b20db53 by @houseroad)
AppVeyorBot(2018-04-04 00:03:13)::white_check_mark: [Build onnx 0.3.2045 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2045) (commit https://github.com/onnx/onnx/commit/f583774fa6 by @houseroad)
bddppq(2018-04-04 00:10:49):@anderspapitto Could you merge DummyBackend in backend_shape_inference_test.py into test_backend_test.py?
AppVeyorBot(2018-04-04 00:43:45)::white_check_mark: [Build onnx 0.3.2049 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2049) (commit https://github.com/onnx/onnx/commit/31f232f3a3 by @houseroad)
bddppq(2018-04-03 20:20:40):it's not good, we encourage backends to support more device types
bddppq(2018-04-03 20:21:52):the tests are just running checker on the models, so it's ok to say this backend supports all devices
houseroad(2018-04-03 20:26:06):I am open to discuss this choice. Turning to False will be safer. It's just the whitelist or blacklist we want to use.
bddppq(2018-04-03 20:29:36):Yes, whitelist and blacklist are equally expressive, but IMO the driving factor is whether we want backends to opt-in or opt-out to running tests.
houseroad(2018-04-03 20:30:44):In this case, checking each model once should be good enough, I think. (We can reduce the CI time, too)
AppVeyorBot(2018-04-04 00:12:50)::x: [Build onnx 0.3.2046 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2046) (commit https://github.com/onnx/onnx/commit/28f00c275b by @anderspapitto)
AppVeyorBot(2018-04-04 21:48:25)::x: [Build onnx 0.3.2084 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2084) (commit https://github.com/onnx/onnx/commit/3b2b25bd42 by @bddppq)
anderspapitto(2018-04-05 21:15:06):built on top of https://github.com/onnx/onnx/pull/655
anderspapitto(2018-04-09 18:34:02):just merging this into the other pr
AppVeyorBot(2018-04-04 08:30:23)::x: [Build onnx 0.3.2068 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2068) (commit https://github.com/onnx/onnx/commit/131d9a7137 by @bddppq)
AppVeyorBot(2018-04-04 08:40:23)::x: [Build onnx 0.3.2069 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2069) (commit https://github.com/onnx/onnx/commit/d5556870d9 by @bddppq)
AppVeyorBot(2018-04-04 09:00:11)::x: [Build onnx 0.3.2070 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2070) (commit https://github.com/onnx/onnx/commit/c11e65eafd by @bddppq)
AppVeyorBot(2018-04-04 09:10:34)::x: [Build onnx 0.3.2071 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2071) (commit https://github.com/onnx/onnx/commit/4a881f54db by @bddppq)
AppVeyorBot(2018-04-04 09:21:21)::x: [Build onnx 0.3.2072 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2072) (commit https://github.com/onnx/onnx/commit/3db1af5315 by @bddppq)
AppVeyorBot(2018-04-04 09:41:57)::white_check_mark: [Build onnx 0.3.2073 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2073) (commit https://github.com/onnx/onnx/commit/3dafa0e7c5 by @bddppq)
bddppq(2018-04-04 18:02:52):@anderspapitto mixing is needed because there are bugs discovered after removing redundant code in setup.py.
anderspapitto(2018-04-04 17:30:20):is this part new?
anderspapitto(2018-04-04 17:31:46):is this bit new behavior or pure refactor
bddppq(2018-04-04 17:54:57):new, this is the cmake fixes that needed for supporting ONNX_ML build
bddppq(2018-04-04 18:00:22):yep, it's needed to copy the cmake generated python files to setup.py's directory. This bug was not discovered because previously setup.py does its own protobuf autogen work (in addition to cmake).

AppVeyorBot(2018-04-04 16:26:01)::x: [Build onnx 0.3.2075 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2075) (commit https://github.com/onnx/onnx/commit/6707ad7d31 by @fumihwh)
AppVeyorBot(2018-04-05 05:25:02)::x: [Build onnx 0.3.2108 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2108) (commit https://github.com/onnx/onnx/commit/5bbe07a488 by @fumihwh)
AppVeyorBot(2018-04-05 05:34:51)::x: [Build onnx 0.3.2109 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2109) (commit https://github.com/onnx/onnx/commit/3594c22f8c by @fumihwh)
AppVeyorBot(2018-04-05 05:45:20)::x: [Build onnx 0.3.2110 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2110) (commit https://github.com/onnx/onnx/commit/f28f124741 by @fumihwh)
AppVeyorBot(2018-04-05 06:15:04)::x: [Build onnx 0.3.2112 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2112) (commit https://github.com/onnx/onnx/commit/0e960f717d by @fumihwh)
AppVeyorBot(2018-04-05 07:54:52)::white_check_mark: [Build onnx 0.3.2113 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2113) (commit https://github.com/onnx/onnx/commit/a4f53d60a5 by @fumihwh)
anderspapitto(2018-04-05 19:02:05):so, if the optimization is unsound when the dimensions don't match, then we should make the optimization only fire when the shapes are known.

There are two ways that we can make sure the shapes are known

- producers of ONNX graphs can manually provide ValueInfoProtos for the intermediate values

OR

- we can add enough shape inference that it's not necessary. This work is ongoing.

I think it would also be fine to emit some warning that says "we want to fuse this operation, but we cannot because size information is missing. Please annotate your graph for extra performance"
AppVeyorBot(2018-04-06 08:26:27)::x: [Build onnx 0.3.2156 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2156) (commit https://github.com/onnx/onnx/commit/eaf9b16e26 by @fumihwh)
AppVeyorBot(2018-04-06 08:37:09)::x: [Build onnx 0.3.2157 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2157) (commit https://github.com/onnx/onnx/commit/2b3b4293d1 by @fumihwh)
AppVeyorBot(2018-04-06 09:07:23)::x: [Build onnx 0.3.2158 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2158) (commit https://github.com/onnx/onnx/commit/c0ae56f9c9 by @fumihwh)
AppVeyorBot(2018-04-06 09:40:40)::white_check_mark: [Build onnx 0.3.2159 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2159) (commit https://github.com/onnx/onnx/commit/66540f02a7 by @fumihwh)
fumihwh(2018-04-06 13:48:54):@anderspapitto @houseroad 
I made following changes
- check if size information exists
- check if bias is 1d
- check if bias dim is equal to second dim of conv
- add 2 test cases
AppVeyorBot(2018-04-07 04:15:15)::x: [Build onnx 0.3.2188 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2188) (commit https://github.com/onnx/onnx/commit/0040e97be8 by @fumihwh)
AppVeyorBot(2018-04-07 04:34:39)::x: [Build onnx 0.3.2189 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2189) (commit https://github.com/onnx/onnx/commit/d5e58c060c by @houseroad)
AppVeyorBot(2018-04-07 04:53:37)::x: [Build onnx 0.3.2190 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2190) (commit https://github.com/onnx/onnx/commit/bee155f35d by @fumihwh)
AppVeyorBot(2018-04-07 05:12:03)::x: [Build onnx 0.3.2191 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2191) (commit https://github.com/onnx/onnx/commit/7b87991617 by @fumihwh)
AppVeyorBot(2018-04-07 05:49:27)::x: [Build onnx 0.3.2193 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2193) (commit https://github.com/onnx/onnx/commit/14b1c178f2 by @fumihwh)
AppVeyorBot(2018-04-07 08:04:32)::x: [Build onnx 0.3.2195 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2195) (commit https://github.com/onnx/onnx/commit/6854b0cb6c by @fumihwh)
AppVeyorBot(2018-04-07 12:09:49)::x: [Build onnx 0.3.2196 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2196) (commit https://github.com/onnx/onnx/commit/96817a4763 by @fumihwh)
AppVeyorBot(2018-04-08 17:00:00)::x: [Build onnx 0.3.2200 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2200) (commit https://github.com/onnx/onnx/commit/44a761338e by @fumihwh)
AppVeyorBot(2018-04-09 20:22:22)::x: [Build onnx 0.3.2202 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2202) (commit https://github.com/onnx/onnx/commit/6d86e0692d by @fumihwh)
fumihwh(2018-04-09 21:27:02):@houseroad 
No idea with appveyor stops at `optimizer_test.py .`
Seems something wrong with second test `test_nop_transpose_default`?
AppVeyorBot(2018-04-10 03:47:11)::x: [Build onnx 0.3.2211 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2211) (commit https://github.com/onnx/onnx/commit/4725634cae by @fumihwh)
AppVeyorBot(2018-04-10 07:17:28)::x: [Build onnx 0.3.2214 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2214) (commit https://github.com/onnx/onnx/commit/7a8884b7df by @fumihwh)
AppVeyorBot(2018-04-10 17:04:11)::x: [Build onnx 0.3.2230 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2230) (commit https://github.com/onnx/onnx/commit/bfdd62dcba by @fumihwh)
AppVeyorBot(2018-04-11 06:11:10)::x: [Build onnx 0.3.2252 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2252) (commit https://github.com/onnx/onnx/commit/20ad1cd1c2 by @houseroad)
AppVeyorBot(2018-04-11 10:12:44)::x: [Build onnx 0.3.2253 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2253) (commit https://github.com/onnx/onnx/commit/7dec00e121 by @houseroad)
AppVeyorBot(2018-04-11 14:31:00)::x: [Build onnx 0.3.2255 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2255) (commit https://github.com/onnx/onnx/commit/1e7fd4de31 by @houseroad)
AppVeyorBot(2018-04-11 18:39:42)::x: [Build onnx 0.3.2259 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2259) (commit https://github.com/onnx/onnx/commit/a475fc6b05 by @fumihwh)
AppVeyorBot(2018-04-11 23:35:46)::x: [Build onnx 0.3.2264 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2264) (commit https://github.com/onnx/onnx/commit/84273cdd4c by @houseroad)
AppVeyorBot(2018-04-11 23:58:02)::white_check_mark: [Build onnx 0.3.2265 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2265) (commit https://github.com/onnx/onnx/commit/f377052b3a by @houseroad)
houseroad(2018-04-04 16:28:34):We use ONNX_NAMESPACE
houseroad(2018-04-04 16:31:06):Add only has and only has two inputs, I think it's fine to have this assumption.
houseroad(2018-04-04 16:34:19):We need to check 1) Add's broadcast flag, 2) the shape of bias. (The first dimension of bias is the same as the Conv's 2nd input's 1st dimension.)
houseroad(2018-04-04 16:36:20):You already have two input assumption here. :-)
houseroad(2018-04-04 16:38:07):I would like to have another case, with broadcast enabled, our optimizer should not have any optimization
fumihwh(2018-04-05 02:33:38):@houseroad 
Yes. And I don't know which input comes from conv op, so I use `idx_of_conv` to get the index.
fumihwh(2018-04-05 03:33:28):@houseroad 
I tried and failed on getting shape of conv by using sizes(). Always 0.....
~~Also broadcast's value. I could use `n->hasAttribute(Symbol)` to check if exists, but can not get the value.~~
Could you help?
fumihwh(2018-04-05 03:52:22):I use `namespace ONNX_NAMESPACE { namespace optimization {` at first. What should I do with this method?
fumihwh(2018-04-05 04:12:05):I think our optimizer should only work with broadcast enabled and axis=1.
So why with broadcast enabled will be unavailable?
Do you mean axis is not C? e.g. broadcast=1, axis=2, bias_shape=(1, 1)
houseroad(2018-04-05 06:25:54):I mean we have some constraints on Conv's bias, it has to be a 1D tensor with size M (which is the feature number of W). So in case broadcasting case, this fusion cannot be conducted.
houseroad(2018-04-05 06:26:41):I will check whether there is a bug in our IR constructor or just we don't have enough information in this case.
houseroad(2018-04-05 06:27:16):I think `onnx::` is not needed, ArrayRef should be visible. I will double check.
fumihwh(2018-04-05 06:42:58):Depends on above comment mentioned that we need conv shape information to check this. 
fumihwh(2018-04-05 06:43:40):Ok. I’ll also check the constructor. 
fumihwh(2018-04-05 07:32:11):checked and removed.
fumihwh(2018-04-05 09:19:32):@houseroad 
For priori,
```
NodeProto doesn't have dim information. And field type of input and output is string.
TensorProto and ValueInfoProto has dim information.
```

In my practice,
I call `def make_graph(nodes, name, inputs, outputs, initializer=None, doc_string=None)` to make graph.

nodes: list of NodeProto
inputs: list of ValueInfoProto
outputs: list of ValueInfoProto
initializer: list of TensorProto

I think it should be a problem that each node does not keep the output shape.
onnx knows shape information of inputs and outputs and initializer.
But after one operator, shape information disappears. We can not find it, because it doesn't exist in graph.
anderspapitto(2018-04-05 18:57:55):yes, there is no guarantee to have size information for intermediate values. We are separately working on adding shape inference, to fill it in in more cases.
houseroad(2018-04-05 21:23:51):@fumihwh yeah, we store the shape info in the graph.value_info. 

In https://github.com/onnx/onnx/pull/726, I just extend the make_graph to accept value_info. This will be helpful while testing your optimization pass.
houseroad(2018-04-06 17:33:50):I would like to call it fuse_add_bias_into_conv.
anderspapitto(2018-04-06 18:34:30):can we be a little more specific? e.g. "Warning: failed to fuse Add into Conv bias due to ..."
houseroad(2018-04-06 20:00:13):nit: n->inputs()[idx]->node()->inputs().size() == 2
houseroad(2018-04-06 20:05:32):To enable this optimization in more cases:
We can also check bias_shape[0].dim != orig_conv->node()->inputs()[1]->sizes()[0].dim (conv's 2nd input weight's first dimension)
houseroad(2018-04-06 20:19:42):Y's shape should be (16, 5, 3, 3), please check the https://github.com/onnx/onnx/blob/master/docs/Operators.md#Conv
houseroad(2018-04-06 20:21:07):Actually, we should also handle this case. By checking Y's first dimension. :-)
houseroad(2018-04-06 20:25:51):If you want, you can try to handle such cases.

Hints: add reshape to change the shape of bias. So it can be fused into conv.
This case is a little bit challenging. :-)
fumihwh(2018-04-07 03:51:11):I am not sure if we need to also check weight dim M same to bias dim.
Why check output from conv is not enough?
fumihwh(2018-04-07 03:55:07):Aha, I know why do you want to check weight dim.
I think checking weight dim should not be included here. It's a Conv specification unconformity. Should check in checker.py? maybe.
houseroad(2018-04-07 04:01:12):I didn't say checking output is not enough. I mean you can enhance the optimization by checking the weights (2nd input) of Conv.

As long as you get either weight's shape or output's shape, you can do the optimization. (Of course, Bias's shape is also necessary.)
houseroad(2018-04-07 04:04:09):I think we can depends on weight's shape information. Because in the operator doc, we explicitly say both weight_shape[0].dim = bias_shape[0].dim = M.
houseroad(2018-04-07 04:33:52):Here should be (conv_shape.size() == 0 && weight_shape.size() == 0).

Either we have conv_shape[1] or weight_shape[1], we can proceed. Because they are equivalent.
anderspapitto(2018-04-12 19:56:27):you're going to get featured in https://accidentallyquadratic.tumblr.com/

but it's probably fine in this case
anderspapitto(2018-04-12 19:57:54):++ thanks for fixing
anderspapitto(2018-04-12 19:58:26):what is the issue here? should we fix it now, or maybe open a task to fix it later?
anderspapitto(2018-04-12 19:59:38):can you add a line that describes what this is transformed into? I guess it would be `B = Conv(X, Y, A)` or something
anderspapitto(2018-04-12 20:00:19):why? shouldn't it still work 
houseroad(2018-04-13 17:48:17):Good point :-)

houseroad(2018-04-13 17:56:19):I think this is fine. Never mind. :-)
houseroad(2018-04-13 18:03:02):If bias is not constant or initializer, then we have to check all of the bias's dependencies has no dependencies on conv, so we can move bias before conv.

We don't have any dependency analysis here yet, so better to give this assumption.
bddppq(2018-04-04 19:11:46):We should even remove this py_cmd stuff from cmake, instead do all the necessary stuffs in setup.py and pass it to cmake as flag.
bddppq(2018-04-04 19:20:11):https://github.com/onnx/onnx/pull/710
CLAassistant(2018-04-04 20:58:31):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=711) <br/>All committers have signed the CLA.
bddppq(2018-04-04 21:48:08):Also need to re-generate the doc
AppVeyorBot(2018-04-04 22:14:54)::white_check_mark: [Build onnx 0.3.2085 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2085) (commit https://github.com/onnx/onnx/commit/c05a0544e0 by @snnn)
snnn(2018-04-04 22:28:16):Where should I put the data?
houseroad(2018-04-04 22:29:24):The script will automatically generate the data and put them in the right place `onnx/backend/test/data/...`
houseroad(2018-04-04 22:29:59):And @bddppq is right, you also need to regenerate the doc too :-)
snnn(2018-04-04 23:09:54):Just curious, who knows I didn't put any malicious data inside this pr?
houseroad(2018-04-04 23:17:11):@snnn that's a good question. We use checker/CI to verify the model. But in theory, they are not exhaustive. If interested, your contribution on security check is welcome.
bddppq(2018-04-05 00:14:45):Ideally we should in the CI check that the new data files you are submitting are exactly the same as what our tool generates. But data files generated on different platforms/environments can have small precision differences, so the comparison is not that straightforward. This is solvable, but not implemented.
houseroad(2018-04-05 00:26:03):As @bddppq mentioned, we could and should check the generated data. Even with such checks, it is still possible to add some unnecessary (maybe malicious) data in the PR to exploit vulnerabilities.
houseroad(2018-04-05 00:29:53):Just created https://github.com/onnx/onnx/issues/715 to track the data verification stuff.
snnn(2018-04-05 23:51:06):Hi @houseroad   Document is updated. 
snnn(2018-04-06 18:23:55):@houseroad @bddppq ping
snnn(2018-04-09 23:06:41):Anything should I do?
bddppq(2018-04-04 21:43:00):put this list of pass names in a tuple and set it as the default value of pass_list function argument?
bddppq(2018-04-04 21:43:13):nit: call it `passes`?
bddppq(2018-04-04 21:44:53):I prefer to have one fixed input & output type.
bddppq(2018-04-06 18:35:59):The signature should not allow string as input anymore. and the output type should remain as ModelProto
bddppq(2018-04-06 18:37:16):this description doesn't look correct. I think it always return ModelProto now
houseroad(2018-04-06 18:41:04):good catch
AppVeyorBot(2018-04-05 06:05:17)::white_check_mark: [Build onnx 0.3.2111 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2111) (commit https://github.com/onnx/onnx/commit/9b175b6535 by @houseroad)
bddppq(2018-04-05 06:50:40):There are lint errors:

```
./onnx/shape_inference.py:25:1: E302 expected 2 blank lines, found 0
./onnx/test/shape_inference_test.py:6:1: F401 'onnx.ModelProto' imported but unused
./onnx/test/backend_shape_inference_test.py:12:1: F401 'onnx.ModelProto' imported but unused
```
houseroad(2018-04-05 18:05:04):@anderspapitto when I wrote the doc, I had to convert between str and ModelProto. This is not user-friendly. Dima also mentioned it. So let's change it before it is used widely.
jspisak(2018-04-05 14:11:58):Thanks for the proposal! Adding Ke as well for review and comment.. @bddppq @linkerzhang 
linkerzhang(2018-04-05 16:38:01):Thank you @postrational for the proposal! Node fusion (of graph optimization) is being discussed. This is the wiki of the proposal being discussed, https://github.com/onnx/onnx/wiki/Proposal----Adding-Function-into-ONNX.

Back to this attribute nesting, I personally would suggest not doing it since having attribute flatten in node level does not introduce any inconvenience. In the proposal I attached, attribute reference is a way designed to allow fusion sub nodes inside to refer to attribute data outside (in parent node). We may discuss more on gitter or messenger.
RanACohen(2018-04-12 10:50:40):Maybe putting node fusion as an example is not the best example as it can be solved in other means as suggested.

But for our needs, we have defined a TensorIterator layer (which resembles the caffe2 iterator layer). This layers has a list of attributes where each attribute is a structure of several different attributes.

I know this can also be flatten but it is not convenient to flatten such complex structures and I do image that someone else might have a need for this also.

CLAassistant(2019-07-24 00:57:44):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=717) <br/>All committers have signed the CLA.
askhade(2021-10-01 16:13:51):@postrational : Can we close this PR?
askhade(2021-10-28 16:33:08):Closing after offline discussion with @postrational 
bddppq(2018-04-05 19:33:30):"default" CI failure is irrelevant
bddppq(2018-04-05 20:33:51):@onnxbot retest this please
AppVeyorBot(2018-05-02 15:54:07)::x: [Build onnx 0.3.2756 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2756) (commit https://github.com/onnx/onnx/commit/5ee45d2c82 by @NiklasGustafsson)
AppVeyorBot(2018-05-02 16:10:45)::x: [Build onnx 0.3.2757 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2757) (commit https://github.com/onnx/onnx/commit/3aff00c19f by @NiklasGustafsson)
NiklasGustafsson(2018-05-03 12:51:13):It must have been the merge after the last 'Update Branch' that resulted in those edits.
AppVeyorBot(2018-05-03 13:06:38)::x: [Build onnx 0.3.2789 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2789) (commit https://github.com/onnx/onnx/commit/3bb292d0ae by @NiklasGustafsson)
NiklasGustafsson(2018-05-03 14:34:22):Lu,
Thank you for all your review comments. My latest commit should have addressed most of them, excluding those where I had a follow-up question for you.

Thanks,
Niklas
AppVeyorBot(2018-05-03 14:50:39)::x: [Build onnx 0.3.2790 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2790) (commit https://github.com/onnx/onnx/commit/b589f896da by @NiklasGustafsson)
AppVeyorBot(2018-05-03 15:06:53)::white_check_mark: [Build onnx 0.3.2791 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2791) (commit https://github.com/onnx/onnx/commit/a179ee23a4 by @NiklasGustafsson)
houseroad(2018-05-03 18:41:59):@NiklasGustafsson thanks for the prompt update. I have added more comments. :-)
gramalingam(2018-04-05 20:34:24):How about "A tensor shape is a list of dimensions"?
gramalingam(2018-04-05 20:46:09):I don't  understand this ... two different node-outputs cannot have the same name, right?
NiklasGustafsson(2018-04-05 20:47:51):Yes, I agree that it's better.
gramalingam(2018-04-05 20:55:00):The last sentence "Graph outputs MAY NOT ..." is unclear. Every graph output X must also appear as the output of some node (or must refer to a graph input). This X could be legally used as an input in some other node as well. I guess the sentence is trying to clarify that no dependence edge is created from the graph-output to such an input (in this case)? But it seems to imply the use of X as an input of some node is not valid. Since dependency edges are created only between nodes, perhaps this clarification is not necessary? 
NiklasGustafsson(2018-04-05 20:55:22):I think this is clumsy wording on my part. 

The motivation was the If() operator, where you would expect the same outputs, or some overlap of them, to be computed by each of the graph attributes, but it's really the operator outputs that are computed by two graphs, not by two nodes, and graph outputs would be associated with If() outputs positionally, not by name, as far as I understand.
NiklasGustafsson(2018-04-05 20:58:20):Regarding your second comment -- we need to specify that node inputs cannot depend on a graph output that hasn't already been computed by another node. So, I think some clarification is necessary, but you are right in that the current one seems imprecise.
gramalingam(2018-04-05 23:10:07):Both inputs and outputs can be optional.
wschin(2018-04-10 05:58:27):Consider LSTM with output_sequence=False as an example. Does "provide a name for the output when it is computed" mean that we can assign an empty string to LSTM's first output in this case?
NiklasGustafsson(2018-04-10 15:03:07):Optional outputs is perhaps the murkiest part of the semantics, hardest to explain, I think. 

Optional inputs make sense -- they don't have to be provided.

Optional outputs, however -- who is it that has an option, the caller or the operator? In other words, is it that the operator has the option of not computing the value, or the model graph that has the option of not picking it up? I believe the former is what's I've heard described, the operator may not compute it. 

If that is correct, then looking at my wording above, the text is focusing on the wrong issue -- it's not about missing to pick up a value that is computed, the issue is giving a name to an output that is not actually computed, and then using it as input to something else.

wschin(2018-04-10 16:14:20):For LSTM/RNN/GRU, I think the existence of optional outputs (or inputs) means that an operator can have multiple static configurations because whether the first output is computed or not is determined by a user-specified attribute. Therefore, the user of such an operator should be in charge of choosing the right outputs. If she turns output_sequence on, the first output name would be a meaningful string. If not, she can use an empty string. My point is that as long as an output configuration is (implicitly) determined by an user, using an empty string is better than setting up a flag+provide an uncomputed tensor name. Anyway, if users always need to specify all output names, probably you want to put a note here because it's different than input behavior.

gramalingam(2018-04-10 17:05:25):Yes, the optional outputs part is somewhat murky. My interpretation is that

(a) Generic (operator-independent) constraints: The model may omit an output (using an empty string or completely omitting it in the case of trailing outputs) only if that output is marked as optional in the operator specification.

(b) There will typically be further operator-specific constraints on when omitting an output is valid, which is currently captured only in the informal operator description.
gramalingam(2018-04-10 17:14:31):Regarding @NiklasGustafsson 's question as to who has the option: I believe it is a contract between both the caller and the operator, which unfortunately is only informally captured in the operator description. Since this part is not formally captured, "model validation" cannot check these aspects of the contract (e.g., such as "an output is computed iff a specific attribute has a specific value"). This, in turn, means that the semantics of what happens during a model's execution if this contract is violated is undefined.
NiklasGustafsson(2018-04-10 17:17:30):This raises a question -- if a node fails to give a name to an output that is computed (optional or not), what is the failure mode? It seems to me that giving a name to an output that is __not__ computed is definitely an error, while the reverse isn't as obviously wrong. I guess that some implementations could have trouble with both.
wschin(2018-04-10 17:31:42):@NiklasGustafsson , what do you mean by failure mode? Is it "missing output tensor name"? Also, why assigning an output name to an uncomputed output is wrong? I think it's fine if no other operator refers to that undefined output. Do you mean "forgetting assigning a name to a computed output is wrong"?
gramalingam(2018-04-10 17:35:21):Giving a name to an output that is not computed: this could happen even with non-optional outputs. When this named output is subsequently used in a different node, it would pretty much behave like the use of an uninitialized variable (the corresponding tensor memory could have random values). (Obviously, this depends on the runtime implementation.) Unfortunately, we cannot always catch this statically.

Failing to give a name for an output that is not optional is easier to handle: model validation can catch this, and a runtime can avoid initiating execution. Even when it can't be caught in model validation, it can be caught at runtime. How this exception is reported to the user is implementation-dependent. I guess ONNX could try to standardize the exception behavior in such cases.

NiklasGustafsson(2018-04-10 17:44:21):By "failure mode," I mean how does an error manifest itself at runtime? 

If an output (optional or not) is not named, it cannot be used as an input or graph output. What downstream error can arise from that? (I'm not saying there can't be one, it's just not clear to me what it could be.) 

Failing to name an output that is computed could be an indication that the model is incorrectly constructed, which is worth at least a warning during validation, but it doesn't seem to be able to cause corruption of data, and if a model doesn't need the output, then why make it assign a name?

By assigning a name to an output that is __not__ computed, errors can arise, depending on the runtime implementation. Since such a name can be used as input to other operators, or as a graph output, uninitialized values may be propagated, which leads to further errors and, in the worst case, crashes. 

Just asking questions...
gramalingam(2018-04-10 17:50:54):Yes, for the second part, we can naturally extend the language to allow callers to indicate that they don't care about a particular output, e.g., perhaps by using a special name like "_". (But my understanding is that this scenario is not what the current optional-output mechanism targets.)
NiklasGustafsson(2018-04-10 17:52:32):OK, I'll change the text to reflect this.
wschin(2018-04-10 18:05:50):I think there are three different questions. Below I summarize our thread.

1. How to specify an operator's optional output when it's not computed? I assume we all agree that it should be an empty string or unspecified if they are trailing outputs.

2. Is it legal if one doesn't provide a name (to the operator) for a computed output? Yes, if it's optional (equivalent to assigning an empty string). No, if it's not optional (we need to throw an exception because it violates the meaning of a required field).

3. Is it legal if one provide a name to a required output which is not computed? Can this really happen? If it happens, does it mean that we should mark that output optional and user may assign it an empty string when constructing the graph?

I have another question for (1). If my assumption is correct, why do we have output_sequence flag in LSTM/GRU/RNN? It looks a bit redundant to me because it's implicitly determined by the first output's name (empty string --> output_sequence=False, meaningful string --> output_sequence=True). 



wschin(2018-04-10 18:40:39):Let me add a table for all possible output configurations. We can use it for further discussion.

| Optional  | Named (non-empty name) |  Computed  |  Status  |
| ------------- | ------------- | ------------- | ------------- |
|  Y  |  Y  |  Y  |  OK  |
|  Y  |  Y  |  N  |  Error  |
|  Y  |  N  |  Y  |  Error  |
|  Y  |  N  |  N  |  OK  |
|  N  |  Y  |  Y  |  OK  |
|  N  |  Y  |  N  |  Error  |
|  N  |  N  |  Y  |  Error  |
|  N  |  N  |  N  |  Error  |

NiklasGustafsson(2018-04-10 21:08:22):Thanks for the summary. I believe that the corrected text reflects the rules of that table.
wschin(2018-04-11 23:39:11):Does it mean that the dimensions defined by a 'value_info"s are mutually independent even if some of them are identical strings? 
wschin(2018-04-11 23:50:31):I think attributes should be non-trainable parameters? What do you think? Many variables such as LSTM's recursion matrices are somehow constant in test phase.
wschin(2018-04-12 00:00:19):It'd be better to give some examples here. attribute values ---> attribute values (e.g., an operator's attributes); runtime values ---> runtime values (e.g., inputs and outputs of an operator). It's not extremely clear to me what runtime values are. As a standard, you might want this to be more specific.
wschin(2018-04-12 00:03:10):Can one variable be used as an output for an operator and an input for another operator? If yes, you might want to add  a tiny note.
wschin(2018-04-12 00:11:33):"a referring node MAY choose not to provide a value for that input" or "a user of such an operator MAY choose not to provide values for some of optional inputs"?
wschin(2018-04-12 00:21:44):all its declared outputs ---> all its outputs? The meaning of "declared" is not clear to me.
wschin(2018-04-12 00:24:34):I believe they also support different operator sets. For example, "LinearClassifier" is not a part of ONNX but belongs to ONNX-ML. Please correct me if I am wrong. Thanks!
wschin(2018-04-12 00:30:41):Would you want to add a note here to explain that symbolic dimension variable is not unknown but variable-length? It's kind of confusing to my tiny brain. Probably the note can be put into next paragraph. :)
NiklasGustafsson(2018-04-12 14:53:19):Wei-Sheng,

I don't quite understand what you are proposing. Are you saying we shouldn't call them "attributes"? If so, I disagree -- we've been using that name since the beginning, and it has little to do with trainability.
NiklasGustafsson(2018-04-12 15:00:46):ONNX doesn't have any formal notion of a "variable," so I assume that you are referring to names of outputs and inputs. Having the same names used as the output of one operator and the input of another operator is how data is transferred in the graph, so yes, and I believe that is what the text already says. If you disagree, please suggest some alternative language that better expresses it.
NiklasGustafsson(2018-04-12 15:01:50):Thanks for bringing this up! 'Choose' in this context is an anthropomorphism that doesn't belong in a spec. I will change the language.
NiklasGustafsson(2018-04-12 15:05:21):Fixed
NiklasGustafsson(2018-04-12 15:06:59):Yes, that is an important point. Thanks!
NiklasGustafsson(2018-04-12 15:15:49):What I'm trying to say here, perhaps not precisely enough, is that for graph inputs and outputs, dimension variables are scoped to the collection of inputs and outputs. This is so that we can associate dimension information between inputs and outputs. For value_info records that are not inputs or outputs, I believe we don't force association across values, but maybe that's a misunderstanding on my part.
wschin(2018-04-12 15:49:59):Sorry for not making it clear. My original opinion is to have a specific rule to distinguish "inputs" and "attributes" from each other. A "constant" is not ultimately clear to me because for example, LSTM's recursion matrices are constant under test mode but they are "inputs." Not sure if changing "literal constants" to "literal constants at inference stage and training stage" is helpful?
wschin(2018-04-12 15:59:11):I believe you're right but this might need help from all praters to confirm. To make this concept clearer, you might want to change **"Dimension variables appearing in a graph's 'value_info' record are scoped to the value"** to **"Dimension variables appearing in a graph's 'value_info' record are scoped to the value so they don't force any association across values which are not graph's inputs or outputs"**
wschin(2018-04-12 16:29:15):Sorry for causing ambiguation. When looking at BatchNormalization where "mean" appears in its input and output lists, I was considering if using a tensor for both of an operator's input and output is legal. If not legal, I would like to add a sentence to explicitly tell users not to do so.
NiklasGustafsson(2018-04-12 16:33:01):Doing so would, it seems to me, create a cycle in the graph from the node's output to its input, which is explicitly disallowed in the spec. 
wschin(2018-04-12 16:36:38):I know it's not allowed because it's a loop, but I would like to emphasize this point. In the meanwhile, CoreML allows this case because they're doing in-place computation.

A typo: All node output **namess** MUST be unique within the graph.
NiklasGustafsson(2018-04-12 16:49:53):In-place computation violates the single static assignment rule, so that is already covered.
wschin(2018-04-12 16:54:23):"single static assignment" is a magic word to some poor users like me. My background is applied math.
wschin(2018-04-13 18:13:33):with  ML algo --> with ML algo (please remove one extra space)
wschin(2018-04-13 18:14:57):pperator --> operator
wschin(2018-04-19 01:10:25):According to some discussions [here](https://github.com/onnx/onnx/pull/755), a graph can have multiple identical output names. Could we clarify this point somewhere, saying that node outputs are different from graph outputs?
wschin(2018-04-19 01:11:19):introduces a value ---> remove an extra space between "a" and "value"
houseroad(2018-05-02 06:11:56):More specific, a 32 bit floating point value?
houseroad(2018-05-02 07:08:26):checker.py is a python wrapper, actually the real implementation is in C++.
houseroad(2018-05-02 07:18:59):Shall we also talk about release version here?
houseroad(2018-05-02 07:22:42):Each opset represents the combination of newest version of each operator.
houseroad(2018-05-02 07:36:59):two `has` here
houseroad(2018-05-02 15:22:11):This is not consist with our current checker. Our checker checks whether opset_import's domain exists, and also ir_version is required.
houseroad(2018-05-02 15:26:03):ONNXOPSET
NiklasGustafsson(2018-05-02 15:48:22):OK, will explain.
NiklasGustafsson(2018-05-02 15:49:17):I'm not sure, that seems to me quite distinct from how ONNX itself uses versioning in its format.
NiklasGustafsson(2018-05-02 15:52:47):So, what should be changed in the text? That isn't clear from your comment. Should I insert a 'SHOULD' before 'use reverse domain names'?
NiklasGustafsson(2018-05-02 15:53:04):Thanks
houseroad(2018-05-02 15:58:03):Attributes are statically determined, and input can be dynamically computed. This is orthogonal to training and reference.
houseroad(2018-05-02 15:58:50):Although calling a graph attribute constant literal sounds a little bit weird to me.
NiklasGustafsson(2018-05-02 16:03:16):Yes, I agree that "literal constants" is redundant, I'll fix that.
I don't understand your comment about 'training and reference,' where does the text mention that with regard to attributes or inputs?
houseroad(2018-05-02 18:39:22):SSA is an important property, why it's missing?
houseroad(2018-05-02 18:40:05):Here, we use value_info to store the shape/type information of non-input/output nodes
NiklasGustafsson(2018-05-02 20:10:07):It's mentioned under 'Nodes' farther down in the document. Does it make the graph explanation clearer if it's included here, too?
houseroad(2018-05-03 04:21:44):Yes, please, this is an important properties of the graph.
houseroad(2018-05-03 04:30:51):Why do we make graph's name required?
houseroad(2018-05-03 04:54:57):Shall we use a different name? Here is quite ambiguous... We describe the values namespace in the previous section, here we keep using values, but we are talking about the attribute values and input/output values.
NiklasGustafsson(2018-05-03 04:56:03):Runtimes are not the only consumers of models; some tools will need something to identify the model. It's also important to identify models in logging, error messages, etc. In the future, if and when we get to supporting model composition, names will be essential for that ability, too.
houseroad(2018-05-03 04:59:37):Here, we probably should explicitly say float is float32.
NiklasGustafsson(2018-05-03 05:00:26):Not sure where the ambiguity comes from. The text is referring to 'attribute values' and 'runtime values,' and elaborates on the two. But clarity is essential, so if there's a better formulation, please suggest one. It seems unambiguous to me, but I wrote it, so that's not the bar.
NiklasGustafsson(2018-05-03 12:49:11):Thanks.
houseroad(2018-05-03 18:33:28):I guess we can explain more in Versioning.md
houseroad(2018-05-03 18:35:27):But we already have the model name, I am totally confused why we need both graph name and model name. Especially, each model can only contain one graph... 
houseroad(2018-05-03 18:40:38):I think you can move the attribute values to the previous subsection attributes, and change the names of this subsection to Operator Input and Output
NiklasGustafsson(2018-05-03 18:41:20):I don't find a name property in ModelProto. The model name is stored in the graph message.

From the current onnx.in.proto, commenting on the domain name of a model:

  // Domain name of the model.
  // We use reverse domain names as name space indicators. For example:
  // `com.facebook.fair` or `com.microsoft.cognitiveservices`
  //
  // Together with `model_version` and GraphProto.name, this forms the unique identity of
  // the graph.
  optional string domain = 4;

NiklasGustafsson(2018-05-03 18:43:35):That makes sense. I intend to tackle Versioning.md next, by the way.
houseroad(2018-04-05 19:13:25):Thanks!

Also the link in README.md?
anderspapitto(2018-04-05 20:58:15):built on top of https://github.com/onnx/onnx/pull/655
AppVeyorBot(2018-04-12 22:51:21)::x: [Build onnx 0.3.2276 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2276) (commit https://github.com/onnx/onnx/commit/43910252b5 by @anderspapitto)
AppVeyorBot(2018-04-13 16:11:28)::x: [Build onnx 0.3.2287 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2287) (commit https://github.com/onnx/onnx/commit/03f76ba585 by @anderspapitto)
houseroad(2018-04-05 21:15:07):2 FLOAT? C++ may miss some float, so we need a second check to catch them?
anderspapitto(2018-04-05 21:28:00):float is a very important datatype and needs extra attention
gramalingam(2018-04-13 03:30:14):Why not return false, instead of throwing an exception? E.g., I see this being invoked for Concat, where this may not be an error.
gramalingam(2018-04-13 03:31:26):I don't understand: there's a check earlier for exactly 1 input type.
gramalingam(2018-04-13 03:45:32):"Cast" -> "Shape"
anderspapitto(2018-04-13 15:49:01):well, this is just a helper that throws an exception. the error is with concat - i'll remove that usage
anderspapitto(2018-04-13 15:49:22):++
anderspapitto(2018-04-13 15:49:28):++
gramalingam(2018-04-13 16:18:00):The spec above doesn't actually say what the behavior is when leftover is non-zero. May be a good idea to extend the spec to do that (though that should probably be a separate PR since it affects the spec).
gramalingam(2018-04-13 16:30:27):Is this supposed to be initialized to some value from the input dims?
anderspapitto(2018-04-13 17:55:21):++ will make a PR for the spec
anderspapitto(2018-04-13 17:57:15):not at the moment - this is more "rank inference" than "shape inference". It can be enhanced in a future diff
gramalingam(2018-04-13 18:03:13):It would be safer to initialize the dim with the empty string param than leave it uninitialized (neither value nor param set).
anderspapitto(2018-04-13 18:26:05):I guess representing this is still a little up in air - but see the comment I just wrote at https://github.com/onnx/onnx/pull/681#issuecomment-381221996
gramalingam(2018-04-13 18:34:33):Okay. As long as the meaning/representation is standardized and well-documented, it's fine.
linkerzhang(2018-04-06 03:43:51):@ebarsoum is going to send a proposal of bumping op_set version together with ONNX release, which makes lots of sense, I think. For now, between 2 ONNX releases, say, 1.0 and 1.1 or 1.1 and 1.2 in future, there may be many op_set version created due to breaking change introduced in between. This makes backend tough to follow and to ensure backward compatibility (too many version needs to be supported). 
bddppq(2018-04-06 03:52:34):I think reducing the number of op_set bump makes senses. But I think we should freeze a version together with a release and bump the version number in master right **after** making a release.
Adding @ezyang to this discussion.
linkerzhang(2018-04-06 03:55:06):YES. basically, after an ONNX release, if there's an op breaking change, bump the op_set version once and keep the version till next ONNX release.
houseroad(2018-04-06 04:14:27):I would like to have two types of opset_import version:
1) develop (all odd)
2) release/stable (all even)

For example, we use opset 7 as a develop opset version. It will be bumped right before next release, the next release will use opset version 8, and after release cut, we will use opset version 9 for development.

The benefit is obvious: we don't need to promise any long term support for develop version.
ezyang(2018-04-06 04:29:08):This is OK, but I also prefer the even-odd stable-unstable numbering scheme, and to also have the checker warn when a model is tracking an unstable opset version.

Also, I'd like to know what the pain points of opset number bumps are, because they are supposed to be cheap (or, more specifically, it should be no more expensive to have five bumps for five changes, than one bump for five changes), so if they are weirdly expensive then it's important to know, because that affects compliance for real releases as well.
houseroad(2018-04-06 04:32:23):One point is the model zoo. Right now, we maintain the models for each opset. :-)
At least before we have a model version converter, we probably have to stick to this solution.
bddppq(2018-04-06 15:49:53):@ezyang I feel two pain points:
1. compliant frontend and backend need to follow each opset and provide backward compatibility
2. it makes onnx afraid of changing any existing schema, delaying the bump to each release gives us a very good staging period.

On the other hand, if we change to tie opset to release, then we should do more frequent (and regular) releases so people know when they can expect the backward compatibility guarantee.
bddppq(2018-04-06 17:36:16):@linkerzhang Let's move the discussion of the opset bump into a separate github issue (you mentioned @ebarsoum is going to create one). In the meantime, please review this `Cast` schema change at your convenience.
ezyang(2018-04-06 17:52:46):OK, let's do it.
AppVeyorBot(2018-04-07 05:31:09)::white_check_mark: [Build onnx 0.3.2192 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2192) (commit https://github.com/onnx/onnx/commit/6c32a8a22c by @bddppq)
AppVeyorBot(2018-04-12 23:40:41)::x: [Build onnx 0.3.2278 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2278) (commit https://github.com/onnx/onnx/commit/e218871ddb by @bddppq)
houseroad(2018-04-13 23:39:27):We also need to change here https://github.com/onnx/onnx/blob/master/onnx/test/shape_inference_test.py#L87

If not bumping up the opset_version, all the node tests have to be updated, too
AppVeyorBot(2018-04-15 04:01:20)::x: [Build onnx 0.3.2321 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2321) (commit https://github.com/onnx/onnx/commit/cf815945b3 by @bddppq)
AppVeyorBot(2018-04-15 04:25:03)::x: [Build onnx 0.3.2322 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2322) (commit https://github.com/onnx/onnx/commit/2f2b0dc81e by @bddppq)
AppVeyorBot(2018-04-15 05:06:51)::white_check_mark: [Build onnx 0.3.2323 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2323) (commit https://github.com/onnx/onnx/commit/28e82b7398 by @bddppq)
houseroad(2018-04-06 01:13:31):Why do we only change this file?
bddppq(2018-04-06 02:21:21):Sorry what other files do I also need to change? 
This one was caught by test
houseroad(2018-04-06 03:26:46):I was wondering why only this notebook needs changes. I think I know why now. Because, in other cases, we only load the existing model, not making. Only in this case, new model is created. :-)
anderspapitto(2018-04-06 18:42:32):as we discussed in person, I believe that the correct way to do this is to add a case to the attribute type enum, which corresponds to TensorProto::DataType, rather than using an integer that happens to line up with the integer values of the DataType enum.

The advantages are that (1) it's clearer, and (2) tooling which understands protobuf will automatically work - for example the `str()` of a `ModelProto` in python will automatically say `TensorProto.FLOAT` instead of `4`.

But if i'm the lone voice here then I won't hold it up
bddppq(2018-04-06 21:00:09):Having the visualization tooling support is indeed more ideal, however the cost is not small:
1. each backend needs to be updated to acknowledge a new AttributeProto data type
2. existing tools (checker, our c++ IR, schema declarations) needs to be updated to acknowledge this as well

And use cases seem to be small, so far I can only think of Cast and GivenTensorFill (experimental).
houseroad(2018-04-06 23:05:15):Nit: export ==> export_nearest
houseroad(2018-04-06 23:05:45):Nit: 'test_upsample' ==> 'test_upsample_nearest'
anderspapitto(2018-04-09 17:56:26):oops
bddppq(2018-04-07 05:57:30):https://github.com/onnx/onnx/pull/383
AppVeyorBot(2018-04-07 06:14:12)::white_check_mark: [Build onnx 0.3.2194 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2194) (commit https://github.com/onnx/onnx/commit/2257bbb468 by @houseroad)
bddppq(2018-04-09 19:08:34):@linkerzhang @ebarsoum could you review this one?
AppVeyorBot(2018-04-09 20:51:49)::white_check_mark: [Build onnx 0.3.2203 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2203) (commit https://github.com/onnx/onnx/commit/399385d0ee by @houseroad)
ebarsoum(2018-04-09 21:58:36):@houseroad can we have test cases that cover the general form?
houseroad(2018-04-09 21:59:01):@ebarsoum sure, I will add them in a different pr. :-)
houseroad(2018-04-09 22:01:01):@ebarsoum BTW, we have some converted cases from pytorch. Before I add new node tests, we can rely on such cases. 

https://github.com/onnx/onnx/tree/master/onnx/backend/test/data/pytorch-converted/test_BatchNorm2d_eval

https://github.com/onnx/onnx/tree/master/onnx/backend/test/data/pytorch-converted/test_BatchNorm3d_eval
bddppq(2018-04-10 03:31:34):`struct` -- so c++ ;)
houseroad(2018-04-10 03:39:09):or pure c :-P
AppVeyorBot(2018-04-10 11:41:57)::x: [Build onnx 0.3.2229 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2229) (commit https://github.com/onnx/onnx/commit/64f7d6b302 by @pk-g)
onnxbot(2018-04-12 21:02:15):Build finished. 

spandantiwari(2018-04-12 21:07:32):Can we add tests for bidirectional LSTMs please?
spandantiwari(2018-04-12 21:09:04):A (few) different combination of input_size and hidden_size would be good to have. 
spandantiwari(2018-04-12 21:10:11):Do we need to check in model.onnx files?
spandantiwari(2018-04-12 21:11:18):Are we planning to add more coverage for different activation functions?
pk-g(2018-04-12 21:47:09):yes, that will be needed by backends that will try to run the tests.
snnn(2018-04-13 00:16:59):Where do the numbers come from? What if someone's code failed this test? How can the author debug his code? He would wondering, why the result should be such numbers, not the others? 
pk-g(2018-04-21 01:02:46):Sure, will add.
pk-g(2018-04-21 01:10:30):The intention for this PR is to provide basic test coverage for forward case, as well as reference numpy implmentation for it. Different variations, including Bidirectional LSTMs, shall be added seperately as part of other PRs.
pk-g(2018-04-21 01:11:57):The intention for this PR is to provide basic test coverage for forward case, as well as reference numpy implmentation for it. Different variations shall be added seperately as part of other PRs.
pk-g(2018-04-21 01:14:20):Please take a look at the updated PR. I removed the hardcoded numbers and instead included the numpy implementations based on formulas provided in ONNX documentation.
spandantiwari(2018-04-25 00:27:39):OK.
spandantiwari(2018-04-25 00:31:40):Can we give capture the 3 as a variable with a suitable name instead of a magic number for the benefit of the reader? Similarly for LSTM tests below we can capture 4 as a variable. 
spandantiwari(2018-04-25 00:32:54):Bidirectional LSTM?
pk-g(2018-04-25 18:39:13):Sure, updated the PR.
pk-g(2018-04-25 18:45:31):I agree that we need tests for bidirectional LSTM, as well as different variations of activations and etc. The intended scope for this PR is to add basic test coverage for forward case only and also provide a base for numpy implementation that shall be extended later. Additional tests, which would also require extension of base numpy implementation, can be added separately as part of other PRs.
spandantiwari(2018-04-25 21:06:16):OK. 
houseroad(2018-04-10 20:44:05):I didn't notice the mypy had been merged. Thanks for fixing this.
AppVeyorBot(2018-04-10 23:05:15)::x: [Build onnx 0.3.2242 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2242) (commit https://github.com/onnx/onnx/commit/7e0e93e543 by @anderspapitto)
AppVeyorBot(2018-04-10 23:27:47)::x: [Build onnx 0.3.2243 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2243) (commit https://github.com/onnx/onnx/commit/36c524236b by @anderspapitto)
AppVeyorBot(2018-04-11 00:16:50)::x: [Build onnx 0.3.2245 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2245) (commit https://github.com/onnx/onnx/commit/056f6360ec by @anderspapitto)
AppVeyorBot(2018-04-11 16:22:54)::white_check_mark: [Build onnx 0.3.2258 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2258) (commit https://github.com/onnx/onnx/commit/adad7f48c6 by @bddppq)
AppVeyorBot(2018-04-11 20:28:33)::white_check_mark: [Build onnx 0.3.2263 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2263) (commit https://github.com/onnx/onnx/commit/27ef6556b6 by @anderspapitto)
AppVeyorBot(2018-04-13 23:59:29)::x: [Build onnx 0.3.2304 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2304) (commit https://github.com/onnx/onnx/commit/0bf7ec4b07 by @anderspapitto)
bddppq(2018-04-16 06:15:46):@houseroad model tests are run via `prepare` + `run`, so `run_model` is not needed.
AppVeyorBot(2018-04-17 21:27:55)::x: [Build onnx 0.3.2342 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2342) (commit https://github.com/onnx/onnx/commit/c829ebd961 by @anderspapitto)
AppVeyorBot(2018-04-18 21:22:04)::x: [Build onnx 0.3.2372 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2372) (commit https://github.com/onnx/onnx/commit/b3ffce0259 by @anderspapitto)
AppVeyorBot(2018-04-19 00:15:51)::x: [Build onnx 0.3.2379 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2379) (commit https://github.com/onnx/onnx/commit/88410833b3 by @anderspapitto)
AppVeyorBot(2018-04-20 00:20:48)::x: [Build onnx 0.3.2408 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2408) (commit https://github.com/onnx/onnx/commit/c1eb732f1d by @anderspapitto)
AppVeyorBot(2018-04-20 22:44:35)::x: [Build onnx 0.3.2451 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2451) (commit https://github.com/onnx/onnx/commit/8b9258d3b1 by @anderspapitto)
AppVeyorBot(2018-04-23 23:12:05)::x: [Build onnx 0.3.2491 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2491) (commit https://github.com/onnx/onnx/commit/0c8d7b2cbf by @anderspapitto)
AppVeyorBot(2018-04-24 20:20:56)::x: [Build onnx 0.3.2513 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2513) (commit https://github.com/onnx/onnx/commit/2099ace3fb by @anderspapitto)
AppVeyorBot(2018-04-24 22:02:53)::x: [Build onnx 0.3.2519 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2519) (commit https://github.com/onnx/onnx/commit/2b8b222401 by @anderspapitto)
AppVeyorBot(2018-04-25 00:08:12)::x: [Build onnx 0.3.2528 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2528) (commit https://github.com/onnx/onnx/commit/4d0d6e73a3 by @anderspapitto)
bddppq(2018-04-13 20:58:20):these imports are not needed anymore
jspisak(2018-04-12 15:18:18):@houseroad can you take a look here?
AppVeyorBot(2018-04-13 16:51:00)::white_check_mark: [Build onnx 0.3.2288 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2288) (commit https://github.com/onnx/onnx/commit/c53f863711 by @linkerzhang)
houseroad(2018-04-12 17:09:18):Looks like we have one unnecessary trailing ')' 
bddppq(2018-04-12 23:21:57):needs to regen doc
AppVeyorBot(2018-04-13 00:04:47)::white_check_mark: [Build onnx 0.3.2279 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2279) (commit https://github.com/onnx/onnx/commit/f18d57ce61 by @linkerzhang)
bddppq(2018-04-12 23:21:13):Common frameworks set this default value to 1.0f
AppVeyorBot(2018-04-13 09:18:43)::x: [Build onnx 0.3.2283 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2283) (commit https://github.com/onnx/onnx/commit/719f186b3f by @fumihwh)
AppVeyorBot(2018-04-13 09:49:38)::x: [Build onnx 0.3.2284 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2284) (commit https://github.com/onnx/onnx/commit/689854f44a by @fumihwh)
AppVeyorBot(2018-04-13 10:20:24)::white_check_mark: [Build onnx 0.3.2285 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2285) (commit https://github.com/onnx/onnx/commit/9344fcbf20 by @fumihwh)
bddppq(2018-04-13 21:13:57):Since ONNX requires SSA and we have removed the in-place annotation "consumed_inputs", I don't think we need Identity operator anymore. Shall we remove it? @linkerzhang @SherlockNoMad 
houseroad(2018-04-13 23:14:51):@fumihwh can you check why the model converted from tensorflow contains `Identity` operator?
houseroad(2018-04-14 00:24:51):Identity is not useful in SSA format. That's why we need to eliminate them from the graph. :-)
I guess Identity can make the model exporting work easier, since all the framework supports Identity-ish operator. If not, we should remove it. @tjingrant @ebarsoum @linkerzhang please confirm. 
AppVeyorBot(2018-04-14 02:03:16)::white_check_mark: [Build onnx 0.3.2310 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2310) (commit https://github.com/onnx/onnx/commit/7db1129f59 by @houseroad)
AppVeyorBot(2018-04-14 11:27:21)::white_check_mark: [Build onnx 0.3.2316 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2316) (commit https://github.com/onnx/onnx/commit/31863aa9e0 by @fumihwh)
fumihwh(2018-04-14 12:01:50):@houseroad 
BTW, in tf, when initialize variables, following code will be run, which create an Identity op in graph.
https://github.com/tensorflow/tensorflow/blob/r1.7/tensorflow/python/ops/variables.py#L386-L391
wschin(2018-04-17 16:22:49):@bddppq Are we going to remove Identity from the ONNX spec? If yes, I would like to consider two places about CoreML where Identity is currently used. (1) CoreML does have an Identity operator. (2) When converting RNN's from CoreML or Keras, sometime they may have two identical outputs and we use Identity to duplicate one from the other. Although it's possible to use a Scalar to mimic what Identity is doing, without Identity it's hard to have a perfect semantic here.
bddppq(2018-04-17 17:43:39):@wschin Let's leave it if it's useful.
fumihwh(2018-04-18 02:16:08):@wschin 
If there is no effect to the computation graph, we could optimize graph by removing them.
I am concerned about 2), what do two identical outputs do? Why we need that?
wschin(2018-04-18 04:14:17):In my opinion, we can remove Identity operators only if it's not going to change the "interface" of a computational graph. Users don't care about what a black box is doing internally. However, if the interface (i.e., graph I/O) connecting user application and the real computation is changed, it's not a black box anymore (it probably becomes a red box) because the poor user needs to modify his application. For (2), it happens when output_sequence=False and output_state=True. Again, as RNNs naturally allow this case, ONNX should have it.
houseroad(2018-04-18 04:36:31):@wschin the property you mentioned is good to have. But we didn't require this at the beginning and we already break it. In some case, it's really hard to keep this property, for example, if the whole graph only contains one identity op or non-op transpose, I think we should definitely remove this op. In general, the optimizer should preprocess the model offline. (Yes, some frameworks will use the optimizer on the fly, but it's easy to update the names of outputs.)

Further more, it's more appropriate to add this feature in a different PR.
wschin(2018-04-18 05:15:41):@houseroad , if the only op, an Identity, is removed, what is the expected behavior of this graph? There is no op, so semantically it shouldn't do anything. Is it a defined behavior of a 0-op graph in ONNX? For RNNs, CoreML can output both of the two identical outputs as a part their graph outputs. If optimization happens, we have two duplicated output names. I don't know if ONNX allows this situation. Therefore, an alternative solution is (1) to explain what users should do for empty graphs (2) clarify that duplicated graph outputs are allowed (it's confusing because operator should also produce unique output names).
houseroad(2018-04-18 17:18:19):The inputs and outputs are defined here: https://github.com/onnx/onnx/blob/master/onnx/onnx.proto#L248--L249. The inputs and outputs of an ONNX model are well defined, even if we don't have any node in the graph. ONNX models can have two identical outputs without any problem, because any tensor/var is only defined in initializer or operator. We can still keep SSA property.

I am not trying to remove Identity operator, actually, I also would like to keep it in the spec, if it will greatly ease the exporting work. :-)
houseroad(2018-04-14 00:15:24):nit: test_eliminate_identity_single_use
houseroad(2018-04-14 00:17:29):check len(node) == 0 and also check the output is "Y"
houseroad(2018-04-14 00:18:05):Please also check len(node) == 2
fumihwh(2018-04-14 11:14:37):@houseroad 
The output is "X" because Identity is eliminated also "Y".
wschin(2018-04-17 21:47:36):If an Identity is connected with a graph output, we should keep the output name?
wschin(2018-04-17 21:50:25):We probably need to keep output name if the considered Identity is a leaf node.
fumihwh(2018-04-18 02:02:30):@wschin You mean case that Identity is an output node right? Agree.
houseroad(2018-04-18 03:31:32):@fumihwh yeah, I mean, check the output is "X" in the graph.
houseroad(2018-04-18 04:08:56):For now, I think no need, because we didn't keep the output name in other passes either.

But it may be worth to create some helper functions to rename the inputs and outputs
AppVeyorBot(2018-04-13 23:17:13)::x: [Build onnx 0.3.2302 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2302) (commit https://github.com/onnx/onnx/commit/997f2509b4 by @houseroad)
AppVeyorBot(2018-04-13 23:39:03)::x: [Build onnx 0.3.2303 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2303) (commit https://github.com/onnx/onnx/commit/e0465b05e2 by @houseroad)
AppVeyorBot(2018-04-14 00:45:26)::x: [Build onnx 0.3.2306 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2306) (commit https://github.com/onnx/onnx/commit/8e2fe6d8d2 by @houseroad)
linkerzhang(2018-04-24 16:45:39):@houseroad , I noticed that the "tiles" and "axis" were designed as input before, and we're using "repeats" for doing tile in multiple dimension. However, question please. do we really need this kind of dynamic "repeats", which introduces performance hits in run-time, I believe.
houseroad(2018-04-24 23:05:21):@linkerzhang Yeah, making `repeats` as inputs can bring us the following benefits:
- Most existing frameworks (such as TensorFlow, Caffe2, PyTorch) take repeats as inputs. We should align with them, so the exporting will be much easier.
- Even in the static cases, we can have an optimization pass to preprocess the onnx graph while importing, so performance won't be a concern.
- We indeed have some dynamic `repeats` cases internally, which means args are not enough.

Above all, I think having `repeats` as inputs is better than making it args.
AppVeyorBot(2018-04-25 00:20:10)::white_check_mark: [Build onnx 0.3.2529 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2529) (commit https://github.com/onnx/onnx/commit/44944883be by @linkerzhang)
linkerzhang(2018-04-25 04:57:16):@houseroad  Thank you for the detail clarification. Looks good to me.
bddppq(2018-04-13 20:39:16):this is of type T
houseroad(2018-04-13 20:42:14):good catch
bddppq(2018-04-13 20:46:54):output should not have same shape as input
AppVeyorBot(2018-04-17 20:42:40)::x: [Build onnx 0.3.2340 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2340) (commit https://github.com/onnx/onnx/commit/0f85ee1844 by @anderspapitto)
AppVeyorBot(2018-04-17 21:54:54)::x: [Build onnx 0.3.2343 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2343) (commit https://github.com/onnx/onnx/commit/8c4746c95e by @anderspapitto)
AppVeyorBot(2018-04-18 00:30:59)::x: [Build onnx 0.3.2349 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2349) (commit https://github.com/onnx/onnx/commit/04b7152dfc by @anderspapitto)
AppVeyorBot(2018-04-18 05:57:47)::x: [Build onnx 0.3.2360 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2360) (commit https://github.com/onnx/onnx/commit/0066ba0471 by @bddppq)
bddppq(2018-04-13 21:15:14):unsqueeze
bddppq(2018-04-13 21:25:58):This requires `axes` to be sorted. (which is unspecified in the spec)
bddppq(2018-04-13 21:28:13):`axes` is required attribute
anderspapitto(2018-04-13 22:12:32):++
anderspapitto(2018-04-13 22:12:47):++
anderspapitto(2018-04-13 22:13:48):so - would you like me to throw an exception, or just assume it succeeds and remove the check? or leave it as is?
houseroad(2018-04-13 22:15:02):we should have assertion here, but I know assertion will cause Appveyor hanging...
houseroad(2018-04-13 22:15:45):throw an exception should be fine, because model is incorrect.
houseroad(2018-04-13 22:25:43):yes, please throw an exception, or use this code: https://github.com/onnx/onnx/blob/master/onnx/common/assertions.h

ONNX_ASSERT is actually an exception, not a real assert
gramalingam(2018-04-14 03:42:24):IMHO, just returning is fine too. I think checking the model for correctness is not the job of shape-inference. It is intended for optimization, and it is fine if doesn't infer the output shape under some conditions. 
bddppq(2018-04-18 05:40:02):exactly because "checking the model for correctness is not the job of shape-inference", here should throw instead of silently return.
AppVeyorBot(2018-04-14 01:08:27)::white_check_mark: [Build onnx 0.3.2307 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2307) (commit https://github.com/onnx/onnx/commit/3ea33cf569 by @linkerzhang)
AppVeyorBot(2018-04-14 01:42:27)::x: [Build onnx 0.3.2309 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2309) (commit https://github.com/onnx/onnx/commit/3f63918a53 by @linkerzhang)
AppVeyorBot(2018-04-14 02:34:07)::x: [Build onnx 0.3.2312 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2312) (commit https://github.com/onnx/onnx/commit/4911217bc4 by @linkerzhang)
AppVeyorBot(2018-04-14 04:36:21)::x: [Build onnx 0.3.2313 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2313) (commit https://github.com/onnx/onnx/commit/77f963ba95 by @linkerzhang)
AppVeyorBot(2018-04-14 05:52:53)::x: [Build onnx 0.3.2314 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2314) (commit https://github.com/onnx/onnx/commit/05fb36b58f by @linkerzhang)
AppVeyorBot(2018-04-14 07:05:00)::x: [Build onnx 0.3.2315 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2315) (commit https://github.com/onnx/onnx/commit/ba8414d709 by @linkerzhang)
AppVeyorBot(2018-04-17 20:18:50)::white_check_mark: [Build onnx 0.3.2339 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2339) (commit https://github.com/onnx/onnx/commit/e319ee427d by @linkerzhang)
AppVeyorBot(2018-04-20 21:28:33)::white_check_mark: [Build onnx 0.3.2445 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2445) (commit https://github.com/onnx/onnx/commit/a9445e10ad by @linkerzhang)
AppVeyorBot(2018-04-21 19:11:48)::white_check_mark: [Build onnx 0.3.2465 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2465) (commit https://github.com/onnx/onnx/commit/f297a9d308 by @linkerzhang)
raymondxyang(2018-04-30 20:22:14):Will reopen with changes tested
raymondxyang(2018-05-02 01:30:40):I am gonna separate the projects (in CMake) later. We may need the tests running first for other developers to contribute test cases first
AppVeyorBot(2018-05-02 01:39:42)::x: [Build onnx 0.3.2746 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2746) (commit https://github.com/onnx/onnx/commit/e6016f3da7 by @raymondxyang)
AppVeyorBot(2018-05-02 01:47:32)::x: [Build onnx 0.3.2747 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2747) (commit https://github.com/onnx/onnx/commit/e6016f3da7 by @linkerzhang)
AppVeyorBot(2018-05-02 03:12:57)::x: [Build onnx 0.3.2750 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2750) (commit https://github.com/onnx/onnx/commit/0a4a9bcabe by @raymondxyang)
AppVeyorBot(2018-05-02 04:09:08)::x: [Build onnx 0.3.2751 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2751) (commit https://github.com/onnx/onnx/commit/28707b0734 by @raymondxyang)
AppVeyorBot(2018-05-02 04:19:45)::white_check_mark: [Build onnx 0.3.2752 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2752) (commit https://github.com/onnx/onnx/commit/e14d6586a6 by @raymondxyang)
linkerzhang(2018-05-16 02:09:41):@bddppq this looks good to you now?
bddppq(2018-05-16 02:49:48):@linkerzhang I will take a look tomorrow
AppVeyorBot(2018-05-17 06:18:03)::white_check_mark: [Build onnx 0.3.3293 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.3293) (commit https://github.com/onnx/onnx/commit/064adbce98 by @linkerzhang)
linkerzhang(2018-05-17 23:52:23):@bddppq  I'm merging this in. Please add more comments if any and will fix them separately. Thank you very much!
bddppq(2018-04-16 18:27:24):why do we need to do an install and uninstall here?
bddppq(2018-04-16 22:01:04):what's the reason to suppress the exit code?
bddppq(2018-04-16 22:04:01):Add "ONNX" to the flag. Otherwise if onnx's cmake files get included by other frameworks's cmake build system, the parent scope variables will leak to onnx's.
bddppq(2018-04-16 22:05:17):IMO it's better to simply add googletest as a submodule in onnx
bddppq(2018-04-16 22:06:39):python build should not build anything that python side doesn't need (in this case cpp tests are not usable from python side).
raymondxyang(2018-04-17 01:05:00):So we need to do a insource develop install for building and run the unittest, then a regular install for other tests defined in script.sh
raymondxyang(2018-04-17 01:05:27):This one is unnecessary. Will remove it
raymondxyang(2018-04-17 01:06:34):Good point.. I ll change this to enabling UT in CI instead
bddppq(2018-04-17 02:08:15):pip doesn't seem to be the right way to trigger the build of c++ test binaries. How about we do it completely separated? I mean we first pip install python part, run python tests as before and then add new steps to directly invoke cmake to build the test binaries and run the tests.
bddppq(2018-05-02 07:45:09):This should be declared as an option (see line 12 and 14).
bddppq(2018-05-02 07:46:31):nit: I prefer to put these files under onnx/test/c++
bddppq(2018-05-02 07:47:36):nit: How about naming it `ONNX_BUILD_TESTS`?
bddppq(2018-05-02 07:50:30):why is this needed?
bddppq(2018-05-02 07:50:39):ditto
bddppq(2018-05-02 07:52:12):cmake boolean flags convention is using "ON" and "OFF"
bddppq(2018-05-02 07:53:30):nit: for simple existence checking, you can use `count`.
bddppq(2018-05-02 07:57:22):Can we add gtest as a submodule, instead of downloading it on the fly?
bddppq(2018-05-02 08:00:29):why do we need to find protobuf here again? I think we have already done this in the top level cmake file?
bddppq(2018-05-02 08:04:53):wait I think this is finding a list of header files, not "dirs". We have set ONNX_INCLUDE_DIRS, will it be enough by just adding it to the include dirs when compiling the test cc files?
bddppq(2018-05-02 08:08:23):I don't think this is needed (and also it's too broad, e.g. it's adding gtest include dir even when compiling non-test cc files). In unittest.cmake it already adds the include dirs by `target_include_directories(${_UT_TARGET} PUBLIC ${googletest_INCLUDE_DIRS} ${onnx_INCLUDE_DIRS} ${PROTOBUF_INCLUDE_DIRS})`
raymondxyang(2018-05-02 22:21:57):make sense for the naming.. I follow the naming for the protobuf_include_dirs, will change it

raymondxyang(2018-05-02 22:24:03):just saw the onnx_include_dirs.. will use that one
raymondxyang(2018-05-02 23:22:19):I agree that I put too many into it... I tested and found out the minimal set we need: root of binary and source is needed for import onnx headers in testcases and using pb.h files... 
raymondxyang(2018-05-02 23:23:15):This one is trying to pass the jenkins build (it failed before).. to be honest I dont know why it fails since I dont know its configuration. I removed the find protobuf and wait to see if it can pass it
raymondxyang(2018-05-02 23:23:47):I am planning to refactor the cmake (submodule, separate projects) together later
bddppq(2018-05-03 01:05:40):I checked the last failure, it should be irrelevant. I think the `if` should be removed.
bddppq(2018-05-03 01:07:39):`PROTOBUF_LIBRARY` and `PROTOBUF_LIBRARIES` are the same. It's just normally in cmake libs maintainer export all these similar names to the same values to protect typos when other people use it.
bddppq(2018-05-03 01:08:04):@raymondxyang Alright, pinky swear :-)
bddppq(2018-05-03 01:11:56):I guess you are doing this so the test binaries are generated in the source dir?
You can add `-b` to the `pip install` command in `script.sh` to point it to use a specified build path, so that later you can find and execute the test binaries.
bddppq(2018-05-03 01:12:08):I don't think this is needed. These binaries should be executable already.
bddppq(2018-05-03 01:12:55):The description needs to be adjusted.
bddppq(2018-05-03 01:13:56):Do we still need to add these include dirs at the top level? You are already doing `target_include_directories` in unittest.cmake.
bddppq(2018-05-03 01:14:21):it should be ONNX_INCLUDE_DIRS.
raymondxyang(2018-05-03 01:28:55):I ll re-test it later.. I tried set TMPDIR and --noclean somehow its not working.. I didnot remmeber if I used -b before
raymondxyang(2018-05-03 01:29:06):for chmod?

raymondxyang(2018-05-03 01:29:19):yes.. that's the minimal set working on windows
bddppq(2018-05-03 02:21:35):yes
bddppq(2018-05-03 02:22:14):@raymondxyang hmm looks weird to me, does it mean `target_include_directories` doesn't work? 
raymondxyang(2018-05-03 21:04:20):haha not so familiar with the linux rule so always 777
raymondxyang(2018-05-03 21:09:41):It just came to me that if we treat it as external project, we only download and build it on the fly when user activate the option. If its a sub-module we'll always have it around.. 
raymondxyang(2018-05-03 21:15:14):sorry i misunderstand ur point.. moved the include dirs from top level to target_include_directories 
raymondxyang(2018-05-04 17:53:01):Just tested the `-b` is not working.. only protobuf is retained in the given directory.. dont know the reason but log can be found https://api.travis-ci.org/v3/job/374997390/log.txt, command used is `pip install -b ${PWD}/.setuptools-cmake-build/ -v .`
snnn(2018-05-11 03:27:06):Another solution maybe: checkin googletests directly at here. The whole project can be packed into only 2 files: one header file and one cc file. It's simpler to manage.
snnn(2018-05-11 03:27:27):whole_archive flag on onnx_proto is not needed?
snnn(2018-05-11 03:28:18):Why /MT?
snnn(2018-05-11 03:28:40):what's the logic of "else"?  They still need protobuf right?
AppVeyorBot(2018-04-17 02:09:26)::x: [Build onnx 0.3.2332 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2332) (commit https://github.com/onnx/onnx/commit/1b1d0917c0 by @raymondxyang)
AppVeyorBot(2018-04-17 03:21:11)::white_check_mark: [Build onnx 0.3.2333 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2333) (commit https://github.com/onnx/onnx/commit/25e7f8bd78 by @raymondxyang)
AppVeyorBot(2018-04-17 19:38:55)::x: [Build onnx 0.3.2338 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2338) (commit https://github.com/onnx/onnx/commit/628c81f860 by @linkerzhang)
AppVeyorBot(2018-04-17 22:28:58)::white_check_mark: [Build onnx 0.3.2344 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2344) (commit https://github.com/onnx/onnx/commit/d2d558f575 by @linkerzhang)
bddppq(2018-04-16 21:50:41):Need to change the python interface to expose this new function signature https://github.com/onnx/onnx/blob/master/onnx/defs/__init__.py#L13.
Actually I think it's better to just do

```
has_schema = C.has_schema
has = has_schema  # for backward compatibility
```


bddppq(2018-04-16 21:56:14):pybind11 supports specifying default value for function argument, so no need to define twice for different function signatures.
raymondxyang(2018-04-17 01:03:12):I tried default value but pytest gave me errors 
```   
def test_typecheck(self):
>       defs.get_schema("Conv")
E       TypeError: get_schema(): incompatible function arguments. The following argument types are supported:
E           1. (arg0: str, arg1: int, arg2: str) -> onnx.onnx_cpp2py_export.defs.OpSchema
E
E       Invoked with: 'Conv'

onnx\test\schema_test.py:12: TypeError
```
raymondxyang(2018-04-17 01:03:38):updated
bddppq(2018-04-17 02:11:08):Have you annotated the default arguments like this: http://pybind11.readthedocs.io/en/stable/basics.html#default-arguments ?
fumihwh(2018-04-17 02:12:24):@raymondxyang 
You've removed default value of max_inclusive_version and get that error.
bddppq(2018-04-17 02:16:30):let's add the default argument in the pybind11 bindings
raymondxyang(2018-04-17 02:55:25):yes... I also tried none(true), not working as well... also I feel a little bit uncertain about what should the default values be, because we already have default ones defined in schema.h
raymondxyang(2018-04-17 02:58:44):Gonna revert this first until we figure out on which step(s) we need to define a default value.. currently we have: 1) py apis(some with default value) 2) pybind(no default value) 3) c apis(default value defined) 
fumihwh(2018-04-17 03:09:43):@raymondxyang 
Both 2 and 3 don't have default value I think.
And we should not add default value to schema.h because of there is another constructor which is using maxInclusiveVersion.
What we need to do I think is just add default value to pybind11 bindings as @bddppq said.
Maybe we can also add default value of domain.
bddppq(2018-04-17 03:30:47):the default value should match the one in c++
btw the error you got seems to be from get_schema, not has_schema
raymondxyang(2018-04-17 18:07:13):they have the same error so I copied one of it. 
bddppq(2018-04-17 18:16:16):ok as this is just implementation details that doesn't affect user interface, let's get this in first and we can revisit it later.
raymondxyang(2018-04-17 18:23:30):Agree.. I am having a discuss with ke dont have a efficient way to give lambda function default values. may revisit later  ~~and please do not merge it first~~
CLAassistant(2018-04-16 22:47:42):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=765) <br/>All committers have signed the CLA.
bddppq(2018-04-17 02:49:54):should simply remove line 122.
Could yu also add a test case to verify this?
wschin(2018-04-17 05:54:48):@bddppq , Sure. I will fix them. Thanks.
houseroad(2018-04-16 23:02:43):This does not look right to me
In `raw` branch, assert `not raw`?
bddppq(2018-04-17 22:14:45):I feel it's probably better to just add a new "type" argument to load and save (default to `ModelProto` for backward compatibility).
houseroad(2018-04-17 22:17:20):Yep, adding another parameter sounds better...

load and load_tensor are really confusing, let me refactor.
AppVeyorBot(2018-04-17 22:59:11)::x: [Build onnx 0.3.2345 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2345) (commit https://github.com/onnx/onnx/commit/4eb345a68d by @houseroad)
AppVeyorBot(2018-04-17 23:28:22)::x: [Build onnx 0.3.2346 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2346) (commit https://github.com/onnx/onnx/commit/ed1e36c51d by @houseroad)
AppVeyorBot(2018-04-18 00:12:56)::x: [Build onnx 0.3.2348 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2348) (commit https://github.com/onnx/onnx/commit/87df25f918 by @houseroad)
AppVeyorBot(2018-04-18 00:58:58)::x: [Build onnx 0.3.2350 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2350) (commit https://github.com/onnx/onnx/commit/0924ae586b by @houseroad)
bddppq(2018-04-18 23:27:29):I think it's better to not expose serialize and deserialize.
For serialize because proto objects already have a "serializeToString" method, and also for the new model format (that offloads tensors to separate files), we actually don't know what should be return.
For deserialize, I think same as load.
houseroad(2018-04-23 17:19:03):@dzhulgakov yes, totally agree. I will update the PR soon.
AppVeyorBot(2018-04-26 19:11:19)::x: [Build onnx 0.3.2615 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2615) (commit https://github.com/onnx/onnx/commit/be09e99cdb by @houseroad)
houseroad(2018-04-27 06:16:57):@yinghai ping for review :-)
anderspapitto(2018-04-17 21:08:39):`model` is an unbound variable?
houseroad(2018-04-17 21:19:07):++
bddppq(2018-04-17 22:09:46):str
bddppq(2018-04-17 22:12:55):Doesn't seem more useful than `proto.SerializeToString`?
houseroad(2018-04-17 22:15:12):err... i ran the wrong scripts to test this ;-)
bddppq(2018-04-18 23:17:10):why not default to ModelProto?
bddppq(2018-04-18 23:19:20):nit: use "f" instead of `obj` to match `save`?
houseroad(2018-04-18 23:30:24):I feel bad to use any mutable object as default values
bddppq(2018-04-18 23:30:28):it's better not to smash these two cases (i.e. passing the proto class vs. passing the proto instance) into one function, they are pretty different (e.g one will modify the proto instance in-place, while in the other case it returns a new instance).
houseroad(2018-04-18 23:36:28):Yep, I was thinking whether we should support in-place deserialization. But it won't bring any benefits in this case. I will stick to class instead of instance.
yinghai(2018-04-25 04:22:30):I think you can just construct the object here and pass it to _deserialize instead of passing a class as an argument. 
anderspapitto(2018-04-17 22:31:36):[ do not review till https://github.com/onnx/onnx/pull/758 is pushed ]
AppVeyorBot(2018-04-17 23:46:26)::x: [Build onnx 0.3.2347 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2347) (commit https://github.com/onnx/onnx/commit/2024ec2297 by @anderspapitto)
anderspapitto(2018-04-18 18:22:48):ready to review. there is some whitespace changes, may be helpful to review them hidden  https://github.com/onnx/onnx/pull/771/files?w=1
AppVeyorBot(2018-04-19 22:56:50)::white_check_mark: [Build onnx 0.3.2404 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2404) (commit https://github.com/onnx/onnx/commit/6f2bea9df4 by @houseroad)
AppVeyorBot(2018-04-19 23:44:37)::white_check_mark: [Build onnx 0.3.2406 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2406) (commit https://github.com/onnx/onnx/commit/78bf35c51b by @anderspapitto)
AppVeyorBot(2018-04-20 15:50:32)::white_check_mark: [Build onnx 0.3.2434 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2434) (commit https://github.com/onnx/onnx/commit/ba4e124964 by @anderspapitto)
bddppq(2018-04-18 22:04:47):probably not worth adding a parameter "opname" just for error message (if you really want to show the opname, you can store the opname in the InferenceContext itself).
bddppq(2018-04-18 22:08:38):maybe assert inputIndex and outputIndex are not out of bound?
bddppq(2018-04-18 22:13:39):actually it might be better to assert inside getInputType and getOutputType
anderspapitto(2018-04-19 22:06:15):done, see implementation.h
bddppq(2018-04-19 23:07:41):use ONNX_NAMESPACE::to_string
AppVeyorBot(2018-04-18 01:12:07)::x: [Build onnx 0.3.2351 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2351) (commit https://github.com/onnx/onnx/commit/dd60679546 by @anderspapitto)
AppVeyorBot(2018-04-18 06:10:07)::x: [Build onnx 0.3.2361 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2361) (commit https://github.com/onnx/onnx/commit/d6c177ec0e by @anderspapitto)
AppVeyorBot(2018-04-19 23:56:47)::x: [Build onnx 0.3.2407 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2407) (commit https://github.com/onnx/onnx/commit/a40b0ff5ab by @anderspapitto)
AppVeyorBot(2018-04-20 17:37:11)::x: [Build onnx 0.3.2440 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2440) (commit https://github.com/onnx/onnx/commit/8ab05115e1 by @anderspapitto)
AppVeyorBot(2018-04-20 21:41:57)::x: [Build onnx 0.3.2448 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2448) (commit https://github.com/onnx/onnx/commit/31b287b016 by @anderspapitto)
anderspapitto(2018-04-20 22:25:57):onnx-fb-universe failure is a spurious network error
AppVeyorBot(2018-04-20 22:57:31)::x: [Build onnx 0.3.2452 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2452) (commit https://github.com/onnx/onnx/commit/da5a70201a by @anderspapitto)
AppVeyorBot(2018-04-23 19:47:55)::x: [Build onnx 0.3.2484 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2484) (commit https://github.com/onnx/onnx/commit/9cc4ee3144 by @anderspapitto)
AppVeyorBot(2018-04-23 20:35:09)::x: [Build onnx 0.3.2486 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2486) (commit https://github.com/onnx/onnx/commit/4f07535c47 by @anderspapitto)
gramalingam(2018-04-23 20:42:38):I have a high-level question/comment: how do we represent a completely unknown shape (of unknown rank)? If we restrict ourselves to the current protobuf definition, then one solution is to have the shape field of TypeProto not be set (so that has_shape() is false). Is there any other alternative?

If we go with the above convention, then this PR needs to be updated accordingly ... (a) don't assume that an input has_shape() ... check if it has_shape() and treat it as an unknown rank otherwise, (b) don't set the output's shape unless it's rank is known.
anderspapitto(2018-04-23 21:02:48):@gramalingam yes, you're correct - it's fully determined by has_shape(). We in fact do check has_shape() and treat it as unknown rank if there is no shape - see the implementation and usage of `hasNInputShapes()`
gramalingam(2018-04-23 21:19:08):Ok, that sounds good. Didn't notice "hasNInputShapes()", just thought it was "hasNInputTypes()".
AppVeyorBot(2018-04-23 21:23:13)::x: [Build onnx 0.3.2487 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2487) (commit https://github.com/onnx/onnx/commit/8e3e583892 by @anderspapitto)
anderspapitto(2018-04-23 22:46:13):It was hasNInputTypes() a few commits ago, I updated it at some point to make this handling better :)
AppVeyorBot(2018-04-23 22:57:54)::x: [Build onnx 0.3.2490 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2490) (commit https://github.com/onnx/onnx/commit/8e58153dc3 by @anderspapitto)
AppVeyorBot(2018-04-24 18:41:34)::x: [Build onnx 0.3.2507 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2507) (commit https://github.com/onnx/onnx/commit/98ede73ecf by @anderspapitto)
AppVeyorBot(2018-04-24 22:16:09)::white_check_mark: [Build onnx 0.3.2520 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2520) (commit https://github.com/onnx/onnx/commit/bee028f2e8 by @anderspapitto)
AppVeyorBot(2018-04-25 20:22:05)::x: [Build onnx 0.3.2564 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2564) (commit https://github.com/onnx/onnx/commit/55bba4f3e7 by @anderspapitto)
AppVeyorBot(2018-04-25 20:53:49)::x: [Build onnx 0.3.2566 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2566) (commit https://github.com/onnx/onnx/commit/58fd12e657 by @anderspapitto)
AppVeyorBot(2018-04-26 21:11:52)::white_check_mark: [Build onnx 0.3.2618 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2618) (commit https://github.com/onnx/onnx/commit/46b213ba1a by @anderspapitto)
AppVeyorBot(2018-04-26 22:47:27)::x: [Build onnx 0.3.2624 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2624) (commit https://github.com/onnx/onnx/commit/34e3667e57 by @anderspapitto)
AppVeyorBot(2018-04-26 23:16:49)::x: [Build onnx 0.3.2626 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2626) (commit https://github.com/onnx/onnx/commit/f96a660595 by @anderspapitto)
houseroad(2018-04-27 00:15:54):@anderspapitto In general, it looks good to me, but some shape_inference test failed. Could you take a look?
gramalingam(2018-04-23 20:29:53):This doesn't seem correct unless the pads value is zero. In particular, if input dim is "N" and pads is non-zero, we shouldn't set output dim to be "N"
anderspapitto(2018-04-23 21:01:04):good catch
houseroad(2018-04-24 05:33:27):Thanks for fixing this.
houseroad(2018-04-24 05:35:42):TODO?
houseroad(2018-04-24 05:37:20):else add some assertion here?
houseroad(2018-04-24 05:48:06):Dumb question: if ctx.getInputType(0) does not have shape info, what will happen?
houseroad(2018-04-24 05:49:19):Could you add the formula here: how do you calculate the output shape... 
houseroad(2018-04-24 06:04:43):The formula here seems incorrect:

For example: pad=0, dilation=1, input=5, kernel=5, stride=2, you will get output=0

I think right formula should be:
Output[i+2] = (Input[i+2] + padding[i] + padding[i+len(kernel_shape)] - dilations[i] *( kernel_shape[i] - 1) - 1) / stride[i] + 1
houseroad(2018-04-24 06:08:41):To answer my own question: I think we depends on hasNInputShapes.
houseroad(2018-04-24 06:09:32):`!hasNInputShapes(ctx, 2)`, we also depends on weights' shape
anderspapitto(2018-04-24 22:19:20):it's in the inline comments below
anderspapitto(2018-04-24 22:19:24):yes
anderspapitto(2018-04-24 22:19:55):not sure what you want me to assert - either kernel_shape is provided explicitly, or else we infer it from the input shape
anderspapitto(2018-04-24 22:20:00):++
houseroad(2018-04-25 18:04:58):Assert kernel_shape == dim_weight, since they are duplicate.
houseroad(2018-04-25 18:20:07):I think it's better to tell function `getRepeatedAttribute` how many elements it should expect from the onnx attributes. Right now, the checker didn't check whether how many values each attribute contains.

Or you can add an assertion to check how many elements dilations contain.

houseroad(2018-04-25 18:21:21):Because, here you use the value without checking the length of `pads`...
anderspapitto(2018-04-25 18:45:55):++ I'll add a check here
anderspapitto(2018-04-25 18:46:08):++ will also add a length check for `pads`
houseroad(2018-04-25 22:51:38):Do you want to sort the axes?
houseroad(2018-04-25 23:07:17):Similar here, we need to check `pads`'s sizes
houseroad(2018-04-26 22:21:35):len(pads) == 2 * dim_size?
linkerzhang(2018-04-18 14:38:26):Yep. You're right. Thank you @bddppq for correcting me!
AppVeyorBot(2018-04-18 06:56:39)::white_check_mark: [Build onnx 0.3.2362 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2362) (commit https://github.com/onnx/onnx/commit/a233524dad by @bddppq)
bddppq(2018-04-18 07:47:02):@linkerzhang @prasanthpul Seems like @raymondxyang doesn't write permission to the repo?
AppVeyorBot(2018-04-18 21:48:41)::white_check_mark: [Build onnx 0.3.2373 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2373) (commit https://github.com/onnx/onnx/commit/c6d2d0cbef by @linkerzhang)
AppVeyorBot(2018-04-19 00:39:42)::white_check_mark: [Build onnx 0.3.2380 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2380) (commit https://github.com/onnx/onnx/commit/5fa7143ebb by @smessmer)
AppVeyorBot(2018-04-19 02:22:22)::white_check_mark: [Build onnx 0.3.2385 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2385) (commit https://github.com/onnx/onnx/commit/0588a526a5 by @houseroad)
bddppq(2018-05-13 04:56:35):Doesn't seem to work, closing. Feel free to reopen if we still need this.
bddppq(2018-04-18 23:53:18):Could you only clang-format the changed parts? :-) It's pretty hard to review.
AppVeyorBot(2018-04-19 02:00:59)::x: [Build onnx 0.3.2384 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2384) (commit https://github.com/onnx/onnx/commit/0d5d98a1e8 by @linkerzhang)
AppVeyorBot(2018-04-19 04:44:20)::x: [Build onnx 0.3.2389 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2389) (commit https://github.com/onnx/onnx/commit/f688feb251 by @linkerzhang)
bddppq(2018-04-20 00:16:14):@anderspapitto Could you take a look?
AppVeyorBot(2018-04-20 01:08:00)::x: [Build onnx 0.3.2412 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2412) (commit https://github.com/onnx/onnx/commit/65b404c305 by @linkerzhang)
AppVeyorBot(2018-04-20 06:54:17)::white_check_mark: [Build onnx 0.3.2430 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2430) (commit https://github.com/onnx/onnx/commit/651a182bd0 by @linkerzhang)
AppVeyorBot(2018-04-20 19:33:35)::white_check_mark: [Build onnx 0.3.2442 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2442) (commit https://github.com/onnx/onnx/commit/3e74ce3209 by @linkerzhang)
gramalingam(2018-04-18 23:33:26):Change name to "propagateTypeFromInputToOutput"?
bddppq(2018-04-19 02:56:12):what if `iter == valueTypesByName.end()`?
linkerzhang(2018-04-19 21:24:53):I found that what it was supposed to do is really coping element type instead of whole type proto.
AppVeyorBot(2018-04-19 02:35:11)::x: [Build onnx 0.3.2386 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2386) (commit https://github.com/onnx/onnx/commit/f9f639443c by @bddppq)
AppVeyorBot(2018-04-19 04:57:28)::x: [Build onnx 0.3.2390 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2390) (commit https://github.com/onnx/onnx/commit/a16f1ecc84 by @bddppq)
AppVeyorBot(2018-04-19 02:56:00)::white_check_mark: [Build onnx 0.3.2387 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2387) (commit https://github.com/onnx/onnx/commit/4fec2d4b7b by @wschin)
linkerzhang(2018-04-19 22:18:35):@houseroad , looks like some of the tests are not following the fact that opset_import should not be there if ir_version <= 2.  Could you help on it please?
AppVeyorBot(2018-04-19 22:31:27)::x: [Build onnx 0.3.2402 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2402) (commit https://github.com/onnx/onnx/commit/bfc1669fef by @wschin)
AppVeyorBot(2018-04-19 23:21:40)::x: [Build onnx 0.3.2405 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2405) (commit https://github.com/onnx/onnx/commit/a101afba21 by @wschin)
houseroad(2018-04-20 00:21:23):https://github.com/onnx/onnx/pull/788 should solve the problem
AppVeyorBot(2018-04-20 03:17:13)::x: [Build onnx 0.3.2418 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2418) (commit https://github.com/onnx/onnx/commit/897f159ca2 by @linkerzhang)
AppVeyorBot(2018-04-20 05:43:18)::x: [Build onnx 0.3.2427 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2427) (commit https://github.com/onnx/onnx/commit/f95c64fec7 by @houseroad)
AppVeyorBot(2018-04-20 06:31:54)::x: [Build onnx 0.3.2429 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2429) (commit https://github.com/onnx/onnx/commit/1f2d3ac5f6 by @bddppq)
AppVeyorBot(2018-04-20 13:15:44)::x: [Build onnx 0.3.2433 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2433) (commit https://github.com/onnx/onnx/commit/af7fef5b47 by @bddppq)
linkerzhang(2018-04-20 14:24:05):@houseroad looks like the tests still fail :(.
houseroad(2018-04-20 16:17:36):@linkerzhang Some test cases were not updated. Checking the reason
AppVeyorBot(2018-04-20 16:54:05)::x: [Build onnx 0.3.2437 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2437) (commit https://github.com/onnx/onnx/commit/9ad9ab73ba by @bddppq)
AppVeyorBot(2018-04-20 19:06:21)::x: [Build onnx 0.3.2441 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2441) (commit https://github.com/onnx/onnx/commit/c826f80c41 by @linkerzhang)
AppVeyorBot(2018-04-20 20:59:34)::x: [Build onnx 0.3.2444 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2444) (commit https://github.com/onnx/onnx/commit/ba96ba9bab by @linkerzhang)
linkerzhang(2018-04-19 01:55:45):we should add #ifdef ONNX_ML and add cases of map/sequence in-place.
bddppq(2018-04-19 02:45:35):`.empty()`
houseroad(2018-04-19 17:59:37):This doesn't seem right to me. Do you want to avoid the check when you have other domain?
wschin(2018-04-19 18:16:26):The following checks are not necessary if at least one domain exists because (1) opset_import is good with IR>=3 (2) default opset_imports should be set only if opset_import is empty.
houseroad(2018-04-19 19:00:45):I think the right check should be:
```
if (model.ir_version() >= 3) {
  // enforce opset_imports is not empty
} else {
  // enforce opset_imports is empty,
  // and set the default value for ONNX_DOMAIN
}
```
wschin(2018-04-19 20:56:43):Looks better.
linkerzhang(2018-04-19 21:22:11):else {
    fail_check(
           "model with IR version < 3 should have no opset_import specified for ONNX");
}
wschin(2018-04-19 21:49:37):Done. Thanks.
wschin(2018-04-19 22:16:19):It breaks a lot of torch tests.
smessmer(2018-04-19 18:55:02):CI says this works. No protoc-gen-mypy error message anymore.

I think we should land this. However, it means that we should encourage people to use pip. People not using pip will have to pre-install protobuf.
smessmer(2018-04-19 18:54:00):doesn't work, CI still shows the protoc-gen-mypy error message
AppVeyorBot(2018-04-20 01:29:11)::white_check_mark: [Build onnx 0.3.2413 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2413) (commit https://github.com/onnx/onnx/commit/c0fcd8287f by @bddppq)
AppVeyorBot(2018-04-20 03:37:24)::white_check_mark: [Build onnx 0.3.2419 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2419) (commit https://github.com/onnx/onnx/commit/7d0e7dff73 by @houseroad)
linkerzhang(2018-04-20 12:49:56):Ahahaha... Thank you very much! @bddppq @houseroad .
AppVeyorBot(2018-04-20 23:20:54)::white_check_mark: [Build onnx 0.3.2453 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2453) (commit https://github.com/onnx/onnx/commit/b6585b0fc9 by @houseroad)
yinghai(2018-04-20 23:19:07):Can we remove this name? 
hlu1(2018-04-20 23:20:31):Sure
Yangqing(2018-04-20 23:37:58):Only 1 place used this string :)

https://github.com/onnx/onnx/search?utf8=%E2%9C%93&q=zhangke&type=
buehrer(2018-04-23 15:44:18):Looks good to me. Thanks!
NiklasGustafsson(2018-04-23 16:02:48):This seems to be a big-enough change to warrant bumping the IR version, right?
linkerzhang(2018-04-23 21:11:29):@NiklasGustafsson , we should bump the IR version when release. Same idea as we're now bumping op_set version together with onnx release.
AppVeyorBot(2018-04-23 21:59:46)::white_check_mark: [Build onnx 0.3.2488 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2488) (commit https://github.com/onnx/onnx/commit/c4de068cf8 by @linkerzhang)
linkerzhang(2018-04-24 20:38:16):@ezyang please kindly review this change and add your comments if any. Thank you very much!
linkerzhang(2018-04-25 04:41:09):@ezyang Thank you very much for reviewing this! For checker logic, yes, they're not substantive, for the function cases. will add that with function registration code in-place.
AppVeyorBot(2018-04-25 19:03:31)::x: [Build onnx 0.3.2559 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2559) (commit https://github.com/onnx/onnx/commit/62beaf9a13 by @linkerzhang)
NiklasGustafsson(2018-04-23 16:05:04):If we can derive the types of inputs and outputs through inference, why can't we do the same for attributes?
linkerzhang(2018-04-23 17:36:37):Good question. We can. so that the attribute here will also be just "names", which will be referred by node inside the body.
ezyang(2018-04-25 03:29:22):Need to mention that this is not currently valid in ModelProto
linkerzhang(2018-04-25 04:33:13):Sure.
AppVeyorBot(2018-04-22 05:15:22)::x: [Build onnx 0.3.2477 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2477) (commit https://github.com/onnx/onnx/commit/f3a26a9f50 by @jamesr66a)
anderspapitto(2018-04-23 18:28:23):yeah, overall I'm fine with this. We're not really committing hard to anything; if we come up with a cleaner place to put this logic down the line (e.g. directly in the caffe2 backend.py) we can switch pretty easily.
anderspapitto(2018-04-23 18:29:55):I think it would be slightly better to put the unresolved references directly inside your __new_attribute, rather than as extra inputs - it would upset the checker less
AppVeyorBot(2018-04-23 20:20:08)::white_check_mark: [Build onnx 0.3.2485 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2485) (commit https://github.com/onnx/onnx/commit/4608c725c1 by @jamesr66a)
anderspapitto(2018-04-23 18:11:28):since this isn't a public api and has like 1 or 2 callsites, i'd slightly prefer to not have the parameter defaulted.
anderspapitto(2018-04-23 18:14:54):```
if (...) return true;
return false;
```
->

```
return (...);
```
houseroad(2018-04-23 18:31:27):Make checking unhappy does not look good. Let's store the input names in the attributes then :-)
houseroad(2018-04-23 19:06:36):Nit: One example will be very helpful here:
what's before, and what's after the optimization
jamesr66a(2018-04-23 20:20:33):@ebarsoum @lupesko does this look okay?
dzhulgakov(2018-04-24 07:16:58):nit, add comma or space
AppVeyorBot(2018-04-23 23:36:18)::white_check_mark: [Build onnx 0.3.2492 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2492) (commit https://github.com/onnx/onnx/commit/6a7e708d86 by @houseroad)
fumihwh(2018-04-24 01:07:06):@houseroad 
We should also update caffe2's doc.
https://github.com/pytorch/pytorch/blob/master/caffe2/operators/filler_op.cc#L43

houseroad(2018-04-24 03:55:29):@fumihwh yeah, feel free to update the doc there :-)
fumihwh(2018-04-24 09:53:35):@linkerzhang 
Great works!
So this PR defines conv and matmul for quantization data types as you said in 2.
input and weight are assumed to int, right?
If true, will the op, called A_Quantizer that processes them from float to int be defined in the future?
linkerzhang(2018-04-24 15:49:26):@fumihwh Great question! 

"If true, will the op, called A_Quantizer that processes them from float to int be defined in the future?"

Yes, it's true. that's why in this PR the op called as "*_Integer".  The op, called A_Quantizer that quantize a float to int, may or may not be defined as part of ONNX. Common quantizer ops may be defined in onnx as a Function #802 . Cusotmized quantizer ops should be owned by customers as custom function (sub-graph).

sergeigofman(2018-04-25 06:52:28):@linkerzhang Hi Ke,
Thanks for pushing this.
A couple of questions/opens:
1.	You have defined Conv_Integer and MatMul_integer operators that accept input and weight. In order to carry out the operation, need to know that zero_points of these tensors. Where is this information contained? Is it a meta-data of the tensor itself? If yes, can you point me where is it defined in the schema?
2.	It would be beneficial that the Bias for those operators would be int32, because we would like to merely add it to the accumulator.
liwchang(2018-04-25 18:06:10):@sergeigofman 
There is a "Z" in Conv_integer containing zero_point information of X for padding.
Z is a tensor, not an attribute, since it MIGHT be computed during runtime.
For MatMul_integer itself, I don't think zero_point is necessary. 
(zero_points are needed for Dequanizing back float later, but MatMul itself just doesn't need it.)

For the Bias, we were thinking users can simply use Add after Conv_integer. 
Since ONNX only captures semantics, it is up to platforms to fuse them to their own implementation or not.


sergeigofman(2018-04-25 18:20:07):@liwchang I understand the bias part.
Regarding the zero_point - it is actually necessary to factor in the zero points of both input and weight to get the correct answer for the linear operators (MatMul, Conv). This is due to the reason that e.g. MatMul actually evaluates res_pixel = sum_common_dim( (x - zero_point_x) * (w - zero_point_weight)). Therefore, for different zero point settings of x and w you will get different answers. Hence, I think it is necessary to provide the information regarding parameters' zero points to these operators.
A very detailed explanation on the role of the zero_point(offset) in linear operators is located at [https://github.com/google/gemmlowp/blob/master/doc/low-precision.md#efficient-handling-of-offsets](https://github.com/google/gemmlowp/blob/master/doc/low-precision.md#efficient-handling-of-offsets) . It confirms my claim above.
liwchang(2018-04-25 18:43:26):@sergeigofman 
That doesn't counteract what I said. 
As you said there are different zero_point setting, 
that is the exact reason we exclude zero_points out of MatMul_Integer.
Since MatMul is linear, we should be used MatMul_Integer plus with other ONNX operators to define a quantized MalMul.

1. Regardless what zero_point definition you use, you can also transform it back to a linear equation. 
x_float = x_int * x_scale + x_base. 
w_float = w_int * w_scale + w_base.
Sum(x_float*x_float) = Sum(x_int * w_int * x_scale * w_scale ) + Sum(x_int * x_scale * w_base)
                                     +Sum ( w_int * w_scale * x_base) +Sum(x_base * w_base).
                                 = x_scale * w_scale * Sum(x_int * w_int) + x_scale * w_base * Sum(x_int)
                                   + w_scale * x_base * Sum (w_int) + x_base * w_base * Sum(1).

MatMul_Integer is used for the first term, which is Sum(x_int * w_int).
The rest should be covered by the existing ONNX operators.

2. If you use the following formula.
x_float = (x_int - x_zero) * x_scale.
 
2a. if x_zero is the same type as x_int.
You can  simply do Sub op before MatMul_Integer.

2b. If x_zero is the float type, 
you can linearize it to
x_float =  (x_int - x_zero) * x_scale  = x_int * x_scale - x_zero * x_scale = x_int * x_scale + x_base

I think using generic MalMul_Integer should be enough to cover all semantics of linear quantization in MatMul.








 

 




sergeigofman(2018-04-25 19:00:10):@liwchang Thanks for th clarifications. I better understand your point now. To your points.
1. That would work, but it will make a simple self-contained operator a sequence of multiple operators which might be not that easy identifiable by graph pattern matching for the HW that support these in a fused fashion. Android NN API has it all fused, with zero_point information. But in general what you propose will work.
By the way, in order to make it work with existing ONNX operators, Sum operator needs to be extended to work on 8/16 bit tensors to int32 accumulator to be consistent with this proposal,
2a. I cannot really do what you propose and stay within the same bit width, because,e.g. if the x is int8 and x_zero is int8, their difference will overflow 8 bits and then I won't be able to use int8 flavor of MatMul_Integer.
2b. I am not sure I understand your point here. In any case, my assumption is that both x and x_zero are of the same data type. Is this a wrong assumption? I think it has to be.
liwchang(2018-04-25 20:03:47):@sergeigofman 
1. We understand self-contained operator might lead to better performance in some specific architectures. However, I think quantization itself is not stable enough to be standardized at this moment. Hope you can understand it. If users really care about performance, as @linkerzhang mentioned, there is another proposed op called Function #802, which provides a hint to contain a sequence of multiple operators. That might help for some vendors.

2a. I will be surprised that it will cause overflow. 
When x_int is int8 and x_zero is also int8, (x_int - x_zero) using Sub op should stay in int8 range, if 0.0f is in the dynamic range of data or near it. If not, that simply means your dynamic range is far to 0.0f. In those cases, don't you think you pick a wrong quantization formula. Switch to the formula in 1, it should work well. If you really want to use formula 2a in this case, you can add a Cast op to promote it from int8 to int32 to avoid overflow. Then you can add Function to force self-containing if you have a concern of API mapping in your platform. Or you can put some normalization to your model to avoid it if you prefer a data scientist's method. :)

2b is the case that x_zero is float. There are tons of quantization methods. You never know what users can come up with. :) 





AppVeyorBot(2018-04-26 00:00:48)::x: [Build onnx 0.3.2580 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2580) (commit https://github.com/onnx/onnx/commit/f85949b6fd by @linkerzhang)
sergeigofman(2018-04-26 09:08:39):@liwchang @linkerzhang 
1. I think I get the “Functions” proposal (#802). It lets users define sequences of nodes that will operate on given inputs, an return resulting outputs. This indeed would allow wrapping the sequence that will be equivalent to a compound node that would need zero points I had in mind. Appreciate your confirmation.
Let’s look at the required sequence however:
(A - zpA) * (B – zpB) = A*B – zpA * SumColumns(B) – zpB * SumRows(A) + zpA * zpB * CommonDimension(A,B)
The first term would map to MatMul_Integer. The last term would map to broadcasted Add (Tensor,Scalar).
The mid terms should map to  broadcasted Mul(zpA, ReduceSum(B, axis=1) and Mul(zpB, ReduceSum(A, axis=0).
In order to be able to execute mid terms, we need to have the ReduceSum() operator to receive inputs of (int/uint)(8/16) and return int32 result. Can we add this extension as a part of this proposal?

2a. If you subtract zero_point from the quantized tensor, it will overflow for sure. Let me give you a concrete example. Let’s assume we have a tensor with real values from 0.0 to 63.75. When we quantize it to int8 with zero_point, the scale will be (63.75-0)/ (127 - -128) = 0.25 and zero_point will be -128.
0.0f is clearly in the dynamic range. Now if I take a positive int8 number from the quantized tensor, say, 3, and subtract the zero_point from it, I get 3 – (-128) = 131 which is not representable by int8. The only case this math will not overflow is when the zero_point is 0, which defeats the purpose of the zero_point. The suggested method of casting to int32 will not let me do GEMM in int8, which is where the efficiency comes from. Therefore, I am actually forced to use the decomposition in my (1).
2b. Your proposal already stipulates that zero_point must be of the same type as input tensor for the reasons of padding. This makes this kind of hypothetical, amn’t I wrong here?

Now to a different point. In order to be able to implement LinearQuantize “function” from existing operators, we need to be able to perform rounding. Currently, ONNX spec contains only the following rounding-related operators: Floor, Ceil. Using these operators, it is only possible to implement the following rounding policies: round-ties-up and round-ties-down (via Floor(Tensor + 0.5), Ceil(Tensor -0.5)). Are we OK to limit ourselves to those rounding methods, or do we want to introduce Round operator which will round ties away from zero, or NearbyInt that lets user chose their preferred rounding method? 
liwchang(2018-04-26 14:34:33):@sergeigofman  @linkerzhang 
1. ReduceSum was actually discussed. 
We tend to use ReduceSum(Cast(A, int32)), instead of introducing a new one.
There is a debate whether ReduceSum(Cast(A, int32)) can capture the semantics of loading A in int8, casting it to int32, then doing ReduceSum on it.

2a. If you reserve 1 bit, then it won't. :)
Or you saturate it?
Let's put it this way. 
There are three (or more?) ways to dodge overflow: promoting, reserving, saturating.
2ai. In promoting, you need to perform multiply in int16.
So what you want is loading in int8, multiply in int16, accumulate in int32.
You can use Cast to int16 to keep accuracy. 
2aii. In reserving, you can simply use Sub.
2aiii. In saturating, the question will become whether it can be easily supported in ONNX and that is disscusable. ( Cast(Clip(Sub(Cast))) vs introducing saturation-supported ops)
Let's know if you really use saturating and have a trouble to do that. 
BTW, @linkerzhang, this reminds us to check/add overflow behavior description in Add and Sub, and others.

2b. Yes, it is hypothetical. It is kind of weird using 2b rather than 1.
But we didn't limit users from doing it. MatMul_Integer also didn't have padding to limit users.
There is also no limit to only one quantization schema in one model. 

3. For the rounding, it is a good point.
C++ does have those functions. 
Though ONNX might have "If" op in future, it might be easier to introduce rounding.
Whether there is a need or we have to introduce it in this phase is discussable. 
Let's know if you really need it. 
AppVeyorBot(2018-04-26 16:56:32)::white_check_mark: [Build onnx 0.3.2609 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2609) (commit https://github.com/onnx/onnx/commit/b1e4d6f96d by @linkerzhang)
Maratyszcza(2018-04-26 18:09:41):@linkerzhang Thanks for putting up the proposal.

Is the expectation that `Conv_Integer` and `MatMul_Integer` produce exact result?
liwchang(2018-04-26 18:29:57):@Maratyszcza 
Do you mean when both matrices' values are all integer, MatMat_Integer will generate the same number you calculate manually?
If so, I think the answer is "yes" when the accumulator doesn't overflow.

I think @linkerzhang also put some description mentioning whether multiplication or accumulation can overflow in what condition to make sure every platform will generate identical results.
(Though I saw you have some comment on it.)

Maratyszcza(2018-04-30 18:00:51):Producing exact 32-bit output feels too restrictive:
- Models using `Conv_Integer` and `MatMul_Integer` wouldn't be able to run on FP16/FP32 hardware, due to inexact nature of results. Being able to run a quantized model on conventional floating-point hardware is a very desirable property.
- I know of an upcoming NN accelerator that operates on 8-bit inputs, accumulates to 32 bits, but doesn't support writing out full 32-bit results. Only quantized outputs can be stored.
liwchang(2018-04-30 18:57:13):@Maratyszcza 
Good points. I actually thought about them before.
- For the first one, I think some rounding error (in the final model output) might be acceptable (though it might be questionable that a hw only has f16/32 without int). Even in regular models, people often change algorithms, or execution order to gain performance. Those can also impact rounding error. Let's call it simulated MatMul_Integer. How large rounding error you think we should put? e-7 for f32? e-4 for f16?

- For the second one, you can think it is a MatMul_Integer with following  (Saturate + or some ops +) Cast. Don't forget ONNX is just capturing semantics. It doesn't tell you when to read from/store to dram. Output of an op can be intermediately stored in scratchpad/register/sram. You can use compiler analysis or Function #802 to achieve it. 

The key design here is that some hw/people use 32bit as output, some hw/people use 8bit after some requantization or saturation tricks, but the common ground is using 32bit as accumulator. 
That will make op interface to dirty. So we pick 32bit as "output" (which not implying store back to dram), and let 8bit output conversion tricks as subgraph followed by MatMul_Integer.



CLAassistant(2018-07-09 14:46:38):[![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/onnx/onnx?pullRequest=809) <br/>Thank you for your submission! We really appreciate it. Like many open source projects, we ask that you all sign our [Contributor License Agreement](https://cla-assistant.io/onnx/onnx?pullRequest=809) before we can accept your contribution.<br/>**1** out of **2** committers have signed the CLA.<br/><br/>:white_check_mark: linkerzhang<br/>:x: root<br/><hr/>**root** seems not to be a GitHub user. You need a GitHub account to be able to sign the CLA. If you have already a GitHub account, please [add the email address used for this commit to your account](https://help.github.com/articles/why-are-my-commits-linked-to-the-wrong-user/#commits-are-not-linked-to-any-user).<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/onnx/onnx?pullRequest=809) it.</sub>
fdwr(2018-10-11 04:32:49):Should redirect readers to PR #1219 now.
prasanthpul(2019-04-10 20:42:12):superseded by https://github.com/onnx/onnx/pull/1908
Maratyszcza(2018-04-26 17:55:51):What do you mean by `The accumulation may overflow if and only if in 32 bits.`?
Maratyszcza(2018-04-26 17:57:35):I'm not sure we need to support it for quantized convolution
Maratyszcza(2018-04-26 17:59:05):Is `Z` the bias?
Maratyszcza(2018-04-26 18:00:57):Typo: `float` -> `fixed-point` tensors
liwchang(2018-04-26 18:32:48):It probably means if it overflows, it only can overflow in 32 bit. (<- this is wrong)
Correct myself. It should probably be "accumulation overflows only in 32 bit".
liwchang(2018-04-26 18:36:13):No, Z is the padding value for Tensor X. 
You might need it for padding.
liwchang(2018-04-26 18:41:59):Hmm... I saw "DEPRECATION NOTE: auto_pad is only intended to support legacy uses, and for framework authors, one is explicitly encouraged to use explicit padding specified in the pads attribute." in Conv.

If we drop it in Conv, then we should also drop it here.
Two should be consistent.
linkerzhang(2018-04-27 02:47:43):Good catch! Will fix it.
linkerzhang(2018-04-27 02:49:56):Yes, it means it may overflow only in 32 bits, normally it should never overflow.
fdwr(2018-10-08 05:06:14):"float tensors" -> "integer tensors"
fdwr(2018-10-08 05:08:31):Although allowing the weights to be distinct types from the input does allow flexibility, it also yields 16 (4x4) different combinations to handle :/.
fumihwh(2018-04-24 09:53:35):@linkerzhang 
Great works!
So this PR defines conv and matmul for quantization data types as you said in 2.
input and weight are assumed to int, right?
If true, will the op, called A_Quantizer that processes them from float to int be defined in the future?
linkerzhang(2018-04-24 15:49:26):@fumihwh Great question! 

"If true, will the op, called A_Quantizer that processes them from float to int be defined in the future?"

Yes, it's true. that's why in this PR the op called as "*_Integer".  The op, called A_Quantizer that quantize a float to int, may or may not be defined as part of ONNX. Common quantizer ops may be defined in onnx as a Function #802 . Cusotmized quantizer ops should be owned by customers as custom function (sub-graph).

sergeigofman(2018-04-25 06:52:28):@linkerzhang Hi Ke,
Thanks for pushing this.
A couple of questions/opens:
1.	You have defined Conv_Integer and MatMul_integer operators that accept input and weight. In order to carry out the operation, need to know that zero_points of these tensors. Where is this information contained? Is it a meta-data of the tensor itself? If yes, can you point me where is it defined in the schema?
2.	It would be beneficial that the Bias for those operators would be int32, because we would like to merely add it to the accumulator.
liwchang(2018-04-25 18:06:10):@sergeigofman 
There is a "Z" in Conv_integer containing zero_point information of X for padding.
Z is a tensor, not an attribute, since it MIGHT be computed during runtime.
For MatMul_integer itself, I don't think zero_point is necessary. 
(zero_points are needed for Dequanizing back float later, but MatMul itself just doesn't need it.)

For the Bias, we were thinking users can simply use Add after Conv_integer. 
Since ONNX only captures semantics, it is up to platforms to fuse them to their own implementation or not.


sergeigofman(2018-04-25 18:20:07):@liwchang I understand the bias part.
Regarding the zero_point - it is actually necessary to factor in the zero points of both input and weight to get the correct answer for the linear operators (MatMul, Conv). This is due to the reason that e.g. MatMul actually evaluates res_pixel = sum_common_dim( (x - zero_point_x) * (w - zero_point_weight)). Therefore, for different zero point settings of x and w you will get different answers. Hence, I think it is necessary to provide the information regarding parameters' zero points to these operators.
A very detailed explanation on the role of the zero_point(offset) in linear operators is located at [https://github.com/google/gemmlowp/blob/master/doc/low-precision.md#efficient-handling-of-offsets](https://github.com/google/gemmlowp/blob/master/doc/low-precision.md#efficient-handling-of-offsets) . It confirms my claim above.
liwchang(2018-04-25 18:43:26):@sergeigofman 
That doesn't counteract what I said. 
As you said there are different zero_point setting, 
that is the exact reason we exclude zero_points out of MatMul_Integer.
Since MatMul is linear, we should be used MatMul_Integer plus with other ONNX operators to define a quantized MalMul.

1. Regardless what zero_point definition you use, you can also transform it back to a linear equation. 
x_float = x_int * x_scale + x_base. 
w_float = w_int * w_scale + w_base.
Sum(x_float*x_float) = Sum(x_int * w_int * x_scale * w_scale ) + Sum(x_int * x_scale * w_base)
                                     +Sum ( w_int * w_scale * x_base) +Sum(x_base * w_base).
                                 = x_scale * w_scale * Sum(x_int * w_int) + x_scale * w_base * Sum(x_int)
                                   + w_scale * x_base * Sum (w_int) + x_base * w_base * Sum(1).

MatMul_Integer is used for the first term, which is Sum(x_int * w_int).
The rest should be covered by the existing ONNX operators.

2. If you use the following formula.
x_float = (x_int - x_zero) * x_scale.
 
2a. if x_zero is the same type as x_int.
You can  simply do Sub op before MatMul_Integer.

2b. If x_zero is the float type, 
you can linearize it to
x_float =  (x_int - x_zero) * x_scale  = x_int * x_scale - x_zero * x_scale = x_int * x_scale + x_base

I think using generic MalMul_Integer should be enough to cover all semantics of linear quantization in MatMul.








 

 




sergeigofman(2018-04-25 19:00:10):@liwchang Thanks for th clarifications. I better understand your point now. To your points.
1. That would work, but it will make a simple self-contained operator a sequence of multiple operators which might be not that easy identifiable by graph pattern matching for the HW that support these in a fused fashion. Android NN API has it all fused, with zero_point information. But in general what you propose will work.
By the way, in order to make it work with existing ONNX operators, Sum operator needs to be extended to work on 8/16 bit tensors to int32 accumulator to be consistent with this proposal,
2a. I cannot really do what you propose and stay within the same bit width, because,e.g. if the x is int8 and x_zero is int8, their difference will overflow 8 bits and then I won't be able to use int8 flavor of MatMul_Integer.
2b. I am not sure I understand your point here. In any case, my assumption is that both x and x_zero are of the same data type. Is this a wrong assumption? I think it has to be.
liwchang(2018-04-25 20:03:47):@sergeigofman 
1. We understand self-contained operator might lead to better performance in some specific architectures. However, I think quantization itself is not stable enough to be standardized at this moment. Hope you can understand it. If users really care about performance, as @linkerzhang mentioned, there is another proposed op called Function #802, which provides a hint to contain a sequence of multiple operators. That might help for some vendors.

2a. I will be surprised that it will cause overflow. 
When x_int is int8 and x_zero is also int8, (x_int - x_zero) using Sub op should stay in int8 range, if 0.0f is in the dynamic range of data or near it. If not, that simply means your dynamic range is far to 0.0f. In those cases, don't you think you pick a wrong quantization formula. Switch to the formula in 1, it should work well. If you really want to use formula 2a in this case, you can add a Cast op to promote it from int8 to int32 to avoid overflow. Then you can add Function to force self-containing if you have a concern of API mapping in your platform. Or you can put some normalization to your model to avoid it if you prefer a data scientist's method. :)

2b is the case that x_zero is float. There are tons of quantization methods. You never know what users can come up with. :) 





AppVeyorBot(2018-04-26 00:00:48)::x: [Build onnx 0.3.2580 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2580) (commit https://github.com/onnx/onnx/commit/f85949b6fd by @linkerzhang)
sergeigofman(2018-04-26 09:08:39):@liwchang @linkerzhang 
1. I think I get the “Functions” proposal (#802). It lets users define sequences of nodes that will operate on given inputs, an return resulting outputs. This indeed would allow wrapping the sequence that will be equivalent to a compound node that would need zero points I had in mind. Appreciate your confirmation.
Let’s look at the required sequence however:
(A - zpA) * (B – zpB) = A*B – zpA * SumColumns(B) – zpB * SumRows(A) + zpA * zpB * CommonDimension(A,B)
The first term would map to MatMul_Integer. The last term would map to broadcasted Add (Tensor,Scalar).
The mid terms should map to  broadcasted Mul(zpA, ReduceSum(B, axis=1) and Mul(zpB, ReduceSum(A, axis=0).
In order to be able to execute mid terms, we need to have the ReduceSum() operator to receive inputs of (int/uint)(8/16) and return int32 result. Can we add this extension as a part of this proposal?

2a. If you subtract zero_point from the quantized tensor, it will overflow for sure. Let me give you a concrete example. Let’s assume we have a tensor with real values from 0.0 to 63.75. When we quantize it to int8 with zero_point, the scale will be (63.75-0)/ (127 - -128) = 0.25 and zero_point will be -128.
0.0f is clearly in the dynamic range. Now if I take a positive int8 number from the quantized tensor, say, 3, and subtract the zero_point from it, I get 3 – (-128) = 131 which is not representable by int8. The only case this math will not overflow is when the zero_point is 0, which defeats the purpose of the zero_point. The suggested method of casting to int32 will not let me do GEMM in int8, which is where the efficiency comes from. Therefore, I am actually forced to use the decomposition in my (1).
2b. Your proposal already stipulates that zero_point must be of the same type as input tensor for the reasons of padding. This makes this kind of hypothetical, amn’t I wrong here?

Now to a different point. In order to be able to implement LinearQuantize “function” from existing operators, we need to be able to perform rounding. Currently, ONNX spec contains only the following rounding-related operators: Floor, Ceil. Using these operators, it is only possible to implement the following rounding policies: round-ties-up and round-ties-down (via Floor(Tensor + 0.5), Ceil(Tensor -0.5)). Are we OK to limit ourselves to those rounding methods, or do we want to introduce Round operator which will round ties away from zero, or NearbyInt that lets user chose their preferred rounding method? 
liwchang(2018-04-26 14:34:33):@sergeigofman  @linkerzhang 
1. ReduceSum was actually discussed. 
We tend to use ReduceSum(Cast(A, int32)), instead of introducing a new one.
There is a debate whether ReduceSum(Cast(A, int32)) can capture the semantics of loading A in int8, casting it to int32, then doing ReduceSum on it.

2a. If you reserve 1 bit, then it won't. :)
Or you saturate it?
Let's put it this way. 
There are three (or more?) ways to dodge overflow: promoting, reserving, saturating.
2ai. In promoting, you need to perform multiply in int16.
So what you want is loading in int8, multiply in int16, accumulate in int32.
You can use Cast to int16 to keep accuracy. 
2aii. In reserving, you can simply use Sub.
2aiii. In saturating, the question will become whether it can be easily supported in ONNX and that is disscusable. ( Cast(Clip(Sub(Cast))) vs introducing saturation-supported ops)
Let's know if you really use saturating and have a trouble to do that. 
BTW, @linkerzhang, this reminds us to check/add overflow behavior description in Add and Sub, and others.

2b. Yes, it is hypothetical. It is kind of weird using 2b rather than 1.
But we didn't limit users from doing it. MatMul_Integer also didn't have padding to limit users.
There is also no limit to only one quantization schema in one model. 

3. For the rounding, it is a good point.
C++ does have those functions. 
Though ONNX might have "If" op in future, it might be easier to introduce rounding.
Whether there is a need or we have to introduce it in this phase is discussable. 
Let's know if you really need it. 
AppVeyorBot(2018-04-26 16:56:32)::white_check_mark: [Build onnx 0.3.2609 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2609) (commit https://github.com/onnx/onnx/commit/b1e4d6f96d by @linkerzhang)
Maratyszcza(2018-04-26 18:09:41):@linkerzhang Thanks for putting up the proposal.

Is the expectation that `Conv_Integer` and `MatMul_Integer` produce exact result?
liwchang(2018-04-26 18:29:57):@Maratyszcza 
Do you mean when both matrices' values are all integer, MatMat_Integer will generate the same number you calculate manually?
If so, I think the answer is "yes" when the accumulator doesn't overflow.

I think @linkerzhang also put some description mentioning whether multiplication or accumulation can overflow in what condition to make sure every platform will generate identical results.
(Though I saw you have some comment on it.)

Maratyszcza(2018-04-30 18:00:51):Producing exact 32-bit output feels too restrictive:
- Models using `Conv_Integer` and `MatMul_Integer` wouldn't be able to run on FP16/FP32 hardware, due to inexact nature of results. Being able to run a quantized model on conventional floating-point hardware is a very desirable property.
- I know of an upcoming NN accelerator that operates on 8-bit inputs, accumulates to 32 bits, but doesn't support writing out full 32-bit results. Only quantized outputs can be stored.
liwchang(2018-04-30 18:57:13):@Maratyszcza 
Good points. I actually thought about them before.
- For the first one, I think some rounding error (in the final model output) might be acceptable (though it might be questionable that a hw only has f16/32 without int). Even in regular models, people often change algorithms, or execution order to gain performance. Those can also impact rounding error. Let's call it simulated MatMul_Integer. How large rounding error you think we should put? e-7 for f32? e-4 for f16?

- For the second one, you can think it is a MatMul_Integer with following  (Saturate + or some ops +) Cast. Don't forget ONNX is just capturing semantics. It doesn't tell you when to read from/store to dram. Output of an op can be intermediately stored in scratchpad/register/sram. You can use compiler analysis or Function #802 to achieve it. 

The key design here is that some hw/people use 32bit as output, some hw/people use 8bit after some requantization or saturation tricks, but the common ground is using 32bit as accumulator. 
That will make op interface to dirty. So we pick 32bit as "output" (which not implying store back to dram), and let 8bit output conversion tricks as subgraph followed by MatMul_Integer.



CLAassistant(2018-07-09 14:46:38):[![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/onnx/onnx?pullRequest=809) <br/>Thank you for your submission! We really appreciate it. Like many open source projects, we ask that you all sign our [Contributor License Agreement](https://cla-assistant.io/onnx/onnx?pullRequest=809) before we can accept your contribution.<br/>**1** out of **2** committers have signed the CLA.<br/><br/>:white_check_mark: linkerzhang<br/>:x: root<br/><hr/>**root** seems not to be a GitHub user. You need a GitHub account to be able to sign the CLA. If you have already a GitHub account, please [add the email address used for this commit to your account](https://help.github.com/articles/why-are-my-commits-linked-to-the-wrong-user/#commits-are-not-linked-to-any-user).<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/onnx/onnx?pullRequest=809) it.</sub>
fdwr(2018-10-11 04:32:49):Should redirect readers to PR #1219 now.
prasanthpul(2019-04-10 20:42:12):superseded by https://github.com/onnx/onnx/pull/1908
Maratyszcza(2018-04-26 17:55:51):What do you mean by `The accumulation may overflow if and only if in 32 bits.`?
Maratyszcza(2018-04-26 17:57:35):I'm not sure we need to support it for quantized convolution
Maratyszcza(2018-04-26 17:59:05):Is `Z` the bias?
Maratyszcza(2018-04-26 18:00:57):Typo: `float` -> `fixed-point` tensors
liwchang(2018-04-26 18:32:48):It probably means if it overflows, it only can overflow in 32 bit. (<- this is wrong)
Correct myself. It should probably be "accumulation overflows only in 32 bit".
liwchang(2018-04-26 18:36:13):No, Z is the padding value for Tensor X. 
You might need it for padding.
liwchang(2018-04-26 18:41:59):Hmm... I saw "DEPRECATION NOTE: auto_pad is only intended to support legacy uses, and for framework authors, one is explicitly encouraged to use explicit padding specified in the pads attribute." in Conv.

If we drop it in Conv, then we should also drop it here.
Two should be consistent.
linkerzhang(2018-04-27 02:47:43):Good catch! Will fix it.
linkerzhang(2018-04-27 02:49:56):Yes, it means it may overflow only in 32 bits, normally it should never overflow.
fdwr(2018-10-08 05:06:14):"float tensors" -> "integer tensors"
fdwr(2018-10-08 05:08:31):Although allowing the weights to be distinct types from the input does allow flexibility, it also yields 16 (4x4) different combinations to handle :/.
CLAassistant(2018-04-24 01:52:28):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=810) <br/>All committers have signed the CLA.
linkerzhang(2018-04-24 01:57:06):#757  also fixed this issue. Let's merge this two.
houseroad(2018-04-25 18:13:22):Fixed in https://github.com/onnx/onnx/pull/757
CLAassistant(2018-04-24 02:50:12):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=812) <br/>All committers have signed the CLA.
linkerzhang(2018-04-24 04:30:32):@kit1980 please sign the CLA accordingly. Thank you!
houseroad(2018-04-24 17:54:40):@onnxbot retest this please
AppVeyorBot(2018-04-26 03:21:53)::x: [Build onnx 0.3.2593 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2593) (commit https://github.com/onnx/onnx/commit/d4432924d3 by @linkerzhang)
snnn(2018-04-27 06:54:42):Ideally, we should not use CMAKE_BUILD_TYPE in cmake files. We shouldn't force user choose a CMAKE_BUILD_TYPE while invoking cmake command to generate VC projects files on Windows
AppVeyorBot(2018-04-24 18:04:17)::x: [Build onnx 0.3.2506 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2506) (commit https://github.com/onnx/onnx/commit/6ea5acba5c by @houseroad)
CLAassistant(2018-04-24 18:30:42):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=817) <br/>All committers have signed the CLA.
AppVeyorBot(2018-04-24 18:59:17)::white_check_mark: [Build onnx 0.3.2508 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2508) (commit https://github.com/onnx/onnx/commit/36c628ddfd by @zdevito)
zdevito(2018-04-25 18:42:41):Closing in favor of doing this transform before export from PyTorch. 
jamesr66a(2018-04-24 22:31:43):Can you use kCaptured here?
jamesr66a(2018-04-24 22:41:11):Can you just use `DescendOnGraphAttributes` here?

https://github.com/onnx/onnx/blob/master/onnx/optimizer/passes/optimize_pass.h#L30

You can capture the capture map in a lambda
jamesr66a(2018-04-24 22:42:11):lol
CLAassistant(2018-04-24 19:02:43):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=818) <br/>All committers have signed the CLA.
linkerzhang(2018-04-24 20:21:43):Thank you very much! Please sign the cla accordingly.
pranavsharma(2018-04-24 21:14:26):@linkerzhang signed the CLA. Looks like it takes sometime to get updated. Here's what I see when I click on the link to sign the CLA: You have signed the CLA for onnx/onnx. 
pranavsharma(2018-04-25 00:14:45):How much time does it take generally for the CLA to recognize that I've already signed the agreement? 
wschin(2018-05-21 16:51:33):Do we need to increate the version number of this op?
linkerzhang(2018-04-24 22:37:55):@gramalingam please also run \onnx\defs\gen_doc.py to update the operator.md and Changelog.md
houseroad(2018-04-25 00:24:05):@linkerzhang I think we need to manually add the default value description to the doc. Please correct me if I am wrong.
linkerzhang(2018-04-25 02:17:36):@houseroad Aha, you're right. My bad :) .......................................
houseroad(2018-04-25 03:48:45):@gramalingam could you add defaults into the descriptions of these two attributes? And update the doc using gen_doc.py?
yinghai(2018-04-25 15:40:53):what's zfnet? :) 
houseroad(2018-04-25 17:12:35):Another model for image classification, won ImageNet 2013
houseroad(2018-04-25 17:12:52):More details here: https://github.com/onnx/models/tree/master/zfnet

:-)
jspisak(2018-04-25 17:50:13):I've never heard of it either. But LGTM.. :) 
houseroad(2018-04-25 17:52:01):The paper has high citation: 3000+ :-)
Let's read it.
AppVeyorBot(2018-04-25 19:21:39)::white_check_mark: [Build onnx 0.3.2560 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2560) (commit https://github.com/onnx/onnx/commit/52b6ae0769 by @houseroad)
AppVeyorBot(2018-04-26 00:15:07)::x: [Build onnx 0.3.2582 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2582) (commit https://github.com/onnx/onnx/commit/a29e7e5610 by @houseroad)
AppVeyorBot(2018-04-26 01:13:04)::x: [Build onnx 0.3.2590 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2590) (commit https://github.com/onnx/onnx/commit/7f72275923 by @houseroad)
AppVeyorBot(2018-04-26 19:31:36)::x: [Build onnx 0.3.2616 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2616) (commit https://github.com/onnx/onnx/commit/bb0bedc61e by @houseroad)
AppVeyorBot(2018-04-26 23:02:26)::white_check_mark: [Build onnx 0.3.2625 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2625) (commit https://github.com/onnx/onnx/commit/2904f7f19d by @houseroad)
AppVeyorBot(2018-04-26 05:16:48)::x: [Build onnx 0.3.2596 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2596) (commit https://github.com/onnx/onnx/commit/fe890deb92 by @linkerzhang)
linkerzhang(2018-04-26 05:23:39):@houseroad , I should create a new branch for it. :)
AppVeyorBot(2018-04-26 06:02:18)::x: [Build onnx 0.3.2597 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2597) (commit https://github.com/onnx/onnx/commit/37bd6fba91 by @linkerzhang)
AppVeyorBot(2018-04-26 06:13:45)::white_check_mark: [Build onnx 0.3.2598 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2598) (commit https://github.com/onnx/onnx/commit/3ee56beec3 by @houseroad)
houseroad(2018-04-26 06:13:56):Windows CI was never happy...
houseroad(2018-04-26 06:15:09):@anderspapitto btw, I think we should also delete `onnx/test/backend_shape_inference_test.py`, since it was merged into `test_backend_test.py`
linkerzhang(2018-04-26 06:16:48):it was not a good idea to check in change that does not pass windows ci :).
bddppq(2018-04-30 07:34:30):Do these test cases already exist in pytorch?
fumihwh(2018-04-30 09:30:32):@bddppq 
You mean https://github.com/onnxbot/onnx-fb-universe/pull/1780?
houseroad(2018-04-30 17:03:04):yeah, I think they are in onnx-fb-universe already. :-)
CLAassistant(2018-04-26 15:31:49):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=832) <br/>All committers have signed the CLA.
AppVeyorBot(2018-04-26 16:37:12)::x: [Build onnx 0.3.2608 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2608) (commit https://github.com/onnx/onnx/commit/4e6a5f25ec by @)
raymondxyang(2018-04-26 18:42:40):@houseroad the changes lgtm but I dont know if its what we want to put libs and headers under environment paths?
ffk0716(2018-04-27 03:35:15):I am using onnx in cmake ExternalProject. Currently I have to locate the libraries in binary_dir. I think it will be connivence if we have offical install step in cmake.
ffk0716(2018-04-27 06:19:48):Sorry, my mistake. There is no need to install onnx_cpp2py_export. I have fixed it.
yinghai(2018-04-27 04:34:42):Why do we need to install this to lib? 
bddppq(2018-04-30 07:24:14):These can be shared libs as well. Simply remove `ARCHIVE`.
bddppq(2018-04-30 07:29:37):`onnx` -> `${ONNX_ROOT}/onnx`
ffk0716(2018-05-01 07:05:59):Should I squash to one commit or commit a new one?
AppVeyorBot(2018-04-26 21:09:10)::x: [Build onnx 0.3.2617 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2617) (commit https://github.com/onnx/onnx/commit/e5c24aad94 by @anderspapitto)
CLAassistant(2018-04-26 22:49:32):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=836) <br/>All committers have signed the CLA.
AppVeyorBot(2018-04-26 23:29:54)::x: [Build onnx 0.3.2627 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2627) (commit https://github.com/onnx/onnx/commit/3b97a61d58 by @)
AppVeyorBot(2018-04-26 23:51:07)::white_check_mark: [Build onnx 0.3.2628 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2628) (commit https://github.com/onnx/onnx/commit/3b97a61d58 by @)
linkerzhang(2018-04-27 20:10:12):Per discussion, let's define it with fixed value for float or double, instead of having it to be truncated. Thank you very much!
AppVeyorBot(2018-04-27 22:29:17)::white_check_mark: [Build onnx 0.3.2655 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2655) (commit https://github.com/onnx/onnx/commit/d9a2017215 by @houseroad)
AppVeyorBot(2018-04-28 00:24:07)::white_check_mark: [Build onnx 0.3.2660 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2660) (commit https://github.com/onnx/onnx/commit/cf5a49de04 by @houseroad)
CLAassistant(2018-04-27 17:22:20):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=843) <br/>All committers have signed the CLA.
houseroad(2018-04-27 17:39:44):@skottmckay could you sigh the CLA
AppVeyorBot(2018-04-27 18:25:45)::x: [Build onnx 0.3.2641 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2641) (commit https://github.com/onnx/onnx/commit/0077089ec3 by @)
AppVeyorBot(2018-04-27 18:40:40)::x: [Build onnx 0.3.2642 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2642) (commit https://github.com/onnx/onnx/commit/ba2488103c by @skottmckay)
linkerzhang(2018-04-27 17:56:29):change "T2" to "tensor(float)", since there's only one type specified. :) Thank you!
AppVeyorBot(2018-04-27 21:01:59)::x: [Build onnx 0.3.2652 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2652) (commit https://github.com/onnx/onnx/commit/103560dccf by @anderspapitto)
AppVeyorBot(2018-04-27 21:30:02)::x: [Build onnx 0.3.2654 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2654) (commit https://github.com/onnx/onnx/commit/01df53f760 by @houseroad)
houseroad(2018-04-27 21:32:15):Can we output the original shape information and the computed one when mismatch happens? It's easier for debug.
anderspapitto(2018-04-30 17:50:29):I've addressed all comments, except for improving the error messages, which I've done in a separate PR (https://github.com/onnx/onnx/pull/855)
gramalingam(2018-04-27 21:00:16):It would be useful to define this function (unaryElementWiseOpInferenceFn?) in one place and refer to it in the several functions that can use this.
gramalingam(2018-04-27 21:01:14):I mean "several ops"
houseroad(2018-04-27 21:03:19):You mean extracting this part into one function `propageteShapeTypeFromFirstInput`?
gramalingam(2018-04-27 21:07:53):One simple extension would be: in the non-broad-cast case, use shape of C, if that is available.
gramalingam(2018-04-27 21:08:55):Yes
houseroad(2018-04-27 21:50:17):I think the problem is here. should be `transB ? 0 : 1`

According to the spec, without any transpose, we should have shapes like A = M x K, B = K x N, 
CLAassistant(2018-04-27 23:12:51):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=846) <br/>All committers have signed the CLA.
AppVeyorBot(2018-04-28 00:49:14)::white_check_mark: [Build onnx 0.3.2663 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2663) (commit https://github.com/onnx/onnx/commit/ba52f03698 by @skottmckay)
ebarsoum(2018-04-28 00:49:31):I like the change, but this is a breaking change. The OP version need to increase.
linkerzhang(2018-04-28 19:40:27):@ebarsoum We'll bump the version when doing ONNX 1.2 release. btw, for this op, probably the type was specified wrongly before, which was a bug.

Meanwhile, for version bump, confirm please, we agreed to use odd and even op version for dev and release version, right?
ebarsoum(2018-04-28 00:25:46):Shouldn't be using the shape of the inputs and not inferring?
houseroad(2018-04-29 05:09:29):Nit: One more space?
bddppq(2018-04-30 07:12:16):Input and output types should not use the same parameter, they can be different.
linkerzhang(2018-04-30 17:10:33):so the out type inference will be, if the "dtype" specified, then output type is "dtype", otherwise, it will use the same type as input?
gramalingam(2018-04-30 17:12:43):yes, that's my understanding too. so, we should use T1 and T2 for input and output type.
skottmckay(2018-04-30 17:13:24):What should the input type constraints be?
gramalingam(2018-04-30 17:18:39):Good question! I guess we have two options:
(1) Allow only the same types permitted for output, or
(2) Allow any type, and have the type-checker separately check that the input type is one of the permitted output types if "dtype" is missing.
(2) seems the right solution since it is less restrictive. (But it is a bit extra work.)
gramalingam(2018-04-30 17:20:28):I suggest (2) unless someone else has objections.
bddppq(2018-04-30 17:54:57):nit: Should add `*.pyd` in the "Compiled Dynamic libraries" instead.
bddppq(2018-04-30 17:56:36):Vote for (2)
bddppq(2018-05-01 17:21:31):There is `OpSchema::all_tensor_types()` for this long list.
linkerzhang(2018-04-28 02:53:09):There's already a default "0" specified. Are you changing the spec to use last axis anyway? if yes, you'll need to remove the default value specified below.
linkerzhang(2018-04-28 02:54:51):replace "OPTIONAL" with static_cast<int64_t>(1) then.
houseroad(2018-04-28 04:15:23):We should align with numpy, if no axes are specified, we should do the reduction along all the dimensions. Caffe2 and Tensorflow follow this rule.
houseroad(2018-04-28 04:33:39):Let's keep 0 as default value, unless we have good reason for that.
houseroad(2018-04-28 04:43:25):In my opinion, this should be a required field
linkerzhang(2018-04-30 22:51:56):@houseroad / @bddppq / @ebarsoum , are we all ok to change to the "blocksize" from "optional" to "required" please? or actually this should be "required" and was a spec bug? 
bddppq(2018-05-01 07:26:57):@linkerzhang I agree this should be required and should be considered as a bug (instead of a breaking change).
liqunfu(2018-05-01 17:34:44):resolved
liqunfu(2018-05-01 17:35:30):resolved
liqunfu(2018-05-01 17:36:39):changed required. this is resolved. 
houseroad(2018-05-02 04:03:11):I think it's spec bug. Usually, users should aware what he is doing, :-)
AppVeyorBot(2018-04-28 01:52:30)::white_check_mark: [Build onnx 0.3.2666 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2666) (commit https://github.com/onnx/onnx/commit/73b743d11f by @linkerzhang)
bddppq(2018-04-28 10:16:05):Please provide an example usage of this API.
linkerzhang(2018-04-28 17:25:11):@bddppq Thank you very much for the comments!

1. This API is used to register a function, as we designed functionproto and are going to use it for composite ops. There has to be a way of register such composite ops/functions, similar with the current op schema registration.
2. Having check* function to return Status instead of throwing exception is something we have ever discussed, but I didn't get chance to introduce the "Status" class into ONNX.
3. Having "OnnxInstance" is giving the upper layer a chance to also have its own local instance of FunctionBuilderRegistry instance. OnnxInstance is always immutable.

Make sense?
bddppq(2018-04-30 06:55:19):@linkerzhang I know at a high level this is a set of new api for registering functions. What I wanted to see is a concrete (toy) example usage of this API, this helps exercising from a user's perspective.
bddppq(2018-04-30 06:57:49):re. separate instances of registries, are you saying for custom domains? so we are handling it differently with the current op schema registry (which uses a single instance)?
linkerzhang(2018-04-30 22:48:03):@bddppq ,toy cases will be added as test cases later in separate PR after https://github.com/onnx/onnx/pull/763 merged in (Raymond is actively working on it).

"re. separate instances of registries, are you saying for custom domains? so we are handling it differently with the current op schema registry (which uses a single instance)?"

Yes, there're cases that upper layer wants to dynamically load/unload customized  function registration. So, we'd allow more than one function registratry instance here.

Make sense?

bddppq(2018-05-01 07:15:47):Re. toy use case I was not requesting to add it into the code, just a code snippet as a comment in this thread would be enough. TBH I don't see how is this going to be used. e.g. I don't see any registration macros like in operator schemas so I don't know how are you going to register functions in the onnx core domain.
Re. dynamically registering functions with custom domains, operator schemas also supports that with single registry instance, right?
AppVeyorBot(2018-05-02 00:11:29)::white_check_mark: [Build onnx 0.3.2742 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2742) (commit https://github.com/onnx/onnx/commit/e3223cdb48 by @linkerzhang)
AppVeyorBot(2018-05-04 05:35:08)::white_check_mark: [Build onnx 0.3.2830 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2830) (commit https://github.com/onnx/onnx/commit/4f61e84eaf by @linkerzhang)
AppVeyorBot(2018-05-04 17:23:27)::white_check_mark: [Build onnx 0.3.2844 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2844) (commit https://github.com/onnx/onnx/commit/3f31791214 by @linkerzhang)
linkerzhang(2018-05-05 14:04:56):@houseroad is the travis ci experiencing some issues please?
linkerzhang(2018-05-05 14:36:46):Finally, travis ci passed, I'm merging it.
bddppq(2018-04-28 09:50:35):All other checker functions are using exceptions, this new `check_function` should be consistent with the others.
bddppq(2018-04-28 09:52:28):what do you need from checker in schema? this will introduce circular dependencies.
bddppq(2018-04-28 09:57:41):This file also needs [these boilerplates](https://github.com/onnx/onnx/blob/master/onnx/onnx_pb.h#L7-L59)
bddppq(2018-04-28 09:58:52):nit: it's already inside the onnx namespace, so IMO no need to have "Onnx" in its name
linkerzhang(2018-04-28 17:59:50):Fixed.
linkerzhang(2018-04-28 18:07:35):I explained below. a while ago, I talked to Dima and folks in some PR review somehow to introduce "Status" class into onnx for error handling instead of throwing exceptions. Now, I'm adding it, it will take some time to change existing exception usage though.
linkerzhang(2018-04-28 18:39:50):btw, I can move this function into function.h/cc for now, if you feel that's better. :)
bddppq(2018-04-30 06:48:30):Oh I didn't know you had discussions with @dzhulgakov already. My main concern is having **inconsistent** styles of error handling in our api. What was the problem of using exceptions?
linkerzhang(2018-05-01 04:27:12):Whether using exception for error handling, I believe it could be a long-debate question. As a core ONNX lib, I personally think it's better to not throw exceptions, especially in some of our static registration codes, throwing exceptions or "abort" process is not a good idea. I agree with you that we should have consistent error handling. I'm targeting to move all error handling to use "Status", instead of throwing exceptions. Thoughts? Again, if there's different opinion here, to avoid long debate on this PR, I can move this function into function.h instead of putting it here. Which way do you prefer please?
bddppq(2018-05-01 07:09:49):I don't think moving into a separate file will help the issue of inconsistency.
If we are doing this at the beginning, I probably won't be against using status code (as you said it's a long-debate question that I don't want us get into it if possible). However, considering this api has been established for a while (so there could be many users already), I prefer to not change it.
linkerzhang(2018-05-01 21:33:08):We can't allow "exception" and "abort" in our static registration code, in windows ml scenario. That's why we should avoid using exception as much as possible. To me, it's acheiveable to not use exception in onnx. Make sense?
bddppq(2018-05-04 03:57:26):ONNX_NAMESPACE::to_string
bddppq(2018-05-04 04:06:35):This file also needs [these boilerplates](https://github.com/onnx/onnx/blob/master/onnx/onnx_pb.h#L11).
bddppq(2018-05-04 04:12:16):this seems to be worth to create a macro/function in status.{h,cc} because there will be quite often such check.
bddppq(2018-05-04 04:15:51):ONNX_NAMESPACE::to_string
bddppq(2018-05-04 04:18:48):CHECKER is more appropriate IMO, there are some checks don't belong to schema validation.
bddppq(2018-05-04 04:23:16):why does it take shared_ptr?
bddppq(2018-05-04 04:23:56):string_utils.h has MakeString to simplify this
bddppq(2018-05-04 04:25:29):Should there be another version of GetFunction that return the function of specific name and specific version (default to latest)?
bddppq(2018-05-04 04:27:42):lol it's indeed very very similar to checking graph
linkerzhang(2018-05-04 05:15:59):Yes. they're having similar structure.
linkerzhang(2018-05-04 05:17:31):I'd not do it but having upper layer to do that, because this get function is indeed calling BuildFunctions to create FunctionProto, I don't want to expose any API for doing this creation again and again.
linkerzhang(2018-05-04 05:18:22):You mean macro to create status, right? given I'm using the MakeString in string_utils, I feel it's not needed. the macro will look similar as the current lines of codes.
linkerzhang(2018-05-04 05:18:49):I'm adding #include "onnx/onnx_pb.h" in this file. 
linkerzhang(2018-05-04 05:19:47):Done
linkerzhang(2018-05-04 05:19:57):Done
bddppq(2018-05-04 05:19:58):oh I see that also works
linkerzhang(2018-05-04 05:20:04):Done.
linkerzhang(2018-05-04 05:20:25):unique_ptr looks better here. Changing to use unique_ptr instead.
linkerzhang(2018-05-04 05:20:34):Done.
bddppq(2018-05-04 05:21:04):Speaking of repeated creation, should we do some caching (and more importantly do not expose the builder function)?
linkerzhang(2018-05-04 18:04:14):the builder function is not exposed. I don't want to do caching here. Upper layer callers can do that.
bddppq(2018-05-04 19:15:41):Why shared_ptr here?
linkerzhang(2018-05-04 20:24:50):it should be unique_ptr. Good catch!
linkerzhang(2018-05-04 20:25:12):it should be unique_ptr. Good catch!
bddppq(2018-05-05 07:31:48):hmm what's the benefit of this (comparing to directly use `""`)?
bddppq(2018-05-05 07:32:51):Maybe macro each case?
bddppq(2018-05-05 07:34:21):Better to implement `==` for class `State` so we don't need to construct the string representation of status.
bddppq(2018-05-05 07:35:37):Could you use `std::multimap<std::string, FunctionProto>` here?
bddppq(2018-05-05 07:36:36):Could you use `FunctionProto*` here?
linkerzhang(2018-05-05 14:27:08):I'm seeing it only having a explicit name :).
linkerzhang(2018-05-05 14:28:03):I'd keep the unique_ptr here to 1) avoid a copy, 2) clarify its unique ownership (folks should not copy it over around).
linkerzhang(2018-05-05 14:37:19):will check this comment later.
linkerzhang(2018-05-05 14:37:33):will check this comment later.
houseroad(2018-04-30 21:41:56):Thanks for fixing the problem!!!
snnn(2018-04-30 21:53:49):Unfortunately, this fix doesn't really work.

After the fix, the ValueInfoProto of graph input y became:
```
name: "y"
type {
  tensor_type {
    elem_type: FLOAT
    shape {
    }
  }
}
```
It's perfect. But,

in onnx\defs\data_type_utils.cc, function  DataTypeUtils::ToString, we have such code:
```c++
 switch (type_proto.value_case()) {
    case TypeProto::ValueCase::kTensorType: {
      if (type_proto.tensor_type().has_shape() &&
          type_proto.tensor_type().shape().dim_size() == 0) {
        // Scalar case.
        return left + ToDataTypeString(type_proto.tensor_type().elem_type()) +
            right;
      } else {
        return left + "tensor(" +
            ToDataTypeString(type_proto.tensor_type().elem_type()) + ")" +
            right;
      }
    }
```

So the type string of this input would be 'float' , not 'tensor(float)'. 
It doesn't match the spec of 'Pow' .





houseroad(2018-04-30 22:37:39):@snnn I think this change works perfectly. I manually checked the tensor proto and model proto the script generated. Their dims' len are both 0. I think what you want is here: https://github.com/onnx/onnx/pull/858. Or you want to add `floa16`, `float`, `double` to the type of second input.
snnn(2018-04-30 22:43:05):Thanks.
AppVeyorBot(2018-04-30 21:31:51)::white_check_mark: [Build onnx 0.3.2700 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2700) (commit https://github.com/onnx/onnx/commit/c3890a7861 by @houseroad)
bddppq(2018-05-01 07:18:30):test failures are unrelated
bddppq(2018-04-30 18:19:44):You can use MakeString in string_utils.h to concat strings.
AppVeyorBot(2018-04-30 18:50:30)::x: [Build onnx 0.3.2694 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2694) (commit https://github.com/onnx/onnx/commit/5b3772d0da by @anderspapitto)
AppVeyorBot(2018-04-30 19:06:25)::x: [Build onnx 0.3.2695 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2695) (commit https://github.com/onnx/onnx/commit/fdd4f94610 by @anderspapitto)
AppVeyorBot(2018-04-30 19:23:07)::x: [Build onnx 0.3.2696 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2696) (commit https://github.com/onnx/onnx/commit/a63029f935 by @anderspapitto)
AppVeyorBot(2018-04-30 20:17:59)::x: [Build onnx 0.3.2699 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2699) (commit https://github.com/onnx/onnx/commit/0ae8778d6a by @anderspapitto)
AppVeyorBot(2018-05-01 19:00:58)::x: [Build onnx 0.3.2731 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2731) (commit https://github.com/onnx/onnx/commit/ebaece53d5 by @anderspapitto)
AppVeyorBot(2018-05-01 19:58:56)::x: [Build onnx 0.3.2732 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2732) (commit https://github.com/onnx/onnx/commit/a9cd602782 by @anderspapitto)
AppVeyorBot(2018-05-01 20:29:52)::x: [Build onnx 0.3.2734 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2734) (commit https://github.com/onnx/onnx/commit/13d2d2367d by @anderspapitto)
AppVeyorBot(2018-05-01 20:55:49)::x: [Build onnx 0.3.2736 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2736) (commit https://github.com/onnx/onnx/commit/ecf532a059 by @anderspapitto)
AppVeyorBot(2018-05-01 21:55:26)::x: [Build onnx 0.3.2739 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2739) (commit https://github.com/onnx/onnx/commit/e5dfec2d61 by @anderspapitto)
AppVeyorBot(2018-05-02 19:20:18)::x: [Build onnx 0.3.2765 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2765) (commit https://github.com/onnx/onnx/commit/2474484c09 by @anderspapitto)
houseroad(2018-04-30 23:22:14):~~This is wrong for when dimension is higher than 2.~~ We checked the spec carefully, this is right. False alarm. 
gramalingam(2018-05-01 03:50:29):getInputType(0)?
gramalingam(2018-05-01 03:50:54):getInputType(0)?
gramalingam(2018-05-01 03:58:46):Add an (unknown) dim to output shape in this case?
gramalingam(2018-05-01 04:03:10):I don't see attribute "dilations" mentioned in these ops ... should it be added to these ops in spec?
anderspapitto(2018-05-01 15:03:05):the pooling ops don't have dilations, but Conv does, and it's harmless to share the code since it is a no-op when unspecified.

However I will add a comment to make this clear in the code
gramalingam(2018-05-01 16:28:20):It's not harmless for models where the pooling ops specify a dilations attribute. The pooling ops implementation will ignore the dilations if specified (unless some attribute-checking throws out the model as an invalid model), but the shape inference will take them into account and produce a wrong shape, leading to possible memory corruption if memory is allocation on this basis.
gramalingam(2018-05-01 16:30:12):May be we could add a Boolean parameter to the inference function to indicate if dilations should be considered or not?
anderspapitto(2018-05-01 17:59:10):do we allow arbitrary extra attributes (e.g. putting a dilation on something that doesn't support it?) If so then yes this is a problem.

In any case yes a flag sounds like a good idea
anderspapitto(2018-05-01 18:04:14):++ so it is
anderspapitto(2018-05-01 18:33:20):yeah, that will work. I guess there's a little more difference between Conv and Pool than I was thinking
anderspapitto(2018-05-01 18:33:24):++
anderspapitto(2018-05-01 18:37:04):++
dzhulgakov(2018-05-02 05:13:46):Do we have unittests for it? If so - why it can't be caught by automatically validating shape inference against the test set?

If we don't - we should consider adding something like "fuzzer" to validate more use cases against some backend
dzhulgakov(2018-05-02 05:15:56):shouldn't it be an error (like throwing an exception) since it's an invalid spec?

what's you preferred error reporting behavior for shape inference?
dzhulgakov(2018-05-02 05:16:38):nit: dilations.assign(n_input_dims, 1)
dzhulgakov(2018-05-02 05:17:50):nit: just use `int` everywhere instead of `size_t`. It's quite easy to get the accidental underflow with size_t and also saves you from some casting
anderspapitto(2018-05-02 18:20:48):we discussed offline - actually this implementation is correct, and yes it's caught by tests if you change it (which I initially tried to do after lu's comment -> broke all the tests)
anderspapitto(2018-05-02 18:24:09):My error-handling approach is to only avoid crashes and incorrect results in shape inference. Anything beyond that belongs in the checker.

Also in this particular case, do we explicitly disallow extraneous attributes? If I add attribute "foo=3" to some node, is the graph now considered to be ill-formed?
anderspapitto(2018-05-02 18:24:28):sure ++
anderspapitto(2018-05-02 18:24:54):sure, will try `int`
houseroad(2018-05-03 06:05:30):Nit: Whether we have `is_test` or not, calling `propagateShapeAndTypeFromFirstInput` should be fine. 
AppVeyorBot(2018-04-30 19:34:42)::white_check_mark: [Build onnx 0.3.2697 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2697) (commit https://github.com/onnx/onnx/commit/56b8e054c1 by @linkerzhang)
gramalingam(2018-04-30 19:55:32):getAttribute() can return null if the attribute is not specified. check for null (and do nothing if it is null). (In the longer run, we should probably add methods to InferenceContext so that we can return an error-code to indicate the exact typing-errors for better error messages.)
gramalingam(2018-04-30 19:58:22):Add a hasNInputTypes(1) check?
linkerzhang(2018-04-30 22:43:17):fixed.
snnn(2018-04-30 22:41:00):Good! This change will also fix another issue in the pow_bcast test case.
houseroad(2018-04-30 22:38:37):This method is only called in the schema. Do we really need to have special handling for the scalar tensor in the type system? If not, this change looks good to me. Otherwise, we should change the spec instead.
fumihwh(2018-05-02 02:07:36):@houseroad 
So in that case we also need to remove it, right?
houseroad(2018-05-02 03:45:31):@fumihwh correct, we should also remove unused input, too. That's why I ask for this case.
anderspapitto(2018-05-02 18:45:24):so, unlike most optimizations, you're actually changing the semantics of the graph here (i.e. some code that really really wants to provide those 'unused' inputs might break when they no longer exist).

I guess that should be fine as long as the optimization is opt-in, which they all are, but what's the motivation here? Do you commonly have graphs with unused inputs?
houseroad(2018-05-02 19:42:51):Here is more context. Since PyTorch is not smart enough, it may export some unused parameters as initializers. These data should be removed. And in general, since the removed inputs are initializers, users still provide the same input and get the same output, this pass does not really change the semantics. To be safe, we can only remove the input when it’s a useless initializer.
fumihwh(2018-05-23 14:09:10):@houseroad Done~~~;)
AppVeyorBot(2018-05-23 16:40:23)::white_check_mark: [Build onnx 0.3.3539 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.3539) (commit https://github.com/onnx/onnx/commit/55cefcd594 by @fumihwh)
dzhulgakov(2018-05-02 05:08:06):this is going to copy potentially big struct. Feel free to change Graph's methods to manipulate initializers if necessary
dzhulgakov(2018-05-02 05:09:11):it assumes sorted ranges which is not the case. Do you want to use std::unordered_set instead?
dzhulgakov(2018-05-02 05:09:36):it will break if the same initializer is used twice in different nodes
fumihwh(2018-05-02 07:00:55):@dzhulgakov 
Thanks. 
By using `std::unordered_set`, it will not be a problem anymore.
Of course, above is based on current strategy. 
If some helper functions could be added to graph, problem can become easier.
houseroad(2018-05-21 14:08:40):Tensor& initializer? otherwise the cost may be very high if initializer is large.
houseroad(2018-05-21 14:11:38):Nit: add some example to explain the behavior of this pass, something like https://github.com/onnx/onnx/blob/master/onnx/optimizer/passes/fuse_add_bias_into_conv.h#L6
houseroad(2018-05-21 14:22:15):Here, we cannot simply call DescendOnGraphAttributes, because we also need record all the inputs of the nodes in the subgraph.
houseroad(2018-05-21 14:32:35):Nit: add one test case with control flow graph (while/if operator, so you can test subgraph case).
houseroad(2018-05-27 10:45:16):We are not able to remove unused Const :-)
To be more precise,  you can say `A, B, C are in the initializer list`
fumihwh(2018-05-27 11:46:06):@houseroad initializer could not be a const? BTW, kConstant and kParam does mean Const, right?
houseroad(2018-05-27 13:58:33):kConstant means you have a `Constant` op, see https://github.com/onnx/onnx/blob/master/docs/Operators.md#constant

kParam means the initializers, I think.
houseroad(2018-05-27 14:04:11):Nit: assert len(input) == 2
fumihwh(2018-05-28 12:33:09):@houseroad Maybe we should clarify them in future. 
linkerzhang(2018-05-01 23:53:12):@houseroad , do we need 3d/5d right now? I'm open on it.
houseroad(2018-05-02 06:08:16):@linkerzhang yeah, I think sooner or later we will add them anyway, let's just add them at one time then. :-)
linkerzhang(2018-05-03 04:34:45):@houseroad  The spec now changed to support n-dimension upsampling. The original one supporting 2 4-D tensor upsampling only asks for 2 scales (for H and W). Now I changed to ask for X's rank scales. It means the scale for N and C may be 1 to fit into the spec. Another option is to have the scale array to have X's rank - 2 elements, which looks a little bit weird. Let me know your thoughts. We can finalize the spec and update the docs and test cases accordingly.
AppVeyorBot(2018-05-03 04:41:25)::x: [Build onnx 0.3.2785 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2785) (commit https://github.com/onnx/onnx/commit/da1d191770 by @linkerzhang)
houseroad(2018-05-03 05:48:28):@linkerzhang looks good to me, let's move forward :-)
AppVeyorBot(2018-05-03 16:30:50)::x: [Build onnx 0.3.2792 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2792) (commit https://github.com/onnx/onnx/commit/4f45a32a9e by @linkerzhang)
AppVeyorBot(2018-05-03 17:47:17)::x: [Build onnx 0.3.2796 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2796) (commit https://github.com/onnx/onnx/commit/01e940fe76 by @linkerzhang)
AppVeyorBot(2018-05-04 05:00:12)::x: [Build onnx 0.3.2828 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2828) (commit https://github.com/onnx/onnx/commit/351da6c022 by @linkerzhang)
linkerzhang(2018-05-04 15:59:32):The last change of this PR will be setting the since_version as "7" or "8" once we make the decision whether we use odd/even idea or not.
houseroad(2018-05-04 16:05:50):Wait for https://github.com/onnx/onnx/pull/876
AppVeyorBot(2018-05-04 16:15:15)::x: [Build onnx 0.3.2838 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2838) (commit https://github.com/onnx/onnx/commit/84f252fc4d by @linkerzhang)
AppVeyorBot(2018-05-04 16:30:09)::x: [Build onnx 0.3.2839 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2839) (commit https://github.com/onnx/onnx/commit/299687942f by @linkerzhang)
AppVeyorBot(2018-05-04 16:43:39)::x: [Build onnx 0.3.2840 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2840) (commit https://github.com/onnx/onnx/commit/ef95e67335 by @linkerzhang)
AppVeyorBot(2018-05-06 21:28:15)::x: [Build onnx 0.3.2898 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2898) (commit https://github.com/onnx/onnx/commit/dab99773e3 by @linkerzhang)
AppVeyorBot(2018-05-08 04:05:23)::x: [Build onnx 0.3.2927 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2927) (commit https://github.com/onnx/onnx/commit/5a758e0ca7 by @linkerzhang)
AppVeyorBot(2018-05-10 02:04:50)::x: [Build onnx 0.3.3000 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.3000) (commit https://github.com/onnx/onnx/commit/17bed11e40 by @linkerzhang)
AppVeyorBot(2018-05-10 02:31:54)::x: [Build onnx 0.3.3001 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.3001) (commit https://github.com/onnx/onnx/commit/ce94abc790 by @linkerzhang)
AppVeyorBot(2018-05-10 02:44:09)::white_check_mark: [Build onnx 0.3.3002 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.3002) (commit https://github.com/onnx/onnx/commit/ddf7ea7203 by @linkerzhang)
fumihwh(2018-06-08 13:26:38):@linkerzhang Should we remove Experimental Upsample?
houseroad(2018-05-01 05:14:20):newline is missing here
linkerzhang(2018-05-03 16:17:35):This is the only change in this file (adding upsample op). All others are format changes.
linkerzhang(2018-05-03 16:17:52):fixed.
houseroad(2018-05-03 17:10:45):nearest (default), and linear (including bilinear, trilinear, etc)
bddppq(2018-05-04 08:02:34):@linkerzhang In case you are not aware of it: there is a `git-clang-format` script/tool that integrates with git to only clang-format the changed parts.
bddppq(2018-05-04 08:05:18):(Can be done in a separate PR) Please remove these example lines from the doc, because:
1. they are not rendered correctly in our markdown file
2. they are not needed anymore since we have embedded test cases's python code into the doc
3. their correctness is not testable
bddppq(2018-05-04 08:08:08):The choices of these supported input types seem to be pretty random. Should be all_tensor_types?
bddppq(2018-05-04 08:18:46):These should be python floats
linkerzhang(2018-05-04 15:50:20):Make sense! removed these texts.
linkerzhang(2018-05-04 15:50:29):changed.
linkerzhang(2018-05-04 15:50:35):changed.
linkerzhang(2018-05-04 15:57:50):thank you! I was hoping to keep doing this whole file formatting until whole code base follows the format :). I
linkerzhang(2018-05-04 15:58:18):as this file is not that complex, so I'll keep the format changes for other parts.
CLAassistant(2018-05-01 04:52:14):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=862) <br/>All committers have signed the CLA.
AppVeyorBot(2018-05-03 04:59:27)::white_check_mark: [Build onnx 0.3.2786 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2786) (commit https://github.com/onnx/onnx/commit/1b0f7b3c98 by @anirudhacharya)
AppVeyorBot(2018-05-03 16:53:55)::white_check_mark: [Build onnx 0.3.2793 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2793) (commit https://github.com/onnx/onnx/commit/1f6ed43bcc by @anirudhacharya)
houseroad(2018-05-03 17:58:23):Please call https://github.com/onnx/onnx/blob/master/onnx/defs/gen_doc.py to update the generated docs.
houseroad(2018-05-03 23:24:57):Please lint the code accordingly. https://travis-ci.org/onnx/onnx/jobs/374574385#L7310
anirudhacharya(2018-05-04 05:08:28):@houseroad please merge this if I have addressed all your comments.
houseroad(2018-05-02 06:35:42):Nit: change the function to export_nokeepdims
houseroad(2018-05-02 06:35:53):Nit: change the function to export_keepdims
houseroad(2018-05-02 06:39:01):to be consistent, better using astype(np.float32)
houseroad(2018-05-02 06:46:45):right now, both cases are descending order, we can have one case with ascending axes.
AppVeyorBot(2018-05-01 20:19:08)::x: [Build onnx 0.3.2733 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2733) (commit https://github.com/onnx/onnx/commit/b123409a87 by @anderspapitto)
AppVeyorBot(2018-05-01 20:44:33)::x: [Build onnx 0.3.2735 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2735) (commit https://github.com/onnx/onnx/commit/a31be15162 by @anderspapitto)
AppVeyorBot(2018-05-01 21:10:43)::white_check_mark: [Build onnx 0.3.2737 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2737) (commit https://github.com/onnx/onnx/commit/99ce915084 by @anderspapitto)
dzhulgakov(2018-05-02 05:23:46):is it worth using size_t everywhere? usually it's easier to use int/int64_t and avoid ugly casting everywhere
AppVeyorBot(2018-05-02 19:34:32)::white_check_mark: [Build onnx 0.3.2766 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2766) (commit https://github.com/onnx/onnx/commit/e73458ddb3 by @anderspapitto)
anderspapitto(2018-05-03 18:34:57):@dzhulgakov does it look good to you now?
AppVeyorBot(2018-05-03 18:57:22)::x: [Build onnx 0.3.2801 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2801) (commit https://github.com/onnx/onnx/commit/b0cf7aa195 by @anderspapitto)
AppVeyorBot(2018-05-03 20:00:16)::white_check_mark: [Build onnx 0.3.2808 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2808) (commit https://github.com/onnx/onnx/commit/1c177c9a55 by @anderspapitto)
CLAassistant(2018-05-01 23:40:59):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=868) <br/>All committers have signed the CLA.
AppVeyorBot(2018-05-01 23:51:21)::x: [Build onnx 0.3.2740 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2740) (commit https://github.com/onnx/onnx/commit/21217b2437 by @chentaMS)
AppVeyorBot(2018-05-01 23:59:41)::x: [Build onnx 0.3.2741 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2741) (commit https://github.com/onnx/onnx/commit/2a8bbbfc4f by @chentaMS)
AppVeyorBot(2018-05-02 00:25:32)::x: [Build onnx 0.3.2743 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2743) (commit https://github.com/onnx/onnx/commit/eb58297063 by @chentaMS)
AppVeyorBot(2018-05-08 04:15:09)::x: [Build onnx 0.3.2928 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2928) (commit https://github.com/onnx/onnx/commit/362868b165 by @souptc)
souptc(2018-05-09 18:21:29):anything else needed to merge this pr?
gramalingam(2018-05-09 19:42:15):Looks good to merge
AppVeyorBot(2018-05-09 22:36:48)::white_check_mark: [Build onnx 0.3.2978 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2978) (commit https://github.com/onnx/onnx/commit/74eb960e2d by @gramalingam)
dzhulgakov(2018-05-02 05:20:50):we should really have a helper function similar to `getRepeatedAttribute ` for getting the right attribute type
bddppq(2018-05-02 08:11:20):We should basically rip off ArgumentHelper from https://github.com/pytorch/pytorch/blob/master/caffe2/utils/proto_utils.h
bddppq(2018-05-02 08:14:02):I would really prefer not to silently swallow illegal error here. IMO once shape inference needs to handle error checking, the code will become very verbose, the rule should be shape inference will assume the whole model file has already passed the checks in checker.
souptc(2018-05-03 02:36:24):I agree, if onnx guarantee the model already resolved correctly, I am happy to remove those checks here.
souptc(2018-05-03 02:40:28):yes, but I didn't find such a helper in onnx, add such a help seems out of this pr's scope, so I prefer to address it later.
linkerzhang(2018-05-03 17:57:28):We'll need to return sth in these cases. I'm going to add "Status" as the return value for the inference function. Discussed with Anders before on this too. It's not added at the beginning is because there's no Status class in ONNX repo at that time.
CLAassistant(2018-05-02 02:33:56):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=869) <br/>All committers have signed the CLA.
zasdfgbnm(2018-05-02 02:36:29):There is something I'm not sure what to do:
One is whether I need to put a `.SinceVersion(?)` in `def.cc` and if needed, which version to put.
Another is I don't know how to generate the pb files in `onnx/backend/test/data/node/`
AppVeyorBot(2018-05-02 02:44:34)::white_check_mark: [Build onnx 0.3.2748 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2748) (commit https://github.com/onnx/onnx/commit/a22bd250e7 by @zasdfgbnm)
fumihwh(2018-05-02 02:47:38):As new ops, since version 1 is OK. As default SinceVersion is 1, so you could ignore it.
Use cmd_tools.py to generate test data.
zasdfgbnm(2018-05-02 02:54:27):@fumihwh Data added, thanks!
linkerzhang(2018-05-02 03:51:48):The since version should be a new version bumped against the latest/max version of a domain. It should be 7 (6+1). (odd version is used as developer version, while even version is used for release). We'll bump the version to be 8 when doing next ONNX release.

btw, the since version (8) in coming ONNX release is telling customers that onnx domain operator set has these new operators in version 8. 

@ebarsoum  is going to improve the documentation about the versioning of operator set, btw.

Thank you!
zasdfgbnm(2018-05-02 05:14:56):@linkerzhang I'm trying to add `.SinceVersion(7)` to operators, but get the following error:
```
Trying to register schema with name Sin (domain:  version: 7) from file /home/gaoxiang/onnx/onnx/defs/math/defs.cc line 668, but it its version is notin the inclusive range [1, 6] (usually, this means you bumped the operator version but forgot to update the version range in DomainToVersionRange in onnx/defs/schema.h).
fish: “python onnx/defs/gen_doc.py” terminated by signal SIGABRT (Abort)
```

Is this an expected behavior which means that I should go ahead and update `onnx/defs/schema.h`?
fumihwh(2018-05-02 05:24:22):@zasdfgbnm Sorry for misleading.
@linkerzhang Thanks for making it clearly.
linkerzhang(2018-05-02 15:24:12):@zasdfgbnm please also update the max version to 7 in schema.h

    DomainToVersionRange() {
      // Increase the highest version when you make BC-breaking changes to the
      // operator schema on specific domain. Update the lowest version when it's
      // determined to remove too old version history.
      map_[ONNX_DOMAIN] = std::make_pair(1, 6);  --->       map_[ONNX_DOMAIN] = std::make_pair(1, 7);

    }

Doc generation is checking whether all ops' since version make sense per this setting. That's why you're seeing errors.
zasdfgbnm(2018-05-02 16:35:04):@linkerzhang Fixed
AppVeyorBot(2018-05-02 16:50:44)::x: [Build onnx 0.3.2758 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2758) (commit https://github.com/onnx/onnx/commit/bdc7a18857 by @zasdfgbnm)
bddppq(2018-05-02 17:11:55):Test data will need to be regenerated as well.
AppVeyorBot(2018-05-02 18:59:16)::x: [Build onnx 0.3.2763 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2763) (commit https://github.com/onnx/onnx/commit/45199cb144 by @zasdfgbnm)
AppVeyorBot(2018-05-02 19:10:51)::x: [Build onnx 0.3.2764 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2764) (commit https://github.com/onnx/onnx/commit/3e1fe94b9d by @zasdfgbnm)
zasdfgbnm(2018-05-02 19:19:47):@bddppq Regenerating test data seems to introduce some extra error...
- HTTP Error 403 on some files.
- And the notebook outputs should also be updated.

Fixing these things seems to be beyond the scope of this PR. Shall we open a separate PR for version bump, and update this branch when that PR is merged?

houseroad(2018-05-02 21:01:33):Hold on, the model zoo is not ready yet, since you bump up the opset version
bddppq(2018-05-03 01:15:49):@zasdfgbnm @linkerzhang @houseroad I agree we should do the version bump in a separate PR (this PR will need to wait for that to be merged first).
linkerzhang(2018-05-03 03:58:38):Agreed on bumping the version for test data separately. Thank you very much!
linkerzhang(2018-05-03 03:59:30):We may again need to do it when doing ONNX release :(. 
houseroad(2018-05-03 04:01:22):as @ebarsoum said, if there is no change on op spec, we should not bump up the opset version. Sorry, I would like to retract my odd-even version proposal. Instead, I think @bddppq's original proposal is better, we only bump up the opset version when the following two conditions are both true:
1) we are about to introduce some breaking changes,
2) and we haven't bumped up the opset version since last release.

@linkerzhang, @ebarsoum, @bddppq what do you think? :-)
AppVeyorBot(2018-05-03 04:06:16)::x: [Build onnx 0.3.2784 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2784) (commit https://github.com/onnx/onnx/commit/6fb9060723 by @linkerzhang)
linkerzhang(2018-05-03 04:10:47):It's OK. The odd/even thing makes the 2nd condition check easier although.
zasdfgbnm(2018-05-03 18:00:12):I'm a bit confused. What is the relationship between version like "1.1.1" and like "7"? Are they independent?

Also, there seems to be dependency cycle between version bump and new op. If we increase the version, then we need new model zoo to pass tests. But without new op set, we can not have a model zoo on this set... So at some point we have to fail tests... 

I would suggest to do the following to avoid this problem:
1. The rule is: new op must always be in a new version
1. postpone the version bump until release
1. Add a macro indicating whether the build is develop or product
1. If the build is develop, allow the version to be 1 larger than the upper bound.

This can be done by changing the following line:
https://github.com/onnx/onnx/blob/7d1e102e73ffd0479a87d62c8aa950e03cfdda95/onnx/defs/schema.h#L560

into
```C++
#ifdef DEVELOP
if (!(lower_bound_incl <= ver && upper_bound_incl + 1 >= ver)) { 
#else
if (!(lower_bound_incl <= ver && upper_bound_incl >= ver)) { 
#endif
```
This would make adding new op much easier.
AppVeyorBot(2018-05-08 00:38:54)::white_check_mark: [Build onnx 0.3.2924 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2924) (commit https://github.com/onnx/onnx/commit/e4b728399f by @linkerzhang)
AppVeyorBot(2018-05-08 18:59:30)::white_check_mark: [Build onnx 0.3.2938 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2938) (commit https://github.com/onnx/onnx/commit/5c06a425e1 by @zasdfgbnm)
AppVeyorBot(2018-05-09 23:38:11)::white_check_mark: [Build onnx 0.3.2984 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2984) (commit https://github.com/onnx/onnx/commit/2dc2b3d4f0 by @zasdfgbnm)
linkerzhang(2018-05-11 23:11:50):@zasdfgbnm do you want to resolve the conflicts please? Thank you!
zasdfgbnm(2018-05-12 00:30:11):@linkerzhang Done
zasdfgbnm(2018-05-12 00:59:04):Can anyone reviewing this give a quick comment on if theses operators in https://github.com/onnx/onnx/issues/870 are contribute welcome? I want to start working on these, but before that I just need to make sure if they are welcome.
bddppq(2018-05-13 04:58:47):CI failure is irrelevant. @zasdfgbnm Could you resolve the merge conflict?
zasdfgbnm(2018-05-13 05:14:42):@bddppq Done.
Looks like op set 7 is having many new ops and ONNX is becoming more complete :)
bddppq(2018-05-13 05:21:17):@zasdfgbnm Thanks. You need to regenerate the backend test cases as well, opset version is part of the test data.
zasdfgbnm(2018-05-13 06:11:36):@bddppq Do you mean files in `onnx/backend/test/data/`?
After doing a `python ./onnx/backend/test/cmd_tools.py generate-data`, I get the following changes:
```
On branch trigonometric
Changes not staged for commit:
  (use "git add <file>..." to update what will be committed)
  (use "git checkout -- <file>..." to discard changes in working directory)

	modified:   onnx/backend/test/data/node/test_exp/test_data_set_0/output_0.pb
	modified:   onnx/backend/test/data/node/test_identity/model.onnx
	modified:   onnx/backend/test/data/node/test_log/test_data_set_0/input_0.pb
	modified:   onnx/backend/test/data/node/test_log/test_data_set_0/output_0.pb
	modified:   onnx/backend/test/data/node/test_lstm_defaults/test_data_set_0/output_0.pb
	modified:   onnx/backend/test/data/node/test_lstm_with_initial_bias/test_data_set_0/output_0.pb
	modified:   onnx/backend/test/data/node/test_lstm_with_peepholes/test_data_set_0/output_0.pb
	modified:   onnx/backend/test/data/node/test_matmul_2d/test_data_set_0/output_0.pb
	modified:   onnx/backend/test/data/node/test_reduce_log_sum_asc_axes/model.onnx
	modified:   onnx/backend/test/data/node/test_reduce_log_sum_default/model.onnx
	modified:   onnx/backend/test/data/node/test_reduce_log_sum_desc_axes/model.onnx
	modified:   onnx/backend/test/data/node/test_softplus/test_data_set_0/output_0.pb
	modified:   onnx/backend/test/data/node/test_tanh/test_data_set_0/output_0.pb

no changes added to commit (use "git add" and/or "git commit -a")
```
All those seems to be unrelated. The data for trigonometric functions are already in this PR. Do I need to generate something else?
bddppq(2018-05-13 07:49:35):@zasdfgbnm Hmm these are unrelated, they should be updated in the PR that bumped up the opset version. I will create a separate PR to fix these.
anderspapitto(2018-05-02 18:28:24):do you have tests? I'd like to have at least a couple simple cases in shape_inference_test.py

implementation looks great
gramalingam(2018-05-02 19:22:20):yes, I need to add tests. I was wondering whether we should use a single test file or different test files, since several of us will be concurrently adding tests. I am okay with either, but a single test file may mean some potential merging conflicts
anderspapitto(2018-05-02 19:34:00):either way is fine with me. Feel free to add `# gramalingam new tests here` somewhere in the middle of the file, so we don't all conflict appending to the end :)
linkerzhang(2018-05-02 23:36:49):I just merged without checking all comments. @gramalingam feel free add tests in separate PR then. :)
AppVeyorBot(2018-05-02 21:37:52)::x: [Build onnx 0.3.2773 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2773) (commit https://github.com/onnx/onnx/commit/fef0c2297f by @anderspapitto)
AppVeyorBot(2018-05-03 19:05:35)::x: [Build onnx 0.3.2802 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2802) (commit https://github.com/onnx/onnx/commit/2acb5a055a by @anderspapitto)
AppVeyorBot(2018-05-03 19:37:01)::x: [Build onnx 0.3.2806 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2806) (commit https://github.com/onnx/onnx/commit/05fb204445 by @anderspapitto)
AppVeyorBot(2018-05-03 19:44:43)::x: [Build onnx 0.3.2807 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2807) (commit https://github.com/onnx/onnx/commit/0275e7b399 by @anderspapitto)
gramalingam(2018-05-03 22:54:35):else set it to paddedShapeR.dim(...) ? (Autocorrect insists of converting lowercase I to uppercase!)
gramalingam(2018-05-03 22:55:08):else use param from paddedShapeL?
gramalingam(2018-05-03 23:01:30):Looks good to me ... I guess the broadcasting-logic alone may be reusable across multiple broadcasting ops, if we can factor it out. (I haven't checked the other broadcasting ops yet.)
anderspapitto(2018-05-04 17:33:14):yes, i think it's fine to extract it out when someone gets around to another op that needs it
anderspapitto(2018-05-04 17:36:09):yes, I suppose that's a bit better, (merge 1 and 'a' -> 'a', rather than to unknown)
anderspapitto(2018-05-04 17:36:17):yes, same here
AppVeyorBot(2018-05-03 03:03:40)::x: [Build onnx 0.3.2783 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2783) (commit https://github.com/onnx/onnx/commit/51d21eba6e by @zasdfgbnm)
zasdfgbnm(2018-05-03 04:39:19):How are the files in `onnx/examples/resources` generated? manually?
houseroad(2018-05-03 17:52:30):@zasdfgbnm yes, manually.

I will update zoo later, and let you know when it's done.
AppVeyorBot(2018-05-04 17:10:45)::x: [Build onnx 0.3.2843 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2843) (commit https://github.com/onnx/onnx/commit/d9eab88c8e by @houseroad)
houseroad(2018-05-04 17:54:18):https://github.com/onnx/onnx/pull/887 will make sure, the ipython notebook won't be affected by bumping up the opset version.
zasdfgbnm(2018-05-07 21:37:44):Looks to be unrelated failures on file `onnx/test/shape_inference_test.py`
AppVeyorBot(2018-05-08 00:27:46)::x: [Build onnx 0.3.2923 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2923) (commit https://github.com/onnx/onnx/commit/927e917d91 by @houseroad)
zasdfgbnm(2018-05-08 02:16:55):@houseroad Do you mean the .onnx files in `onnx/backend/test/data`? Or something like `output_0.pb` files
houseroad(2018-05-08 03:32:01):Such as `onnx/backend/test/data/node/test_log/test_data_set_0/input_0.pb`
zasdfgbnm(2018-05-08 04:14:43):changes to these file are reverted now
AppVeyorBot(2018-05-08 04:25:13)::white_check_mark: [Build onnx 0.3.2929 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2929) (commit https://github.com/onnx/onnx/commit/42bcd96962 by @zasdfgbnm)
CLAassistant(2018-05-03 19:12:00):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=879) <br/>All committers have signed the CLA.
AppVeyorBot(2018-05-12 01:50:35)::x: [Build onnx 0.3.3055 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.3055) (commit https://github.com/onnx/onnx/commit/1b7498f427 by @walrusmcd)
tjingrant(2018-05-12 01:57:18):@walrusmcd In addition to the wording comments, I would suggest you don't try to draw a parallel between other types of denotations and dimension denotations. And I will outline my reason:

Dimension denotation is a unique subject whose complexity may go beyond TensorDenotation or TypeDenotation. The primary goal of dimension denotation is **not** the storage of meta data, but rather the **propagation and verification of the meta data**, which is no where near the goal of other types of denotations, you are wasting spaces trying to explain that the other meta datas are not propagated and verified. Furthermore, you are distracting readers who need to realize the complexity and purpose of dimension denotation. What you are interested in is a very specific sub feature of Dimension Denotation, so the best way to refer to Dimension Denotation is via a link to the original proposal.

Not to mention that, dimension denotation was the result of lengthy discussion, debate and deliberations (c.f., https://github.com/onnx/onnx/pull/443). Even the word `denotation` was carefully chosen to express the connotation of "literal meaning" or "meaning at its lowest level of abstraction", which is BTW an unnecessary word choice in this context and may make your proposal a bit less understandable to people whose mother tongue is not English. I would kindly ask you please do not move the original proposal around unless it is absolutely necessary.
AppVeyorBot(2018-05-13 21:10:49)::x: [Build onnx 0.3.3082 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.3082) (commit https://github.com/onnx/onnx/commit/e693a9c814 by @linkerzhang)
AppVeyorBot(2018-05-14 16:25:01)::x: [Build onnx 0.3.3094 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.3094) (commit https://github.com/onnx/onnx/commit/c9001e36b0 by @linkerzhang)
AppVeyorBot(2018-05-14 16:40:11)::x: [Build onnx 0.3.3095 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.3095) (commit https://github.com/onnx/onnx/commit/dace12a51c by @walrusmcd)
AppVeyorBot(2018-05-14 16:54:24)::x: [Build onnx 0.3.3096 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.3096) (commit https://github.com/onnx/onnx/commit/87abe5394a by @walrusmcd)
linkerzhang(2018-05-15 02:45:57):@bddppq @houseroad  please help to review and share your comments if any.
AppVeyorBot(2018-05-16 19:10:24)::x: [Build onnx 0.3.3259 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.3259) (commit https://github.com/onnx/onnx/commit/85a0419264 by @walrusmcd)
AppVeyorBot(2018-05-16 19:24:20)::white_check_mark: [Build onnx 0.3.3260 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.3260) (commit https://github.com/onnx/onnx/commit/eac3d059e4 by @walrusmcd)
AppVeyorBot(2018-05-23 01:35:24)::x: [Build onnx 0.3.3523 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.3523) (commit https://github.com/onnx/onnx/commit/d7077107c2 by @walrusmcd)
AppVeyorBot(2018-05-23 15:47:27)::x: [Build onnx 0.3.3537 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.3537) (commit https://github.com/onnx/onnx/commit/cfe8f6730c by @linkerzhang)
linkerzhang(2018-05-03 20:51:47):We'd put this denotation in TypeProto instead of Tensor so that other types (say Map, Sequence) will also be able to use it?
linkerzhang(2018-05-03 20:54:35):Merge all denotation documents into one file?
prasanthpul(2018-05-03 23:49:10):n is missing -> Denotation
prasanthpul(2018-05-03 23:50:27):can you call out that keys and values are case insensitive?
tjingrant(2018-05-04 15:55:59):NCHW format on a model level is not easily enforceable and thus brings about nightmares in terms of compliance checking. ONNX, as it stands today, only enforces node-level input format, which does not translate to any restrictions on model level format. C.f. my comment copied from the thread discussing Dimension Denotation:


1. > ONNX as a standard will keep as it is to support only NCHW.

    We need to be careful about what you mean. There are no operations supporting NHWC format in ONNX; however, that does not preclude models from supporting NHWC format input. In fact, you cannot preclude such use case as long as you have a **transpose** in the operator set because you can always transpose the input once and enter the NCHW world thereafter. Thus, unless you explicitly pattern match and prohibit these cases in the checker, there is no way to prevent a user from creating  ONNX **models** that accept NHWC input but uses NCHW **operators**. Thus it is my opinion that 
    > This means all ONNX models should be in NCHW format,

    does not hold true as an implication from your previous statement.
linkerzhang(2018-05-04 16:33:24):it's --> its ?
walrusmcd(2018-05-09 16:58:54):Yes, I had thought of that option as well.   I think the real challenge here is that we have Tensor as our base …. and all of these other things are sub classes of tensor.   Map and Sequence are first class citizens right now.   I imagine after this experiment, we would promote Image/Audio/Text to first class citezens as well.    Thus my though was to keep it on tensor to allow us to experiment for candidates to promote to first class types.    
Also, maps and sequences don't have the denotations we define in this first iteration.   A map cannot be an image/audio/text.   Nor can a sequence.   I think if we find candidates for denotation on map & sequence (Things that are common to all TypeProto's) ; we could add one then.
 thoughts?
walrusmcd(2018-05-09 16:59:33):I thought about that , but then it becomes on larger doc.     Preference / thoughts?

if you really like the idea I can take the time to merge them and see how it looks.   lmk.
walrusmcd(2018-05-09 17:00:25):good catch.   fixed in the the next iteration.
walrusmcd(2018-05-09 17:01:24):nice !   added to the next iteration.
walrusmcd(2018-05-09 17:02:06):fixed in next iteration
walrusmcd(2018-05-09 17:17:12):agreed.   with this we are not proposing enforcing or being strict about NCHW, but instead allowing enough metadata and denotation for a model builder to let a model consumer know what to do.    This PR is less about NCHW (dimension denotation talks about that).  This PR is more about the other things metadata talks about (bitmap pixel format, etc..etc..) .

thoughts?   Would you propose changing anything in the docs or strings for this PR ?  thx !
tjingrant(2018-05-12 01:17:52):Hi, sorry for the delay, my opinion is that SHOULD is too big a word in this scenario for reasons I outlined earlier. My suggestion is to switch to `may`.

And why did you capitalize it?
houseroad(2018-05-16 13:05:18):A typical SqueezeNet takes [1, 3, 224, 224] as input.
houseroad(2018-05-16 13:11:12):~~This doc is called metadata prop, so we should not introduce the additional experimental metadata here.
Let's move the introduction of the experimental metadata to metadata section in IR doc.~~

We can extract the useful pieces in this doc, and merge it into metadata section in IR.doc.

At least, we should have a better name for this section...
houseroad(2018-05-16 13:15:27):Why do we mention Dimension Denotation here?
houseroad(2018-05-16 13:17:09):I think instead of using type denotation, saving the following information in the metadata is more straightforward.
houseroad(2018-05-16 13:32:19):How does this work? Any example?
walrusmcd(2018-05-16 14:26:15):Good catch, I meant 3.  fixed.
walrusmcd(2018-05-16 18:42:26):I am adding an example to this MD, that shows why you actually need to use all three features (type denotation, dimenstion denotation, and metadata) .   all three are needed to make it work end to end.
walrusmcd(2018-05-16 18:43:30):We experimented with that approach first, but it's more semantically correct to *first* denote that the type is an IMAGE.  only then do you know to go look at the metdata to see how the model requires it's images.   If you had multiple inputs into the model, you need a type denotation to know *which* of those types is the image.
walrusmcd(2018-05-16 18:43:47):great catch !!  I think an example would be perfect here.   I'm adding one.
walrusmcd(2018-05-16 18:44:49):The example should help to show that we have 3 separate pieces here, and how to tie them all together.
lupesko(2018-05-20 19:48:08):nit: no need for a new line here
lupesko(2018-05-20 19:51:40):+1 the need to support multiple inputs is a good call
lupesko(2018-05-20 19:54:00):What about good old numerical tensors, for other tasks such as recommendations, forecasting, anomaly detection, etc?
Should we add another type TENSOR for those?
lupesko(2018-05-20 19:55:11):Please see my comment above about a generic tensor
houseroad(2018-05-22 00:12:34):In some cases, means for each channel is also needed to preprocess the input.
rednoah91(2018-05-22 15:00:03):Do you consider to add more metadata like "crop_size", "mean_value", or "scale"? (Just like caffe data layer "transform_param")
walrusmcd(2018-05-22 18:48:04):fixed !
walrusmcd(2018-05-23 01:07:18):Makes sense !    I added the 3 we had been using the most in our models (image/audio/text) and added explicit metadata for image.   I assumed that there would be follow up proposals and PR's as we add more "types" here.    What you thinking ?     Also, we do have the normal case where there is no denotation .  In that case the tensor is also a good old numerical tensor.     since denotation is optional, it still has all the fun stuff, like shape and type.

walrusmcd(2018-05-23 01:08:46):I was thinking we make denotation optional, and "generic" would be that there is no denotation at all.    Are you saying we should add a TENSOR to the constproto as the default/0 case so that you could provide a tensor denotation and say it is just a "tensor" ?  Assuming that , what do you think?   I'll add a GENERIC to this PR for completeness, but leave the denotation as optional.     Let me know what you think and if you are proposing we make the denotation required.   thanks !
walrusmcd(2018-05-23 01:12:04):How do you think that would look ?    Are you OK if we add this as a follow up PR later ?   I think this follows this model of adding more and more metadata as we find it to be useful.   love it !
walrusmcd(2018-05-23 01:13:23):Yes.   We considered this and did not go that route in that the goal was not to do the transformation for you, but instead provide the final values that the model expects.   This allows people using the model to perform their own transformation outside the model.    
lupesko(2018-05-23 06:43:19):Agreed.
lupesko(2018-05-23 06:45:44):nit: "a" not "an"
rednoah91(2018-05-23 08:09:11):I mean if the metadata contains the transformation information, the model user can know how to perform the transform that the model expects. For example, specify the mean value [123.68, 116.78, 103.94] for VGG net in the metadata in order to hint model user to do the image pre-processing.
walrusmcd(2018-05-23 17:59:15):fixed :)
CLAassistant(2018-05-03 21:52:16):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=881) <br/>All committers have signed the CLA.
houseroad(2018-05-03 23:23:24):We should merge this one with https://github.com/onnx/onnx/pull/874
souptc(2018-05-03 23:54:54):Oh, not aware there is already a pr on this. If we already plan to merge that, I am ok to close this one.
bddppq(2018-05-04 07:57:55):@souptc Since #874 has earlier PR creation date, I'm going to close this one, sorry!
@houseroad @anderspapitto Probably create a list like in #426 for shape inference?
AppVeyorBot(2018-05-03 23:44:17)::x: [Build onnx 0.3.2821 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2821) (commit https://github.com/onnx/onnx/commit/116646f970 by @linkerzhang)
AppVeyorBot(2018-05-03 23:54:29)::white_check_mark: [Build onnx 0.3.2822 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2822) (commit https://github.com/onnx/onnx/commit/2e85d07886 by @linkerzhang)
bddppq(2018-05-04 05:17:56):This PR needs to wait for https://github.com/onnx/onnx/pull/876 which bumps the opset_version since last release
AppVeyorBot(2018-05-04 05:25:15)::x: [Build onnx 0.3.2829 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2829) (commit https://github.com/onnx/onnx/commit/c0905b57d0 by @bddppq)
bddppq(2018-05-09 06:47:14):ping @linkerzhang @ebarsoum for reviewing schema changes
AppVeyorBot(2018-05-10 00:06:52)::white_check_mark: [Build onnx 0.3.2987 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2987) (commit https://github.com/onnx/onnx/commit/d94de0b863 by @bddppq)
wschin(2018-05-23 18:37:47):Without this PR, it's still possible to compose this attribute. See [here](https://github.com/onnx/onnxmltools/blob/ccf3acb629d90d4a5634252951c2ea530302cc03/onnxmltools/convert/coreml/operator_converters/neural_network/Pool.py#L83). Anyway, I am glad to see that ONNX accepts composed operators (for optimization reasons).
anirudhacharya(2018-05-07 21:45:02):Shouldn't this specify that the default value = 0.
bddppq(2018-05-07 22:28:24):@anirudhacharya It has been specified to default to zero in the schema, but our doc doesn't show that (we probably should, need someone working on correctly rendering AttributeProto in the markdown).
ebarsoum(2018-05-09 20:14:22):Should we add something similar to MaxPool? So if all elements are negatives out zeros.
bddppq(2018-05-12 04:10:32):Yeah for the sake of leaving no ambiguity, let's add that as well (in a separate PR).
bddppq(2018-05-04 07:52:58):needs to set `broadcast=1`
houseroad(2018-05-07 15:40:04):This case is a little bit tricky on broadcasting... which is not aligned with our current broadcasting rule in other ops...

And in Gemm's doc there is no description on how to do the broadcast.
anirudhacharya(2018-05-07 18:40:21):Yes, I just noticed that. I went by the mxnet standards on broadcasting -
```python
import numpy as np
import mxnet as mx
e=mx.nd.array(np.random.rand(4,5))
f=mx.nd.array(np.random.rand(1,1))
mx.nd.broadcast_add(e,f)
```

would it be possible to expand the definition of broadcast in ONNX. It would be useful.
houseroad(2018-05-07 20:27:19):Yeah, I know, we are also about to move to numpy style broadcast. Let's first move the 1-element case aligned with numpy. https://github.com/onnx/onnx/pull/902
bddppq(2018-05-07 22:30:04):The broadcast clarification/standardization in the schema probably should not block adding these tests?
houseroad(2018-05-08 06:05:49):The only concern is that, in our current broadcast solution, this case is not allowed. We have to relax the 1-element case a little bit to support this case.
houseroad(2018-05-24 07:55:59):@fumihwh could you add type info to the new cases?
bddppq(2018-06-02 04:32:30):@fumihwh @houseroad Just wanna double check, are the test data of opset 6 or opset 7?
fumihwh(2018-06-02 08:57:19):@bddppq I didn't get your point. I think ConvTranspose has only opset 1..
bddppq(2018-06-02 15:43:26):@fumihwh When exporting the node test cases, they are wrapped as a ModelProto (with its graph contains only one single node), I was asking the what's the opset version encoded in the new ModelProto added in this PR. I guess this doesn't matter as you said ConvTranspose hasn't been changed since version 1.
houseroad(2018-05-04 19:31:03):@bddppq now i just don't output opset_version, the model is still the newest one.
zasdfgbnm(2018-05-04 19:55:50):I think check_model.ipynb, load_model.ipynb, and optimize_onnx.ipynb also print opset version. 
AppVeyorBot(2018-05-04 19:56:50)::x: [Build onnx 0.3.2854 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2854) (commit https://github.com/onnx/onnx/commit/3a1e4a7200 by @houseroad)
houseroad(2018-05-04 22:34:20):@zasdfgbnm but their model's opset version won't change when bumping up opset in the master.
bddppq(2018-05-07 17:53:44):CI failure unrelated
bddppq(2018-05-04 19:06:56):This is should declared as an option at the top of the cmake file as well (see ONNX_BUILD_PYTHON).
anderspapitto(2018-05-04 21:04:25):if this pattern shows up a third time it should probably be pulled out into a function
gramalingam(2018-05-04 20:56:14):add ".TypeAndShapeInferenceFunction(propagateShapeAndTypeFromFirstInput)" (anyway we are probably going to see this in a merge conflict!)
linkerzhang(2018-05-06 16:33:27):fixed.
CLAassistant(2018-05-04 22:41:42):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=894) <br/>All committers have signed the CLA.
bddppq(2018-05-05 07:38:19):Please resolve merge conflict (and make sure the CI passing).
bddppq(2018-05-05 07:46:42):>Second, it avoids a pattern of static object usage which can cause downstream compilation and performance issues.

Could you elaborate?
bddppq(2018-05-16 00:40:56):@jeffbloo @linkerzhang @shschaefer @dzhulgakov How about we move the schema instantiation from global static variables to a function static variable in the `map()` function? i.e. something like:
```
class SchemasRegisterer {
  SchemasRegisterer() {
   // AUTOGENERATED BEGIN
    _map.insert(std::make_pair("Aaa", OperatorSchema(Aaa)....));
    ...
   _map.insert(std::make_pair("Zzz", OperatorSchema(Zzz)....));
   // AUTOGENERATED END
  }
}

Map* map() {
  static auto SchemasRegisterer{};
  return _map
}
```
This way we only do the initialization at the first usage time, and there is no need for an Init and it's thread safe. One downside is this will make the registry a close registration, all user schemas will need to either register dynamically or register into a separate registry.
To achieve this we will need to move the source of truth of the schemas to say yaml files (instead .cc files) and then have a script to auto-generate the chunk between `AUTOGENERATED BEGIN` and `AUTOGENERATED END` above.

jeffbloo(2018-05-16 03:53:41):@bddppq - It's possible to populate the map by using the same couple code lines as added to cpp2py_export.cc.  Is it your goal to make this change non-breaking, so that updates are not required in any callers?

Another downside to that change would be coupling the policy of whether to use OpSchemaRegistry with the policy of which operator sets and versions will be used, in which registry.  

This could also make it less clear whether other code in the ONNX library should rely on this map.  For a similar reason, I was considering removing OpSchemaRegistry's internal statics altogether, and making callers create their own static instances of OpSchemaRegistry and share them as needed.  This is more disruptive to calling code though.

I'd prefer to keep this simple and define operator sets directly for now.  Code generation could be a more involved change if designed holistically (considering the most efficient format to generate, how to best handle inferencing callbacks, etc.).   The iteration routines in the existing PR also provide more flexibility for the library consumer, without requiring them to use a code-gen step.


AppVeyorBot(2018-05-16 20:29:20)::white_check_mark: [Build onnx 0.3.3265 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.3265) (commit https://github.com/onnx/onnx/commit/09dbbed05b by @jeffbloo)
AppVeyorBot(2018-05-16 20:42:43)::x: [Build onnx 0.3.3266 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.3266) (commit https://github.com/onnx/onnx/commit/c407ad9229 by @jeffbloo)
AppVeyorBot(2018-05-16 23:17:13)::white_check_mark: [Build onnx 0.3.3278 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.3278) (commit https://github.com/onnx/onnx/commit/8850aa477b by @jeffbloo)
jeffbloo(2018-05-18 01:01:26):The latest update works around a bug in older GCC compilers that the build hit.  Raw strings inside macros were not handled correctly.

I also removed usages of std::abort to avoid affecting terminating a whole process, and added a conditional to allow large doc strings to be compiled out to save memory.  
bddppq(2018-05-18 07:52:43):Also what's the strategy for users adding their own schemas? As mentioned before, moving the schemas instantiation into function static variable makes the registration closed.
jeffbloo(2018-05-18 22:32:06):@bddppq - As to binary size, this was indeed a slight increase.  Measured on our compiler, about 85K larger while optimizing for speed, and 55K for size.  This is without using the new macro to remove doc strings.

A lot of bloat is related to constructing STL objects before each registration call.  I'm fixing this by simply wrapping the OpSchema methods consuming STL types.  This will make the size 175K smaller than the master branch, and another 56K smaller if removing doc strings.
AppVeyorBot(2018-05-19 00:32:30)::white_check_mark: [Build onnx 0.3.3383 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.3383) (commit https://github.com/onnx/onnx/commit/3d2588a098 by @jeffbloo)
linkerzhang(2018-05-05 14:40:09):Generally, I'm nervous about adding this kind of option, similar with the one we added for ONNX_ML. it makes the code diverge and different partners focus on each option. It's generally hard to maintain. for example, enable CI for with different option values. For such divergence, either all partners agree on same direction, or put the specific stuff in local code base.

@jeffbloo is going to update the PR without the diverge option. Thank you!
shschaefer(2018-05-14 19:14:08):I agree that having different patterns is not ideal.  However, not all vendors may be able to leverage the code in its current state, but the code contains a lot of necessary information.  To answer the question above @bddppq, control of the timing of initialization is important to manage potential startup cost for the compilation unit.  As we are looking at this, the code can be brought into many different processes, some where deferring this cost is important.
bddppq(2018-05-14 19:20:58):@shschaefer Could you provide some benchmark numbers about the startup cost?
shschaefer(2018-05-14 20:46:36):For non-framework scenarios, when this code is loaded as part of an ONNX library, it can be consumed by numerous processes.  We do a lot of work to eliminate this overhead, delay initialization until the library is used, etc...  The dynamic initializer makes this code hard to consume.  Not sure if providing benchmarks changes this.
bddppq(2018-05-14 22:01:58):@shschaefer I'm asking benchmark numbers mainly because I think the overhead of these static initialization should be negligible.
bddppq(2018-05-14 22:06:52):Actually could you elaborate how does it work? Where is this `GetOpSchema` declared and how is user code supposed to call it?
jeffbloo(2018-05-15 01:09:41):I measured that registering schema consumes 4 milliseconds and increases memory consumption by 1.1 MB.  This is very non-negligible for a single component, especially if it may be loaded into many processes.
jeffbloo(2018-05-15 01:18:14):GetOpSchema is declared slightly above this.  Macro call sites define its specializations.  

Based on the feedback here, I'm preparing a change to remove the original mode of registration during static initialization.  This will include a new file with related declarations for each schema, and functions which retrieve and/or register the schema of specific operator sets.
bddppq(2018-05-18 07:29:34):What's the reason of moving this string outside? This makes the schema (in c++ files) less readable.
bddppq(2018-05-18 07:33:17):most schemas are of the onnx core domain. can you change this to a helper macro and leave `ONNX_OPERATOR_SCHEMA` dedicated to the onnx core domain? For ONNX_ML I think `ONNX_ML_OPERATOR_SCHEMA` can be used.
bddppq(2018-05-18 07:34:34):Could you not clang-format the unchanged part? It's really hard to review. :-)
bddppq(2018-05-18 07:36:02):Need to guard this include with ONNX_ML for non ONNX_ML builds
bddppq(2018-05-18 07:37:44):So this file should be autogenerated, it's so easy to forget to update when adding new schemas and also unfriendly for new contributors.
bddppq(2018-05-18 07:39:23):Can we remove this variant? It's not happening at initialization time already, what's the concern?
bddppq(2018-05-18 07:40:54):Can you separate the doc string stripping change to anther PR? I prefer not to let these two discussions don't slow down each other. Also according to our previous discussion in #623, removing doc strings really doesn't save you too much (~50kb **uncompressed** size, <10kb **compressed**)
jeffbloo(2018-05-18 19:50:06):I can do that
jeffbloo(2018-05-18 19:50:11):OK
jeffbloo(2018-05-18 19:50:14):The headers had internal conditionals, but I can add them here.
jeffbloo(2018-05-18 19:53:59):I see a similar gain - 56 KB.  I also have to make the strings static and avoid consuming onnx.lib with whole-archive (which is now possible with this change).  I think this is worthwhile for such an isolated change.  Savings like this one add up.
jeffbloo(2018-05-18 19:59:17):In either case, the operators won't be available or get generated into documentation, so I don't think this will be missed.  

Not requiring code-gen steps is simplifying in itself.  While I can imagine other benefits if we combine code-gen with other refactoring, I don't want to block this PR on that larger project.
jeffbloo(2018-05-18 20:08:22):This was intended to allow the static registry to be used by a caller which wants to constrain it to specific operator sets.  For example, to deprecate an older operator set or withhold others.  
jeffbloo(2018-05-18 22:35:18):This applies to your question below about custom schema. They may also implement ISchemaRegistry.
bddppq(2018-05-19 00:40:44):This will be missed. We can't rely on reviewers to remember how many docs changes they need to see so the changes are legit, the only viable way to guard such boilerplate changes is adding a check in the CI. Also think about new coming/occasional contributors, it's impossible for them to remember.
bddppq(2018-05-19 00:46:12):What's the total size in your measurement? If it's the same as in the previous PR, the total size is around 8MB, which means 50 KB is really not worth adding this complexity into the code base (note as you have found this not only needs to add those macros, but also require moving couple strings and changing some function signature in the OpSchema class, and in some code you will need to add guards preventing accessing the doc strings).
In any case, I feel this really should be a separate discussion, could you split it out into a separate PR.
bddppq(2018-05-19 00:52:30):Could you elaborate what's the purpose of constrain the registration to a specific operator sets? If it's for overriding an operator definition in the onnx core I'm pretty against it.
jeffbloo(2018-05-19 01:05:57):This is the workaround I mentioned for a GCC bug.  Raw strings in macros were busted.
https://gcc.gnu.org/bugzilla/show_bug.cgi?id=57824
jeffbloo(2018-05-19 01:44:57):The baseline size was around 2.5 MB.  In the context of a larger product this isn't as relevant.  Components of different sizes sharing a common budget should be equally judicious with size deltas, and this is an easy win.   It also reduces heap usage, which is multiplied across every single process loading the binary.

Doc strings are optional, so guards aren't necessary.  The string movements were necessary for different reasons - the GCC bug workaround.  The signature changes also also needed for other reasons - minimizing static STL string initialization and avoiding instruction bloat in the macro-defined functions.

jeffbloo(2018-05-19 01:56:22):No problem.  It's not actually intended for overriding ones that exist with another added to the same map.  One goal is for a caller to have more control over the contract it exposes to its own customers.  This naturally includes the operator versions that it supports, when it introduces them, and if it eventually ends support for them.

Another goal is to enable callers to handle certain operator sets outside the core library, based in part on its strategy for deploying updates.  

jeffbloo(2018-05-19 02:00:16):Our thinking was that exposing operators which were never tested was not as important, but I understand wanting to make this easier to discover.

I'll see if I can cleanly add check in debug mode that the number of registration macro calls matches the total of schema exposed by the iteration methods.  Does that sound reasonable?

jeffbloo(2018-05-20 23:44:00):I've made that change.  This also fixes a pre-existing bug in assert implementations which caused them to always pass.
bddppq(2018-05-23 01:13:57):Could you preserve ONNX_DOMAIN as well?
bddppq(2018-05-23 01:16:04):In other places (e.g. checker and shape inference) we define a custom exception class inherit from std::runtime_error and macros (e.g. `fail_check`, `fail_shape_inference`).
CLAassistant(2018-05-06 07:22:24):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=896) <br/>All committers have signed the CLA.
linkerzhang(2018-05-07 06:12:32):This is an auto-generated document. Please fix the description in the related defs.cc file and run /onnx/defs/gen_doc.py for updating this document. Thank you!
jsenellart(2018-05-07 07:18:08):ok - thanks. just did it!
CLAassistant(2018-05-16 00:11:28):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=897) <br/>All committers have signed the CLA.
AppVeyorBot(2018-05-16 18:35:16)::x: [Build onnx 0.3.3258 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.3258) (commit https://github.com/onnx/onnx/commit/6f29ddc979 by @linkerzhang)
bddppq(2018-05-17 02:44:55):@wchao1115 There are windows line ending in onnx/defs/generator/defs.cc.
AppVeyorBot(2018-05-18 16:25:26)::white_check_mark: [Build onnx 0.3.3356 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.3356) (commit https://github.com/onnx/onnx/commit/3bc59cfbc5 by @linkerzhang)
bddppq(2018-05-12 04:28:44):input_type and output_type can be `nullptr`
bddppq(2018-05-12 04:30:59):This can't be true for a valid Operator. I suggest shape inference code don't do validation and always operate on the assumption that the model file has passed the checker. (At least here don't silently return, it should raise some errors or similar) @anderspapitto @gramalingam What's the error reporting mechanism in shape inference?
bddppq(2018-05-12 04:31:35):I'm curious does compiler complain if you don't static_cast<int>?
bddppq(2018-05-12 04:39:20):How is the backend supposed to know which output type it should take?
bddppq(2018-05-12 04:40:39):Please clarify the meaning the values in this input tensor.
linkerzhang(2018-05-13 21:09:33):About error handling in shape inference, I'd suggest to add "Status" as return value. @anderspapitto  and @gramalingam  let me know whether you're OK on it or not please. The "Status" class was added recently. I had ever given @anderspapitto this proposal (using status as return value) when the interface was checked in.
gramalingam(2018-05-13 22:49:22):I think returning a Status code is reasonable. While I agree that it is reasonable to assume that the model file has passed the checker before inference is invoked, it is still a good idea to safeguard the inference code against null-dereference etc. (E.g., this ensures that even if the op signature is changed to make an attribute optional, we don't fail with a null dereference.) Shape inference is (currently) viewed as optional (for optimization): there will be models where shape inference won't successfully infer shapes. For this reason, returning silently is a reasonable compromise for shape inference (in such edge cases). At least, that's my personal opinion.
gramalingam(2018-05-13 22:55:55):You can use getAttribute(ctx, "sample_size", -1) and return if the value is negative.
gramalingam(2018-05-13 22:57:19):Yes, this is not clear to me either. If we want both, we probably should add a "dtype" attribute that specifies the output type
bddppq(2018-05-14 03:21:08):@linkerzhang @gramalingam Using status code sounds good to me.

@gramalingam @anderspapitto There are differences between cannot successfully do full graph shape inference due to missing clues vs. invalid model file. And IMO it's reasonable to silently return in the former case while for the latter it's not. The reason I advocate this is adding validations into the shape inference code will make it more complicated and introduce duplicated logic into both shape inference and checker, which will make the onnx core code hard to maintain in the long run. E.g. in this specific case, after checking the existence of a required attribute, should this code also check whether the attribute is of the correct type (i.e. `[int]`)? If you feel unsafe to rely on users to invoke checker before invoking shape inference, we can insert the `check_model` call  into the shape inference engine.
anderspapitto(2018-05-14 18:20:07):@bddppq do we have a list or documentation somewhere of what guarantees the checker provides about the model?
anderspapitto(2018-05-14 18:25:42):add some test cases?
bddppq(2018-05-14 18:32:27):@anderspapitto Nope, but it's a good idea to document this. 
houseroad(2018-05-15 00:31:09):Shape inference test will also be helpful here.
wchao1115(2018-05-16 00:23:55):I'm collaborating with linkerzhang on this change. I've updated the checks in the type inference code in my new revision but did not make it return Status. Looking at other op schemas, it appears no other have done that (returning Status). If that is needed, I think it should be done for the entire set of op schemas and not specifically for just multinomial and it should be done in a separate PR as I feel that change is probably out of scope for this PR.
wchao1115(2018-05-16 00:24:46):I've removed the static_cast in the new revision. Built fine.
wchao1115(2018-05-16 00:25:12):I've added the dtype attribute to select the output data type as suggested. I've also updated the comments for both the input and output in my new revision.
wchao1115(2018-05-16 00:25:24):Fixed.
wchao1115(2018-05-16 00:26:56):This is fixed in the new revision.
wchao1115(2018-05-16 00:38:20):As for the backend test case, I've looked at it and realized that this is a generator op. Although I think one could implement an AB testing comparing the result of a random function against another (say std::mt19937), it would be too stringent to assume that all backend implementation must conform to a specific algorithm.

anirudhacharya(2018-05-16 21:18:58):this is a new operator, It has not been available since op_set version 1.
anirudhacharya(2018-05-16 21:37:40):why isn't this experimental?
anirudhacharya(2018-05-16 21:38:45):sample size should default to 1
wchao1115(2018-05-16 23:58:06):Fixed default sample size to 1.
bddppq(2018-05-17 02:46:11):The default value can be specified in the schema here.
bddppq(2018-05-17 02:49:10):`sample_size` is an optional attribute and so here you can possibly get a `nullptr`
bddppq(2018-05-17 02:59:48):Why log-probability here (instead of just the plain probability)?
wchao1115(2018-05-17 16:59:59):@linkerzhang produces this. I think simply by running gen_doc. Ke, can you look into this feedback? I somehow had problem generating docs in my machine with gen_doc.py. It decimated the doc as a result but without an error. 
wchao1115(2018-05-17 17:01:33):Why shouldn't it be real if there is a clear use case for it in a model? This operator is requested by Unity. @prasanthpul can you help facilitate this conversation? 
prasanthpul(2018-05-17 17:39:04):@anirudhacharya generally ops are added as experimental because their use or value is uncertain. for example it could be an op that is only used in 1 framework or an op that potentially could be decomposed into other ops without a perf hit. In this case, the op is supported by multiple frameworks such as TF, PyTorch, mxnet, etc. and can be considered a primitive. So the proper course is to add it as a core op - similar to how the trig ops were added recently.
linkerzhang(2018-05-17 18:00:03):fixed.
linkerzhang(2018-05-17 18:00:07):fixed.
linkerzhang(2018-05-17 18:00:21):good catch. fixed.
wchao1115(2018-05-17 19:22:44):sample_size is not optional. I had null check before and, unless I'm mistaken, one of your earlier feedbacks was to remove that check because it's not possible to get null from a non-optional attribute. 
wchao1115(2018-05-17 19:23:01):I have no problem with Ke adding the check back in.
bddppq(2018-05-18 07:57:39):@wchao1115 It is optional, you currently have set a default value for it, which defines how should the backend should interpret when it's missing. My previous comment was because at that point it was required, but after adding default value, it becomes optional.
AppVeyorBot(2018-05-07 06:20:45)::white_check_mark: [Build onnx 0.3.2899 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2899) (commit https://github.com/onnx/onnx/commit/5b28ba698b by @linkerzhang)
bddppq(2018-05-08 05:44:36):@onnxbot retest this please
linkerzhang(2018-05-09 14:20:21):Exception thrown from static initializer codes is not useful. As https://github.com/onnx/onnx/issues/696 pointed  out, ideally, we should avoid using static variable for registration. That is the pr#894 trying to do (ONNX_NON_STATIC_SCHEMA_INITIALIZATION defined case). 

Either this PR or the pr#894 (change the pr to only have one case instead of having two options) will ask caller to call a function to init registration. 

It is good to not cover thread safe in this case in onnx. Caller should handle it in a better way. Covering it in onnx enforces a lock for every caller, which is not good. Say caller wants to do the init during runtime start up, it could be a call without lock.

Make sense?

Of course, if the pr #894 (ONNX_NON_STATIC_SCHEMA_INITIALIZATION defined case) is agreed, we should go for that.

Let me know your thoughts please. 
bddppq(2018-05-09 19:14:38):@linkerzhang No #894 will make the code super hard to maintain (not to mention playing with macros is really not fun).
So errors in schema registration phase should rarely happens (for op schema it only happens when we accidentally declared duplicated schemas), and when error happens it's an undefined behavior in onnx. If we can not afford abort during this phase, how about let's just log to stderr and keep going?
linkerzhang(2018-05-11 23:03:57):#894 could be discussed in-place in the PR.

@bddppq  having an init() function makes the order of doing initialization clearly, which was kind of discussed and agreed in #696 . logging the error and keep going is good for other apps except windows, I think :), although I agree with you that rare error cases actually, just in case there's one.
bddppq(2018-05-11 23:23:57):@linkerzhang No I don't think #696 is a good idea, I have put my concerns there from the very beginning. And although it's true adding `init` can control the order of doing registration, my argument was there is no need for it. Also it's worth pointing out, adding `init` will be an api level breaking change.
bddppq(2018-05-11 23:26:38):Another choice would be having separate registries for onnx core schemas and user defined schemas, this can hugely improve the guarantee of no duplications.
linkerzhang(2018-05-11 23:56:14):@shschaefer may share more thoughts on having a deterministic order of initialization please. Thank you!
linkerzhang(2018-05-13 21:05:23):btw, @bddppq  to have all the function proto cached, it's also good to have an init function :). Otherwise, the initialzation logic will be put in the getter covered by a "if" statement, which is not good.
bddppq(2018-05-14 03:34:55):@linkerzhang Well FunctionProto instantiation can be done the same way as op schema registration (i.e. the FunctionProto will be instantiated and put into the registry during initialization time), in that case there will be no need for caching or Init.
If you really insist to make the schema's initialization order predictable, we can archive it by putting the schemas meta data into yaml files and then write a script to generate one single .cc file that has the c++ part of all the op schemas in desired order, the instantiation order of static variables in one single compilation unit is deterministic.
houseroad(2018-05-07 21:30:22):Travis CI is broken. Could you lint your code and fix the CI?
houseroad(2018-05-07 21:30:27):https://travis-ci.org/onnx/onnx/jobs/376069894
gramalingam(2018-05-07 22:18:07):Sorry about that. So, I guess the Travic CI checks are different from the checks before merge is enabled? What else can I check before merging changes in (to avoid this in the future)?
ffk0716(2018-05-07 08:44:56):The root cause is FindProtobuf.cmake take uppercase vairable as old style and the backwards compatibility does not update cache variable.

first find_package(): PROTOBUF_INCLUDE_DIR -> Protobuf_INCLUDE_DIR(not in cache)
first find_package(): mark_as_advanced(Protobuf_INCLUDE_DIR)
second find_package(): find_path(Protobuf_INCLUDE_DIR) <- Protobuf_INCLUDE_DIR has been cleared

ffk0716(2018-05-15 09:25:28):hello?
raymondxyang(2018-05-15 21:02:37):LGTM.. Thanks for the contribution 👍 
AppVeyorBot(2018-05-07 22:57:27)::white_check_mark: [Build onnx 0.3.2921 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2921) (commit https://github.com/onnx/onnx/commit/0be8c898bf by @houseroad)
houseroad(2018-05-09 17:49:57):ongoing PR is here for the record: https://github.com/onnx/onnx/pull/907
houseroad(2018-05-10 05:55:58):@anirudhacharya @ebarsoum @linkerzhang ping for review :-)
anirudhacharya(2018-05-12 02:01:09):This change should be shown in the documentation of the following operators too, which also have broadcast attribute - AND, Equal, Xor, Greater, Lesser, and GEMM.
bddppq(2018-05-13 05:02:32):Repeatedly putting this chunk in each relevant op seems cumbersome. Shall we put the description of our broadcasting rule as a separate doc, and then refer to it in each op? This can also help keeping the broadcast semantics consistent across all relevant ops.
anirudhacharya(2018-05-13 05:46:55):sounds good.
houseroad(2018-05-18 14:36:47):Yeah, I did so in the other PR.
houseroad(2018-05-18 23:44:23):@anirudhacharya do you want to approve this PR? We will solve the repeated broadcasting text in the other PR #907, otherwise we cannot check in the Gemm test cases.
anirudhacharya(2018-05-19 23:14:12):@houseroad approved.
AppVeyorBot(2018-05-07 22:44:52)::x: [Build onnx 0.3.2920 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2920) (commit https://github.com/onnx/onnx/commit/da3b06197f by @houseroad)
houseroad(2018-05-16 01:22:44):https://github.com/onnx/onnx/pull/965 clarify the spec.
houseroad(2018-05-21 12:11:58):@anirudhacharya friendly ping, we have updated LRN's spec, do you mind updating the pr accordingly?
anirudhacharya(2018-05-21 21:37:19):@houseroad will do.
anirudhacharya(2018-05-26 01:05:28):@houseroad ping for review
onnxbot(2018-05-29 20:40:46):Build started sha1 is merged.

houseroad(2018-05-30 14:42:39):Lint the code please: https://travis-ci.org/onnx/onnx/jobs/385386568
anirudhacharya(2018-06-02 01:53:45):@houseroad ping
gramalingam(2018-06-06 06:17:32):@anirudhacharya : a question: the ONNX spec for LRN mentions the upper-limit of the local region as min(C - 1, c + ceil((size - 1) / 2) - 1), while the test case at https://github.com/onnx/onnx/blob/master/onnx/backend/test/case/node/lrn.py uses min(4, c + int(math.ceil((nsize - 1) / 2)) + 1) … that is, + 1 instead of - 1. Can you please clarify?
anirudhacharya(2018-06-06 18:24:10):@gramalingam The spec should have the region as the following - 
``max(0, c - floor((size - 1) / 2)) <= i <= min(C - 1, c + ceil((size - 1) / 2))``  Even the alexNet paper uses this region for computing LRN. 

In fact if the local region is defined as ``max(0, c - floor((size - 1) / 2)) <= i <= min(C - 1, c + ceil((size - 1) / 2) - 1)`` then the local region will not be symmetric about the ``i``th element.

The spec has a ``-1`` probably due to oversight. I can make a PR to correct it.

My test case has a +1 because numpy indexing is upper-bound exclusive.
gramalingam(2018-06-06 19:43:33):Yes, the spec seems to have an erroneous "-1" … otherwise, even the description of "size" as "The number of channels to sum over" does not match. It would be great if you can update the spec. Thanks.
houseroad(2018-05-29 12:44:21):default is float64, please convert it to float32.
houseroad(2018-05-29 12:44:50):default is float64, please convert it to float32.
ke1337(2018-05-08 18:22:38):Should we also fix other ops? I have a proposal [here](https://gist.github.com/KeDengMS/76337ca350aaa222318a3dda47a6839a) for Add/Div/Mul/Pow/Sub, Equal/Greater/Less, And/Or/Xor, PReLU, Gemm.
houseroad(2018-05-08 18:29:28):WIP ==> work in progress, don't worry, we will cover all these cases :-)
snnn(2018-05-08 18:59:13):Please also add node test cases to reflect this change. 
houseroad(2018-05-08 20:05:13):@snnn sure, will and have to do that, otherwise, CI won't be green. :-)
AppVeyorBot(2018-05-17 08:11:19)::x: [Build onnx 0.3.3296 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.3296) (commit https://github.com/onnx/onnx/commit/40f614df38 by @houseroad)
AppVeyorBot(2018-05-17 09:21:48)::x: [Build onnx 0.3.3297 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.3297) (commit https://github.com/onnx/onnx/commit/4ee9ad54b1 by @houseroad)
AppVeyorBot(2018-05-17 09:34:14)::x: [Build onnx 0.3.3298 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.3298) (commit https://github.com/onnx/onnx/commit/beddc3be9f by @houseroad)
AppVeyorBot(2018-05-17 10:40:11)::x: [Build onnx 0.3.3300 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.3300) (commit https://github.com/onnx/onnx/commit/baa3a4cb35 by @houseroad)
AppVeyorBot(2018-05-17 13:05:09)::x: [Build onnx 0.3.3302 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.3302) (commit https://github.com/onnx/onnx/commit/04c63b5658 by @houseroad)
AppVeyorBot(2018-05-17 14:25:09)::x: [Build onnx 0.3.3303 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.3303) (commit https://github.com/onnx/onnx/commit/f7a8c31168 by @houseroad)
AppVeyorBot(2018-05-17 14:38:32)::x: [Build onnx 0.3.3304 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.3304) (commit https://github.com/onnx/onnx/commit/463169a564 by @houseroad)
AppVeyorBot(2018-05-17 14:51:28)::x: [Build onnx 0.3.3305 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.3305) (commit https://github.com/onnx/onnx/commit/13a55ada69 by @houseroad)
AppVeyorBot(2018-05-17 15:06:24)::x: [Build onnx 0.3.3306 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.3306) (commit https://github.com/onnx/onnx/commit/3c6b5b5305 by @houseroad)
AppVeyorBot(2018-05-17 15:20:15)::x: [Build onnx 0.3.3307 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.3307) (commit https://github.com/onnx/onnx/commit/91e6f9d198 by @houseroad)
AppVeyorBot(2018-05-18 02:59:44)::x: [Build onnx 0.3.3340 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.3340) (commit https://github.com/onnx/onnx/commit/0f5aeb7682 by @linkerzhang)
AppVeyorBot(2018-05-18 14:37:29)::x: [Build onnx 0.3.3354 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.3354) (commit https://github.com/onnx/onnx/commit/edc317c559 by @houseroad)
AppVeyorBot(2018-05-18 15:31:10)::x: [Build onnx 0.3.3355 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.3355) (commit https://github.com/onnx/onnx/commit/bd593a8b9d by @houseroad)
AppVeyorBot(2018-05-19 09:38:05)::x: [Build onnx 0.3.3392 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.3392) (commit https://github.com/onnx/onnx/commit/25af0e0591 by @houseroad)
AppVeyorBot(2018-05-20 03:50:54)::x: [Build onnx 0.3.3396 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.3396) (commit https://github.com/onnx/onnx/commit/9438ec55cc by @houseroad)
AppVeyorBot(2018-05-20 11:20:31)::x: [Build onnx 0.3.3397 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.3397) (commit https://github.com/onnx/onnx/commit/ecabb1c29b by @houseroad)
AppVeyorBot(2018-05-20 11:36:47)::x: [Build onnx 0.3.3399 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.3399) (commit https://github.com/onnx/onnx/commit/9701f32076 by @houseroad)
AppVeyorBot(2018-05-20 12:07:10)::x: [Build onnx 0.3.3402 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.3402) (commit https://github.com/onnx/onnx/commit/9f9c87bd26 by @houseroad)
AppVeyorBot(2018-05-20 12:26:54)::x: [Build onnx 0.3.3403 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.3403) (commit https://github.com/onnx/onnx/commit/da2e0cd8ec by @houseroad)
AppVeyorBot(2018-05-20 12:47:26)::white_check_mark: [Build onnx 0.3.3404 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.3404) (commit https://github.com/onnx/onnx/commit/12a84b9c00 by @houseroad)
linkerzhang(2018-05-21 17:05:39):Thank you very much! @houseroad for pushing this forward! Overall it looks good to me. Some minor editing comments was given.
AppVeyorBot(2018-05-22 16:31:47)::x: [Build onnx 0.3.3479 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.3479) (commit https://github.com/onnx/onnx/commit/a6ef18258f by @linkerzhang)
AppVeyorBot(2018-05-22 16:46:49)::x: [Build onnx 0.3.3480 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.3480) (commit https://github.com/onnx/onnx/commit/b7e5c54b36 by @linkerzhang)
AppVeyorBot(2018-05-22 17:08:20)::white_check_mark: [Build onnx 0.3.3481 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.3481) (commit https://github.com/onnx/onnx/commit/ffa7a0f0b3 by @linkerzhang)
AppVeyorBot(2018-05-22 18:04:51)::white_check_mark: [Build onnx 0.3.3484 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.3484) (commit https://github.com/onnx/onnx/commit/2c8cd52481 by @linkerzhang)
AppVeyorBot(2018-05-22 21:22:04)::x: [Build onnx 0.3.3502 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.3502) (commit https://github.com/onnx/onnx/commit/1b6e60f8af by @linkerzhang)
houseroad(2018-05-22 23:57:34):@wchao1115 the unidirectional broadcasting rule is slightly different from numpy broadcasting. So we cannot simply refer to numpy rules.
AppVeyorBot(2018-05-23 01:46:38)::white_check_mark: [Build onnx 0.3.3524 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.3524) (commit https://github.com/onnx/onnx/commit/9be34bf499 by @houseroad)
linkerzhang(2018-05-23 03:56:51):Thank you very much! @houseroad 
anderspapitto(2018-05-23 16:56:01):@houseroad did you re-enable all the shape inference tests?
kit1980(2018-05-23 23:00:27):What about Min and Max? Currently, ONNX Min and Max don't support broadcasting at all, while numpy elementwise minimum/maximum have broadcasting.
linkerzhang(2018-05-21 16:25:47):following
linkerzhang(2018-05-21 16:45:45):In numpy, if the dimension value is not 1, then they should be the same, right? (common length means same length here?).
linkerzhang(2018-05-21 16:56:50):@gramalingam @anderspapitto  please be aware of that type and shape inference function for math ops need to be updated. it's removed in this PR :).
linkerzhang(2018-05-21 17:02:05):unidirectional broadcastable?
linkerzhang(2018-05-21 17:04:09):unidirectional broadcasting, I think.
anderspapitto(2018-05-21 17:21:18):specifically, the output shape is the same as the shape of A, right? If B is bigger than you can't do unidirectional broadcasting
anderspapitto(2018-05-21 17:32:40):@gramalingam are you interested in re-implementing these to account for the new broadcasting or shall I?
gramalingam(2018-05-21 18:02:50):@anderspapitto if you can, that would be great!
linkerzhang(2018-05-22 16:20:30):yes, I think.
houseroad(2018-05-22 23:48:02):yes
AppVeyorBot(2018-05-08 18:34:09)::white_check_mark: [Build onnx 0.3.2936 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2936) (commit https://github.com/onnx/onnx/commit/ce42b85323 by @anderspapitto)
bddppq(2018-05-08 17:54:54):I believe environment variable doesn't work, needs to be a cmake variable
snnn(2018-05-08 19:00:49):Would better be: "Treat warning as error"

I hope this option also works for Windows.
AppVeyorBot(2018-05-08 21:04:50)::x: [Build onnx 0.3.2941 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2941) (commit https://github.com/onnx/onnx/commit/d7968ce955 by @houseroad)
AppVeyorBot(2018-05-10 22:37:46)::white_check_mark: [Build onnx 0.3.3023 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.3023) (commit https://github.com/onnx/onnx/commit/4ee9fa4540 by @houseroad)
snnn(2018-05-08 21:42:36):Why not add a template argument to std::min? It would be safer is someone changed the type of "ends" in the latter.
AppVeyorBot(2018-05-08 21:30:39)::white_check_mark: [Build onnx 0.3.2942 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2942) (commit https://github.com/onnx/onnx/commit/e3370e270e by @smessmer)
bddppq(2018-05-08 22:50:19):Are these not unused imports?
bddppq(2018-05-08 22:58:27):it actually inherits from `Exception`
bddppq(2018-05-09 08:06:21):duplicated with line 15
bddppq(2018-05-09 08:07:23):why do we need to mark ignore here?
bddppq(2018-05-09 08:08:21):proto could be of type `bytes`
bddppq(2018-05-09 08:08:59):Move this above so `_serialize` can also use it?
smessmer(2018-05-09 19:57:33):because mypy doesn't support hasattr() checks yet and complains about the `f.read` (since Text doesn't have a .read member). But I removed it now and replaced it with a type cast, which is a bit less intrusive.
smessmer(2018-05-09 19:59:32):Not needed above. The reason for this is that _Proto makes _deserialize() a generic function. If you call it with ModelProto, it returns a ModelProto. If you call it with NodeProto, it returns a NodeProto. _serialize() above only uses the proto parmeter once, so default variance is doing the job.
smessmer(2018-05-09 21:57:47):fixed
AppVeyorBot(2018-05-11 02:17:35)::white_check_mark: [Build onnx 0.3.3028 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.3028) (commit https://github.com/onnx/onnx/commit/474cf37814 by @smessmer)
AppVeyorBot(2018-05-08 22:32:45)::x: [Build onnx 0.3.2947 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2947) (commit https://github.com/onnx/onnx/commit/4481c25159 by @smessmer)
AppVeyorBot(2018-05-09 00:28:10)::x: [Build onnx 0.3.2956 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2956) (commit https://github.com/onnx/onnx/commit/30a1581136 by @smessmer)
AppVeyorBot(2018-05-09 21:05:13)::x: [Build onnx 0.3.2973 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2973) (commit https://github.com/onnx/onnx/commit/11e312f932 by @smessmer)
AppVeyorBot(2018-05-09 23:18:25)::x: [Build onnx 0.3.2982 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2982) (commit https://github.com/onnx/onnx/commit/ef6cf09532 by @smessmer)
AppVeyorBot(2018-05-11 23:38:17)::x: [Build onnx 0.3.3045 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.3045) (commit https://github.com/onnx/onnx/commit/26ba3aa855 by @smessmer)
AppVeyorBot(2018-05-14 22:15:08)::x: [Build onnx 0.3.3121 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.3121) (commit https://github.com/onnx/onnx/commit/e9a04f68d8 by @smessmer)
AppVeyorBot(2018-05-14 22:25:25)::x: [Build onnx 0.3.3122 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.3122) (commit https://github.com/onnx/onnx/commit/bc5ec26bf2 by @smessmer)
AppVeyorBot(2018-05-14 23:07:28)::x: [Build onnx 0.3.3126 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.3126) (commit https://github.com/onnx/onnx/commit/3b562a1dc1 by @smessmer)
bddppq(2018-05-15 03:31:00):@smessmer Shall we close this one?
smessmer(2018-05-15 16:03:16):Yes, this was split into sub-PRs.
pranavsharma(2018-05-09 17:04:12):@linkerzhang @gramalingam can one of you approve? thanks.
AppVeyorBot(2018-05-17 10:27:01)::white_check_mark: [Build onnx 0.3.3299 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.3299) (commit https://github.com/onnx/onnx/commit/f7ad67e7e7 by @fumihwh)
AppVeyorBot(2018-05-17 10:54:55)::white_check_mark: [Build onnx 0.3.3301 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.3301) (commit https://github.com/onnx/onnx/commit/c0ee7c17b9 by @fumihwh)
AppVeyorBot(2018-05-19 15:34:36)::x: [Build onnx 0.3.3393 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.3393) (commit https://github.com/onnx/onnx/commit/88f45dc176 by @fumihwh)
AppVeyorBot(2018-05-22 02:53:57)::x: [Build onnx 0.3.3468 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.3468) (commit https://github.com/onnx/onnx/commit/a88545a49f by @fumihwh)
AppVeyorBot(2018-05-23 14:57:30)::x: [Build onnx 0.3.3536 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.3536) (commit https://github.com/onnx/onnx/commit/ec730aaec7 by @fumihwh)
AppVeyorBot(2018-05-23 16:24:46)::x: [Build onnx 0.3.3538 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.3538) (commit https://github.com/onnx/onnx/commit/8953279c84 by @fumihwh)
linkerzhang(2018-05-21 20:38:53):instead of having this step function/implementation, shall we have the output result explicitly set in the test case please? otherwise, I'm seeing this case is testing "test code" (this step function). 
pk-g(2018-05-21 21:01:50):Same as the other comment: Is there any specific reason why we are removing these parameters from the class and instead rely on local variables? this may put unnecessary limitations on reusing the values elsewhere.
pk-g(2018-05-21 21:16:22):Is there any specific reason why we are removing these parameters from the class and instead rely on local variables? this may put unnecessary limitations on reusing the values elsewhere.
pk-g(2018-05-21 23:43:26):Where are these numbers coming from and what is the need to have them here instead of relying on reference implementation? Using precomputed numbers may create unnecessary complications going forward. For instance, If a particular framework generates numbers that do not match these numbers in future, how can we resolve conflicts?
fumihwh(2018-05-22 02:25:32):@linkerzhang 
I didn't get you.
Basically, we need this numpy implemented function to calculate the output result. 
Maybe the name of function `step` is not good for you?

fumihwh(2018-05-22 02:37:03):I think we don't need to reuse `hidden_size`. All things (init b, p, h, c) need `hidden_size` is done when initialize class.
Even until last version doesn't have it.
The author of this version wanted to use `hidden_size` in step (to reshape the result) and put it in to class.

fumihwh(2018-05-22 02:38:48):It comes from pytorch.
Yes, make sense. I will remove this.
linkerzhang(2018-05-23 00:03:56):haha,  I was seeing this test case is testing the "step" function. I understood that it's used to generate the data. ignore this.
pk-g(2018-05-23 20:19:03):Thanks, sounds good.
AppVeyorBot(2018-05-11 15:36:01)::white_check_mark: [Build onnx 0.3.3032 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.3032) (commit https://github.com/onnx/onnx/commit/7852b572b9 by @liqunfu)
liqunfu(2018-05-14 17:13:10):@houseroad @bddppq, @ebarsoum kindly ping. This is to fix the expected output shape of RNN ops.  
AppVeyorBot(2018-05-16 18:03:19)::white_check_mark: [Build onnx 0.3.3257 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.3257) (commit https://github.com/onnx/onnx/commit/fbf59b0daf by @linkerzhang)
gramalingam(2018-05-16 22:37:03):Hi, can you please also check if the output type is correctly specified as "float" (when the inputs are "float")? The model that is currently in ONNX seems to have the type wrong (as "double" instead of "float"). I can't view the model produced by this PR, so haven't checked this yet.
liqunfu(2018-05-17 02:07:52):Hi @gramalingam  the mode produced have inputs/output being in float format. I also passed the test with the generated test model and data. Thanks.
Here is the model dump:  (node the output is Y_h instead)
Name
test_simple_rnn_defaults
Inputs
X: float[1,3,2]
W: float[1,4,2]
R: float[1,4,4]
Outputs
Y: float[1,3,4]


fumihwh(2018-05-17 10:26:18):`np.reshape(H, (self.num_directions, self.batch_size, self.hidden_size))`
This is not cool.
Luckily current test case has only one direction. Bidirectional will calculate separately and `concat` them finally.
And `H` already has shape `(batch_size, hidden_size)`, we don't need pass them to `step` by putting in `self`.
Anyway, now `reshape` is no problem in calculation but not correct.
AppVeyorBot(2018-05-09 21:17:28)::x: [Build onnx 0.3.2974 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2974) (commit https://github.com/onnx/onnx/commit/831bfc12ac by @anderspapitto)
AppVeyorBot(2018-05-09 22:12:51)::x: [Build onnx 0.3.2976 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2976) (commit https://github.com/onnx/onnx/commit/5cff571b9c by @anderspapitto)
AppVeyorBot(2018-05-09 23:26:12)::x: [Build onnx 0.3.2983 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2983) (commit https://github.com/onnx/onnx/commit/def1e6fe1e by @anderspapitto)
AppVeyorBot(2018-05-09 23:56:29)::x: [Build onnx 0.3.2986 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2986) (commit https://github.com/onnx/onnx/commit/a75cf635fe by @anderspapitto)
pranavsharma(2018-05-09 21:46:24):@linkerzhang @gramalingam please take a look. thanks.
gramalingam(2018-05-09 21:54:06):Looks good ... can you please resolve the conflicts? Thanks.
AppVeyorBot(2018-05-09 22:00:18)::white_check_mark: [Build onnx 0.3.2975 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2975) (commit https://github.com/onnx/onnx/commit/1c7f93cb8e by @pranavsharma)
AppVeyorBot(2018-05-09 23:48:22)::x: [Build onnx 0.3.2985 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.2985) (commit https://github.com/onnx/onnx/commit/55cf6bcefb by @pranavsharma)
pranavsharma(2018-05-10 02:20:56):@gramalingam please merge this after travis CI build is done. thanks. 
anderspapitto(2018-05-09 23:56:10):so, the process for generating and testing these things is kind of involved, but I have confirmed locally that this fixes the failure in the linked caffe2 diff
AppVeyorBot(2018-05-10 06:00:19)::white_check_mark: [Build onnx 0.3.3006 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.3006) (commit https://github.com/onnx/onnx/commit/19df5a00c4 by @houseroad)
AppVeyorBot(2018-05-10 08:04:43)::x: [Build onnx 0.3.3013 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.3013) (commit https://github.com/onnx/onnx/commit/40cffb5d7f by @jaliyae)
AppVeyorBot(2018-05-10 17:50:29)::x: [Build onnx 0.3.3017 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.3017) (commit https://github.com/onnx/onnx/commit/094aeea8b3 by @jaliyae)
AppVeyorBot(2018-05-11 02:08:02)::x: [Build onnx 0.3.3027 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.3027) (commit https://github.com/onnx/onnx/commit/93cf98a72d by @linkerzhang)
gramalingam(2018-05-10 21:10:02):I think this logic should be combined with lines 83-99 below: check if kernel_shape is available via attribute, and then determine whether we need 1 or 2 input shapes. (I think that the check on NumInputs below probably should be has-1-InputShape, right?)
gramalingam(2018-05-10 21:40:06):Sorry, it looks like you need the second input shape for the conv case even if kernel_shape attribute is specified. So, just choose 1 or 2 as required shapes based on require_kernel_shape
gramalingam(2018-05-10 21:45:52):doesn't match preceding comment
gramalingam(2018-05-10 21:47:00):Check if dim(0).has_value() first.
jaliyae(2018-05-11 21:35:51):Fixed.
jaliyae(2018-05-11 21:39:06):Change to   *output_shape->add_dim() = rios_shape.dim(0); it could be an inferred dim right?
bddppq(2018-05-13 04:55:17):Isn't that getInputType can return `nullptr`? @anderspapitto @gramalingam 
gramalingam(2018-05-13 05:00:58):The preceding hasNInputShapes check verifies that the inputType is nonnull.
AppVeyorBot(2018-05-10 18:19:54)::x: [Build onnx 0.3.3019 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.3019) (commit https://github.com/onnx/onnx/commit/02c6e44eab by @gramalingam)
AppVeyorBot(2018-05-10 18:35:57)::white_check_mark: [Build onnx 0.3.3020 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.3020) (commit https://github.com/onnx/onnx/commit/34881ef378 by @gramalingam)
anderspapitto(2018-05-10 21:29:35):is this the default indentation of your editor? I've never seen this style of `}    else if` before
anderspapitto(2018-05-10 21:31:24):I feel that dividing dimensions is pretty rare, and what to do when they don't divide evenly is not necessarily so clear and universal that it should necessarily be wrapped up into an operator.
gramalingam(2018-05-10 22:35:43):Probably not, but I was sticking to manual formatting to try to keep it similar to the code around. (If I were choosing, I would stick to this style, but I am happy to follow any specified style guidelines.)
AppVeyorBot(2018-05-10 19:20:48)::x: [Build onnx 0.3.3022 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.3022) (commit https://github.com/onnx/onnx/commit/2c05afc31c by @NiklasGustafsson)
AppVeyorBot(2018-05-13 21:30:49)::white_check_mark: [Build onnx 0.3.3083 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.3083) (commit https://github.com/onnx/onnx/commit/0be1988b1b by @linkerzhang)
prasanthpul(2018-05-15 17:58:49):@ezyang does this look good to you?
AppVeyorBot(2018-05-11 12:22:41)::x: [Build onnx 0.3.3031 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.3031) (commit https://github.com/onnx/onnx/commit/0e95814bbc by @ffk0716)
raymondxyang(2018-05-11 23:19:42):+ #933 for reference
AppVeyorBot(2018-05-12 04:04:42)::x: [Build onnx 0.3.3056 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.3056) (commit https://github.com/onnx/onnx/commit/710691b23b by @bddppq)
bddppq(2018-05-11 18:37:17):PROTO_SRCS got overridden to only have the onnx_operators.pb.cc now, this is the root cause of the current CI linkage error
ffk0716(2018-05-12 03:07:04):oh!! my mistake, sorry!
I will fix this!
AppVeyorBot(2018-05-11 20:29:09)::x: [Build onnx 0.3.3036 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.3036) (commit https://github.com/onnx/onnx/commit/a1dcca0cb4 by @gramalingam)
anderspapitto(2018-05-11 20:38:59):looks great! One complication is that there's a bug in the pytorch->onnx->caffe2 conversions that is causing the tests to fail. The bug is https://github.com/onnx/onnx/issues/921 and I'm actively working on it, but in the meantime we need to keep the tests green somehow
gramalingam(2018-05-11 21:36:34):Any suggestions what to do? If the bug is only LSTM (and not RNN/GRU), we could register the inference function for RNN/GRU and not for LSTM.
anderspapitto(2018-05-11 21:52:47):it's kind of annoying, because the tests are scattered across repos, and also anyone running pytorch export will hit this, even outside the tests.

I have two potential suggestions, does either work for you?

1) hold of on pushing this for a day or two until I fix the linked bug
2) add a try-catch around the invocation of checkShapesAndTypes in implementation.h, so that if an exception is thrown for RNN/LSTM/GRU, we simply continue. I will then remove this when I fix the bug.

and yes, it affects all three of RNN/LSTM/GRU.
gramalingam(2018-05-11 22:00:16):Waiting for one or two days is fine. But it would be good to get this into the 1.2 release scheduled for 15th.
AppVeyorBot(2018-05-15 04:48:55)::x: [Build onnx 0.3.3169 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.3169) (commit https://github.com/onnx/onnx/commit/bdc731221a by @gramalingam)
AppVeyorBot(2018-05-15 05:01:15)::x: [Build onnx 0.3.3170 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.3170) (commit https://github.com/onnx/onnx/commit/a75c72f2b8 by @gramalingam)
anderspapitto(2018-05-15 17:34:38):@gramalingam I've merged https://github.com/pytorch/pytorch/pull/7511, so as far as I'm aware the incorrect shapes are fixed, and everything should agree with shape inference now. If you notice anything still being off please let me know.
gramalingam(2018-05-15 17:36:05):Hi @anderspapitto , I noticed that you updated the tests in test/data/pytorch_operator. But there seem to be similar problems with test/data/node (the lstm/rnn/gru cases) ... 
anderspapitto(2018-05-15 17:46:13):@gramalingam well all tests are passing in master. It looks like in this PR there are still some errors saying "rank does not match" - could you figure out which inputs/outputs are failing to match, and whether it's to do with this PR or with some existing code?
gramalingam(2018-05-15 17:59:03):@anderspapitto if I look at the ONNX model in the folder https://github.com/onnx/onnx/tree/master/onnx/backend/test/data/node/test_simple_rnn_defaults .and view it, the output shape in the model seems incorrect. (Similarly for the other ops like LSTM.) I don't know where these models come from yet.
gramalingam(2018-05-15 18:04:48):I guess perhaps the model is generated from https://github.com/onnx/onnx/blob/master/onnx/backend/test/case/node/rnn.py ?
gramalingam(2018-05-15 18:09:46):@pk-g can you please take a look whether the output shape of https://github.com/onnx/onnx/blob/master/onnx/backend/test/case/node/rnn.py is consistent with the ONNX spec?
gramalingam(2018-05-15 18:20:11):Ok, I also notice an ambiguity in the spec description "It is optional if `output_sequence` is 0." for the first output. I guess this means that: if output_sequence is non-zero, the first output is Y (and the second output, if present, is Y_h). if output_sequence is zero, the first output is Y_h. At least, this is how I am going to interpret it ... please correct me if this is wrong.
anderspapitto(2018-05-15 18:31:49):I think we decided that optional outputs are still present but just contain empty tensors or something like that (so the outputs wouldn't shift) - not sure if we have it written down somewhere.

regarding the test, yes seems like @pk-g is the right person to comment on that - it's unrelated to the fixes I was making
anderspapitto(2018-05-15 18:32:48):"There are two ways to leave an optional input or output unspecified: the first, available only for trailing inputs and outputs, is to simply not provide that input; the second method is to use an empty string in place of an input or output name." - from https://github.com/onnx/onnx/blob/master/docs/IR.md
gramalingam(2018-05-15 18:44:13):@anderspapitto , your interpretation is the one I made. However, I checked our internal implementation and noticed it to be different. It would be good to double check. I don't want to rely too much on IR.md because that was Niklas' attempt at a rational reconstruction in the general case and we need to understand what was intended for this specific operation. Furthermore, with our original interpretation, the attribute "output_sequence" is redundant. @ebarsoum @bddppq can you please confirm which interpretation is right?

skottmckay(2018-05-15 22:58:30):For the three operators all outputs are optional, but only the first output explicitly says "It is optional if `output_sequence` is 0.". Should that statement be made for outputs, resulting in setting output_sequence to non-zero returning valid values in them all? That seems like more deterministic behaviour to me vs. maybe getting an empty string in some places. 
gramalingam(2018-05-16 00:00:20):If we are making a new choice, I would suggest dropping the "output_sequence" attribute, and the different outputs appear in fixed positions (0, 1 or 2) (using empty strings as in other ops). However, I am not so sure about breaking compatibility: if existing implementations all agree on some behavior, we could go with that behavior for compatibility.
skottmckay(2018-05-16 00:17:23):Y_h is redundant if Y is returned, so is output_sequence a flag that chooses which of these should be returned?

Not clear to me how to determine if Y_c in LSTM should be returned though. 
anderspapitto(2018-05-16 16:02:05):well, I would say that in the scope of this diff, let's just go with what's written in the spec - i.e. the outputs do not shift position. In the particular case of shape inference, when output_sequence == 0, that would mean simply don't infer anything for the first output, and infer as usual for the second and third.

If any one of us really wants to change this representation, let's have a separate discussion.

If you are really averse to committing to an interpretation here, then I would suggest to just bail out if `output_sequence == 0` for now so that we can get this merged, and then come back to it later.
anderspapitto(2018-05-16 16:04:36):@skottmckay the way the spec is written currently, Y_h must always be returned, even if it happens to be redundant with Y. 

Y_c is in a bit of a strange place - I believe there has separately been discussion about adding an additional output to LSTM which is the concatenation of cell states (i.e. <new output> is to Y_c as Y is to Y_h). But it doesn't affect this PR
skottmckay(2018-05-16 21:23:33):I thought Y_h was required in GRU v1 but was made optional in v3. 
Optional in LSTM and RNN from v1 on. 

        schema.Output(1, "Y_h",
                      "The last output value of the hidden. It has shape "
                      "`[num_directions, batch_size, hidden_size]`.", "T", OpSchema::Optional);

Is that not the case?
pk-g(2018-05-16 22:11:12):@gramalingam thanks for brining this up. With regards to the output shape mistmatch, as you correctly mentioned, it appears that with [Pull Request 923](https://github.com/onnx/onnx/pull/923) @liqunfu is providing the required fix.
AppVeyorBot(2018-05-17 06:07:08)::x: [Build onnx 0.3.3292 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.3292) (commit https://github.com/onnx/onnx/commit/caaf82edf4 by @gramalingam)
AppVeyorBot(2018-05-18 01:05:43)::x: [Build onnx 0.3.3330 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.3330) (commit https://github.com/onnx/onnx/commit/f2701b5a8f by @gramalingam)
AppVeyorBot(2018-05-18 01:17:13)::x: [Build onnx 0.3.3331 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.3331) (commit https://github.com/onnx/onnx/commit/198c199ec0 by @gramalingam)
AppVeyorBot(2018-05-18 20:14:39)::x: [Build onnx 0.3.3367 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.3367) (commit https://github.com/onnx/onnx/commit/6ad6c1d6c6 by @gramalingam)
AppVeyorBot(2018-05-18 22:07:23)::white_check_mark: [Build onnx 0.3.3376 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.3376) (commit https://github.com/onnx/onnx/commit/9b35d53ecb by @gramalingam)
AppVeyorBot(2018-05-18 22:28:29)::x: [Build onnx 0.3.3378 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.3378) (commit https://github.com/onnx/onnx/commit/dd776038e3 by @gramalingam)
AppVeyorBot(2018-05-18 23:04:27)::white_check_mark: [Build onnx 0.3.3379 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.3379) (commit https://github.com/onnx/onnx/commit/f1f97b15ce by @gramalingam)
bddppq(2018-05-14 03:39:14):The spaces of these lines aren't aligned up. Could you clang-format them?
smessmer(2018-05-14 19:54:51):@onnxbot retest this please
bddppq(2018-05-15 03:20:55):Better to avoid overriding the builtin type `file`.
bddppq(2018-05-15 16:57:40):Is this needed?
smessmer(2018-05-15 17:06:57):no. removed.
smessmer(2018-05-14 19:54:57):@onnxbot retest this please
smessmer(2018-05-14 19:55:02):@onnxbot retest this please
AppVeyorBot(2018-05-14 18:40:31)::x: [Build onnx 0.3.3103 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.3103) (commit https://github.com/onnx/onnx/commit/c20a9ca9bc by @smessmer)
smessmer(2018-05-14 19:55:07):@onnxbot retest this please
AppVeyorBot(2018-05-14 22:05:08)::x: [Build onnx 0.3.3120 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.3120) (commit https://github.com/onnx/onnx/commit/d6ccbe9e56 by @smessmer)
AppVeyorBot(2018-05-14 18:52:48)::white_check_mark: [Build onnx 0.3.3104 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.3104) (commit https://github.com/onnx/onnx/commit/9b53505bc6 by @smessmer)
smessmer(2018-05-14 19:55:11):@onnxbot retest this please
AppVeyorBot(2018-05-15 17:35:51)::x: [Build onnx 0.3.3177 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.3177) (commit https://github.com/onnx/onnx/commit/230d4b60cb by @smessmer)
bddppq(2018-05-15 03:28:33):dtype is of numpy.number type. How does this type annotation even pass the CI? :-)
smessmer(2018-05-15 16:54:25):Because we `type: ignore` the numpy include, so (1) when calling `_test_numpy_helper_float_type` with `numpy.float32` as argument, it doesn't know anything about `numpy.float32` and (for all it knows) it might be `Text`. (2) inside the function, `dtype` is passed back to numpy again in `.astype(dtype)`. Again, because of the `type: ignore` it doesn't know what argument `astype` is supposed to take and lets it through.

Fixed it.
bddppq(2018-05-15 16:56:04):no need to import Text anymore
smessmer(2018-05-14 19:55:14):@onnxbot retest this please
smessmer(2018-05-14 19:55:18):@onnxbot retest this please
AppVeyorBot(2018-05-14 23:19:35)::white_check_mark: [Build onnx 0.3.3127 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.3127) (commit https://github.com/onnx/onnx/commit/341df26090 by @smessmer)
smessmer(2018-05-14 19:55:21):@onnxbot retest this please
smessmer(2018-05-15 17:38:24):@onnxbot retest this please
AppVeyorBot(2018-05-15 17:56:10)::white_check_mark: [Build onnx 0.3.3178 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.3178) (commit https://github.com/onnx/onnx/commit/25e27c3490 by @smessmer)
smessmer(2018-05-15 18:08:58):@onnxbot retest this please
smessmer(2018-05-15 18:47:05):@onnxbot retest this please
smessmer(2018-05-15 19:22:47):@onnxbot retest this please
smessmer(2018-05-15 20:06:47):The CI here was still failing, but (probably) unrelated. Let's see if this breaks CI on master.
bddppq(2018-05-15 03:25:12):Backend is intended to be used as singleton
smessmer(2018-05-14 19:55:25):@onnxbot retest this please
gramalingam(2018-05-14 22:24:33):Hi @linkerzhang @anderspapitto @bddppq : with regards to error-checking in the inference code:

What is the expectation for the pointer returned by "getOutputType(n)" ? 

(a) If non-null, can we assume that (1) it is a fresh object that can be over-written by the type-inference code, or (2) do we need to check for compatibility with a possibly pre-existing type? I think (1) is simpler, in which case we can drop some of the checks in inference code.

(b) How do we want to handle optional outputs (or even inputs), both the trailing and non-trailing kinds? Can getOutputType(n) return null for an optional (non-trailing) output? Or not?

anderspapitto(2018-05-14 22:30:01):@gramalingam the implementation we provide (in implementation.h) has the behavior of

- getOutputType(n) is always a valid, fresh pointer (as long as n < numOutputs)

I think we shouldn't have to really deal with optional inputs/outputs - by the time shape inference runs, there already is some complete graph, and each node we see has a fixed number of inputs/outputs. The shape inference function should simply infer shapes for as many outputs as it can.
AppVeyorBot(2018-05-14 22:33:02)::x: [Build onnx 0.3.3123 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.3123) (commit https://github.com/onnx/onnx/commit/30d6ddd7ed by @linkerzhang)
gramalingam(2018-05-14 22:48:12):@anderspapitto : ok (but this just hides optional non-trailing outputs; for optional trailing outputs, the inference code still needs to check "numOutputs" to figure out if that is present or not)
gramalingam(2018-05-14 23:10:53):@anderspapitto : is there a potential problem in implementation.h? if an optional output indicated by an empty string name has an inferred type, then it gets saved as the type of the empty string, and may be used as the type of another optional input/output elsewhere ... at least implementation.h needs to handle input/outputs with an empty string name, right?
AppVeyorBot(2018-05-15 01:07:36)::x: [Build onnx 0.3.3137 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.3137) (commit https://github.com/onnx/onnx/commit/17aa8d4bc0 by @linkerzhang)
AppVeyorBot(2018-05-15 02:05:54)::x: [Build onnx 0.3.3142 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.3142) (commit https://github.com/onnx/onnx/commit/484a2afeed by @linkerzhang)
AppVeyorBot(2018-05-15 02:16:13)::x: [Build onnx 0.3.3143 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.3143) (commit https://github.com/onnx/onnx/commit/d339d3cbe6 by @linkerzhang)
AppVeyorBot(2018-05-15 03:30:34)::x: [Build onnx 0.3.3153 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.3153) (commit https://github.com/onnx/onnx/commit/690accaabc by @linkerzhang)
bddppq(2018-05-15 03:32:10):@linkerzhang The CI reported there are core dump in the tests, probably some nullptr dereferencing?
anderspapitto(2018-05-15 17:38:23):@gramalingam yes, there might be an issue like that. Do we currently have any operators with optional, non-trailing outputs? We may actually want to also just come up with a cleaner representation for optional inputs/outputs than "empty string name".
dzhulgakov(2018-05-15 20:29:12):Hey folks, maybe it was discussed before - but why status codes instead of exceptions? It's much easier to propagate errors that way and we already do use exceptions in onnx codebase
bddppq(2018-05-15 20:44:31):@dzhulgakov I totally agree we should continue using exceptions instead of switching to status code. As I have explained in couple PRs, having a consistent style of error handling in our api is very very important (especially now we have introduced onnx c api), and there are many existing onnx core code have been using exceptions since the very beginning so switching to status code would be api breaking changes. But apparently there are some miscommunications here because in #848 @linkerzhang says he has already discussed this with you. :-)
linkerzhang(2018-05-15 22:48:12):@dzhulgakov I thought we discussed this before that we agreed to have "Status" introduced into ONNX for error handling. Not all upper callers would like to follow using exception for error handling. For shape inference specifically, it's not an exception if it fails. Runtimes may still be able to handle the case without shape inference.

@gramalingam  is kindly going to use exception for the type and shape inference implementation. Thank you very much!

Let's try to clean it before 1.2 release. We'd not release codes without error handling.
AppVeyorBot(2018-05-16 00:12:24)::x: [Build onnx 0.3.3210 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.3210) (commit https://github.com/onnx/onnx/commit/165487ce8a by @linkerzhang)
bddppq(2018-05-16 00:30:43):@linkerzhang If shape inference throws exception, can your code try && catch? I'm really not trying to say exception is better than status code in general, it will be a endless debate. My point is onnx started with using exceptions, there is no good reason to do such an api breaking change, and we definitely need to have a consistent style of error checking.
gramalingam(2018-05-14 21:36:11):When should we use "INVALID_PROTOBUF" and when "INVALID_ARGUMENT"?
gramalingam(2018-05-14 22:02:27):I think this won't work. We need to allow optional outputs (which may be missing, and that is valid). Unfortunately, we can distinguish between the two cases only inside "getOutputShape".
linkerzhang(2018-05-14 22:33:37):I may remove INVALID_ARGUMENT if it's confusing. It's not used in this PR.
anderspapitto(2018-05-14 22:40:34):I would suggest "ShapeInferenceError" in a user-facing message, since Inference is an overloaded term
gramalingam(2018-05-14 23:01:53):Ok, please ignore my comment above, as per discussion elsewhere
gramalingam(2018-05-14 23:03:38):But, essentially, this condition will never arise as per our discussion.
linkerzhang(2018-05-15 01:34:26):But type inference also covered in the function :). So, TypeShapeInference? if you want.
gramalingam(2018-05-15 02:33:33):Missing !
linkerzhang(2018-05-15 02:53:53):Thank you so much for the catch and fix!
snnn(2018-05-14 22:14:58):Updated.
AppVeyorBot(2018-05-14 22:46:02)::white_check_mark: [Build onnx 0.3.3124 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.3124) (commit https://github.com/onnx/onnx/commit/2f5f0d92f0 by @snnn)
AppVeyorBot(2018-05-15 01:19:42)::white_check_mark: [Build onnx 0.3.3138 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.3138) (commit https://github.com/onnx/onnx/commit/390e3e3f6a by @snnn)
bddppq(2018-05-14 22:08:33):Could you attach the diff of the proto text? Currently this is really un-reviewable :-)
anderspapitto(2018-05-14 22:15:07):@bddppq well, mostly you can just review the associated code diff. but yes i can pretty-print the proto files here
anderspapitto(2018-05-14 22:22:06):here is a diff of before and after (i got the + and - mixed up though - the squeezes are introduced)
https://gist.github.com/anderspapitto/f0d905c06c0d4de55651f38bb9edf75d
bddppq(2018-05-14 22:27:34):@anderspapitto ok thanks. the change look reasonable to me @houseroad 
CLAassistant(2018-05-14 22:51:56):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=961) <br/>All committers have signed the CLA.
AppVeyorBot(2018-05-15 05:39:48)::x: [Build onnx 0.3.3173 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.3173) (commit https://github.com/onnx/onnx/commit/1011a70243 by @rdzhabarov)
AppVeyorBot(2018-05-16 00:25:37)::white_check_mark: [Build onnx 0.3.3211 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.3211) (commit https://github.com/onnx/onnx/commit/76cb6b62ba by @rdzhabarov)
AppVeyorBot(2018-05-16 05:25:18)::white_check_mark: [Build onnx 0.3.3236 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.3236) (commit https://github.com/onnx/onnx/commit/768ba5ccc0 by @rdzhabarov)
rdzhabarov(2018-05-17 00:21:25):Any chance to merge this?
bddppq(2018-05-17 08:44:28):@linkerzhang @ebarsoum Could you look at this simple schema change (technically not a change)?
AppVeyorBot(2018-05-17 16:59:20)::white_check_mark: [Build onnx 0.3.3308 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.3308) (commit https://github.com/onnx/onnx/commit/0a66fae3a6 by @rdzhabarov)
houseroad(2018-05-18 13:40:07):Shall we have a more general solution? I mean clarifying the error out mechanism about invalid axis.
bddppq(2018-05-18 18:20:10):@houseroad I don't think it's wise to force all backends to use the same error reporting mechanism. Just clarifying this is an invalid case is already enough.
AppVeyorBot(2018-05-20 00:50:59)::white_check_mark: [Build onnx 0.3.3395 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.3395) (commit https://github.com/onnx/onnx/commit/502a9583ea by @rdzhabarov)
houseroad(2018-05-20 00:55:41):@bddppq sorry for the confusion, I mean we should do similar things to other operators.
rdzhabarov(2018-05-23 17:56:27):@houseroad @bddppq It's somewhat tough to get the branch not out of date with the frequent master branch updates. I do not see any conflicts with the master branch though. Can you merge the PR?
bddppq(2018-05-15 02:42:10):Hmm I don't think we intended to support negative pads. Have you seen this in our backend test cases/models in the model zoo?
RyanUnderhill(2018-05-15 18:30:49):Yep, it's in a bunch of tests: ConstantPad2d ReflectionPad2D ReplicationPad2D ZeroPad2d and more.

We discovered it due to the public test cases failing due to negative padding values.

prasanthpul(2018-05-15 21:00:58):@RyanUnderhill the documentation is auto generated so you need to update the def files and then run the document generator, rather than updating operators.md directly
linkerzhang(2018-05-17 21:34:26):@houseroad @bddppq  if we think negative padding is not needed, let's fix the tests please. :). Thank you very much!
bddppq(2018-05-18 08:20:20):@linkerzhang TF and PyTorch support negative padding, and looks to me it's easy to support in a backend, maybe let's support it?
houseroad(2018-05-18 14:17:26):@linkerzhang Yeah, I agree with @bddppq, it's better to support this semantics in ONNX. Since this is  just a clarification on the doc, I just merged it. :-)
jspisak(2018-05-15 23:53:05):@anirudhacharya and @prasanthpul Can you take a look and let us know if we are good to merge?

AppVeyorBot(2018-05-16 00:52:13)::white_check_mark: [Build onnx 0.3.3213 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.3213) (commit https://github.com/onnx/onnx/commit/05b2817f25 by @houseroad)
anirudhacharya(2018-05-16 16:39:09):thanks for the PR. The new description looks good to me.
anderspapitto(2018-05-15 19:18:02):We could commit this and perhaps add the output into the docs somewhere, so that we stay up-to-date as more operators are added, or I can just keep it around as a one-off patch to run manually
bddppq(2018-05-15 19:29:02):not used
bddppq(2018-05-15 19:30:54):In general all python files should have these boilerplate lines at the top

```
from __future__ import absolute_import
from __future__ import division
from __future__ import print_function
from __future__ import unicode_literals
```
bddppq(2018-05-15 19:33:54):nit: too magic
bddppq(2018-05-15 19:36:11):Maybe add a comment saying this is the default one that intend to do nothing.
AppVeyorBot(2018-05-15 23:12:07)::x: [Build onnx 0.3.3204 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.3204) (commit https://github.com/onnx/onnx/commit/17d85f0b27 by @smessmer)
AppVeyorBot(2018-05-16 00:38:10)::x: [Build onnx 0.3.3212 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.3212) (commit https://github.com/onnx/onnx/commit/48bf56ffc5 by @smessmer)
bddppq(2018-05-17 08:41:47):looks like all this breaks all python3 CI
smessmer(2018-05-17 20:58:16):This PR depends on https://github.com/onnx/onnx/pull/982. That one is landed now and I rebased this one - in short, it should be fixed now.
AppVeyorBot(2018-05-15 22:11:52)::x: [Build onnx 0.3.3199 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.3199) (commit https://github.com/onnx/onnx/commit/09d5c3df17 by @smessmer)
AppVeyorBot(2018-05-15 23:20:03)::x: [Build onnx 0.3.3205 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.3205) (commit https://github.com/onnx/onnx/commit/29722a1bc3 by @smessmer)
smessmer(2018-05-16 00:05:44):superseded by https://github.com/onnx/onnx/pull/972
smessmer(2018-05-15 21:36:11):mypy might not be able to infer the type of these two arrays. If CI doesn't complain, you're fine. If it does, just add a `# type: ` comment with the concrete type to these lines.
AppVeyorBot(2018-05-15 23:32:46)::white_check_mark: [Build onnx 0.3.3206 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.3206) (commit https://github.com/onnx/onnx/commit/14e30f99ca by @anderspapitto)
AppVeyorBot(2018-05-17 18:42:31)::x: [Build onnx 0.3.3312 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.3312) (commit https://github.com/onnx/onnx/commit/00eced819f by @anderspapitto)
AppVeyorBot(2018-05-19 02:25:33)::white_check_mark: [Build onnx 0.3.3388 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.3388) (commit https://github.com/onnx/onnx/commit/d169b05c00 by @anderspapitto)
bddppq(2018-05-16 21:37:06):let's put this blacklist/whitelist logic into a separate function
bddppq(2018-05-16 21:38:05):what's special about dropout?
anderspapitto(2018-05-17 00:19:57):++
anderspapitto(2018-05-17 00:21:01):second output is the mask, which is unused in at inference time and shape inference doesn't work for it yet - but i'd still like to check the rest of the model
bddppq(2018-05-18 18:13:43):nit: there is syntax sugar `{...}` for `set([...])`
linkerzhang(2018-05-19 17:28:46):what will happen in onnx_ml case?
anderspapitto(2018-05-21 17:07:29):we may have to extend this logic if we add models using the onnx_ml types to the whitelist
AppVeyorBot(2018-05-16 17:49:54)::x: [Build onnx 0.3.3256 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.3256) (commit https://github.com/onnx/onnx/commit/8ed3cb75da by @smessmer)
smessmer(2018-05-16 20:50:41):note: https://github.com/onnx/onnx/pull/982 needs to land first and this needs to be adapted to it. Otherwise things will break.
bddppq(2018-05-18 08:22:19):Inherit from `ONNXCommand` to saves you some boilerplate.
bddppq(2018-05-18 08:26:20):1. Use `sys.executable` instead of `"python"` (this is not nit, because e.g. virtualenv can be used without activation, and so the correct python is actually not in `$PATH`, and also as we recently discovered, many users can have `python3`, `python3.6` as their main python installation).
2. Use absolute path for the `mypy-onnx.py`, otherwise it requires user to invoke from the onnx top directory.

bddppq(2018-05-18 08:27:04):this can be combined with last line as `subprocess.check_call`.
bddppq(2018-05-18 08:28:24):what's the purpose of this try-catch?
bddppq(2018-05-18 08:29:06):nit: Do `os.path.abspath` before doing `os.path.dirname` to avoid symlink confusion.
Use `os.pardir` instead of `..` , or you can just add another `os.path.dirname`.
bddppq(2018-05-18 08:32:13):use `__file__` and some path resolution functions to get the top level directory path
smessmer(2018-05-18 18:13:26):Yes, but check_call throws an exception, which then outputs a backtrace, cluttering the type errors listed by mypy. So it's either check_call and try/catch or call and sys.exit.
smessmer(2018-05-18 18:13:51):See explanation above. I added an explanatory comment.
smessmer(2018-05-18 18:19:45):I moved it below the `chdir`, that should fix it
AppVeyorBot(2018-05-16 16:48:23)::x: [Build onnx 0.3.3253 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.3253) (commit https://github.com/onnx/onnx/commit/7aa5ac57a5 by @jaliyae)
AppVeyorBot(2018-05-16 17:30:25)::x: [Build onnx 0.3.3255 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.3255) (commit https://github.com/onnx/onnx/commit/68ea60a70e by @jaliyae)
AppVeyorBot(2018-05-16 20:17:56)::x: [Build onnx 0.3.3264 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.3264) (commit https://github.com/onnx/onnx/commit/37cbb15e76 by @jaliyae)
AppVeyorBot(2018-05-16 22:34:40)::white_check_mark: [Build onnx 0.3.3274 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.3274) (commit https://github.com/onnx/onnx/commit/2092f848b5 by @jaliyae)
gramalingam(2018-05-16 03:02:46):Please change comment to "Input tensor should have at least two dimensions" ... This will be important if we convert these into error messages (as we plan to do over next few days).
gramalingam(2018-05-16 03:12:10):tras npose => transpose in all these function names?
gramalingam(2018-05-16 16:27:29):Build failure seems to be complaining about a comparison of signed and unsigned integers somewhere here ...
jaliyae(2018-05-16 16:35:29):This comparison is both size_t, I think the issue is few lines above. I pushed a new change. 
bddppq(2018-05-16 01:27:42):This is actually more error-prone than we thought. On some systems that doesn't by default ship python3, users likely have multiple python installations (python, python2, python3.6), but we can't pass `PYTHON_EXECUTABLE` from cmake to `protoc-gen-mypy.py` and so it always invokes invokes `python`, it can easily run into situation that the default python installation doesn't have protobuf installed.
smessmer(2018-05-16 03:00:08):Let's output a warning in setup.py if protobuf isn't found so users who want type stubs get a clue as to why they don't work.

What do you mean with it being fragile? I assume protoc-gen-mypy is called with the python running setup.py, or is it not?
bddppq(2018-05-16 03:02:29):@smessmer print a warning sounds good to me.
Users can call setup.py with e.g. `python3.6 setup.py install` (or even when they do `pip install` the pip might belong to a python installation that is different with the `python` on the system.
bddppq(2018-05-16 03:04:11):@smessmer actually, I'm going to remove the check at setup.py, because it might not be checking the correct python installation.
CLAassistant(2018-05-16 02:01:47):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=976) <br/>All committers have signed the CLA.
bddppq(2018-05-16 04:17:31):could you highlight it's from a github source code checkout?
bddppq(2018-05-16 02:46:20):@onnxbot retest this please
AppVeyorBot(2018-05-16 04:49:53)::x: [Build onnx 0.3.3234 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.3234) (commit https://github.com/onnx/onnx/commit/02ccdf0406 by @fumihwh)
AppVeyorBot(2018-05-16 05:04:53)::x: [Build onnx 0.3.3235 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.3235) (commit https://github.com/onnx/onnx/commit/35d8c7c45d by @fumihwh)
AppVeyorBot(2018-05-18 04:57:45)::x: [Build onnx 0.3.3343 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.3343) (commit https://github.com/onnx/onnx/commit/ee96c17392 by @fumihwh)
AppVeyorBot(2018-05-18 05:03:20)::x: [Build onnx 0.3.3344 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.3344) (commit https://github.com/onnx/onnx/commit/45d32328c6 by @fumihwh)
AppVeyorBot(2018-05-20 11:57:50)::white_check_mark: [Build onnx 0.3.3401 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.3401) (commit https://github.com/onnx/onnx/commit/0f37a98a98 by @houseroad)
houseroad(2018-06-14 08:03:15):@fumihwh could you use https://github.com/onnx/onnx/blob/master/onnx/backend/test/stat_coverage.py to update the TestCoverage page?
anirudhacharya(2018-05-18 04:12:50):seeing that autopad is now deprecated? why is this test being added?
fumihwh(2018-05-18 04:29:24):@anirudhacharya 
Tensorflow uses something like `auto_pad` and I am working on backend of tf. 
At least for test case coverage for tf, I added this. 
Of course I am ok with removing it.
anirudhacharya(2018-05-18 04:32:09):It would be good to remove this and then evaluate a plan when the deprecated attributes/operators will be EOL'ed.
anirudhacharya(2018-06-06 22:09:47):nit: input_shape
AppVeyorBot(2018-05-16 20:55:10)::white_check_mark: [Build onnx 0.3.3267 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.3267) (commit https://github.com/onnx/onnx/commit/e6951375e2 by @smessmer)
AppVeyorBot(2018-05-17 17:11:46)::x: [Build onnx 0.3.3309 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.3309) (commit https://github.com/onnx/onnx/commit/43748f101f by @smessmer)
AppVeyorBot(2018-05-17 17:26:13)::white_check_mark: [Build onnx 0.3.3310 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.3310) (commit https://github.com/onnx/onnx/commit/f6b850eb41 by @smessmer)
bddppq(2018-05-17 05:50:12):what does it imply? with python3 we can't perform type checking?
AppVeyorBot(2018-05-16 22:22:41)::x: [Build onnx 0.3.3273 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.3273) (commit https://github.com/onnx/onnx/commit/8f04773082 by @gramalingam)
linkerzhang(2018-05-16 23:01:15):Thank you! @gramalingam . Let's also make the CI happy.
AppVeyorBot(2018-05-16 23:06:13)::x: [Build onnx 0.3.3277 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.3277) (commit https://github.com/onnx/onnx/commit/c68c25aafc by @gramalingam)
bddppq(2018-05-17 05:22:00):Does it mean we are aligned on using exception as the error reporting mechanism in onnx? Shall we close  #958 then?
gramalingam(2018-05-18 00:55:33):yes, we are agreed on exception as the error reporting mechanism here. yes #958 can be closed.
bddppq(2018-05-17 05:22:47):Could you do a clang-format on this code?
gramalingam(2018-05-18 00:52:27):sorry about that ... let me do that separately (especially when there is no code change to review)
AppVeyorBot(2018-05-18 03:50:46)::white_check_mark: [Build onnx 0.3.3341 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.3341) (commit https://github.com/onnx/onnx/commit/caf5151c08 by @gramalingam)
linkerzhang(2018-05-18 00:38:12):at least
AppVeyorBot(2018-05-18 00:42:02)::x: [Build onnx 0.3.3328 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.3328) (commit https://github.com/onnx/onnx/commit/129d8bd5ce by @linkerzhang)
AppVeyorBot(2018-05-18 00:54:16)::white_check_mark: [Build onnx 0.3.3329 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.3329) (commit https://github.com/onnx/onnx/commit/8e34e0ec7b by @linkerzhang)
AppVeyorBot(2018-05-18 08:07:21)::x: [Build onnx 0.3.3350 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.3350) (commit https://github.com/onnx/onnx/commit/5d70ae5ef0 by @bddppq)
AppVeyorBot(2018-05-18 19:48:39)::x: [Build onnx 0.3.3365 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.3365) (commit https://github.com/onnx/onnx/commit/81cc87bfe5 by @smessmer)
bddppq(2018-05-21 19:03:02):good catch
AppVeyorBot(2018-05-18 20:27:14)::white_check_mark: [Build onnx 0.3.3368 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.3368) (commit https://github.com/onnx/onnx/commit/08e2c14e11 by @smessmer)
AppVeyorBot(2018-05-19 00:21:48)::x: [Build onnx 0.3.3382 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.3382) (commit https://github.com/onnx/onnx/commit/5c6293b1e4 by @smessmer)
AppVeyorBot(2018-05-18 21:56:04)::x: [Build onnx 0.3.3375 failed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.3375) (commit https://github.com/onnx/onnx/commit/08f09c58e3 by @smessmer)
AppVeyorBot(2018-05-19 02:46:38)::white_check_mark: [Build onnx 0.3.3390 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.3390) (commit https://github.com/onnx/onnx/commit/cf7886dde5 by @lutzroeder)
houseroad(2018-05-21 01:11:57):I think I already fix this in https://github.com/onnx/onnx/pull/907 :-)
lutzroeder(2018-05-25 01:44:28):@houseroad This is fixed in `Gemm` v7 but the older versions still have the comma on the wrong line.
houseroad(2018-05-25 02:12:59):@lutzroeder yes, you are right, I forgot to fix the problem in the old.cc.

Could you create another PR to solve it?

Thanks!
lutzroeder(2018-05-25 02:34:59):@houseroad #1035
bddppq(2018-05-21 17:31:26):I blame cmake on this :-)
bddppq(2018-05-21 22:03:35):Just to double check, this is only temporary and once your PRs to the upstream google protobuf repository are merged we can change to refer the upstream instead of having these files copied into onnx repo?
smessmer(2018-05-21 22:10:38):yes
linkerzhang(2018-05-22 01:47:42):Thank you very much! @gramalingam 

Do you want to document "optional" input/output semantics somewhere please? including if an optional input or output does not exist for a graph, then an empty string needs to be specified as place holder.
gramalingam(2018-05-22 03:38:49):@linkerzhang : but isn't that the same treatment of optional inputs/outputs for all ops? does it require special mention?
wschin(2018-05-22 16:44:37):RNNs use optional inputs and outputs a lot. It'd be very helpful if you can add a small note. You can also point out where to find doc to specify unused I/O.
anderspapitto(2018-05-22 16:50:22):I (weakly) vote against explicitly talking about optional outputs in the documentation of RNN. After this change, the outputs are no more special than the outputs of e.g. MatMul. In particular, it is not necessary to provide an empty string - any name can be provided as an output, and then you can simply choose not to provide that name as an input to any further ops.
wschin(2018-05-22 17:02:52):Let's imagine that a poor user wants to implement converter for his RNNs. Now, he needs to go through all documents to find the place defining the way to specify optional I/O's. I was one of the poor users. It took me 1 hour to figure out the ONNX way to specify optional I/O's and I even spent another 1 hour to double-confirm with runtime/hardware providers. This can be avoided with a tiny note. Also, I don't think many operators have "missing" inputs between specified inputs; most missing inputs happen at the end of an operator's input list.
anderspapitto(2018-05-22 17:52:15):@wschin I'm fine with adding a pointer to docs related to optional inputs, because they are a real special case that needs special handling. I'm just saying that outputs don't need any special handling (and this diff happens to be about outputs)
gramalingam(2018-05-22 21:40:50):@NiklasGustafsson I updated a line describing optional outputs in IR.md. Hope this looks fine.
AppVeyorBot(2018-05-22 21:51:32)::white_check_mark: [Build onnx 0.3.3503 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.3503) (commit https://github.com/onnx/onnx/commit/07631593a8 by @gramalingam)
AppVeyorBot(2018-05-22 00:04:50)::white_check_mark: [Build onnx 0.3.3455 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.3455) (commit https://github.com/onnx/onnx/commit/9df1340e2a by @houseroad)
linkerzhang(2018-05-22 01:44:04):@bddppq this is not related to this PR. I realized that "shape" was also not since version 1, right? :)
bddppq(2018-05-22 04:31:38):@linkerzhang yeah that was not :(
linkerzhang(2018-05-22 01:43:25):6 or 7?
houseroad(2018-05-22 01:46:10):We introduced this change in opset 6, I think it's better to keep the truth in our code.
houseroad(2018-05-22 01:46:59):Any particular reason to use 7? If there is, we can use 7. 
bddppq(2018-05-22 04:32:14):This should be 6, which was the time we introduced this op.
AppVeyorBot(2018-05-22 03:41:17)::white_check_mark: [Build onnx 0.3.3469 completed](https://ci.appveyor.com/project/onnx/onnx/build/0.3.3469) (commit https://github.com/onnx/onnx/commit/5df2a2cc14 by @houseroad)
wschin(2018-05-23 18:00:01):For those very specific operators, in addition to high-level descriptions, users may also need mathematical documentation for implementing them. Otherwise, it might lead to some reverse engineering and hard to sync with all partners.
houseroad(2018-05-24 07:59:12):@wschin yeah, I will add more description, and make sure people can understand the logic of the definition.
Maratyszcza(2018-07-14 02:05:57):@houseroad Would be great to use this opportunity to find better names for operators.
houseroad(2018-07-14 03:16:30):@Maratyszcza i am totally open to it. Let's have some offline discussion next week :-)
linkerzhang(2018-12-04 22:27:16):@houseroad  is this still on track please? thank you!
houseroad(2018-12-04 22:30:16):Some design decision needs to be made here. I think RoIAlign is fine. For the other three, we probably need to reconsider how to define the API.
houseroad(2018-12-04 22:31:39):@linkerzhang if you guys want to propose other design, feel free to create new PR(s). I can just drop this one.
zhangxiang1993(2019-01-16 18:38:47):Is this PR still in Progress? Thanks! @houseroad 
daquexian(2019-01-28 07:55:29):@houseroad What's the current states of detection ops? :) Is there something I can help with (like tests)?
houseroad(2019-01-28 18:15:41):@daquexian thanks for offering help. I think RoIAlign is good to go. Could you create a separate PR for it and adding some tests for RoIAlign? Other ops, not sure they are general enough to be pushed into ONNX spec, since most of frameworks don't have those ops. 
daquexian(2019-01-30 10:26:03):@houseroad All right :) I'll start to do it. 
Maratyszcza(2019-01-31 09:03:11):These operators bear a lot of legacy issues. In particular, storing batch index in a float tensor is a 100% bad idea. There are refactored versions of Mask R-CNN ops in pytorch/pytorch#15889, they would be a better starting point for ONNX standartization.
houseroad(2019-02-03 07:34:08):@daquexian before working on RoIAlign, checking @Maratyszcza's proposal should be a good idea. :-)
daquexian(2019-02-10 07:32:10):@houseroad Hi, sorry for the delay :) Happy lunar new year! I have read @Maratyszcza's implementation. Does it mean it is better to add a separate `batch_split` input to `RoIAlign`, instead of storing `batch_id` in `rois`? 
prasanthpul(2019-03-21 22:42:42):@houseroad should this be closed since there are now PRs for the individual ops?
CLAassistant(2018-05-22 19:14:12):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=1012) <br/>All committers have signed the CLA.
houseroad(2018-05-23 04:02:54):@nickfeeney could you sign the CLA?
prasanthpul(2018-05-23 20:00:50):Nick has signed the CLA (verified in the CLA logs). Might be an email mismatch. In any case, let's not block on that.
houseroad(2018-05-23 01:18:28):Yes, there are many test cases in shape_inference_test.py for matmul.
bddppq(2018-05-23 01:02:16):Call it numpyBroadcast?
houseroad(2018-05-23 01:10:56):I think we won't have any function called `unidirectionalBroadcastShapeInference`, so calling it `numpyBroadcasting` is fine. 
gramalingam(2018-05-22 21:49:03):Omit "Must be in-place with the input mean"
gramalingam(2018-05-22 21:49:18):Omit reference to in-place update
linkerzhang(2018-05-22 21:53:41):will fix.
linkerzhang(2018-05-22 21:53:46):will fix.
anderspapitto(2018-05-23 01:55:52):What's an example of state that you want to close over? It's possible the need for this indicates that we should expand InferenceContext
jeffbloo(2018-05-23 15:22:21):@anderspapitto - Enabling indirection to an inference function of a different signature which is only made available at runtime
linkerzhang(2018-05-23 01:17:03):@bddppq  ONNX_ML=1 is a super set of ONNX_ML=0. It only adds two more types in proto and some related c++ codes. So, I'm assuming that the code should be always be built with ONNX_ML =0 once it could be built when ONNX_ML=1. Make sense please? Are there any scenarios that're not covered and I missed please?  This change will help us improve the ci efficiency a lot :).
bddppq(2018-05-23 05:55:52):@linkerzhang 
> "the code should be always be built with ONNX_ML =0 once it could be built when ONNX_ML=1" 

Not really, we have spotted couple times in PRs that people forgot adding ifdefs to guard access to onnxml stuffs.


wschin(2018-05-23 17:55:27):@bddppq , does it mean it's a good reason to add ONNX_ML=1 for test? Those situation can be avoided if we have tests.
bddppq(2018-05-23 22:38:25):@wschin Sorry I don't quit follow your suggestion. Could you elaborate how is that going to help?
wschin(2018-05-23 22:59:25):I should said that ONNX_ML=1 is a super set of ONNX_ML=0. Errors found by ONNX_ML=0 should be found by ONNX_ML=1 as well.
bddppq(2018-05-23 23:06:43):@wschin code example of such tests?

wschin(2018-05-24 15:57:09):ONNX-ML=1 proto covers all data structures used in ONNX-ML=0 proto. For ONNX-ML=0 tests, I assume that they always ignore the data structures defined in ONNX-ML=1 so those tests may produce the same result with ONNX-ML=1. If ONNX-ML=1 runs all tests under ONNX-ML=0, I'd expect that we still have 100% tests coverage even if we don't have ONNX-ML=0 test branch.  I am not sure what examples are we looking for. Do you think [this](https://github.com/onnx/onnx/blob/640a4ec5d28db8c30d13e30732b35ed5c26fb746/onnx/test/basic_test.py#L19) one is ok? Thanks.
bddppq(2018-05-24 16:24:14):@wschin Unfortunately no, no existing tests can protect us from forgetting adding ifdefs around e.g. this code block https://github.com/onnx/onnx/blob/640a4ec5d28db8c30d13e30732b35ed5c26fb746/onnx/checker.cc#L48-L58, and if that happens, the code will even not be able to build in ONNX_ML=0 case.
wschin(2018-05-25 16:46:49):That test makes sense! If we have ONNX_ML=0 and ONNX_ML=1, we need to test both of them. I have another question; couldn't everyone use ONNX_ML=1? We can still separate traditional ML ops from default onnx domain. 
bddppq(2018-06-04 16:36:25):Closing. We can revisit this if someday ONNX_ML={0,1} are unified.
CLAassistant(2018-05-24 21:50:42):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=1028) <br/>All committers have signed the CLA.
houseroad(2018-05-27 09:52:40):@benbarsdell could you please sign the CLA?
houseroad(2018-05-31 13:37:56):@benbarsdell friendly ping to sign the CLA.
houseroad(2018-07-19 22:55:58):cc @jspisak 
jspisak(2018-07-19 23:43:03):@benbarsdell - you have a corporate CLA in place but will still need to click through the one on GitHub in order to get your PR merged. TL;DR is that your corp CLA will supersede the click through. 
benbarsdell(2018-08-07 23:27:21):I can't tell what failed in the CI; was it something to do with this PR?
spandantiwari(2018-08-08 03:27:27):@benbarsdell This is a welcome addition. May I recommend adding a test point that tests the round trip from `DepthToSpace`, i.e. starting with a tensor and doing `DepthToSpace` followed by `SpaceToDepth` and check that the output matches the original input. That way we are sure to be consistent between the two.
spandantiwari(2018-09-21 23:17:28):@benbarsdell Yes, this may have failed due to stale test_coverage.md. Can you try running `python onnx/backend/test/stat_coverage.py` and updating the same? 
vandanavk(2018-10-09 21:32:58):Is this PR WIP? would be great to have this test added.
houseroad(2018-10-09 21:35:05):This needs to update TestCoverage.md. I may pick it up later.
jcwchen(2021-10-01 18:57:31):This is definitely good to have. Target this PR for the upcoming 1.11 release. @benbarsdell Could you please signoff DCO if you still have bandwidth? Thank you!
askhade(2021-10-26 21:11:10):Closing this PR since #3786 is merged 
bddppq(2018-05-24 22:55:59):typo
bddppq(2018-05-25 00:14:12):@smessmer If the CI breaks, could you help adjust it to skip the type checking stuffs to unblock release?
linkerzhang(2018-05-25 01:30:02):@smessmer  please kindly fix the CI failure and we may discuss whether we need type stubs and also come out of a solution working well for both windows and linux. Thank you very much!
anderspapitto(2018-05-31 18:27:31):The math operators support broadcasting, but I don't believe that scale/bias parameters to BatchNorm do. You need to disable this optimization when broadcasting is used (and then please add a test for this).
fumihwh(2018-06-03 01:08:46):@anderspapitto from opset 7, there is no more broadcast attribute.
I think add a constraint of `arithmetic param dim should not be greater than batch norm dim` is good enough.
What do you think?
zakk0610(2018-06-25 03:57:19):Could it support below case?
Before:
Y = BatchNorm(X, scale, bias, mean, var)
Z = MUL(Y, A, broadcast=1, axis=1)
After
Y = BatchNorm(X, scale, bias, mean, var)

like https://github.com/onnx/onnx/pull/1139?

BTW, this optimization cannot handle tensor operand, right?
fumihwh(2018-06-26 00:11:04):@zakk0610 Sorry for unexplaining. From opset version 7, we introduced a new strategy for broadcasting which is almost same to numpy.
According to that, we don't need to support case you mentioned above.
zakk0610(2018-06-26 07:40:16):Hi @fumihwh, 
Sorry i don't understand why this optimization need to insert arithmetic and squeeze node in graph.
Could you explain why this optimizer does not like fuse_bn_into_conv optimizer, which only need to update tensor.



fumihwh(2018-06-27 00:32:35):@zakk0610 
For example,
```
X = BN(input, scale, bias)    with shape_scale=(C), shape_bias=(C)
Y = Add(X, A)    with shape_A=(1, C, 1, 1)
↓
Z = Squeeze(A)    shape=(C)
B = Add(bias, Z)    shape=(C)
C = BN(input, scale, B)
```
`bias` always has shape (C) and if `A` has shape (1, C, 1, 1), we should do squeeze first and add them. Then pass new_bias as an input to BN.

zakk0610(2018-06-27 03:07:04):@fumihwh Got it, thanks.

If A is tensor (weight), can we fuse A into bias and remove Add operator?
For example, 
```
X = BN(input, scale, bias)    with shape_scale=(C), shape_bias=(C)
Y = Mul(X, A)    with shape_A=(1, C, 1, 1)  ; A is Tensor (weight) 
Z = Add(Y, B)    with shape_B=(1, C, 1, 1)  ; B is Tensor (weight) 
↓
C = BN(input, new_scale, new_bias)    with new_scale = scale * A ; 
                                      with new_bias = bias * A + B
 ```
If the model is converted from Caffe->Caffe2->ONNX, the above case will be happened, because Caffe uses two layer (BatchNorm+Scale) to implements BatchNorm operator.

or this fuse optimization should be done in converter tool?

Thanks



fumihwh(2018-06-27 06:05:29):@zakk0610 
This optimizer can handle case you mentioned.
But Add and Mul op will be added for making new_bias and new_scale. (before BN)
zakk0610(2018-06-27 12:53:08):@fumihwh 
Why not to replace old tensor by new tensor?
like [this]( https://github.com/onnx/onnx/pull/1139/files#diff-3c66244f77c2f73f1041ba1e989aeda3R98).
Then we don't need to add Add and Mul op.
Thanks.

```
 Tensor scale = *scale;
 Tensor bias = *bias;
 Tensor A = *A;
 Tensor B = *B;
 scale.multiply(A);
 bias.multiply(A);
 bias.add(B);
// update bn's inputs
```
fumihwh(2018-06-29 01:10:46):@zakk0610 
~Yes, we could do this.~
If two tensors has different shape, maybe we can't replace old tensor.
Let me confirm it.

UPDATE:
We cannot add two tensors if their shapes do not match.
For example, `bias=(C,)`, `A=(1,C,1,1)`, we can not use `bias.add(A)` to update tensor inplace.
zakk0610(2018-06-30 07:40:18):@fumihwh 
Yes, you are right. it's my mistake, I use the wrong example.
This opt will fuse two tensors if they have the same shape.

for example, I get the following case when the model is converted form Caffe
```
X = Conv ...              with shape_X = (N, C, H, W)
Y = BN(X, scale, bias)    with shape_scale=(1, C, 1, 1), shape_bias=(1, C, 1, 1)
Z = Mul(Y, A)             with shape_A=(1, C, 1, 1)  ; A is Tensor (weight) 
W = Add(Z, B)             with shape_B=(1, C, 1, 1)  ; B is Tensor (weight) 
```
If we can fuse tensor(BN+Mul-->BN, BN+Add-->BN) and apply this optimization https://github.com/onnx/onnx/pull/1106, 
The result will be very simple.
```
X = Conv with new_weight, new_bias
```
It's why I post the PR https://github.com/onnx/onnx/pull/1139.
Thanks.







fumihwh(2018-06-30 08:21:14):@zakk0610 
In your example, that model does not meet the spec.
`Y = BN(X, scale, bias)    with shape_scale=(1, C, 1, 1), shape_bias=(1, C, 1, 1)`
scale and bias MUST have shape `(C,)`, otherwise, the model is wrong.

So, with correct model,
```
X = Conv ...              with shape_X = (N, C, H, W)
Y = BN(X, scale, bias)    with shape_scale=(C,), shape_bias=(C,)
Z = Mul(Y, A)             with shape_A=(1, C, 1, 1)  ; A is Tensor (weight) 
W = Add(Z, B)             with shape_B=(1, C, 1, 1)  ; B is Tensor (weight) 
```
We can't just update scale and bias in-place by using A and B now.
So I add squeeze and arithmetic node before BN.
```
X = Conv ...              with shape_X = (N, C, H, W)
NEW_A = Squeeze(A)    with shape_A=(1, C, 1, 1)  ; A is Tensor (weight), and shape_NEW_A=(C,)
NEW_B = Squeeze(B)    with shape_B=(1, C, 1, 1)  ; B is Tensor (weight), and shape_NEW_B=(C,)
NEW_SCALE = Mul(scale, NEW_A)      scale is original scale, shape_scale=(C,), shape_NEW_SCALE=(C,)
NEW_BIAS = Mul(bias, NEW_A)
NEW_BIAS = Add(B, NEW_BIAS)    bias is original bias, shape_bias=(C,), shape_NEW_BIAS=(C,)
Y = BN(X, NEW_SCALE, NEW_BIAS)
```
zakk0610(2018-06-30 15:07:17):@fumihwh 
Got it, thanks for your patience. 
I made a wrong example again , so sorry... 

The root cause i want to optimize is the model converted from Caffe. (caffe->caffe2->onnx)
Caffe implement BatchNorm by BN + Scale layer.
```
BN = ( X - mean ) / (sqrt(var))
Scale = BN  * gamma + beta   //  (channel-wise)
```
in the opset version 6, the converter will generate below case
```
X = Conv ...              with shape_X = (N, C, H, W)
Y = BN (X, scale(1), bias(0), mean, var)
Z = Mul(Y, gamma ,axis=1, broadcast=1)
W = Add(Z, beta, axis=1, broadcast=1)
```
as you mentioned before, in the opset version 7, there is no more broadcast attribute, 
But in naturally, I can use only one BN Operator to instead of BN + Mul + Add, Right?
or maybe this opt should be done in converter tool?




fumihwh(2018-07-03 00:53:37):> But in naturally, I can use only one BN Operator to instead of BN + Mul + Add, Right?

Yes, we are aiming to achieve this also. But currently, I can't update tensor inplace as I said before. (Else if there is a way to squeeze tensor, for example, improving `tensor.h` to support more operators.)
What I can do is using Squeeze + Mul + Add + BN instead of BN + Mul + Add.
BTW, optimization does not have version compatibility. We will not consider opset v6.
zakk0610(2018-07-03 14:43:03):@fumihwh  Got it, Thanks for the explanation.
BTW, currently is there any benefit of this fuse optimization?
fumihwh(2018-07-04 01:05:46):@zakk0610 
I think although number of nodes does not reduce by this optimization, the dependencies of node BN be more concise than before.
In another word, all arithmetic node after BN are removed and operands are changed to constant, it will be easier to deal with in edge side (complier or something else).
zakk0610(2018-07-05 06:14:25):@fumihwh Thanks for your opinion. 

CLAassistant(2019-07-24 00:57:50):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=1046) <br/>All committers have signed the CLA.
askhade(2021-10-01 17:20:56):Closing this PR since the optimizers are now moved to a separate repo. @fumihwh if this is still need please open this PR in optimizers
anderspapitto(2018-05-31 18:19:37):I have a slight preference to change from 

```
if (kind matches) {
  do_stuff();
}
```

to

```
if (! kind matches) {
  continue;
}
do_stuff();
```
anderspapitto(2018-05-31 18:23:19):this `continue` is dead code - change it to either `break` or simply remove it
anderspapitto(2018-05-31 18:24:05):why is this necessary?
anderspapitto(2018-05-31 18:25:12):would be great to pull out some of the copy-pasted code into a helper function
anderspapitto(2018-06-04 18:31:13):what if the size of A were

- (5, 1, 1)
- (1, 5, 1, 1)
- (4, 5, 3, 3)
- (4, 5, 1, 1)

these are all valid graphs, which should disable this optimization, right? Because when they use an explicit Add operator, they use generalized numpy broadcasting, but when they are converted to use scale/bias, there is no broadcasting allowed


fumihwh(2018-06-07 01:46:48):@anderspapitto I got your point. Yes, it's right. Let me fix it.
bddppq(2018-05-31 04:14:26):nit: maybe only use the if-else to set a variable and then put this long `list(APPEND ...)` in common code path?
smessmer(2018-06-01 21:19:05):merged into #1047 
linkerzhang(2018-05-30 03:58:44):no. it was discussed and agreed to keep the element number of scales the same as the rank of inputs. In this test case, the input's rank is 4, and we don't want to upsample the 1st and 2nd dimension, so it's 1.0. Make sense?
linkerzhang(2018-05-30 03:59:38):As the upsample op now support multiple-D input tensor now (instead of 4-D tensor previously).
CLAassistant(2018-05-30 02:20:57):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=1050) <br/>All committers have signed the CLA.
anderspapitto(2018-05-30 19:43:04):the windows build failure is legit - if you click through, you'll see that it's complaining because you're implicitly converting between integer types somewhere.

Once you fix that and all tests are passing, looks great!
zrphercule(2018-05-30 19:57:31):Yes, there is an implicity converting of int64_t to int, and maybe more. Let me try to find some way out.
bddppq(2018-05-31 15:25:59):@houseroad why not make sense to put in the subgraph? I think it's cleaner to keep the scopes of the blobs, also if you extract all of them to the global graph you can potentially get name conflict.
bddppq(2018-05-31 15:28:06):CI failures are unrelated, it's the model download from the model zoo being flaky.
dzhulgakov(2018-05-31 16:32:18):Actually @houseroad - has a point: do we have across the board support for nested initializers?

e.g. nothing in https://github.com/pytorch/pytorch/blob/master/caffe2/python/onnx/backend.py handles it properly. We should decide one way or another. We might even consider banning nested initializers in checker if we don't handle them yet.
bddppq(2018-05-31 17:10:41):IMO subgraph should be just a normal graph, and it's better to not make it to be different with the top level graph. Extracting blobs from subgraph to parent level makes the global graph looks messy, and as mentioned above it can potentionally hit name conflict. Re. support in c2, I feel it's lacking proper support of nested graph in general, a lot of places don't descend into subgraphs to do the same processing as for the top-level graph, so I don't want to use it  to judge whether a feature should be supported in onnx or not.
wschin(2018-06-01 22:05:05):If constants are moved to initializer list, does it mean that constant can be changed anyway? For example, to store labels of a classifier, I prefer to use a constant than an initializer.
bddppq(2018-06-02 04:22:17):@wschin No initializer can not be changed at runtime
wschin(2018-06-02 16:28:42):Right. But user could potentially provide an input to overwrite constants like labels. I am not sure if it is ok.
bddppq(2018-06-02 20:12:45):@wschin Nope users are not supposed to provide an input to override initialzier either. https://github.com/onnx/onnx/commit/3e560dd7cb76006c15591c9180eedfd8d4c68587
bddppq(2018-05-30 02:46:47):You can inspect the shape and elem type from the Tensor attribute as well
bddppq(2018-05-30 18:37:51):nit: const
dzhulgakov(2018-05-30 20:38:29):style guide: add {} over all loops and if statements even if they're one line
dzhulgakov(2018-05-30 20:40:33):std::move(t) ?
dzhulgakov(2018-05-30 20:43:45):It's interned - I guess you can just use `kValue`
zrphercule(2018-05-30 21:16:44):I guess you mean kvalue?
bddppq(2018-05-30 23:15:39):@zrphercule @anderspapitto Should we change the original version to take `int64_t`? I think then the compiler won't complain for neither passing in `int` nor `int64_t`.
zrphercule(2018-05-30 23:31:33):I guess you are right. I see no other usage of this "int" version, but I dunno if there is any.
houseroad(2018-05-31 13:01:45):Yeah, I also vote for `int64_t`. We use `int64_t` for dimensions in onnx proto, so it would be nice to keep consistency. 
houseroad(2018-05-31 13:07:15):Nit: add pseudo code to explain the effect of the optimization pass.

Something like the following code:
https://github.com/onnx/onnx/blob/master/onnx/optimizer/passes/eliminate_unused_initializer.h#L7
anderspapitto(2018-05-31 18:13:26):sounds good to me
linkerzhang(2018-06-01 15:34:53):this is not ok after std::move(t), right?
bddppq(2018-06-01 16:03:58):Yep need to move the initializer after this
linkerzhang(2018-06-01 15:23:04):Comments resolved. Merging it.
bddppq(2018-05-31 04:12:07):Hmm, such move is a breaking change
houseroad(2018-05-31 15:15:32):I think `using Common::ONNX_DOMAIN;` should be enough in most of the cases.
linkerzhang(2018-05-31 17:04:33)::). OK, removed the namespace Common.
linkerzhang(2018-05-31 17:04:50):Common namespace is not used for now.
snnn(2018-05-31 18:24:47):Just pushed more changes. Please review.
gramalingam(2018-05-31 22:49:27):Hmmm … the Travis CI build keeps timing out … any idea what we can do?
gramalingam(2018-05-31 00:27:17):if (! ((0 <= fromDimIndex) && (fromDimIndex < shape.dim_size()))
snnn(2018-05-31 01:01:37):Thanks. Will change it.
bddppq(2018-05-31 03:51:08):you should throw error here instead of silently return
bddppq(2018-05-31 03:52:14):please run clang-format 
bddppq(2018-05-31 03:58:15):`hasNInputShapes` already checks `has_tensor_type`
bddppq(2018-05-31 03:59:40):use the macro `fail_shape_inference`
bddppq(2018-05-31 04:02:24):Why do you need to print the dim values? what user should know is `shape.dim_size()` right?
snnn(2018-05-31 05:30:54):Fixed.
snnn(2018-05-31 05:31:05):Fixed.
snnn(2018-05-31 05:31:14):Fixed.
snnn(2018-05-31 05:32:04):Fixed.
snnn(2018-05-31 05:42:24):It's not so obvious. Because they may be inferred from another node.  Currently, there is no way to view it. 
bddppq(2018-05-31 03:50:03):How are they "not compatible"?
snnn(2018-05-31 03:56:13):Hi @bddppq 
See #1055

test_PReLU_1d_multiparam\model.onnx is one example of that.
test_PReLU_1d_multiparam\model.onnx is using opset 6.



bddppq(2018-05-31 04:07:19):So it's the opset not being the latest, but they are still valid test data right? A backend is recommended to inspect the opset version, and then decide what's the corresponding semantics of this model at the specific opset version (of course a backend can raise not supported error when the opset is not supported). Even in the backend test suite you can skip the tests once it's an opset version that this backend doesn't support.
snnn(2018-05-31 04:35:39):It depends on how you judge it's valid or not.
From an end user's perspective, I'll read onnx 1.2's document, to get know how this OP(PRelu) works.  If it doesn’t work like the document says, it’s wrong. The document says it’s from OPSet 6, and the test data is in opset 6, and the backend I'm using supports opset 6.  Therefore, they should be compatible. This is how version check works. I wouldn’t bother to read all the history versions of “Operators.md”.

Otherwise, this thing could be very complicated. Caffe2’s document would say: Oh, PRelu has two major versions: PRelu(new) and PRelu-1(old). For each major version, it has multiple subversions. We only support PRelu(new) for opset 6 and PRelu-1 for opset 1 but we don’t support PRelu(new) for opset 7 or opset 5. If so, why we need to distinguish them as “PRelu” and “PRelu-1”? Just “PRelu” is enough. Because each op can have different behavior in each opset. 


bddppq(2018-05-31 04:56:34):Ok I think I have misunderstood your issue, I thought you were claiming that **all** opset 6 models are invalid because I saw you are deleting all pytorch generated tests.
PRelu's opset indeed should be bumped to opset 7. But I don't issues for other ops so far, should we instead only delete PRelu test data?
snnn(2018-05-31 17:40:53):Most of these data are fine, except 5: PReLU_1d_multiparam, PReLU_2d_multiparam, PReLU_3d_multiparam, operator_transpose, operator_type_as

For PRelu related, I suggest upgrading PRelu version to 7
For operator_transpose and operator_type_as, please delete them

bddppq(2018-05-31 18:53:23):@onnxbot retest this please
bddppq(2018-05-31 21:08:42):@onnxbot retest this please
linkerzhang(2018-06-01 05:50:50):we may add test case to cover it.
houseroad(2018-06-02 15:26:58):Thanks for fixing the problem.
bddppq(2018-06-02 04:24:59):nit: alignment 
bddppq(2018-06-02 04:26:02):nit: change  `PRelu_ver6_doc` to `PRelu_ver7_doc`
linkerzhang(2018-06-03 02:36:44):fixed. thanks!
bddppq(2018-06-01 21:15:05):@anderspapitto it does, but safe to remove them first to unblock the patch release early next week
houseroad(2018-06-02 15:33:28):Nit: indent
houseroad(2018-06-02 15:42:06):Nit: indent
bddppq(2018-06-02 16:00:32):oops good catch
pranavsharma(2018-06-02 01:11:25):@linkerzhang please review.
linkerzhang(2018-06-03 02:11:58):atleast -> at least. please also run clang-format for the file. Thanks!
houseroad(2018-06-05 15:33:43):@onnxbot retest this please
fumihwh(2018-06-05 16:17:04):@houseroad 
Current node's Param will be triggered first. 
Return true means current Param is before target Param.
Although no order between them actually.
houseroad(2018-06-05 16:21:44):@fumihwh If they are both kParams, just return their order in the graph. Does this sound good to you? :-)
fumihwh(2018-06-05 16:25:46):@houseroad ....kParam is not inGraphList......
houseroad(2018-06-05 16:30:24):@fumihwh ok, then it does not make sense to return their order in the graph... thanks!
anderspapitto(2018-06-04 18:36:21):this seems to be getting longer/more complicated than it needs to be. Can't we just use

```
  // Check whether this node is before node n in the graph.
  bool isBefore(Node* n) {
    ONNX_ASSERT(n != nullptr);
    ONNX_ASSERT(n->inGraphList());
    for (Node * p = next(); p != nullptr; p = p->next()) {
      if (p == n) {
        return true;
      }
    }
    return false;
  }
```
fumihwh(2018-06-04 23:31:08):@anderspapitto 
No..
https://github.com/onnx/onnx/blob/master/onnx/common/ir.h#L389
`this circular is a doubly-linked list`
Means `p->next()` will never get `nullptr`, it will go to the first node of graph after reach the last one.
So I use `*graph_->end()` instead.

If `kind_ == kParam`, we need return true because self is in initializer, it is before any nodes in graph.
As same reason, we need to return false if `n->kind() == kParam`.

linkerzhang(2018-06-04 21:08:55):'X.'  --> 'X'.
linkerzhang(2018-06-04 21:09:20):adding some description here is better.
linkerzhang(2018-06-04 21:11:31):'Set’ is better than "Replace". The input tensor is NOT replaced. it's 0 or 1 in the output tensor.
linkerzhang(2018-06-04 21:14:10):same as above. 
linkerzhang(2018-06-04 21:18:59):provides -> provided.
linkerzhang(2018-06-04 21:20:56):Ids ids -> node ids
linkerzhang(2018-06-04 21:22:37):original description contains the enum strings. Keep them please so that readers will know what are the potential string values for the attribute.
linkerzhang(2018-06-04 21:26:03):the updated description is confusing. when the value is nan, use the 'true' branch.
shauheen(2018-06-05 13:13:32):>more than the number of dimensions of [](start = 60, length = 37)

Can this be further improved? no more than dimension of 'X'.?
shauheen(2018-06-05 13:18:48):>Replaces [](start = 4, length = 8)

I think @linkerzhang is correct, in which case this should also be set.
gramalingam(2018-06-05 20:50:20):How about "are mapped to 1, others to 0" ?
gramalingam(2018-06-05 20:51:47):How about "Maps the values of the input tensor to either 0 or 1 …"?
NiklasGustafsson(2018-06-05 22:07:54):I agree. The one I found there was way too generic and didn't add any information. I'll go through and write something useful for each type constraint.
NiklasGustafsson(2018-06-05 22:24:31):Addressed systematically.
NiklasGustafsson(2018-06-05 22:28:42):@shauheen, I'm not sure I understand your comment. What are you referring to?
NiklasGustafsson(2018-06-05 22:29:13):I went with 'mapped'
NiklasGustafsson(2018-06-05 22:30:16):Thanks
NiklasGustafsson(2018-06-05 22:30:48):They are still there. The line got long, but the values are there.
gramalingam(2018-06-05 22:32:57):I assumed he meant change "Replace" to "set" … 
NiklasGustafsson(2018-06-05 22:34:27):I tried to clarify. Which branch to use actually depends on whether the array has a 0 or 1 for a give node.
NiklasGustafsson(2018-06-05 22:34:59):Fixed.
gramalingam(2018-06-05 23:22:03):change "by either" to "to either"?
wschin(2018-06-06 23:46:13):It's not quite clear on selecting elements based on the current description. It sounds reasonable to me to support two different input shape [N, C] and [C]. If I am correct, it'd be nice to clarify two things.

1. Only accepts 1-D and 2-D tensors
2. If the input is 1-D/2-D, the second input specifies the coordinates along the first/second axis we are going to select.

wschin(2018-06-06 23:49:53):What are acceptable input shapes? I guess this can support [C], [N, C], … [N, C_1, …, C_n].
wschin(2018-06-07 00:50:25):What is its output shape? [1, C] or [C]?
wschin(2018-06-07 06:06:25):Do you want to explicitly mention that input shape can be [C] or [N, C]? If input's shape is [C]/[N, C], I would expect that the output shape is also [C]/[N, C].
wschin(2018-06-07 06:09:04):Does it mean that output shape is always 1-D tensor (i.e., shape is [C])? Not sure if we should use [1, C] following NN operators' convention.
wschin(2018-06-07 06:13:41):What are allowed input shapes and how this operator deals with them? I guess there are two cases. First, all input shapes are [C] and the inputs are concatenated along axis=0. Second, all input shapes are [N, C] and we concatenate them along axis=1.
wschin(2018-06-07 06:18:44):Does this operator accept arbitrary numerical tensor? If yes, it'd be nice to note it. On the other hand, if only tensors with shape [C] or shape [N, C] can be inputs, we can add such a restriction here.
wschin(2018-06-07 06:21:02):What are input/output shapes here? Are they [C] and [N, C] again? It'd be great to write down detailed behavior of all operators so that all implementations can be easily aligned with the standard.
wschin(2018-06-07 06:35:38):Could 1-D array be its input? Looks like the output probability tensor's shape implies that the input must be [N, D]. 
wschin(2018-06-07 06:38:19):Can this operator consume inputs with shape [C]? If not, do we need to rule out that case explicitly (saying something like inputs must be 2-D tensors)?
wschin(2018-06-07 06:40:51):What are the expected behaviors when the input becomes [N, C] or higher-dimension tensors?
wschin(2018-06-07 16:28:17):What are its input and output restrictions? I feel like we need to have a input/output format for all traditional classifiers and regressor somewhere. The format document may include something like:
- Input shape can be [C] or [N, C]
- For classifiers, if the input shape is [C], the output label/probabilities shape will be [1]/[L]. If the input shape becomes [N, C] the output labels/probabilities shape will be [N]/[N, L], where L is the number of total labels.
- For regressors with a scalar output, if the input shape is [C], the output shape will be [1]. If the input shape becomes [N, C] the output shape will be [N].
- For regressors with a vector output, if the input shape is [C], the output shape will be [L]. If the input shape becomes [N, C] the output shape will be [N, L], where L is the length of an example's output vector.
wschin(2018-06-07 16:37:40):Does it mean that the input shape must be [N, C] (where C is the feature length)? If I have an input shape [N, C, H, W], this statement looks not very clear.
NiklasGustafsson(2018-06-25 20:50:19):Is it correct that AFE() only accepts 1- and 2-D tensors? I had not caught on to that. Do you have any reference to it that you can forward to me?
NiklasGustafsson(2018-06-25 20:51:51):It should accept any shape.
NiklasGustafsson(2018-06-25 20:57:07):I believe it's [1,C].
NiklasGustafsson(2018-06-25 21:02:04):Done
NiklasGustafsson(2018-06-25 21:05:42):The implementation I have looked at source code for accepts 1-D tensors and that N defaults to 1 if there's just one dimension. Do you believe that is an error?
NiklasGustafsson(2018-06-25 21:08:01):The implementation I have looked at allows [C]. Do you feel that doesn't make sense?
NiklasGustafsson(2018-06-25 21:13:57):This operator always adds another dimension to the end of the shape, with the size of the new dimension equal to the number of categories.
NiklasGustafsson(2018-06-25 21:21:29):Right. Clarified to indicate special case for [N,F] situation.
NiklasGustafsson(2018-06-25 21:24:16):That's a good idea, but where would we place it? Should we just repeat it on each operator?
wschin(2018-07-17 15:58:40):I believe it'd be better to repeat it for each operator because the definitions of different operators are essentially independent. A shortcut is to define on an operator (for example, only specify those restrictions for linear classifier and regressor) and the refer it at other places.
wschin(2018-07-17 16:09:55):Do you mean that Scaler only accepts [N,C]-inputs? If so, we need to say something like "This operator only consumes 2-D inputs" in its document string. If it can handle both of 1-D (aka [C]) and 2-D (aka [N, C]) inputs, we can add "This operator only consumes 1-D and 2-D inputs. The right-most axis is considered as the feature dimension."
wschin(2018-07-17 16:15:19):Thanks. Could you add your illustration into the doc string?
wschin(2018-07-17 16:21:09):It's ok to accept [C]. My point was that the following formulation cannot be applied to [C] and [N, C] at the same time. More specifically, the new formulations here are perfect for inputs with shape [C], but they are not mathematically meaningful when input becomes [N, C]. You probably want to say something like
  1. This operator accepts only [C] and [N, C]
  2. The following formulations are applied along C-axis. That is, different examples in one batch are normalized independently.
wschin(2018-07-17 16:25:24):It's fine to accept 1-D input, but the spec needs to explicitly indicate what shapes can be consumed by this operator and how the corresponding output shapes are calculated.
wschin(2018-07-17 16:39:05):If so, it's not consistent with DictVectorizer's output shape [C]. Anyway, it's not problem to have different behaviors for different operators. We just need to document the desired I/O shape down.
wschin(2018-07-17 16:41:04):Yeah. Would you mind to add "This operator may accept a tensors with an arbitrary shape" to the end of this operator's introduction?
wschin(2018-07-17 16:53:09):If it is not 1-D (i.e. shape [C]) or 2-D (i.e., shape [N, C]), which axis we should extract values along? Considering a similar ONNX operator, Slice, in ONNX, we can see that for arbitrary shape's extraction, an attribute `axes` is required. Also, you can take a look at our latest runtime code.
linkerzhang(2018-08-27 08:11:18):no more than number of dimensions of 'X' is misleading. number of dimensions means "rank" of 'X".

The meaning here is, no more than the size of 'X'.
linkerzhang(2018-08-27 08:25:08):two “the”s, remove one.
wschin(2018-08-28 17:00:16):The behavior of AFE is ambiguous in N-D cases (N > 2).  For 1-D and 2-D inputs (shapes are [C] and [N, C], respectively), could you add that the indices are applied to the last axis of the input (so that most use cases are clarified)?
wschin(2018-08-28 17:08:52):Could you explain how the inputs get concatenated in the doc (maybe reuse my comment above)? The restriction on batch size is removed from the previous doc of this operator, which is the only hint that [N, C_1], ..., [N, C_n] will be merged into [N, C_1 + … + C_n].
wschin(2018-08-28 17:18:27):Could you add a note, saying that this operator doesn't change the input shape and accepts 1-D and 2-D tensors?
wschin(2018-08-28 17:22:37):Could you explicitly say that this operator accepts [N, C]-input because its 2nd output shape is [N, E]? Also adding a note "When input is [C], it will be implicitly reshaped into [1, C]." Thanks.
wschin(2018-08-28 17:51:35):Ping. What do you think about my comment above?
wschin(2018-08-28 17:55:34):What is the input shapes accepted?
wschin(2018-08-28 17:56:33):What are the input and output shapes?
NiklasGustafsson(2018-08-28 19:35:15):I'll address it in an update.
NiklasGustafsson(2018-08-28 19:50:05):LabelEncode accepts tensors of all shapes.
zrphercule(2018-06-06 00:47:09):@bddppq Looks like finally it passed all checks, could you please review it for me? Thanks!
zrphercule(2018-06-06 23:50:51):Just ignore these stupid commits I pushed hahaha... Finally learnt how to run Travis locally.
Thanks for your review and help Junjie!
bddppq(2018-06-07 05:39:29):lol travis just provided you a good chance to test this change: https://travis-ci.org/onnx/onnx/jobs/389062433

```
Start downloading model inception_v2 from https://s3.amazonaws.com/download.onnx/models/opset_7/inception_v2.tar.gz
Failed to prepare data for model inception_v2: [Errno socket error] [Errno 110] Connection timed out
0 times tried
Start downloading model inception_v2 from https://s3.amazonaws.com/download.onnx/models/opset_7/inception_v2.tar.gz
Failed to prepare data for model inception_v2: [Errno socket error] [Errno 110] Connection timed out
1 times tried
Start downloading model inception_v2 from https://s3.amazonaws.com/download.onnx/models/opset_7/inception_v2.tar.gz
Failed to prepare data for model inception_v2: [Errno socket error] [Errno 110] Connection timed out
2 times tried
```

This means:

1. this change is working correctly (retried three times)
2. we probably should add `time.sleep` between different tries

bddppq(2018-06-05 18:13:03):kwargs
bddppq(2018-06-05 18:14:04):better to return the return value of `func` to caller

```
try:
  return func(*args, **kwargs)
...
```
bddppq(2018-06-05 18:14:50):nit: better to print how many times you are retrying (instead of remaining)
bddppq(2018-06-05 18:15:57):no need to use args, better to use explicit arguments
bddppq(2018-06-05 18:16:34):self._retry_execute
bddppq(2018-06-05 18:18:35):Let's put the creation and deletion of the tmp file into the _download_model? (I think it's ok to create tmp files couple times)
zrphercule(2018-06-05 18:31:25):Sure, and move the "os.remove" into it as well
bddppq(2018-06-06 02:08:56):don't need to try except here anymore
bddppq(2018-06-06 02:10:11):Do not except everything (this will catch unexpected events, e.g. user pressing ctrl-c). `except Exception` is more suitable.
bddppq(2018-06-06 02:12:39):in case `NamedTemoraryFile` throws, download_file will be not yet defined, and so in your finally cause, download_file will be an undefined variable.
bddppq(2018-06-06 02:41:32):you can still support both `*args` and `**kwargs`:
```python
def _retry_execute(self, func, *args, **kwargs):
   times = kwargs.pop("times", 3)
   ...
```
What I normally would do is to separate the special arguments in a decorator:
```python
def retry(times):
    assert times >= 1
    def wrapper(func):
        @functools.wraps(func)
        def wrapped(*args, **kwargs):
            ...
        return wrapped
    return wrapper
```
and of course then the call side becomes:

```python
@retry(3)
def download_model(...):
    ...

def _prepare_model_data(...):
    ...
    self.download_model(...)
    ...
```

bddppq(2018-06-07 06:21:10):`i + 1` or maybe just change the for loop to `for i in range(1, times + 1)` since you are currently doing `i == times - 1` below.
zrphercule(2018-06-07 17:21:52):Yes hhh I finally got a testcase lol
bddppq(2018-06-05 03:52:57):@onnxbot retest this please
neginraoof(2020-06-04 23:46:14):@ArmenAg @houseroad Do you happen to know a use case model/op that could use this optimization?
houseroad(2018-06-05 01:32:43):Check the first node’s type and axes value
houseroad(2018-06-05 01:40:11):Nit: check the node type
houseroad(2018-06-05 01:41:44):Add some comments (pseudo code) to describe the behaviors of the pass: 1) when to fuse squeezes, 2) when to remove the first squeeze
houseroad(2018-06-05 01:53:10):The axes computation is probelmatic, such as axes=(0,2) and axes=(0,) in squeeze 1 and 2, we will get wrong axes in the fused op
houseroad(2018-06-05 09:19:16):The method still cannot handle all the cases. For example, squeeze1 with axes=(0,1,2,4) and squeeze2 with axes=(0,). Since axes should be a small vector, you can create a mapping from squeeze2 dims to squeeze1 dims.

Also squeeze1’s axes should be sorted.
houseroad(2018-06-06 05:03:24):Nit: check squeeze-2’s input (X not Y)
houseroad(2018-06-06 05:06:36):Nit: std::vector<int64_t> sorted_axes_1(axes_1.begin(), axes_2.end());
bddppq(2018-06-07 00:15:34):hmm I intentionally didn't add the type mapping for string because it's not very clear when dealing with encodings https://github.com/onnx/onnx/blob/6bd04dd59ad7c39394df2a47023edeea4216bf72/onnx/numpy_helper.py#L37-L38
fumihwh(2018-06-07 01:21:21):@bddppq Thanks. There are some ops accept `tensor(string)`. Maybe it's time to clarify how to deal with encodings.
bddppq(2018-06-06 23:42:08):@houseroad We aim to use it as materialized broadcast.
houseroad(2018-06-06 23:44:42):@bddppq how about directly enhancing `Tile`?
bddppq(2018-06-06 23:56:40):@houseroad that requires some calculation to get the corresponding `repeats`, and this calculation is not easy to express with our existing operators (for dynamic shape). I think broadcast is common enough to worth it's own operator. This will help avoiding error-prone conversion logic based on static shapes when exporting from a dynamic frontend to onnx.
bddppq(2018-06-07 00:06:49):I would like this operator to mimic the full numpy broadcast semantics, e.g.
- It should support expanding to higher rank: `Expand([1.0], (2, 2)) => [[1.0, 1.0], [1.0, 1.0]]`
- It should support expanding in both directions: `Expand(ones(1, 2, 1), (2, 1, 2))` => ones(2, 2, 2)`

Basically I think it should be roughly equivalent to `Expand(lhs, shape) => lhs + zeros(shape)` with `+` being the broadcast add.
zrphercule(2018-06-07 00:15:39):@bddppq I wonder if you mean "Expand(ones(1, 2, 1), (2, -1, 2)) => ones(2, 2, 2)"? Or we need to ignore the second dim of shape (2, 1, 2) if it is smaller than the original shape?
bddppq(2018-06-07 05:34:09):@zrphercule No I meant `shape=(2, 1, 2)`. onnx has now fully aligned our broadcast semantics with numpy https://docs.scipy.org/doc/numpy/user/basics.broadcasting.html
zrphercule(2018-06-07 17:44:52):@bddppq Then what about -1? Should we support -1 like pytorch?
https://pytorch.org/docs/master/tensors.html#torch.Tensor.expand
bddppq(2018-06-07 17:50:49):@zrphercule I think we shouldn't support that semantics (and we should be able to simply change all -1 to 1 when exporting from pytorch to onnx).
linkerzhang(2018-06-08 16:28:08):I don't quite understand this op right now. For some cases, Tile can handle, for some other cases, reshape + pad could handle. No?
bddppq(2018-06-08 17:53:09):@linkerzhang For **static shape**, and if the dims of one side is greater than the other side, Tile and Expand are convertible. However, it's super hard to express the numpy broadcast logic for dynamic shapes (because that means we can only use Operators to express it). Besides, this will help backends to adopt out new braodcast semantics because they can split a broadcast-add to first do materialize broadcast (aka. expand here) and then do non-broadcast-add.
houseroad(2018-06-13 14:05:15):@zrphercule could you also add some node tests for the new operator? Please follow [the doc](https://github.com/onnx/onnx/blob/master/docs/OnnxBackendTest.md)
zrphercule(2018-06-13 18:09:35):@houseroad Sure, I will submit some node test data and docs after I finished the caffe2 side of this operator.
zrphercule(2018-06-14 22:25:21):@bddppq @houseroad Could you please review if the code is fine here? Then I will upload some data :)
linkerzhang(2018-06-17 18:03:37):Per the two test cases, I'm seeing they should be exactly using "tile", No? if yes, may you guys give an example that tile can't be used to represent please? calculation to get the "repeats" in tile should be similar with getting the "shape"? 
bddppq(2018-06-18 16:59:18):@linkerzhang @zrphercule  Yes this is because NodeTests in onnx backend tests are more focus on isolated single node and therefore there is no way to express dynamic shape in it. Let's use simple model tests (like [here](https://github.com/onnx/onnx/blob/master/onnx/backend/test/case/model/single-relu.py)) to add tests for dynamic shape:

```
Graph(
  input=['X', 'shape'],
  output=['Y'],
  initializers=[
    Tensor(
      name='X',
      shape=(1, 3, 1),
      floats=(...),
    )
  ],
  nodes=[
    Node(
      op_type='Expand',
      input=['X', 'shape'],
      output=['Y'],
    ),
  ]
)
```

and multiple `expect(...)` with different input `shape` to demo the different cases that expand supports (e.g. `[1, 3]`, `[3, 1, 3]`, `[3, 3, 2]`).
linkerzhang(2018-06-19 15:47:26):@bddppq Tile now supports the "repeats" as an input, which means it could be dynamic. Am I missing anything? Looks to me the two operators this "expand" and "tile", we only need one.
bddppq(2018-06-19 17:36:12):@linkerzhang Yes `repeats` is input and it can express dynamic values. But if onnx only has `tile`, this means when a frontend needs to export a model that has `expand` to onnx, it needs to do conversion from the expand's `shape` input to tile's `repeats` input, and when the shape is dynamic, the frontends needs to express all the conversion logic using existing onnx operators, instead of some python/c++ code, and this is pretty hard (or even impossible right now) for the `shape` => `repeats` conversion.
AFAIK both tf and pytorch have `expand` (also numpy).
linkerzhang(2018-06-20 17:10:18):@bddppq so I'm seeing that adding "expand" is for model conversion convenience. If this is the case, then we should either remove "tile" or have "expand" as a function using "tile". 

As ONNX is going to cover more models from other frameworks, I don't think that we'll have an operator union from all frameworks. This is also why we agree that several experimental ops were kept and not checked-in to ONNX standard.

Make sense please?
bddppq(2018-06-20 17:40:16):@linkerzhang 
1. it's not just for convenience, as I have explained countless times in this thread, there are fundamental troubles when expressing the conversion logic between tile and expand via onnx operators
2. function is an experimental feature, and support (both tooling wise and framework wise) for it is just half done or even less
3. there needs to be a boundary on what should go through function and not. should we express BatchNormalization as a function? apparently not. Same reason for expand, it represents broadcast which is a fundamental operation in any DL framework, and this can help us avoid adding broadcast support to all of our operators.
zrphercule(2018-06-21 21:44:57):@bddppq @houseroad @linkerzhang So do you think we should land it?
zrphercule(2018-07-02 22:29:28):@linkerzhang Hi Ke, I wonder if you have anything else to add regard this diff? Or we can land it now? Thank you!
bddppq(2018-07-06 22:28:24):@linkerzhang Is there any update on your exploration of using function to represent expand?
bddppq(2018-07-26 04:43:27):Final ping @linkerzhang
linkerzhang(2018-07-27 01:51:08):I am sorry that I forgot to get back to this PR.

To me, "expand" and "tile" can be converted to each other, as "shape" is able to be converted to "repeats" (an issue there is "repeats" is int64, the "shape" may not be n times of input's dim value), or "repeats" is able to be converted to "shape" too (this is more straight forward to be converted without any issue). 

@bddppq  @zrphercule  do you want to retire "tile" or replace it with a function if folks still need it please?

Thank you!

btw, a composed operator "MeanVarianceNormalization" was changed to use Function as an example for your reference here, https://github.com/onnx/onnx/blob/master/onnx/defs/experiments/experiments_functions.cc
bddppq(2018-07-27 07:27:27):@linkerzhang The problem of using function here is how to use onnx operators to express our numpy (bindirectional) broadcast semantics. 
bddppq(2018-07-31 08:51:57):ping @linkerzhang 
springishere(2018-08-03 17:35:01):@bddppq Looks like this commits added Expand operator. Can you add it to the export as well? Currently we can't export pytorch to onnx. From code here expand is not handled: https://github.com/pytorch/pytorch/blob/master/torch/csrc/jit/export.cpp#L47
bddppq(2018-06-06 23:01:01):Need to bump the opset here https://github.com/onnx/onnx/blob/033f956f41c55fd409e1c4a0d09795ae5411447f/onnx/defs/schema.h#L588, since 7 has been frozen in release 1.2 already.
houseroad(2018-06-06 23:37:56):In this case, we can also use `Tile`.
bddppq(2018-06-15 09:30:20):move it to a second `export_*` function so it will appear as a separate code snippet in the doc.
bddppq(2018-06-15 09:32:51):This description is too ambiguous. User might think the `given shape` will be exactly the `output shape` (which is not).
bddppq(2018-06-15 09:33:15):the description is incorrect
bddppq(2018-06-15 09:34:05):all types should be supported (there is a variable defined in schema.h for representing the whole set of types).
houseroad(2018-06-15 10:16:00):Agree with @bddppq. This operator is not very popular (For example, in tensorflow and numpy, people use expand_dim + tile instead). I think that only with the vague description users cannot get the precise definition of the op. Please put more details and make sure the backend developers will know how to implement it after reading our description.
houseroad(2018-06-18 22:34:41):You can explicitly say that this operator is similar to numpy.broadcast_to, and describe the difference here.
zrphercule(2018-06-18 22:43:45):sure :)
bddppq(2018-07-27 13:55:17):@zrphercule the second input is shape, so its type should be int64, and it doesn't need to be the same as input(0). use a different type variable here, and add a typeconstraint says this type variable can only be int64.
linkerzhang(2018-07-30 08:24:44):replace "S" with "tensor(int64)" given there's only one kind of type for this input.
linkerzhang(2018-07-30 08:25:07):formatting this file.
bddppq(2018-07-30 08:38:43):@linkerzhang fixed the type description. re. format, this chunk is actually already clang-formatted.
fumihwh(2018-08-07 23:21:06):Dtype is inconsistency.
Default dtype of `np.ones` is `float64`. But in graph, `onnx.TensorProto.FLOAT` is just `float32`.
zrphercule(2018-08-13 22:35:58):You are right, I have submitted a pr for this:
https://github.com/onnx/onnx/pull/1283
anirudhacharya(2018-06-08 20:54:32):sure I will add both.
linkerzhang(2018-06-10 18:54:39):Hmmm, given it's an optional attribute with default value specified, looks to me version needs not to be bumped. However, the default value is not 0, so models with using current version lpnormalization will have a slightly different result when running with backend using this updated op spec. Do we agree that this is still backward compatible? :) @ebarsoum @bddppq  @houseroad @ezyang @gramalingam 
bddppq(2018-06-10 19:14:49):> However, the default value is not 0, so models with using current version lpnormalization will have a slightly different result when running with backend using this updated op spec.

This means backward incompatible :-)
houseroad(2018-06-19 00:08:48):Turn out the operator is a general version of `L2Normalization` in [TF](https://www.tensorflow.org/api_docs/python/tf/nn/l2_normalize). @anirudhacharya could you put more details to the description, so users can easily understand the semantics of this operator. Thanks!
anirudhacharya(2018-06-19 01:02:47):@houseroad sure I can make the changes. So do I take it that ReduceL1, ReduceL2 and LpNormalization are all going to stay?

If LpNormalization stays, do we restrict p value to 1 and 2? Should we restrict the axes along which L2Norm can be performed? I am inclined to yes for both these questions.

With regards to your question in your own PR, most frameworks have some form of both ReduceL1/2 and L2Normalization. MXNet, for example, has [L2Normalization](http://mxnet.incubator.apache.org/api/python/symbol/symbol.html#mxnet.symbol.L2Normalization) and [norm](http://mxnet.incubator.apache.org/api/python/symbol/symbol.html#mxnet.symbol.norm) which is very similar to ReduceL1/2.
houseroad(2018-06-19 20:11:12):@anirudhacharya yes, they will stay all, unless you want to make LpNormalization as a function op. :-)

For p, ~~yes, restricting to 1 and 2 sounds good to me~~ after some thought, I think there is no need to add the restriction on p, although people only use 1 and 2 for now.
For axis, I think we don't need to set any restriction.
anirudhacharya(2018-06-20 21:08:42):The reason to restrict is that we can have a concrete definition for the operator and we can have test cases to verify the behavior of the operator. For instance how would we define the operator for a 6 dimensional tensor with p=3 and axis=4. Will we have tests for such cases?
anirudhacharya(2018-06-25 20:33:36):@houseroad ping for review. I still need to update the data files and generate the docs(currently running into errors while generating them. will fix it). but can you please take a look at the definition and the tests I have added?
houseroad(2018-07-06 18:36:45):@anirudhacharya friendly ping for update :-)
anirudhacharya(2018-07-12 07:36:50):@houseroad I addressed your comments. pls take a look. Regarding the op_set version bump I think @bddppq has a point. Why did you suggest that this does not require an op_set bump? 
houseroad(2018-07-27 18:05:40):It's more like a bug fix instead of adding new features, and I don't think we have any existing models using this ops. Let's why I would suggest not bumping up the opset version.
houseroad(2018-07-27 18:06:13):@anirudhacharya could you rebase to master and pass the CI?
anirudhacharya(2018-07-27 18:40:50):@houseroad okay, will do.
anirudhacharya(2018-08-26 23:52:10):@houseroad sorry for the delay on this PR. Instead of resolving merge conflicts in this PR, I have created another PR with these changes here - https://github.com/onnx/onnx/pull/1330

The comments raised in this PR have been addressed there.
linkerzhang(2018-06-11 15:07:02):run clang-format please
houseroad(2018-06-26 00:02:23):please set the type of x using astype(np.float32)
houseroad(2018-06-26 00:04:27):please set the type of x using astype(np.float32)
houseroad(2018-06-26 00:06:51):please set the type of x using astype(np.float32)
houseroad(2018-06-26 00:07:25):please set the type of x using astype(np.float32)
houseroad(2018-06-26 00:11:45):Tensorflow defines L2Normalization like: `output = x / sqrt(max(sum(x**2), epsilon))`
@ebarsoum what's your thought?
anirudhacharya(2018-06-15 00:11:16):the window size parameter for LRN is supposed to take only odd values. Could you please update the spec?
houseroad(2018-06-15 00:40:22):@anirudhacharya I think there is no need to forbid even values.
anirudhacharya(2018-06-09 00:39:19):no I tested this on both numpy and with mxnet as backend. here 5 is the size of the array and 4 is the index. infact having 5 here will give ArrayIndexOutOfBoundsException.
gramalingam(2018-06-09 04:17:57):As I understand, numpy indexing is upper-bound exclusive. You mentioned previously that the "test case has a +1 because numpy indexing is upper-bound exclusive." The +1 appears only in the second argument of "min". Shouldn't you either move the +1 outside the min or, equivalently, have a +1 for both arguments of "min"?
gramalingam(2018-06-11 00:08:28):When I run this modified version, it works fine for me. If I use the original value of 4, I get incorrect results. Is it possible that you have different parenthesization in your mxnet version (with +1 outside min) than in the above file?
houseroad(2018-06-12 01:47:48):I agree with @gramalingam, since the upper bound is exclusive, we should use C (i.e., 5) instead of C-1 in this case. I locally tested the script, it worked.

anirudhacharya(2018-06-12 02:57:00):@gramalingam yes in my test example, I had the +1 outside the parentheses.
rdzhabarov(2018-06-08 18:47:49):missing clang-format
linkerzhang(2018-06-10 17:03:00):@bddppq @houseroad @lupesko please help to review this op spec clarification.
linkerzhang(2018-06-12 04:53:33):@skottmckay  for your reference, keep op history example as https://github.com/onnx/onnx/blob/master/onnx/defs/nn/old.cc.
skottmckay(2018-06-12 17:16:14):Could you clarify the request for me? Does this require a version number increment in order to keep the old information (the 'output_sequence' attribute was removed from RNN ops in V7 - https://github.com/onnx/onnx/commit/4642745765188bde07de434fa5413dc98d7f8c2c#diff-b5717083f7a2a8796673400e589552af), and if so where would that need to be done?


gramalingam(2018-06-12 17:23:16):If the specification of the op is changed, the old definition/specification is moved to the corresponding old.cc file (in the same folder), and the new definition/specification is given an incremented version-number. I am guessing that is what Ke is referring to.
skottmckay(2018-06-12 17:39:49):Just wanted to check that updating the version number was the desired update. 

This change just clarifies how the R weights are interpreted so things are consistent, and there are no changes to the attributes/inputs/outputs. Additionally we didn't change the version number when the test case python for some of the operators changed recently. Happy to make whatever change is needed. 
gramalingam(2018-06-12 17:51:03):Ok … I hope @houseroad can answer that question. Changing the equations changes the specification of the operator,  which would suggest increasing the version number ...
houseroad(2018-06-13 14:20:15):@skottmckay even it is just clarification of R, it already changes the semantics of the operator's input. The RNN/GRU/LSTM support may already depend on this. @anderspapitto could you confirm this?
If so, we should bump up the opset_version, and add the old def in `old.cc`.
linkerzhang(2018-06-13 16:55:45):@houseroad  I just realized that this PR is clarifying semantics and fixing the documentation issue, so it should not be taken as a breaking change. Yep. please confirm whether fb's implementation is following the clarified documentation in this PR. Thank you!
linkerzhang(2018-06-14 15:45:14):@houseroad  @anderspapitto please confirm whether fb's implementation is following the clarified documentation in this PR. Thanks a lot!
houseroad(2018-06-15 02:33:31):Keep tracking https://github.com/pytorch/pytorch/issues/8544
linkerzhang(2018-06-15 05:22:28):Thank you very much! @houseroad 
CLAassistant(2018-06-11 17:34:25):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=1106) <br/>All committers have signed the CLA.
bddppq(2018-06-11 19:09:41):You have modified the `third_party/benchmark` submodule, which should be intentional.
bddppq(2018-06-16 00:01:46):@onnxbot retest this please
bddppq(2018-06-16 00:02:36):@onnxbot retest this please
bddppq(2018-06-11 18:49:04):Why `void*`? Should be `T*`
bddppq(2018-06-11 19:03:39):move these to tensor.h, and define add, sub, mul, div on the Tensor class that applies the corresponding operation to the whole tensor.
bddppq(2018-06-11 19:06:43):use `std::find`, which will returns you an iterator (similar to a pointer) to the corresponding initializer (or end() which indicates not found).
yinghai(2018-06-11 22:19:17):`const auto& v`? 
yinghai(2018-06-11 22:22:17):`a_T_data_.data()`? 
yinghai(2018-06-11 22:22:36):nit: the naming of the parameters are not very intuitive. 
yinghai(2018-06-11 22:23:17):`std::accumulate`?
yinghai(2018-06-11 22:25:12):Also I suggest we use template class F instead of function pointer. 
yinghai(2018-06-11 22:26:20):Explicit memory allocation here is not very safe. We can just we vector<T>. 
yinghai(2018-06-11 22:28:14):What exception type are we throwing here? 
yinghai(2018-06-11 22:34:49):Shouldn't it be `const auto&`? We are incurring a copy here. 
yinghai(2018-06-11 22:45:43):Can't we just use something like `std::multiply<T>`? 
varunjain99(2018-06-11 23:18:43):included this function because I need the tensor from a name; however initializers and initializers_name are separate lists. 
varunjain99(2018-06-11 23:22:32):perhaps i should instead define a function getInitializerByName?

varunjain99(2018-06-11 23:22:46):fixed
varunjain99(2018-06-11 23:30:14):changed void to T and moved template to private as the only purpose is to serve apply_binary_function
varunjain99(2018-06-11 23:30:19):changed
bddppq(2018-06-12 00:46:03):Yeah you can just name it as getInitializer with `const std::string&` as input type
bddppq(2018-06-12 00:47:48):Agree
varunjain99(2018-06-12 06:52:22):changed to vector
yinghai(2018-06-12 16:48:47):`1L` also works. :) 
yinghai(2018-06-12 18:57:15):@bddppq Do we have Exception types in ONNX? 
yinghai(2018-06-12 19:00:56):Can we have
```
template<typename T, typename F, int64_t block_size>
void BinFunc(F&& f, Tensor* a, std::vector<T>* T_data, std::vector<T>* a_T_data)
```
as signature? First, it conforms to the Google code style better. Second, it avoids passing by function pointers. Simple functions like binary ops should really be inlined. 
varunjain99(2018-06-13 01:10:14):This would get rid of type checks on f right? Is there a need for that?

houseroad(2018-06-13 02:14:06):Two test cases are so similar, why not merge them together.
yinghai(2018-06-13 05:15:42):Compiler will do type check and throw error if substitution doesn't work. Making a function call to each of the element in a tensor is potentially costly compared to inlined. 
bddppq(2018-06-13 08:29:31):Here shouldn't throw exceptions, because it's just data type that this optimization pass doesn't support (yet), not that the model is invalid.
bddppq(2018-06-13 08:34:22):Create two local variables `conv` and `bn` to refer to the corresponding nodes (aka. `n->inputs()[0]->node()` and n) will make the rest of this function more readable.
bddppq(2018-06-13 08:36:38):You need to check whether these inputs are really in initializers.
bddppq(2018-06-13 08:41:12):a should be const
bddppq(2018-06-13 08:47:16):I think you only need `a_T_data` (or `a`) here? Also they should be const
bddppq(2018-06-13 08:50:39):++i
bddppq(2018-06-13 08:52:39):There are std::plus, std::minus, std::multiplies and std::divides
bddppq(2018-06-13 08:55:36):Looks to me these functions are quite similar, maybe better to use macros to reduce copy-paste like here https://github.com/onnx/onnx/blob/master/onnx/defs/schema.cc#L353-L379?
varunjain99(2018-06-13 16:39:31):changed

varunjain99(2018-06-13 16:41:52):made const, but you need both because you don't know where to look for the data if the tensor is stored as raw data - unless you do another switch on the types
varunjain99(2018-06-13 16:44:12):changed all to const
varunjain99(2018-06-13 16:47:08):this is checked in modify_conv - if not initializers then continue; so this function is not run
varunjain99(2018-06-13 16:55:05):renamed
varunjain99(2018-06-13 17:19:33):changed handling of different data types so that inputs have the same tensor type and optimization is not run if not float or double
varunjain99(2018-06-13 17:35:08):Thought about it, but the functions may differ slightly in implementation for FLOAT16, COMPLEX64, COMPLEX128. I'm not too familiar with schema macros though so feel free to suggest if refactoring with macros is still plausible
varunjain99(2018-06-14 00:48:50):changed to use

varunjain99(2018-06-14 00:49:48):changed template structure to reflect above
bddppq(2018-06-15 09:37:56):`name == it->name()`
bddppq(2018-06-15 09:40:22):`std::vector<Dimension> dim_sizes{initializer.sizes().cbegin(), initializer.sizes().cend()}`
bddppq(2018-06-15 09:49:20):it's better to add a function in the Graph class to expose the next_unique_
```
size_t next_unqiue() {
  return next_unqiue_++;
}
```
and then this function should be simply delegate to `addInitializerAndInput(Tensor &initializer, std::string name)`:
```
Value* addInitializerAndInput(const Tensor &initializer) {
  return addInitializerAndInput(initializer, ONNX_NAMESPACE::to_string(owningGraph()->next_unqiue()));
}
bddppq(2018-06-15 09:51:59):better make initializer const, and then in this function you construct a new copy and std::move it to addInitializer
bddppq(2018-06-15 09:54:17):eraseInput already will assert this (indirectly in eraseOutput)
bddppq(2018-06-15 10:03:10):The type switch in Tensor::add, Tensor::subtract, Tensor::multiply, Tensor::divide are pretty similar, use macros to reduce duplicate.
bddppq(2018-06-15 10:04:35):I guess these `*_nums` are not used anymore so they can be deleted?
bddppq(2018-06-15 10:09:58):no need to cast `.size()` and `sizeof`
varunjain99(2018-06-15 14:01:51):changed
varunjain99(2018-06-15 14:15:58):changed
varunjain99(2018-06-15 14:16:01):modified - without function as we are in the graph class anyway
varunjain99(2018-06-15 14:22:11):deleted
varunjain99(2018-06-15 14:26:25):deleted _nums, changed to size_t
varunjain99(2018-06-15 14:39:35):changed
bddppq(2018-06-15 17:15:34):`const auto`, `cbegin`, `cend`
bddppq(2018-06-15 17:20:08):Agree, should merge to once test function and use a for loop like:

```
for dtype in [TensorProto.FLOAT, TensorProto.DOUBLE]:
  ...
```

bddppq(2018-06-15 17:20:52):use `self.assertEqual`, which will print the mismatched values in case the test failed.
bddppq(2018-06-15 17:22:45):put the function signature into the macro as well, right now the macro is assuming the input variable is named `a`. 
bddppq(2018-06-15 17:24:31):put the macro right above these functions, also one good convention to avoid macros name clashing is undef right after all usage:
```
#define my_macro
  ...

my_macro(add, std::plus)
my_macro(subtract, std::minus)

#undef my_macro

```
bddppq(2018-06-15 17:32:34):looks to me `apply_binary_function_*` functions are doing similar type switch again to determine the underlying field and forward to `bin_func`, can we collapse those into here and remove another level of indirection?
varunjain99(2018-06-15 18:48:57):iterator isn't const though??
varunjain99(2018-06-15 22:56:53):compressed
varunjain99(2018-06-15 22:57:13):organized
varunjain99(2018-06-15 22:57:22):combined signature
varunjain99(2018-06-15 23:02:11):added loop and assertEqual
bddppq(2018-06-16 02:38:10):const, cbegin, cend
bddppq(2018-06-18 07:57:17):no need to cast `ptr[i]` and `a_ptr[i]`
bddppq(2018-06-18 08:00:38):put elem_type_ into the error message to make it clear
bddppq(2018-06-18 08:00:44):ditto
bddppq(2018-06-18 08:03:32):create a dedicated exception subclass for optimizer like here https://github.com/onnx/onnx/blob/master/onnx/checker.h#L13-L33 and throw that instead of general exception
bddppq(2018-06-18 08:11:30):these two cases are almost identical, use another macro to avoid duplication?
bddppq(2018-06-18 08:13:14):here why setting raw_data? s should have stored the values in floats field already?
bddppq(2018-06-18 08:13:44):no need to explicit cast
bddppq(2018-06-18 08:14:04):no need to explicit cast
bddppq(2018-06-18 08:19:36):it's possible the original conv doesn't bias but the fused one has, so here should add a new bias to the fused conv instead of stop doing optimization
bddppq(2018-06-18 08:21:35):orig conv can have no bias, in that case here will be out of bound access
varunjain99(2018-06-18 16:04:50):changed
varunjain99(2018-06-18 16:11:14):residue of previous implementation - removed
varunjain99(2018-06-18 16:17:50):This code is not stopping the optimization if the original conv doesn't have bias. It is checking if there are 3 inputs (i.e. input of a bias). If so, we get the initializer; if that input is not an initializer, then we cannot do the optimization as we do not know the value. If there aren't 3 inputs, we still have a bc tensor - defined above - and we replace it in the replace_inputs function
varunjain99(2018-06-18 16:18:28):Same idea as above - in the case of no bias, this branch will not run as conv_inputs will have size 2 not 3
bddppq(2018-06-18 16:39:30):oops you are right sorry I missed the line above
bddppq(2018-06-18 16:41:19):In that case could you move this bias check to above (where you checked whether other inputs are initializers).
varunjain99(2018-06-18 18:40:14):fixed
varunjain99(2018-06-18 18:40:20):fixed
varunjain99(2018-06-18 18:40:24):added
varunjain99(2018-06-18 18:40:32):used macro
varunjain99(2018-06-18 18:40:58):moved
varunjain99(2018-06-18 20:11:40):get error on CI without cast
varunjain99(2018-06-18 20:11:58):error on CI without cast
bddppq(2018-06-18 22:45:07):You need to check `s` and `var` don't have other uses in rest of the graph before modifying them inplace
varunjain99(2018-06-19 00:01:56):they don't because I'm copying the tensors into s and var (L99, L102) 
auto s = *s_iter;
auto var = *var_iter;
They aren't tensors in the graph
bddppq(2018-06-19 03:23:58):ah I didn't notice you are copying all the initializers before. that's going to be expensive because initializers are normally pretty big. let's try to reuse the existing initializes as much as possible (i.e. check uses(), if it's 1 then just steal that copy).
houseroad(2018-06-12 01:10:32):Actually, we can keep the page, and put the new link `https://github.com/onnx/onnx/#installation` in the page.
houseroad(2018-06-12 23:41:05):@anirudhacharya the purpose of this page is to automatically generate the stats on the test cases. So we can know what the quality of our test cases is. We discussed the idea long time ago (probably in Feb.), now I kick off the work. Currently, the stat page only shows the information based on the node test, later I plan to add more information to the page, such as the stats on model test, and the coverage on the attributes, etc (I have created an issue to track the progress). Adding the script to CI is to make sure that the page is always up-to-date, just like `Operators.md`. If a contributor forgot to update the page, the CI will yell.

I hope my explanation has addressed all your concerns. If not, please let me know.
anirudhacharya(2018-06-13 04:59:56):@houseroad so the script will eventually generate % of coverage for each operator based on all the node, model and pytorch model converted tests?
houseroad(2018-06-13 05:01:56):Yes, eventually we will get there. This is the first step. :-)
houseroad(2018-06-13 07:42:27):After merging this PR, we can refer to the generated doc in https://github.com/onnx/onnx/issues/426, instead of manually updating the progress.
houseroad(2018-06-13 07:43:06):@anirudhacharya please let me know if you have any concern or question. Thanks.
bddppq(2018-06-13 08:24:14):>such as the stats on model test, and the coverage on the attributes

Doesn't the current coverage report already include model tests and also have coverage on attributes? If no, why can't these to be added to the current coverage report and instead we need to add a new one?
houseroad(2018-06-14 01:11:30):@bddppq yes, current model tests have already covered the model test and attributes. But it's not very straightforward to ask users to check the coverage report in the CI every time. Also, we can break the coverage to model level, for example, what kind of operators and attributes are covered by one specific model, this is not covered by the current coverage report. Again, this is only the first step. (I will leave the major part to my intern. :-)) Later, we will have more powerful model analyzer and we can add more useful information to the test coverage page without a real backend.
houseroad(2018-06-14 01:18:39):@bddppq I will figure out how to reuse the existing coverage report logic when dealing with the model coverage and overall coverage parts (Not reinventing the wheel).
linkerzhang(2018-06-27 16:24:20):Let's also add test case for this function, like test cases for other operators.
linkerzhang(2018-07-11 22:39:43):Thank you!
raymondxyang(2018-07-19 07:04:25):Thanks for the detailed review :)
linkerzhang(2018-06-18 04:01:27):please keep it here as an example.
linkerzhang(2018-06-18 04:02:01):Let's switch MeanVarianceNormalization to be a function. FC was removed/deprecated.
linkerzhang(2018-06-22 15:43:56):this if should be removed?
linkerzhang(2018-06-22 15:50:22):if function's since version is greater than the version_range.second, it should be an error, I think.
raymondxyang(2018-06-22 17:16:12):The if logic here determine whether this is a solid attribute or a placeholder attribute. I extract the if- logic here for checks on placeholder attr_proto going under this condition. IMO it is a valid condition checker here. 
wschin(2018-06-25 23:16:51):Cannot Pow_exponent be a initializer inside the function? You can use Constant if a function has no initializer.
wschin(2018-06-25 23:29:28):Could we merge the setup of each node into a helper function? The code here looks a bit long and redundant.
wschin(2018-06-25 23:40:35):Needs to document done its value for across_channels=True/False. Please also make sure the description is understandable for people from outside.
wschin(2018-06-25 23:41:02):This is not necessary. Use Constant, attribute, or initializer.
wschin(2018-06-25 23:45:51):A better output name might be "EX_Squared" or "Squared_EX." The token "Pow" doesn't really carry too much information to me. 
wschin(2018-06-25 23:46:39):The "Pow" in "Pow_exponent" looks redundant.
wschin(2018-06-25 23:50:48):Maybe replace "X_POW" with "X_Squared"?
wschin(2018-06-25 23:55:23):Need to add epsilon >= 0 before division to avoid divided by 0. The epsilon should be a float attribute.
wschin(2018-06-25 23:57:39):VARIABLE? VARIANCE?
wschin(2018-06-25 23:58:20):STD is clear enough to me. May we drop "_VAR"?
wschin(2018-06-26 00:00:36):Calculating --> Calculate
wschin(2018-06-26 00:05:01):Need detailed description and mathematical formulations. As a standard, a clear document is very important. The supported input shape is likely any dimension. Please describe at least [N, C, H, W] and N-D cases.
linkerzhang(2018-06-26 05:00:34):Yes, function does not have initializer right now.
raymondxyang(2018-06-26 21:13:02):I noticed this by looking at Caffe's code. However per https://github.com/onnx/onnx/blob/master/docs/Operators.md#div there is no attribute/input for Div op in ONNX. Do we need to manually add an add op before div? 
linkerzhang(2018-06-28 14:35:41):This is a common nodeproto construction. You may change it with a general name, say, BuildNode or... Make sense?
linkerzhang(2018-06-28 14:46:16):Let's figure out a better way to show function body. 

We may add a netron picture or link in function description (we may ask Netron to be able to visualize a function).
linkerzhang(2018-06-28 14:47:49):question please: no attributes needed?
raymondxyang(2018-06-28 16:56:40):I guess this can be next step as we prob need to explore the ways to do that
linkerzhang(2018-06-29 19:24:41):Remove the nodes section for now then. It's not readable and useful.
wschin(2018-07-03 09:36:03):"S"hape --> "s"hape
wschin(2018-07-03 09:41:07):It looks like this function can support N-D cases. For example, with axes=[0, 1, 2, 3, 4, 5], it should be able to handle 6-D tensors. I don't see any related explanation about N-D cases. Is there any reason for not to mention it?
wschin(2018-07-03 09:42:35):"S"hape --> "s"hape
wschin(2018-07-03 09:46:31):We don't have "epsilon" as an attribute? Before doing the division, we should add a configuration positive number to the denominator to avoid being divided by 0.
wschin(2018-07-03 15:29:01):for no across channel ---> for calculating means and variances along channels. Two variables with the same C-coordinate are associated with the same mean and variance.
for across channel ---> to calculate global mean and  global variance. In this case, all variables share the same mean/variance.

wschin(2018-07-03 15:31:56):A clearer description: Exponent (i.e., 2) to element-wisely calculate the square of a tensor
wschin(2018-07-03 15:33:43):Not sure if creating attribute should be a part of BuildNode(…) like the python helper for making NodeProto.
wschin(2018-07-03 15:37:35):This is similar to a function in helper.py. If C++ doesn't have such a helper module, we should create one. Otherwise, it'd be better to reuse it.
raymondxyang(2018-07-03 17:21:19):I saw this in caffe's code but Div op in onnx does not take epsilon as an attribute..
raymondxyang(2018-07-03 17:24:13):Double checked and there is no node constructing functions in C++
raymondxyang(2018-07-03 17:31:25):I thought about it before but it is complicated as there can be two type of attributes: full attributeprotos and/or referencing attributeprotos. For the full attrs I cant find an universal set of parameters with varied required fields.. 
wschin(2018-07-04 05:30:54):You need to create an Add operator for this.
linkerzhang(2018-07-04 13:11:01):In this way, can we move this function to a new common file please? say, ModelHelper.h/cc in common folder.
linkerzhang(2018-07-04 13:13:46):let's rename this file to experiments_functions.cc. Otherwise, there will be many functions.cc (like defs.cc, which is not a good idea). It makes hard to find some definition codes. :) Thank you!
linkerzhang(2018-07-04 13:16:02):the type constraint info should also be inferred from its body and printed here. You may add it in separate PR and push this in firstly.
raymondxyang(2018-07-05 21:30:34):I tested with Eigen if we use the eps value in caffe, 1e-9, in some cases it showed no differences (0 for determinant and inf for inversed tensors).. what value shall we have in order to avoid it?
linkerzhang(2018-07-11 16:50:06):put it into common folder.
linkerzhang(2018-07-11 16:58:50):caculation --> calculation.
raymondxyang(2018-07-11 18:27:42):out parameter in the end.
raymondxyang(2018-07-11 18:35:55):also check the lower version.
bddppq(2018-07-11 19:00:37):why is this needed?
bddppq(2018-07-11 19:07:30):looks to me this check is also applicable when the attr doesn't have ref_attr_name (however you will need to guard the has_type check by ir_version, IIRC type was added in ir_version 2
bddppq(2018-07-11 19:09:44):where does such cast happen? do you also need to ignore this warning in other targets?
raymondxyang(2018-07-11 21:04:15):This happens when I convert fields in functionproto in cpp2py.. I only notice the warning in onnx target..
raymondxyang(2018-07-11 22:05:58):Actually if it is a solid attr_proto the validation will be covered under checker.cc L199-208.. This validation logic is aimed to be only conducted on the referencing attr_proto.
bddppq(2018-07-12 00:41:02):at least you would also need to ignore it in other platforms as well. maybe what's better is to find the place in pybind11 binding and fix it?
bddppq(2018-07-12 01:00:06):This is not good, because python land already has a `FunctionProto` class from the auto-generated protobuf python files (`onnx.FunctionProto`), and its underlying memory layout is totally different from the c++ protobuf structs, and so python land users will see two classes with the same name while they are completely different. what we have been doing when passing protobuf structs across c++ and python is serialize on one side and then deserialize on the other side (look at check_model)
raymondxyang(2018-07-13 18:46:22):Currently the solution is blocked by #1194 (and seems the import issue is in discussion in protobuf repo as well). I could rename the generated pybind class (to function_schema probably) and explore a way to resolve #1194 (probably need to manually run text replacement scripts), switch to use function_proto later. This class is only used for the display of function information now so there is no use case for users to really create one using function classes in python module.
raymondxyang(2018-07-14 00:20:19):Found a way to manually correct the import in build. Pushed.
bddppq(2018-07-14 16:39:14):change to use the ONNX_DOMAIN instead of hardcode ""?
raymondxyang(2018-07-17 17:17:23):Comment resolved
raymondxyang(2018-07-17 19:40:00):fixed. Thanks for pointing out 👍 
bddppq(2018-07-18 05:52:58):duplicated
bddppq(2018-07-18 05:57:23):sorry if this check is already done in checker, why do we check it again here?
bddppq(2018-07-18 05:59:32):Actually how about putting Functions and Operators together?
bddppq(2018-07-18 06:00:22):onnx.defs has `ONNX_DOMAIN` defined, maybe also move ONNX_ML_DOMAIN to there and here will just refer them through onnx.defs?
bddppq(2018-07-18 06:00:49):FunctionProto should be imported from onnx
bddppq(2018-07-18 06:02:01):nit: move this include to function.cc? (since it's used there)
bddppq(2018-07-18 06:03:25):check status?
bddppq(2018-07-18 06:04:18):std::move bytes?
bddppq(2018-07-18 06:06:04):you can use `[]` of unordered_map to implicitly create an entry if the key doesn't already exist.
raymondxyang(2018-07-18 17:59:12):There is no type check for referencing attr_proto, I tested removing this and there is no enforcement for such attr during the registration (ie change `attr0->set_type(AttributeProto_AttributeType_INTS)` to INT and it can still pass the check)
raymondxyang(2018-07-18 18:03:30):IMO the formatting and visualization could be a future task. Currently I am trying to have type/shape inference of function working and may change the visualization accordingly. And we also talked with Netron people and try to have image generated for the functions. 
linkerzhang(2018-07-19 00:55:32):good suggestion, let's merge the two documents into one in next PR. Thank you!
raymondxyang(2018-07-19 02:39:23):all comments except this resolved
houseroad(2018-06-15 01:51:05):@anirudhacharya since we just removed `is_test` tag, very likely we won't add it back soon. Even we change the schema later, we will adjust the test cases accordingly. It does not make sense to block the merge here.

I have test the cases end to end.
houseroad(2018-06-15 01:53:10):Very likely, we won't add `is_test` back soon. I think it's good to check this PR in at the moment. The cases I introduce only contain 1 output. So even we change the output number in the schema, it won't affect the cases. Bottom line is adjusting the test case while changing the schema.

I have tested the cases end to end.
houseroad(2018-06-15 06:30:25):@anirudhacharya opset version is designed for bc changes in operator spec. Each time, we introduce bc changes, we should bump up the opset. Those models in `onnx/model` will be updated soon (est, next week).
fumihwh(2018-06-15 12:53:26):@houseroad `momentum` is not considered in test cases. Did I miss something?
bddppq(2018-06-15 21:00:11):This is a breaking change and I think our current opset mechanism doesn't support removing operator. I think we need to add "UntilVersion()" (similar to "SinceVersion()").
anirudhacharya(2018-06-15 21:15:34):I am okay with removing LpNormalization but there is/can be a difference between ReduceL2 and L2Normalization. 

ReduceL2 is ``sqrt{sum(x_i**2)}`` where ``0<= i <=n``

and L2Normalization is  ``x_i / sqrt{sum(x_i**2)}`` where ``0<= i <=n``

The above definition is for 1-d arrays, but similar logic can be extended to high dimension arrays with different axes.
anirudhacharya(2018-06-15 21:28:53):For higher dimension tensors -
```python
import numpy as np
import math
norm_eps=1e-10

#Reduce L2
in_data = np.random.rand(3,4,5)
reduceL2 = np.linalg.norm(in_data, axis=1, keepdims=True)
print(reduceL2)
```

```python
# L2Normalization
np_norm = np.linalg.norm(in_data, axis=1) + norm_eps
np_norm = np.repeat(1. / np.expand_dims(np_norm, axis=1), in_data.shape[1], axis=1)
np_out = np.multiply(in_data, np_norm)
print(np_out)
```
houseroad(2018-06-18 18:51:41):Shall we consider this as a bug? I think no one really use this operator due to its vague description.

@bddppq @anirudhacharya @ebarsoum @linkerzhang @dzhulgakov 
houseroad(2018-06-18 20:00:17):@anirudhacharya could you list the frameworks which implement `LpNormalization` ops?
ebarsoum(2018-06-19 00:01:53):This isn't norm OP in TF, it is l2_norm: https://www.tensorflow.org/api_docs/python/tf/nn/l2_normalize. Initially it was called L2Normalization, then we change it to LpNormalization to be more generic. So as @anirudhacharya said they aren't the same.
houseroad(2018-06-19 00:03:11):Close the PR, since the fix is not right.
MrGeva(2018-11-19 10:03:30):The definition of LpNormalization is unclear.  
TF implements them as ReduceL1 or ReduceL2 depends on value of p. It is evident in https://github.com/onnx/onnx-tensorflow/blob/master/onnx_tf/handlers/backend/lp_normalization.py , LpNormalization is implemented using tf.norm, which is same as Reduce . Moreover, when running TF on a network with LpNormalization layers, we get results identical to Reduce.
If the layer is identical to ReduceL1 or ReduceL2, it is duplicate and better be removed.

On the other hand, in this thread LpNormalization was described differently. For p=2, result is x_i/( sqrt( sum(x_j) ) ).

If the latter definition is correct, pls confirm and document it clearly.
CLAassistant(2018-06-19 15:38:18):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=1122) <br/>All committers have signed the CLA.
houseroad(2018-06-20 21:19:24):Please run `python onnx/defs/gen_doc.py` and `python onnx/backend/test/stat_coverage.py` to update Operators.md and TestCoverage.md.
newling(2018-06-21 06:39:08):I'm going to start this PR again from scratch as I messed up (pulled instead of rebased).
newling(2018-06-21 06:54:54):please see the second attempt at this PR : https://github.com/onnx/onnx/pull/1130
newling(2018-06-21 07:30:37):@houseroad it's still failing the final CI test (now at https://github.com/onnx/onnx/pull/1130) after i ran your suggestion:

> Please run python onnx/defs/gen_doc.py and python onnx/backend/test/stat_coverage.py to update Operators.md and TestCoverage.md.

Maybe I should try a "dummy" pull request (just a blank line somewhere)  to see if that passes? 
houseroad(2018-06-20 20:23:51):Call it ElementwiseMultiOpDocGenerator?
Because we also have unary and binary element-wise ops.
houseroad(2018-06-20 20:24:31):The shape inference function is incorrect. Now we support broadcasting. :-)
linkerzhang(2018-06-20 22:56:03):Yep. I'm fixing it.
linkerzhang(2018-06-20 23:31:18):Fixed.
linkerzhang(2018-06-20 23:32:49):done.
houseroad(2018-06-22 06:28:23):Delete this line?
linkerzhang(2018-06-22 14:32:52):Great catch! Thank you! :)
houseroad(2018-06-25 18:40:53):indent here
houseroad(2018-06-25 18:47:15):What's the purpose of symbolic_dim? The logic seems incomplete.
linkerzhang(2018-06-25 21:59:21):offline explained, symbolic dim is used,
1, if there's one symbolic dim and all others are 1, then symbolic dim will be used in result shape.
2. if there're symbolic dims and one dim value > 1, then the result shape will take the >1 value.
3. if there're more than 1 symbolic dim and all others are 1, then the result shape will not be set.
CLAassistant(2018-06-21 01:50:08):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=1129) <br/>All committers have signed the CLA.
bstriner(2018-06-21 05:32:56):@jamesr66a thanks for the quick turnaround! pytorch build back to working on latest onnx.
houseroad(2018-06-26 22:33:49):https://travis-ci.org/onnx/onnx/jobs/394884269

The log shows that
```
        modified:   docs/Changelog.md
	modified:   docs/Operators.md
	modified:   docs/TestCoverage.md
```

Run gen_doc.py and stat_coverage.py to update them please.
CLAassistant(2019-07-25 10:29:34):[![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/onnx/onnx?pullRequest=1130) <br/>Thank you for your submission! We really appreciate it. Like many open source projects, we ask that you all sign our [Contributor License Agreement](https://cla-assistant.io/onnx/onnx?pullRequest=1130) before we can accept your contribution.<br/>**5** out of **9** committers have signed the CLA.<br/><br/>:white_check_mark: linkerzhang<br/>:white_check_mark: yinghai<br/>:white_check_mark: Ac2zoom<br/>:white_check_mark: houseroad<br/>:white_check_mark: fumihwh<br/>:x: a127a127<br/>:x: vchuravy<br/>:x: anderspapitto<br/>:x: newling<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/onnx/onnx?pullRequest=1130) it.</sub>
askhade(2021-10-01 19:22:41):Closing this PR since it is very old. @newling: If you want to forward this please reopen after merging with master
smessmer(2018-06-21 18:29:33):Can you also add code to call this from the backend tests and skip a test case if it returns false?
smessmer(2018-06-21 18:09:34):nit: typo
smessmer(2018-06-21 18:09:56):Why `Optional`?
houseroad(2018-06-21 21:55:58):Probably we should remove `roughly based on LLVM IR`, since they are quite different :-)
houseroad(2018-06-21 21:58:14):Mention how we store the shape and type inference?
houseroad(2018-06-21 21:59:29):List at least one example here?
bddppq(2018-06-22 06:23:50):(unrelated to this PR) IMO this `schema_registry` is really an unfortunate complexity added to the API
bddppq(2018-06-22 06:30:19):Is `_registerOptimizer ` supposed to be a public function? If so shall we remove the `_`?
bddppq(2018-06-22 06:32:39):It can also operate on protobuf (and it's sometimes more convenient, e.g. if someone is going to add a pass that populates all the missing optional attributes, it's easier to write it in protobuf, because the default value is stored as AttributeProto in schemas).
bddppq(2018-06-22 06:34:19):nit: on ONNX models
dzhulgakov(2018-06-23 21:39:00):add that every time <unknown> would produce a new "symbol", however that symbol would propagate further in the graph. E.g. MatMul of (M,2) and (2,4) would produce (M,4).
CLAassistant(2018-06-21 21:30:44):[![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/onnx/onnx?pullRequest=1135) <br/>Thank you for your submission! We really appreciate it. Like many open source projects, we ask that you all sign our [Contributor License Agreement](https://cla-assistant.io/onnx/onnx?pullRequest=1135) before we can accept your contribution.<br/>**12** out of **18** committers have signed the CLA.<br/><br/>:white_check_mark: fumihwh<br/>:white_check_mark: anirudhacharya<br/>:white_check_mark: linkerzhang<br/>:white_check_mark: smessmer<br/>:white_check_mark: snnn<br/>:white_check_mark: gramalingam<br/>:white_check_mark: pranavsharma<br/>:white_check_mark: bddppq<br/>:white_check_mark: skottmckay<br/>:white_check_mark: houseroad<br/>:white_check_mark: kit1980<br/>:white_check_mark: Ac2zoom<br/>:x: jeffbloo<br/>:x: zrphercule<br/>:x: varunjain99<br/>:x: Maratyszcza<br/>:x: Akshay Chalana<br/>:x: bstriner<br/><hr/>**Akshay Chalana** seems not to be a GitHub user. You need a GitHub account to be able to sign the CLA. If you have already a GitHub account, please [add the email address used for this commit to your account](https://help.github.com/articles/why-are-my-commits-linked-to-the-wrong-user/#commits-are-not-linked-to-any-user).<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/onnx/onnx?pullRequest=1135) it.</sub>
houseroad(2018-06-25 22:00:48):@Ac2zoom please rebase this PR to the master branch
Ac2zoom(2018-06-27 20:54:21):Revising test cases for clip
Ac2zoom(2018-06-29 17:22:38):@houseroad If I delete model.onnx, do I need to re-run generate-data?
houseroad(2018-06-29 17:55:50):@onnxbot  retest this please
houseroad(2018-06-29 19:27:27):Please also run `python stat_coverage.py` to update the TestCoverage doc.
houseroad(2018-06-29 19:28:44):Also `python gen_doc.py` to regenerate `Changelog.md` and `Operators.md`
houseroad(2018-07-06 05:32:18):@Ac2zoom regenerate the doc please :-)
houseroad(2018-07-05 22:04:47):This case is incorrect
houseroad(2018-07-05 22:05:05):This case is also incorrect.
houseroad(2018-07-05 23:13:48):Just keep this one.
houseroad(2018-07-05 23:14:14):remove this one, it's the same as the previous one, don't forget to delete the corresponding binary data. :-)
houseroad(2018-07-05 23:14:31):Please also remove this one, it's the same as the previous one, don't forget to delete the corresponding binary data. :-)
CLAassistant(2018-06-23 03:04:17):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=1138) <br/>All committers have signed the CLA.
bddppq(2018-06-23 04:39:29):@peterjc123 Thanks for working on this!
peterjc123(2018-06-23 04:45:36):@bddppq Hi, the problem is that although `clang-cl.exe` is properly installed on Appveyor, it cannot be correctly detected by MSBuild without some extra file copying work.
There're two ways to solve this problem:
1. Copy the files in the using the environment scripts according to this [post](https://gitlab.kitware.com/cmake/cmake/issues/17930).
2. Use ninja-build and specify environmental variables like `CC` and `CXX`. It will also make the build faster.

What do you think?
bddppq(2018-06-23 05:04:05):@peterjc123 2. sounds less intrusive to me (to the user's environment), because it only requires users to install one more thing, instead of moving files around.
@raymondxyang @linkerzhang 
raymondxyang(2018-06-23 22:04:50):+@snnn into the thread

peterjc123(2018-06-24 05:52:22):Test item `onnx/test/shape_inference_test.py::TestShapeInference::test_transpose_preexisting_incorrect_shape` seems to be flaky when compiled using clang-cl. The 32-bit one fails on `onnx/test/checker_test.py::TestChecker::test_check_graph`. I don't know the exact reason.
peterjc123(2018-06-26 10:48:35):I guess the problem is that clang-cl could not work happily with Python libraries, like pybind11. That may be the reason for the random crashes.
peterjc123(2018-06-26 15:18:36):@bddppq Since the problem is on the compiling of the python extension, what about just skipping them now? Let's consider the tests for clang-cl later.
bddppq(2018-06-23 04:37:15):Does `distutils.spawn.find_executable` work on windows?
peterjc123(2018-06-23 04:41:47):Yes, I'll try!
bddppq(2018-06-23 05:07:03):Actually I would prefer to not auto opt in to use clang once it's available, instead, user can choose to use it by passing this extra flag via CMAKE_ARGS environment variable. Same for ninja. What do you think?
peterjc123(2018-06-23 06:46:39):@bddppq Okay then, I'll revert the changes and add the additional tests into the build matrix.
bddppq(2018-06-30 04:41:40):Doesn't MSVC support c++11?
@raymondxyang @linkerzhang 
peterjc123(2018-06-30 04:50:57):@bddppq The flag is useless in MSVC. Starting from C++14, it can be controlled by some flags like `/c++:14`. But for c++ 11, it can be just determined by MSVC version and is enabled by default.
snnn(2018-07-02 17:11:47):@peterjc123   Then by default it would be?
peterjc123(2018-07-03 02:25:45):By default, there is no flag to specify C++ 11 in MSVC. It can't be enabled or disabled explicitly. By using `set(CMAKE_CXX_STANDARD 11)` with MSVC, the flag `-std=c++11` will be appended but it will not be recognized as a valid argument.
snnn(2018-07-04 02:55:58):I don't think onnx can support VC verions earlier than 2015. According to https://docs.microsoft.com/en-us/cpp/build/reference/std-specify-language-standard-version, after your this change, it will default to C++14. Am I right?
peterjc123(2018-07-04 05:22:37):Yes, you are right.
fumihwh(2018-06-25 01:26:19):@zakk0610 Maybe duplicated with #1046 , you can review that PR to see if that can be improved.
zakk0610(2018-06-25 01:40:31):@fumihwh Sorry, this PR is duplicated.
CLAassistant(2018-06-25 23:53:27):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=1144) <br/>All committers have signed the CLA.
houseroad(2018-07-11 20:43:23):jenkins CI failure is unrelated.
bddppq(2018-07-11 20:43:30):@onnxbot retest this please
Ac2zoom(2018-07-20 03:00:24):Are all the test failures on caffe2 my fault?
Ac2zoom(2018-07-20 03:02:39):Quite strange that it seems like all the CI is passing otherwise
houseroad(2018-07-20 04:56:29):@Ac2zoom ignore the default CI, the image is broken. 
Ac2zoom(2018-07-20 23:29:53):Seems like some issue from autograd is failing the build
bddppq(2018-07-21 04:28:01):@onnxbot retest this please
smessmer(2018-07-24 20:16:31):Btw, just for fun, the references to dead values I noted above are actually not causing crashes because C++ has a corner case feature for exactly this (see https://herbsutter.com/2008/01/01/gotw-88-a-candidate-for-the-most-important-const/ ).

However, this feature is little known and comes with a whole bunch of pitfalls that are easy to get wrong, so I strongly recommend against using it.
Ac2zoom(2018-07-27 17:22:19):@smessmer Will implement your recommended changes on https://github.com/onnx/onnx/pull/1184
bddppq(2018-06-30 04:45:07):ModelProto can have a list of opset_versions: https://github.com/onnx/onnx/blob/33e9cd4182fe468675241fba4ae8a16c2f0bd82f/onnx/onnx.proto#L198
Ac2zoom(2018-07-09 18:13:09):Change 0 to i
Ac2zoom(2018-07-09 18:26:08):ConvertVersion should take OperatorSetIdProto, parse to OpSetID
Ac2zoom(2018-07-09 18:27:00):Add comment explaining how this eases the workflow as opposed to using OperatorSetIdProto
Ac2zoom(2018-07-09 18:29:23):Instead of printing a warning, throw an exception (ONNX_ASSERT) and catch in this test instead of asserting the opset_version to be the same
Ac2zoom(2018-07-09 18:32:04):Intake OperatorSetIdProto, serialize to string in implementation
houseroad(2018-07-11 16:38:52):Shall we call this OpsetVersionConverter?
houseroad(2018-07-11 16:58:43):"<domain" ==> "<domain>"
houseroad(2018-07-11 17:03:13):not optimization, it's version conversion.
houseroad(2018-07-11 17:05:51):not pass, it's adapter :-)
houseroad(2018-07-11 17:32:06):Later, we should add `Model` in IR.
houseroad(2018-07-11 17:43:22):pass init_version to `make_model`
houseroad(2018-07-11 17:44:54):Is None right for a function in mypy?
houseroad(2018-07-11 17:46:59):Just assert the equality of node's op_type should be fine.
houseroad(2018-07-11 17:48:03):X1, X2? or X, Y, Z
houseroad(2018-07-11 17:48:09):X1, X2? or X, Y, Z
houseroad(2018-07-11 17:48:46):No need to make this experimental. 
houseroad(2018-07-11 17:56:01):Here, you should print the type of initial_version and target_version.
houseroad(2018-07-11 17:56:33):Raise Exception when it is not supported.
houseroad(2018-07-11 18:00:11):Add the implementation code? Just throw an exception.
houseroad(2018-07-11 18:07:26):Release all the adapters?
houseroad(2018-07-11 18:08:39):This may not apply to the converts if domains are not the same.
houseroad(2018-07-11 18:10:06):We need more thoughts and discussion on this
houseroad(2018-07-11 18:17:07):Make it as a constructor?
houseroad(2018-07-11 18:17:28):add as a member function?
houseroad(2018-07-11 18:17:44):add as a member function of OpSetID?
houseroad(2018-07-11 18:48:52):Yeah, ONNX_ASSERTM which throws exception should be ok here
houseroad(2018-07-11 18:49:01):Yeah, ONNX_ASSERTM which throws exception should be ok here
houseroad(2018-07-11 18:52:43):As long as our base convert's API is able to handle inter-domain converter, we will be good.
houseroad(2018-07-11 19:03:16):Add comments about the key and the values
houseroad(2018-07-11 20:41:33):instead of having next_version, you can use step ==> -1 or 1 
houseroad(2018-07-11 20:42:24):Probably you don't want to always output this, adding a debug switch?
bddppq(2018-07-11 20:48:23):Shouldn't the `initial_version` be taken from the model?
houseroad(2018-07-11 20:51:58):The model may contain several opset_imports. I agree with that if no initial_version is specified, we just use the only opset_import in the model.
houseroad(2018-07-11 21:19:21):~~After the discussion, let's stick to the opset_imports in model.~~

~~The new API should only contain model and target_version.~~
Ac2zoom(2018-07-11 23:45:44):Given that the bulk of a ModelProto is really just a GraphProto, does this have much use outside of providing a separate container for the OpsetVersion?
Ac2zoom(2018-07-12 00:21:40):Is this in reference to the output type or the output type of the Callable parameter?  I believe it should be appropriate for both: this is the syntax currently used in the optimizer.
Ac2zoom(2018-07-12 18:44:56):Beyond the consideration of concerns specific to the default domain, where else should we focus?  I've written a brief new proposal regarding modularizing the downwards vs. cross-domain vs. upwards adapters such that they can be invoked by the BaseConverter and each separate module can be invoked by the DefaultConverter.
Ac2zoom(2018-07-12 18:45:41):Thank you!
Ac2zoom(2018-07-12 19:06:33):This could actually just be a constructor of OpSetID, and I could even make better use of the object in the existing calls to destringify.
smessmer(2018-07-16 21:16:25):Isn't `initial_version` redundant information, i.e. can be read from the model proto?
smessmer(2018-07-16 21:22:15):nit: final
smessmer(2018-07-16 21:22:36):nit: explicit

Also, passing in the string as a const reference is faster.
smessmer(2018-07-16 21:23:44):structs/classes should be either data structures, which means they have no constructors or methods but just public data members, or they should be classes, which means all data members are private.
smessmer(2018-07-16 21:25:43):Let's add some error checking here in case `seglist` has less than two entries. Otherwise the error message could be quite confusing.
smessmer(2018-07-16 21:26:32):Also here, it should be explicit and the argument a const reference
smessmer(2018-07-16 21:26:42):should be explicit
smessmer(2018-07-16 21:27:29):It's faster to use constructor initialization lists instead of setting value members in the constructor body. This should also be applied to your other constructors.
smessmer(2018-07-16 21:27:48):Please no public data members in classes.
smessmer(2018-07-16 21:30:09):`const OpSetID&` instead of `OpSetID` or it's going to copy each entry
smessmer(2018-07-16 21:31:24):Hm this list of ifs sounds dangerous to me. I know you just copied this and it isn't new code. But what if somebody adds a new field and forgets to add it here? Can we instead copy the whole thing and then just change what we want to change afterwards?
smessmer(2018-07-16 21:33:38):nit let's use multi-line type annotations for functions with multiple arguments. Better readable when there is on argument per line.
smessmer(2018-07-16 21:34:26):None is the correct return type for a function that doesn't return a value.
smessmer(2018-07-16 21:35:35):Is this actually bytes or ModelProto?
smessmer(2018-07-16 21:36:39):This is a base class meant for inheritance. The destructor must be virtual or objects are only destructed partially.
smessmer(2018-07-16 21:37:48):Instead of calling destructors of owned objects explicitly, use smart pointers (`unique_ptr` if possible). Then you can be sure you didn't make a mistake with ownership and everything is much safer. Even if exceptions are thrown you can be sure everything is destructed correctly.
smessmer(2018-07-16 21:38:26):pass arguments by reference
smessmer(2018-07-16 21:39:15):Why is the domain unused in this body? Also, you might want to pass it as reference if it's not retained. If it's retained somehow, you might want to consider move semantics.
smessmer(2018-07-16 21:40:58):pass arguments by reference. Note: in general, all arguments that are expensive to copy (e.g. larger than say 8 bytes) should be passed by reference, or should be moved if they're retained.
smessmer(2018-07-16 21:41:45):put this in an anonymous namespace to avoid potential linker clashes.
smessmer(2018-07-16 21:42:14):you should never use raw pointers that have ownership. Return a `unique_ptr` instead.
smessmer(2018-07-16 21:43:24):`= default` instead of `= 0` is better here. You then won't need line 25 below.
smessmer(2018-07-16 21:43:53):This is also a class with public data members and methods. Use either or.
smessmer(2018-07-16 21:44:40):also move `OpSetID`s.
smessmer(2018-07-16 21:47:05):Make this final
smessmer(2018-07-16 21:47:16):no public data members in classes
smessmer(2018-07-16 21:47:29):if it's empty, you can use `=default` instead.
smessmer(2018-07-16 21:49:34):You've already called adapters.find(op_name) above, which is expensive. Here, each call to `adapters[op_name]` is calling `adapters.find(op_name)` internally again. You can instead retain the result of `adapters.find(op_name)` above and reuse it here so you don't have to search the whole map again.
smessmer(2018-07-16 21:50:25):If `op` lives as long as you're accessing `op_name` somewhere, it's faster to not copy the string but just keep a `const std::string&` reference around.
smessmer(2018-07-16 21:50:47):also here, without reference, you're copying the whole map.
smessmer(2018-07-16 21:51:19):This method is quite large. Can we split it into multiple methods? A good rule of thumb is to not have methods larger than ~5 lines.
smessmer(2018-07-16 22:01:21):Taking these as const references doesn't need to copy it
smessmer(2018-07-16 22:03:18):This is a memory leak
smessmer(2018-07-16 22:08:23):Is this always true, that there will only be exactly one up- or downwards adapter? If yes, we can encode it into the data structure and make it faster by having a nesting level less. You don't need a map if it always has only one entry.
smessmer(2018-07-16 22:09:13):pass by reference
smessmer(2018-07-16 22:09:48):take by reference
smessmer(2018-07-16 22:10:47):`const Node*` instead of `Node*` is a bit safer
Ac2zoom(2018-07-16 23:08:20):Will add comment
Ac2zoom(2018-07-17 16:44:09):@houseroad thoughts on providing another layer of indirection here by primarily providing a method without initial_version to the C++ client?
Ac2zoom(2018-07-17 19:29:45):Yeah, I think that makes sense!  @houseroad who would be the best person from the Optimizer project to verify this with?
Ac2zoom(2018-07-17 19:30:53):Fwiw, we can entirely bypass using PrepareOutput in the VersionConverter and instead just copy mp_in directly in convert_version
Ac2zoom(2018-07-17 19:38:01):Should I change this in optimizer_test, too?
Ac2zoom(2018-07-17 19:40:53):In fact, should I abstract these to a test_helper class @houseroad?
Ac2zoom(2018-07-17 19:44:31):Is it best practice to make constructors virtual, as well, so as to prevent variables instantiated as BaseConverter from solely invoking BaseConverter's methods?
Ac2zoom(2018-07-17 19:45:44):domain parameter removed
Ac2zoom(2018-07-17 19:47:23):Is the current approach of instead initializing stack variables instead of heap variables appropriate?
Ac2zoom(2018-07-17 19:48:18):Another carryover from Optimizer.  I wonder why they did it there.
Ac2zoom(2018-07-17 21:18:30):Ok so here's the design issue right now: If we attempt to refill the properties based on the new graph, we face the problem of re-adding existing inputs/outputs/attributes.  As such, to appropriately implement this, we would need to write a new version of ExportModelProto which replaces elements/nodes rather than adding them.  If @houseroad thinks this could be useful, I'd be happy to do so.
Ac2zoom(2018-07-17 21:20:32):DefaultVersionConverter extends this, so it can't be final.  However, that one might be.  I haven't done so in case we want to be able to extend it (for example to implement more restrictions on the ops we convert/the process of doing so).
Ac2zoom(2018-07-17 21:22:25):We might ultimately add adapters here, so I figured I'd leave it be.
Ac2zoom(2018-07-17 21:29:17):To properly take advantage of this, I may need to unfold some of the block at lines 63-70.
Ac2zoom(2018-07-17 21:32:16):Yes, absolutely.  Should we do the same for convert_version @houseroad?
Ac2zoom(2018-07-17 21:48:11):Even the pointer?
Ac2zoom(2018-07-17 21:50:18):For this purpose, as we continue to evaluate the current version of adapter_lookup, it may make sense to move it back to the BaseConverter.
houseroad(2018-07-18 06:08:22):We need to update the doc for python API.
houseroad(2018-07-18 06:11:43):I think this one is not necessary.
Ac2zoom(2018-07-18 16:59:20):Perfect.  I was originally using it somewhere, but I seem to have removed that usage
houseroad(2018-07-19 17:29:59):within default domain 
houseroad(2018-07-19 17:30:30):just release, `stoi` is more robust.
houseroad(2018-07-19 18:04:10):For primitive type, no need to use/return reference, instead, return value is even faster.
smessmer(2018-07-19 18:04:28):Constructors can't be virtual. Virtual means that it's deciding which function to call based on what the object actually is - but for a constructor call you don't have an object yet to base that decision on.
smessmer(2018-07-19 18:05:51):ok makes sense, didn't see that DefaultVersionConverter inherits from it.
houseroad(2018-07-19 18:05:59):This is a legacy problem. Let's fix it in a different pr.
houseroad(2018-07-19 18:06:50):It's inside ONNX_NAMESPACE. It should be ok to just use ModelProto.
smessmer(2018-07-19 18:07:25):I didn't mean the function arguments but the `OpSetId iv` and below. Function arguments might also be made const references if not retained, but as you noticed that does not make sense for the pointer.
smessmer(2018-07-19 18:10:53):yes
houseroad(2018-07-19 18:12:08):I guess this method is not necessary.
houseroad(2018-07-19 18:14:35):You probably want to make this case more complicated. Add has broadcast and axis attribute. Or probably choose another Op, such as `Reshape` from 2 input case to 1 input.
Ac2zoom(2018-07-19 18:15:41):This is all covered in the actual adapter tests on the adapters branch: https://github.com/onnx/onnx/pull/1184/files#diff-fe3eada8cd6b3b792277582934f39198
houseroad(2018-07-19 18:17:15):My point is that let's have a case, in which the adapter is not visible.
houseroad(2018-07-19 18:19:49):Let's mention this is a opset version converter only for default domain.
houseroad(2018-07-19 18:26:14):Add comment to tell it's the end of which namespace.
houseroad(2018-07-19 18:26:26):Not experimental :-)
houseroad(2018-07-19 18:30:52):how to use this flag?
houseroad(2018-07-19 18:37:31):Take unique_ptr as parameter?
houseroad(2018-07-19 18:43:49):Add some comments talking about when `adapter_lookup` should be called?
houseroad(2018-07-19 18:46:09):This is not a TODO, just a note :-)
houseroad(2018-07-19 18:46:54):Also do we really want to register any adapter directly in intra one? probably just delete this comment.
houseroad(2018-07-19 20:16:16):follow the tensorerror in https://github.com/onnx/onnx/blob/master/onnx/common/assertions.h#L16.
Create a new type of error, or just use ONNX_ASSERTM

Please don't directly throw in .h file.
houseroad(2018-07-19 20:17:19):add `override` specifier.
houseroad(2018-07-19 20:17:45):virtual = 0
houseroad(2018-07-19 20:17:56):virtual = 0
houseroad(2018-07-19 20:21:31):Why not make this as a general look up? What's the optimization here?
houseroad(2018-07-19 20:21:46):override specifier
houseroad(2018-07-19 20:22:27):just directly using `==` to compare two std::string should be fine
houseroad(2018-07-19 20:23:31):You can also overload `==` for OpSetID.
houseroad(2018-07-19 20:27:32):Can we just use `initial_version.domain() == ""`?
houseroad(2018-07-19 20:30:30):probably you can remove all the ONNX_NAMESPACE in this file, because at the beginning, we already specify that we are using ONNX_NAMESPACE.
houseroad(2018-07-19 20:33:48):This whole logic should be moved to default converter. Because other intra-domain converters may not follow our logic.
houseroad(2018-07-19 20:38:05):No need to create a new OpSetID. Just set the opset field should be enough, and then break the loop, since you already find the default domain one
Ac2zoom(2018-07-19 21:03:33):Unfortunately didn't work: compilation error with strings not being possible to pass as an expression
Ac2zoom(2018-07-19 21:04:32):We're really only concerned with the domain; we don't really check equality elsewhere.  If it'd still be good protocol, though, I can definitely overload ==.
Ac2zoom(2018-07-19 21:04:49):Exclusively?
smessmer(2018-07-19 22:46:34):no need for reference because reference is 64bit and int64_t is also just 64bit - i.e. copying an int64_t is not more expensive than copying the reference. Then, the indirection of a reference here introduces a small performance hit.
smessmer(2018-07-19 22:47:02):You can move in the domain, i.e. have it by-value in the argument and assign it to the member with `std::move`.
smessmer(2018-07-19 22:47:43):you can move `new_domain` into the `OpSetID`
smessmer(2018-07-19 22:48:23):Can you return a const reference to make sure people don't modify it and break class invariants?
If people need to be able to change it but only one kind of change (say they only need to be able to add stuff), add a member `addOpSetVersion()` specifically only allowing this one kind of change. That makes it easier to uphold class invariants.
smessmer(2018-07-19 22:48:57):Use `emplace_back` and `std::move`
smessmer(2018-07-19 22:51:34):Why do we still have the three-level map? Didn't we say a table (i.e. two-level) is enough?
smessmer(2018-07-19 22:52:45):`.toString()` likely returns a temporary object, not a reference to some object stored inside `op`. So if you just keep a reference (i.e. pointer) to it, the object itself will not live anymore when you try to access it later through the reference. You have to use non-references here.
smessmer(2018-07-19 22:54:03):`adapters.find()` returns an iterator object. This is basically just a pointer and cheap to copy, so you don't need a reference here. Also, the reference here actually breaks your code, because (as in the previous comment), the iterator object gets destroyed if you don't store it and your reference will point to something dead.
smessmer(2018-07-19 22:54:14):same here
smessmer(2018-07-19 22:54:23):and here
smessmer(2018-07-19 22:55:01):There's no need to use `&&` here. Just pass in `(unique_ptr<Adapter> a_ptr)`
smessmer(2018-07-19 22:55:39):You can probably move in `op_name`, `initial` and `target`.
smessmer(2018-07-19 22:57:07):no reference
smessmer(2018-07-19 22:57:23):use `make_unique` instead, never directly call `new` or `delete`.
smessmer(2018-07-19 22:58:20):Here you're also storing a reference to a dead object.
Ac2zoom(2018-07-20 00:36:48):So I could do this, but it would require removing the appropriate OpSetVersion at each iteration of my converter (or, I guess, at the end).  By contrast, if I just edit the vector, it's simpler there.
Ac2zoom(2018-07-20 00:39:36):I think we decided that we wanted to maintain 3-level for the cross-domain case, since it made sense to maintain BaseConverter for that purpose.  That may have changed due to the abstraction of adapter_lookup to BaseConverter and the removal of IntraDomainConverter, though.  What do you think, @houseroad?
Ac2zoom(2018-07-20 00:41:57):@houseroad mentioned it as an indication to users that a_ptr will be unusable
Ac2zoom(2018-07-20 00:43:02):I also need to do this for BackwardsCompatibleAdapter
Ac2zoom(2018-07-20 00:46:56):Doesn't compile due to Adapter being an abstract class.  @houseroad?
Ac2zoom(2018-07-20 00:47:30):search_domain, right? (not versions_map)
Ac2zoom(2018-07-20 00:54:57):Will change name of method
Ac2zoom(2018-07-20 01:00:48):need to change this for adapters, too
houseroad(2018-07-20 05:23:09):remove ONNX_NAMESPACE?
houseroad(2018-07-20 05:25:59):what is the format of target? Shall we add it to the comment?
If target starts with "$", this logic does not seem right.
houseroad(2018-07-20 05:26:37):change the name?
houseroad(2018-07-20 05:28:13):Update the comment please
houseroad(2018-07-20 05:35:38):2 level domain only works for intra/default domain case. The map and its lookup won't be a performance bottleneck. I would suggest to stick to this design, since later, we may have inter-domain converter.
Ac2zoom(2018-07-20 18:32:08):More legacy relative to adapters branch
Ac2zoom(2018-07-20 22:27:37):@houseroad I'm not sure what other instances of using something other than == for string comparison to look for.  Do you have any other pointers?  All these strcmps are due to the ONNX_ASSERTM requirement of char* over string
houseroad(2018-07-21 05:23:22):This method is quite long, let's move it to the cpp file.
houseroad(2018-07-21 05:24:02):This part can be automatically done by iterating the all_schemas.
houseroad(2018-07-21 05:25:22):is this part really necessary?
houseroad(2018-07-21 05:28:52):Another check we need to do is that, make sure that opset version of the model is known (<= than the highest version in current schema, I mean checking initial_version is valid, i.e., in the range.) 
houseroad(2018-07-21 05:37:46):directly set ""?
houseroad(2018-07-21 05:38:47):actually, please confirm what string ("" or "ai.onnx") we use in `versions_map`
houseroad(2018-07-21 05:45:16):You can iterate the schemas here, and maintain a map [opname==> first_version]. With this, you can automatically create and register `NoPreviousVersionAdapter`.
houseroad(2018-07-21 05:56:20):This current_opschemas seems not quite useful.
Ac2zoom(2018-07-21 14:40:31):This seems to introduce an issue with overriding
Ac2zoom(2018-07-21 14:42:24):The issue is that it's difficult to know which adapters to register for each.  Is there a particular approach for doing this that would be better?
Ac2zoom(2018-07-21 14:44:29):Made it easier to debug, but not really, yeah.
Ac2zoom(2018-07-21 14:45:25):This is done on line 66 of the new convert.cc
Ac2zoom(2018-07-21 14:46:03):We always use "" in versions_map, so this builds in compatibility for users providing "ai.onnx" in reference to the same domain.
Ac2zoom(2018-07-21 14:46:35):Perfect. This makes your initial point much clearer.
Ac2zoom(2018-07-21 14:47:16):The idea is just to store the current version of each op, which was useful before I switched to using since_version.  I will attempt to phase it out.
Ac2zoom(2018-07-23 18:35:08):One thing about this: would it be fair to move the generation of all_schemas to the constructor?
smessmer(2018-07-24 18:57:22):You can move in `domain`.
smessmer(2018-07-24 18:58:07):nit: The first const here actually doesn't do anything - after returning, the value is assigned to some variable anyhow and that one's constness is independent from this.
smessmer(2018-07-24 19:54:27):This gives users too much freedom to hurt class invariants. Can you instead have a dedicated `addOpsetVersion()` method?
smessmer(2018-07-24 19:58:15):here and below, you're still storing a reference to a dead object
smessmer(2018-07-24 19:59:42):This is still not moved in, pass by value if you want to move.
smessmer(2018-07-24 20:01:06):you might be able to make these two into references if `.domain()` returns a reference. If it returns by value, then you can't because you'd be keeping a reference to a dead object.
smessmer(2018-07-24 20:02:12):This method is quite long and hard to read. Can we split it into submethods?
smessmer(2018-07-24 20:02:35):this is a reference to a dead object because `.toString()` returns a value, not a reference.
smessmer(2018-07-24 20:05:13):reference to dead value, `get_all_schemas_with_history` returns a value, not a reference.
smessmer(2018-07-24 20:05:59):also please check here that `.Map()` actually returns a reference. If it doesn't it'd be a reference to a dead value. If it does return a reference, you're fine.
Ac2zoom(2018-07-24 20:48:44):As @houseroad and I discussed, the challenge with this, as mentioned above, is that it would require significant refactoring of the modification of these members in DefaultVersionConverter.  If it would be particularly important, I can do this.
Ac2zoom(2018-07-24 21:19:29):It does, so I will do that!
Ac2zoom(2018-07-24 21:25:45):@houseroad?
Ac2zoom(2018-07-24 21:26:46):I believe we checked this last week and it does
smessmer(2018-07-24 22:55:49):Remember that you have to pass it in by value if you want it to be moved
houseroad(2018-07-26 18:34:06):We can add another method in DefaultConverter. Let's keep it as it is in C++ for now and change it in the following PRs.
houseroad(2018-07-26 18:38:31):Remove the std::move please. It's a const string reference, we are not suppose to change its content.
houseroad(2018-07-26 18:40:01):let's do it in the following PR.
houseroad(2018-07-26 18:42:53):Optimization: probably do it in place
houseroad(2018-07-26 18:46:07):Usually, I would prefer "const std::string& name" + name_(name). You will copy it once anyway. Same strategies apply to OpSetID params.
houseroad(2018-07-26 18:49:40):We can merge `step` and `up`
houseroad(2018-07-26 18:51:19):The sanity check part can be lifted into another function.
houseroad(2018-07-26 18:51:46):@Ac2zoom some of the logic can be polished. 
houseroad(2018-07-26 19:08:57):Confirmed, this is fine :-)
Ac2zoom(2018-07-26 19:17:18):Realized that this introduces challenges with over-writing existing nodes/inputs/outputs, so creating the new ModelProto makes a lot of sense, rather than messing with large existing ModelProtos.
smessmer(2018-07-27 00:29:13):this is not addressed yet
smessmer(2018-07-27 00:30:11):you can move in `name` too (i.e. pass it in by value and add an std::move)
smessmer(2018-07-27 00:30:53):You can move in `op_name` (i.e. pass by value and add an std::move)
peterjc123(2018-06-27 16:34:44):Well, I finally figured out the reason for that crash on `test_transpose_preexisting_incorrect_type`. It supposed to return `RuntimeError` and it was actually returned. But due to the poor support of SEH exception handling in clang-cl, it was not sometimes not caught at the Python side. So there are two solutions:
1. Forcibly compile code that contains pybind11 in MSVC (Bad and complex)
2. Wrap the code entries that throws the exception with a try..catch and then rethrow a Python exception (Good, but need to determine where the call is from)
yinghai(2018-06-28 21:15:27):What's the usage of this function? 
Maratyszcza(2018-06-28 22:47:40):Typical use-case:
1. Create `graph` with `onnxInitGraph`
2. Set `inputFence.type = ONNXIFI_SYNCHRONIZATION_EVENT`
3. Create `inputFence.event` with `onnxInitEvent`
4. Set `outputFence.type = ONNXIFI_SYNCHRONIZATION_EVENT`
5. Set `outputFence.event = NULL` (this event will be created by `onnxRunGraph`)
6. Call `onnxRunGraph(graph, &inputFence, &outputFence)`
CLAassistant(2018-06-28 17:34:52):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=1155) <br/>All committers have signed the CLA.
houseroad(2018-06-28 19:14:15):Thanks!
snnn(2018-06-28 21:18:31):It should be 
```
git submodule update --init --recursive
```
houseroad(2018-06-28 21:27:22):Just checked, indeed, `benchmark` contains one submodule.

https://github.com/onnx/onnx/pull/1157
houseroad(2018-06-29 00:47:49):rebase to master please :-)
fumihwh(2018-06-29 11:29:32):@zrphercule  Any difference between https://github.com/onnx/onnx/blob/master/onnx/backend/test/case/node/convtranspose.py#L208 ?
zrphercule(2018-06-29 17:42:27):@fumihwh Yes there are three major differences:

1. The old tests implementing models without giving one crucial attribute: kernel_shape. This attr is neccesary in caffe2 now, and we cannot run the test of ConvTranspose without giving it.

2. Since the interface of caffe2 is quite different from onnx in ConvTranspose, we now choose to skip all tests of ConvTranspose: see https://l.facebook.com/l.php?u=https%3A%2F%2Fgithub.com%2Fpytorch%2Fpytorch%2Fblob%2F01f5ee77e3354d6892685309fa302349942eff13%2Fcaffe2%2Fpython%2Fonnx%2Ftests%2Fonnx_backend_test.py%23L40&h=AT0pJmneWCfKyWnGyFsWUxeddf2LIZbMXapgr0Da3W7LH4Fxm-Y7whR8ag1V-m-XZhC_tD_OXZf6DpOk76qrJSxNrgJ5Zibfx6lMwWjMZXezksPbcZhGPy_zg3oUJ3CXPQTgUlHuW50Tv2eLQT1ypzOZac8
Therefore, I wrote this new test for our new version of ConvTranspose. Also see task: T30052256. We have already solved the first two problems in this task and the diffs are waiting to be merged now. The third task has a lot of dependency, and it will take some time before it is solved. This test is designed to test the function of first two problems without triggering the buggy thrid one.

3. This is a model test, and we really do need to implement more model tests for this operator in the future.
fumihwh(2018-06-30 02:43:25):@zrphercule 
As `kernel_shape` is not a required attr, we should not add a particular test case just for one framework to pass their backend test. In other word, if a backend can not pass test in all conditions, it should be fixed first....
So the implementation in Caffe2 could not get `kernel_shape` from input W?
bddppq(2018-06-30 02:56:47):@fumihwh since this is an optional attribute, we should have test cases for both with and without it. Basically in general test cases should cover as many possible valid models as possible.
bddppq(2018-06-30 03:01:47):@zrphercule the cases you have added should be node test cases as they don't have initializers. Once you move them to node test cases you can largely reuse the python & numpy codd of existing test cases (aka those without kernel_shape). You should also add test cases that put W as initializers of the model as simple model test case, since that's what normally the real world models will be.
zrphercule(2018-07-02 17:56:48):@bddppq Sure then I will change this test to a node test. I will submit another model test after I fixed the kernel_shape part of ConvTranspose =) 

Another problem is in order to enable this test, we cannot put it into the current node test of ConvTranspose, since it is ignored by backend. And since the kernel_shape is still not fixed, we cannot enable the whole ConvTranspose test which will be failed...
zrphercule(2018-07-02 23:18:26):@onnxbot retest this please
zrphercule(2018-07-03 00:26:17):@bddppq @houseroad @fumihwh I re-wrote this testcase and updated all related documents. I didnt enable the "test ConvTranspose" flag in backend, so it will not be tested for real until the flag is removed; However, I tested it locally and it works well.
fumihwh(2018-07-03 09:53:19):@zrphercule 
> I will submit another model test after I fixed the kernel_shape part of ConvTranspose =)

I didn't get `fix the kernel_shape part`...
bddppq(2018-07-03 10:09:16):@fumihwh @zrphercule is referring to extending the caffe2 backend to support "kernel_shape". I believe before we have at least one backend supporting it or have an implementation by ourselves in onnx, there is no easy way to generate the reference inputs & outputs.
fumihwh(2018-07-03 11:48:32):@bddppq onnx-tf backend supports ConvTranspose. About `kernel_shape`, we just use it to check if it is same to shape of weight.
bddppq(2018-07-03 16:31:19):@fumihwh good to know. A minor correction to my previous comment: onnx-caffe2 supports convtranspose with kernel_shape, what @zrphercule is going to work on is when kernel_shape is absent, the converter needs to figure it out by looking into the initializers.
zrphercule(2018-07-03 17:28:45):@bddppq Thanks for your explanation. @fumihwh So far some machine learning frameworks like caffe2 still has no support for dynamic kernal shape in transposed conv operator, which means kernel_shape must be a given attribute when it is initialized. While I am working on this in caffe2 side, I would like to add more test cases of given kernal_shape and other attributes.
fumihwh(2018-07-04 00:55:04):@zrphercule OK, cool.
zrphercule(2018-07-05 23:58:07):@fumihwh @bddppq Since test_convtranspose_output_shape, test_convtranspose_pad and the new added test_convtranspose_kernel_shape are using the same test data, I merged these three test into one function instead of three.
bddppq(2018-07-06 05:55:40):There is python lint error:
```
./onnx/backend/test/case/node/convtranspose.py:207:9: E303 too many blank lines (2)
```

shinh(2018-08-17 13:31:35):Not directly related to this PR, but let me ask a question. Is `output_shape` of the original test, test_convtranspose_output_shape, right? The original test is passing [1,2,10,8], which is the entire shape of the output while the new test's output_shape is [10,8]. Reading the document, [1,2,10,8] seems incorrect, but I'm not sure.
bddppq(2018-07-05 21:47:56):Sorry I don't quite understand this line, could you elaborate?
bddppq(2018-07-05 21:49:44):you need to add kernel_shape attribute to the expect call here (kernel_shape=`W.shape`)
bddppq(2018-07-05 21:52:21):actually you can just add an additional `expect` call to existing test cases (with kernel_shape attribute provided), so we can avoid copy-paste these pre-computed reference inputs & outputs.
zrphercule(2018-07-05 21:55:12):Yes I was thinking if it is a better way...
zrphercule(2018-07-05 21:58:33):@houseroad and I discussed about our definition of "output_pads" and "pads", and found that both of them are output padding, although we claim this "pads" is input padding. In order to translate this "pads" to input padding, we need this formula. 
See http://deeplearning.net/software/theano/tutorial/conv_arithmetic.html#transposed-convolution-arithmetic Relationship 8
zrphercule(2018-07-05 23:23:03):@bddppq btw I wonder why we need to add "kernel_shape = W.shape" here, since I have already added it to the node attribute.
bddppq(2018-07-06 06:09:36):the `pads` are the number of pixels going to be **subtracted** in the output shape, while `output_padding` is going to be **added** (at only the end side) in the output shape.
Also worth noting is the number of `pads` attribute is 2x of the rank of the output tensor while for `output_padding` is equal to the rank of the output.
bddppq(2018-07-06 18:36:55):@zrphercule you are right, it should be in the node.
CLAassistant(2018-07-01 22:25:49):[![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/onnx/onnx?pullRequest=1165) <br/>Thank you for your submission! We really appreciate it. Like many open source projects, we ask that you sign our [Contributor License Agreement](https://cla-assistant.io/onnx/onnx?pullRequest=1165) before we can accept your contribution.<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/onnx/onnx?pullRequest=1165) it.</sub>
dotlambda(2018-07-03 15:39:39):I'd prefer not to be held liable for not

> notify[ing] Facebook and Microsoft of any facts or circumstances of which you become aware that would make these representations inaccurate in any respect.

As this minimal and trivial change can obviously not be copyrighted, feel free to commit the change yourself and not use my PR.
dotlambda(2018-07-06 16:07:56):@bddppq I'm closing this and hope you're going to make the change yourself.
dotlambda(2018-07-30 17:33:21):ping @bddppq 
yinghai(2018-07-08 07:39:03):Does this actually same a copy each time we call this function?
bddppq(2018-07-08 07:43:38):I put it as function static, so there should be only one copy got instantiated
yinghai(2018-07-08 21:11:25):Yeah, it's inlined: https://godbolt.org/g/iGp6QB

:) 
bddppq(2018-07-09 02:53:39):wow good point! updated
gramalingam(2018-08-09 23:45:09):Will update this as per discussion in the ONNX workshop.
houseroad(2018-07-10 03:22:26):Need further discussion. Close for now
CLAassistant(2018-07-10 17:47:00):[![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/onnx/onnx?pullRequest=1181) <br/>Thank you for your submission! We really appreciate it. Like many open source projects, we ask that you sign our [Contributor License Agreement](https://cla-assistant.io/onnx/onnx?pullRequest=1181) before we can accept your contribution.<br/><hr/>**Patrice Vignola** seems not to be a GitHub user. You need a GitHub account to be able to sign the CLA. If you have already a GitHub account, please [add the email address used for this commit to your account](https://help.github.com/articles/why-are-my-commits-linked-to-the-wrong-user/#commits-are-not-linked-to-any-user).<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/onnx/onnx?pullRequest=1181) it.</sub>
houseroad(2018-07-10 20:33:20):@PatriceVignola please do NOT directly change operators.md and changelog.md. After updating the text in defs.cc, run `python setup.py develop` to rebuild the project, and run `python onnx/defs/gen_doc.py` to re-generate operators.md and changelog.md.
PatriceVignola(2018-07-10 21:23:34):Sorry about that! I ran `gen_doc.py` but didn't know I had to also run `setup.py`, so it gave me a weird diff. Everything should be fine now? I ran `python setup.py develop && python onnx/defs/gen_doc.py` and it fixed `Changelog.md`.
houseroad(2018-07-10 21:41:10):Looks like something is still mismatching: https://travis-ci.org/onnx/onnx/jobs/402406720

Could you fix that?
raymondxyang(2018-07-10 22:11:11):I actually tried https://github.com/silx-kit/pyFAI/pull/663/commits/a03f2bafe9ca963c4fbe7faadb7c38cb892e92c8#diff-2eeaed663bd0d25b7e608891384b7298 but the wheel still cause seg fault with caffe2.. but is it definitely a right practice to hide the symbols
bddppq(2018-07-10 22:15:34):@raymondxyang I found a similar issue in pybind11's repo https://github.com/pybind/pybind11/issues/1262, which says two pybind11 extensions built with different compilers can't work well together. although we think as long as both extensions are built with gcc >= 4.9 then it should be fine. IIRC onnx wheels are built with gcc 4.9?
raymondxyang(2018-07-10 22:27:42):Its 4.8.2 on the docker image..
bddppq(2018-07-10 22:30:30):@raymondxyang damm alright then we need to update it to >= 4.9 :-) 
raymondxyang(2018-07-10 22:33:37):https://github.com/pypa/manylinux/issues/118 I guess they are intentionally doing that for supporting centos5.. 
bddppq(2018-07-10 22:37:09):@raymondxyang IIUC from the post it says they initially chose to use gcc4.8 because they couldn't find newer version of gcc on centos5, however the follow up threads say they have built newer gcc for centos5
raymondxyang(2018-07-10 22:42:39):Agree.. I ll try updating it to 4.9 gcc to see if it can be solved
bddppq(2018-07-11 06:53:27):test failure is irrelevant (flaky appveyor infra)
raymondxyang(2018-07-17 22:33:30):Tried to upgrade to gcc >= 4.9. No official distribution found but used https://github.com/squeaky-pl/centos-devtools/releases . now pip install hangs during copying test files. Any ideas?
https://travis-ci.org/raymondxyang/wheel-builder/builds/405116217
bddppq(2018-07-11 20:44:01):@onnxbot retest this please
Ac2zoom(2018-07-20 03:04:59):Seems like we're seg-faulting because the Adapters aren't being properly stored (calling adapt results in a segfault)
Ac2zoom(2018-07-20 03:44:17):Some massive problem with how things are being returned to the Python interface.  something's being freed where it shouldn't.  Need to figure out why
houseroad(2018-07-27 17:05:56):@Ac2zoom rebase the PR to the master?
houseroad(2018-07-31 04:52:11):@Ac2zoom according to CI results, you may run converter on too many models.
houseroad(2018-08-02 17:19:34):@onnxbot retest this please
houseroad(2018-07-31 21:34:16):Probably give some example here?
houseroad(2018-07-31 21:49:49):ONNX_NAMESPACE please. This is for Android compatibility. 
houseroad(2018-07-31 21:52:41):Change to `test_coverage_whitelist`?
houseroad(2018-07-31 21:54:19):Better find another case here. For example, we have two node graph, and shape's content is unknown.
houseroad(2018-07-31 22:14:09):We can merge test_add_7_6 with test_add_6_5. So only `test_add_7_5`
houseroad(2018-07-31 22:15:42):Similar here: probably we can integrate them into one :-)
houseroad(2018-07-31 22:17:42):Add all supported adapters here?

Also an unsupported section?
houseroad(2018-07-31 22:31:03):Reorder the order, group by Op names?
houseroad(2018-07-31 22:32:02):since it's already const, please use const reference, instead of simple const.
Ac2zoom(2018-07-31 22:38:11):Should the unsupported by explicit (i.e. should I go through and identify all of these)?
houseroad(2018-07-31 23:51:54):also use const reference please
houseroad(2018-08-01 17:40:40):Either const std::string& + copy constructor, or std::string + move constructor. 

Cannot be a const string + move constructor... 
houseroad(2018-08-01 17:43:19):name is a little bit weird, shall we change it to 8_5?
houseroad(2018-08-01 17:44:38):We can do the same merge here, from 8->5 and 5->8 on mul and other ops which have more than one converters.
houseroad(2018-08-01 17:45:36):unsupported part is more for documentation purpose. So later we know which adapter we need to implement
houseroad(2018-08-01 17:46:59):Remove the comment?
houseroad(2018-08-01 17:47:35):Again, the std::move should not combine with const
Ac2zoom(2018-08-01 17:52:56):Should we establish, especially given review from @smessmer, that protocol is typically to use the copy constructor with an lvalue reference?
houseroad(2018-08-01 17:53:26):Here, we need to also check if is_test is 0,
if it is 0, throw exception
houseroad(2018-08-01 17:55:56):Same here, let's use const& + copy constructor also in OpSetID cases.
Ac2zoom(2018-08-01 17:55:58):More detail on protocol: const references for strings, but move semantics for other objects?
houseroad(2018-08-01 17:59:02):Check has_sizes before you use the real size: https://github.com/onnx/onnx/blob/master/onnx/common/ir.h#L297
houseroad(2018-08-01 18:07:14):No need to try all axis... because numpy broadcasting is always right side aligned.
houseroad(2018-08-01 18:08:10):This is incorrect, since you have to verify whether it is safe to remove the axis and broadcast flags.
houseroad(2018-08-01 18:21:24):We also need to check whether sizes are available.
houseroad(2018-08-01 18:22:11):Earlier version of maxpool cannot include indices :-)
houseroad(2018-08-13 06:15:36):Good job, this is quite important :-)
houseroad(2018-08-13 17:45:22):inconsistent?
houseroad(2018-08-13 17:45:36):inconsistent?
houseroad(2018-08-13 17:45:45):Inconsistent?
houseroad(2018-08-13 17:45:53):Inconsistent?
houseroad(2018-08-13 17:46:03):Inconsistent?
houseroad(2018-08-13 17:46:12):Inconsistent?
houseroad(2018-08-13 17:46:21):Inconsistent?
houseroad(2018-08-13 17:46:28):Inconsistent?
houseroad(2018-08-13 17:46:37):Inconsistent?
houseroad(2018-08-13 17:46:48):Inconsistent?
houseroad(2018-08-13 17:46:57):Inconsistent?
houseroad(2018-08-13 17:47:06):Inconsistent?
houseroad(2018-08-13 17:47:14):Inconsistent?
houseroad(2018-08-13 17:47:24):Inconsistent?
houseroad(2018-08-13 17:47:34):Inconsistent?
houseroad(2018-08-13 17:48:05):up, since the other test is called down?
houseroad(2018-08-13 17:48:15):same here.
houseroad(2018-08-13 17:48:24):same here
houseroad(2018-08-13 17:49:44):Why `Add`?
houseroad(2018-08-13 17:50:46):Shape of `real name`? or call it first input is not available?
houseroad(2018-08-13 17:50:59):Similar here.
houseroad(2018-08-13 17:54:52):You need to bail out if `A_sizes.size() < B_sizes.size()`. Don't switch the order. 1) you cannot switch the order on ops such as sub, div, etc. 2) switching order is kind of dangerous, let's be conservative.
houseroad(2018-08-13 17:55:48):`Add`?
houseroad(2018-08-13 17:56:03):Similar here as above
houseroad(2018-08-13 17:56:13):Similar here as above
houseroad(2018-08-13 17:57:33):Let's have a function called `bool c2_broadcastable(input1_sizes, input2_sizes)` and another function called `bool numpy_broadcastable(input1_sizes, input2_sizes)`
houseroad(2018-08-13 18:00:10):Although it's safe to assume that the shapes are correct, but you can still put an assertion here. :-)
houseroad(2018-08-13 18:01:18):You need insert before the add. Otherwise, it will violate the constraints on the graph. (topological order)
houseroad(2018-08-13 18:03:13):Testing is supported, but training.
houseroad(2018-08-13 18:38:18):Not must be statically determined. Better to say, no enough shape information to convert the model. :-)
houseroad(2018-08-13 18:39:25):Probably have a function called `unibroadcastable_numpy`?
houseroad(2018-08-13 18:40:09):I think shape inference also use/provide similar functions, which may want to take a look.
houseroad(2018-08-13 18:43:05):Be more specific, which version(s)?
houseroad(2018-08-13 18:48:13):Do you need to check shape in this case? You only need to check the content of the `Constant` or `Initializer`, right?
houseroad(2018-08-13 18:50:09):Remove the todo?
houseroad(2018-08-13 18:52:07):Let's use the general `broadcastable_numpy` to replace the adhoc/repeated check in each adapter. 
Ac2zoom(2018-08-13 23:53:29):Doesn't seem like there are any helpers in shape_inference, but I can try to abstract out the ones I created (in adapter) if the same semantics are used elsewhere.
Ac2zoom(2018-08-14 00:10:04):May have to change this once the Broadcast Adapters are merged
Ac2zoom(2018-08-14 00:21:08):Addressed in https://github.com/onnx/onnx/pull/1284
Ac2zoom(2018-08-14 00:27:56):Addressed in https://github.com/onnx/onnx/pull/1284
Ac2zoom(2018-08-14 00:28:11):Addressed in https://github.com/onnx/onnx/pull/1284
Ac2zoom(2018-08-14 00:28:19):Addressed in https://github.com/onnx/onnx/pull/1284
Ac2zoom(2018-08-14 00:28:24):Addressed in https://github.com/onnx/onnx/pull/1284
Ac2zoom(2018-08-14 00:28:29):Addressed in https://github.com/onnx/onnx/pull/1284
Ac2zoom(2018-08-14 00:28:35):Addressed in https://github.com/onnx/onnx/pull/1284
Ac2zoom(2018-08-14 00:28:41):Addressed in https://github.com/onnx/onnx/pull/1284
Ac2zoom(2018-08-14 00:28:46):Addressed in https://github.com/onnx/onnx/pull/1284
Ac2zoom(2018-08-14 00:28:58):Otherwise Addressed in https://github.com/onnx/onnx/pull/1284
Ac2zoom(2018-08-14 00:29:04):Addressed in https://github.com/onnx/onnx/pull/1284
Ac2zoom(2018-08-14 00:29:12):Addressed in https://github.com/onnx/onnx/pull/1284
Ac2zoom(2018-08-14 00:29:17):Addressed in https://github.com/onnx/onnx/pull/1284
Ac2zoom(2018-08-14 00:29:44):Reshape will be branched from https://github.com/onnx/onnx/pull/1284
Ac2zoom(2018-08-14 01:40:17):Addressed in https://github.com/onnx/onnx/pull/1286
Ac2zoom(2018-08-14 01:40:25):Addressed in https://github.com/onnx/onnx/pull/1286
Ac2zoom(2018-08-14 01:57:40):Addressed in https://github.com/onnx/onnx/pull/1288
Ac2zoom(2018-08-14 01:57:45):Addressed in https://github.com/onnx/onnx/pull/1288
Ac2zoom(2018-08-14 01:57:49):Addressed in https://github.com/onnx/onnx/pull/1288
Ac2zoom(2018-08-14 01:57:54):Addressed in https://github.com/onnx/onnx/pull/1288
Ac2zoom(2018-08-14 17:17:39):Addressed in https://github.com/onnx/onnx/pull/1289
Ac2zoom(2018-08-14 17:17:44):Addressed in https://github.com/onnx/onnx/pull/1289
Ac2zoom(2018-08-14 22:25:48):Addressed in https://github.com/onnx/onnx/pull/1291
Ac2zoom(2018-08-14 22:25:53):Addressed in https://github.com/onnx/onnx/pull/1291
Ac2zoom(2018-08-14 22:26:21):Addressed in https://github.com/onnx/onnx/pull/1291
Ac2zoom(2018-08-14 22:26:25):Addressed in https://github.com/onnx/onnx/pull/1291
Ac2zoom(2018-08-14 23:25:41):Addressed in https://github.com/onnx/onnx/pull/1292
Ac2zoom(2018-08-14 23:25:53):Addressed in https://github.com/onnx/onnx/pull/1292
Ac2zoom(2018-08-14 23:25:57):Addressed in https://github.com/onnx/onnx/pull/1292
Ac2zoom(2018-08-14 23:26:02):Addressed in https://github.com/onnx/onnx/pull/1292
Ac2zoom(2018-08-14 23:26:07):Addressed in https://github.com/onnx/onnx/pull/1292
Ac2zoom(2018-08-15 00:22:29):Addressed in https://github.com/onnx/onnx/pull/1293
Ac2zoom(2018-08-15 00:22:33):Addressed in https://github.com/onnx/onnx/pull/1293
Ac2zoom(2018-08-15 00:22:43):Addressed in https://github.com/onnx/onnx/pull/1293
Ac2zoom(2018-08-15 00:22:48):Addressed in https://github.com/onnx/onnx/pull/1293
houseroad(2018-07-11 16:36:38):It seems that we also need to adjust onnxifi_op in Caffe2.
yinghai(2018-07-11 17:11:48):> It seems that we also need to adjust onnxifi_op in Caffe2.

Yes, I'm sending a PR. 
raymondxyang(2018-07-12 17:06:55):will reopen after changes
snnn(2018-07-12 17:12:56):find_package(Threads) doesn't add '-pthreads' to compiling flags. 
Maratyszcza(2018-07-13 10:03:38):@snnn You need to add it to dependencies of the target.
snnn(2018-07-13 18:49:32):I have to say, compare to autoconf, The quality of cmake's FindXXX is too low.

This is how we do the same thing in autoconf:
http://git.savannah.gnu.org/gitweb/?p=autoconf-archive.git;a=blob_plain;f=m4/ax_pthread.m4

And this is how cmake works:
https://github.com/Kitware/CMake/blob/master/Modules/FindThreads.cmake


zrphercule(2018-07-19 00:17:46):Sry @Maratyszcza I am still testing it using travis, so there might be some stupid debugging thing. Please dont mind.
yinghai(2018-07-19 00:21:44):This is not yet ready for review, right? 
zrphercule(2018-07-19 17:10:40):@Maratyszcza Yes I need to change the interface and do more test now. I will @ you guys once it is ready, thanks!
bddppq(2018-07-25 17:02:10):@zrphercule need to resolve merge conflict
bddppq(2018-07-26 06:23:55):@onnxbot retest this please
Maratyszcza(2018-07-18 22:07:17):Please no changes to `onnxifi.h`
Maratyszcza(2018-07-18 22:08:36):How is it supposed to be used if no backend IDs are exposed?
Maratyszcza(2018-07-18 22:14:47):Please don't modify ONNXIFI loader either. If you want error reporting, define `ONNXIFI_LOADER_LOGGING=1` via `target_compile_definitions`
Maratyszcza(2018-07-18 22:16:34):`dummy_backend.functions[0]` is cryptic. Why not use `dummy_backend.onnxGetBackendIDs`?
rdzhabarov(2018-07-18 22:21:50):numBackends/backendIDs could be nullptr also. Would be great to reflect that in the proper response flag returned, etc.
Having this will facilitate easier extension for concrete backends (No need to worry about this small things).
zrphercule(2018-07-19 00:15:35):okay, just some grammar things...
zrphercule(2018-07-19 00:16:35):oh sry this is only for debugging, I havent erase them yet.
zrphercule(2018-07-19 17:51:50):(As I said I was just testing this way of using onnxGetBackendIDs. It is not ready for review.
yinghai(2018-07-23 19:27:56):Let's see if this unblocks the failure. If it works, we need to put it through cmake. 
bddppq(2018-07-24 21:42:15):better to use absolute path here, also should quote the variable
bddppq(2018-07-24 21:44:07):(In a follow up PR) Please create a cmake function to avoid repeating this chunk
bddppq(2018-07-24 21:47:21):undef it at the end
zrphercule(2018-07-24 22:01:27):OK

bddppq(2018-07-25 17:25:38):use "$top_dir" defined in setup.sh
bddppq(2018-07-25 17:28:35):indent
bddppq(2018-07-25 17:30:18):also you should set backendIDs here
bddppq(2018-07-25 18:30:25):could you change numBackends to 1? right now you are setting only one backendID
zrphercule(2018-07-25 18:33:49):ok, hhh
Maratyszcza(2018-07-25 21:05:40):This will crash if `numBackends == NULL`
Maratyszcza(2018-07-25 21:05:58):This will crash if `backendIDs == NULL`
Maratyszcza(2018-07-25 21:07:34):You should check if `infoValueSize == NULL`
Maratyszcza(2018-07-25 21:08:03):You should check if `onnxModel == NULL` or `onnxModelSize == 0`
Maratyszcza(2018-07-25 21:08:17):Crash here if `backend == NULL`
Maratyszcza(2018-07-25 21:08:26):Crash
Maratyszcza(2018-07-25 21:08:45):Same
rdzhabarov(2018-07-25 23:54:16):as far as the documentation in the onnxifi.h goes:
if (backendIDs == NULL) => ONNXIFI_STATUS_FALLBACK
if (numBackends == NULL) => ONNXIFI_STATUS_INVALID_POINTER
bddppq(2018-07-14 17:15:13):seems to be related to relative import in python3
upstream protobuf issue https://github.com/google/protobuf/issues/1491
bddppq(2018-07-14 18:59:17):@raymondxyang should now be fixed by using absolute import path in our proto files
raymondxyang(2018-07-16 17:11:08):Will merge it if there is no further comments today
CLAassistant(2018-07-15 21:08:03):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=1200) <br/>All committers have signed the CLA.
tengyifei(2018-07-17 06:06:57):@houseroad done and regenerated docs
CLAassistant(2018-07-16 18:33:09):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=1201) <br/>All committers have signed the CLA.
NiklasGustafsson(2018-07-16 23:34:29):This was a mistake. Please disregard. Meant to do this in my own repo.
Maratyszcza(2018-07-17 20:08:30):cc @varunjain99 
rdzhabarov(2018-07-18 19:14:25):Curious how pytorch depends on the particular version of onnxifi interface? Is that based on some good known SHA or it just uses whatever is on the latest onnx master?

Also, what's the policy for breaking changes in ONNXIFI?
Maratyszcza(2018-07-18 22:03:47):@rdzhabarov We are preparing to finalize ONNXIFI, and thats why there are some breaking changes. After ONNXIFI is finalized, we'd have to preserve stable API and ABI.
yinghai(2018-07-17 23:08:54):Why do we remove suffix? 
Maratyszcza(2018-07-18 06:59:39):Suffix was needed to avoid symbol conflict where several ONNXIFI implementations are loaded with `RTLD_GLOBAL` within the same process. As we are no longer using `RTLD_GLOBAL`, and suffix didn't have any other purpose, its best to remove it to simplify loader, wrapper, implementations and users of ONNXIFI.
peterjc123(2018-07-18 06:40:37):How to add the export annotations here? Actually, we need to export `onnx_torch::GetEmptyStringAlreadyInited(void)` on Windows when building PyTorch/Caffe2.
ebarsoum(2018-07-18 22:48:52):Why don't we make the output indices similar to TopK. Non flatten? To avoid row vs column major?
bddppq(2018-07-22 07:20:52):As discussed offline, let's turn it on in the CI instead of be default in cmake.
yinghai(2018-07-20 14:36:31):Looks like it flushed out new issues. 
zrphercule(2018-07-20 22:46:22):Seems like we dont need to merge this pr anymore, since we are having other solutions but not turn on the ONNX_BUILD_TEST flag =)
yinghai(2018-07-20 23:50:35):Test done. 
yinghai(2018-07-20 06:50:03):One line change. 
Maratyszcza(2018-07-20 07:09:54):Why does every line show up as changed?
yinghai(2018-07-20 14:36:14):Changed file from Dos to Linux format. 
bddppq(2018-07-22 05:04:48):Thanks for fixing. Could you sign the CLA? :-)
daquexian(2018-07-22 05:50:27):@bddppq I'm glad to sign the CLA. Where could I find the link? :)
bddppq(2018-07-22 07:17:39):@daquexian You should see a "license/cla" CI entry shown as pending status, please click the "details" link next to it and follow the instructions to finish the process.
daquexian(2018-07-22 15:03:54):@bddppq Sorry there is not "details" link next to license/cla in this PR. But I have found this link in other PRs and signed it. 
bddppq(2018-07-22 17:56:25):@daquexian Thanks
Maratyszcza(2018-07-24 03:30:31):Currently no operators in ONNX have an underscore in the name, any reasons why these are not named `ConvInteger`, etc?
linkerzhang(2018-07-26 00:16:03):@Maratyszcza  any more comments please?
bddppq(2018-07-26 02:53:25):@onnxbot retest this please
Maratyszcza(2018-07-30 07:38:42):The spec should specify whether the result is exact or approximate. Note that targeting exact result would exclude popular implementations for x86, e.g. see the paper by Vanhoucke in the comments
linkerzhang(2018-08-02 00:54:07):@bddppq @Maratyszcza @houseroad  any more comments please? thank you!
prasanthpul(2019-04-10 20:42:50):superseded by https://github.com/onnx/onnx/pull/1908
ebarsoum(2018-07-23 04:01:20):auto_pad is deprecated in the standard conv, so it shouldn't be there.
linkerzhang(2018-07-23 13:18:07):ok. was thinking to remove it in separate PR for both conv and conv_integer later. will remove it right now.
Maratyszcza(2018-07-24 03:29:15):If inputs are 16 bits wide, the product will, generally, overflow. We should either remove 16-bit input types, specify overflow behavior, or specify 64-bit output for 16-bit inputs.
linkerzhang(2018-07-24 06:13:59):Let's remove 16 bits for now then.
linkerzhang(2018-07-24 06:15:24):Think it twice, I'll still have the auto_pad removal in a separate PR for both conv and conv_integer soon.
bddppq(2018-07-26 03:01:27):Is `axes` required to be sorted?
bddppq(2018-07-26 03:05:09):you should get the default value from the schema
bddppq(2018-07-26 03:05:53):const
bddppq(2018-07-26 03:12:52):Set the output type to be int32
bddppq(2018-07-26 03:14:45):set output type
bddppq(2018-07-26 03:16:14):dumb question: can X and W have different input types?
bddppq(2018-07-26 03:17:40):Wait padding become inputs now?
bddppq(2018-07-26 03:17:49):what about bias?
linkerzhang(2018-07-27 00:07:53):output type is already int32 specified in type constraints, don't need to be inferred in this function.
linkerzhang(2018-07-27 00:07:59):output type is already int32 specified in type constraints, don't need to be inferred in this function.
bddppq(2018-07-27 00:14:59):You do need. Shape/type inference is different with your type constraints (e.g. shape inference engine will put the inferred information into the model as ValueInfoProto).
linkerzhang(2018-07-27 00:17:09):they can.
linkerzhang(2018-07-27 00:17:37):yes, padding value is indeed the zero point of X, which is input.
linkerzhang(2018-07-27 00:20:02):bias is kept to be separate as fusing it together with conv asks its scale and zero point to meet some equation. meanwhile, bias quantization is not that critical as input X and kernel W.
linkerzhang(2018-07-27 00:20:13):so, no bias in convinteger here.
linkerzhang(2018-07-27 00:22:51):Good suggestion, however, I don't think the default value in opschema was designed to be accessible here. will discuss with @anderspapitto in separate thread and make it if doable in separate PR.
linkerzhang(2018-07-27 00:25:15):do you mean to add const for input_shape please? it should be "const" with auto& already, I think. Thank you!
liwchang(2018-07-27 00:27:27):In most quantization, bias requires its scale and zero satisfying some relationship of the scales and zeros of kernels and inputs. I think we can add it as optional, but I don't think we can check it, since users might also use this conv dynamically. We just simply assume whatever the model describes is correct. Is this ok for you? As @linkerzhang said, it is not that performance-critical. Let us know whether you really need it. It should be a easy fix.

liwchang(2018-07-27 00:31:27):I think yes. There is a popular one, in which one is int8 and another is uint8. I think it is from Intel
linkerzhang(2018-07-27 00:33:36):it's not needed based on current design and implementation.
linkerzhang(2018-07-27 00:37:36):why? type constraints is saying the output type is int32 only, it does not need any type inference. Type inference done by this function is when there're multiple options in output type constraints and no way to infer it with type constraints only. For example, input type is T1 and output type is T2, and T2 could be int32, double. From constraints, it's not doable to get to know which output type should be picked up. In the cases that output type constraint only have one (this case) or input and output types are the same (with type constraints specified both as "T"), type inference function is not needed. Make sense?
linkerzhang(2018-07-27 00:39:01):@liwchang , if it's not check-able, then we should not add it. I assume that it will introduce ambiguity and correctness bugs.
linkerzhang(2018-07-27 02:25:07):Fixed.
Checked around the other existing type and shape inference function implementation. looks like they're already been taken as part of type constraints. I'm adding the output type setting for these ops too.Thanks for the comments!
Maratyszcza(2018-07-30 06:59:59):`The production MUST never overflow. The accumulation may overflow if and only if in 32 bits.`
An ONNX backend can't guarantee that result never overflows (which is implied by "MUST"). IMO, a better wording would be "If the any intermediate computation overflows 32-bit integer, the result is implementation-defined"
Maratyszcza(2018-07-30 07:04:40):If padding value is `zero_point`, we must require that the same `X` always use the same `zero_point` value. Otherwise, the same quantized tensor could have several dequantized representations.
Maratyszcza(2018-07-30 07:05:44):Should we require `uint32` output types when both inputs are `uint8`?
Maratyszcza(2018-07-30 07:24:22):Do you intend to support all four combinations (uint8+uint8, int8+int8, uint8+int8, and int8+uint8)? I can imagine use-cases only for 3 of them:
- `uint8 X + uint8 W`: [DP4A](https://devblogs.nvidia.com/mixed-precision-programming-cuda-8/) instruction on NVIDIA GPUs, ARMv8.2 UDOT instruction, ARM GPUs with [cl_arm_integer_dot_product](https://www.khronos.org/registry/OpenCL/extensions/arm/cl_arm_integer_dot_product.txt) extension, upcoming mobile accelerators with NNAPI support
- `int8 X + int8 W`: [DP4A](https://devblogs.nvidia.com/mixed-precision-programming-cuda-8/) instruction on NVIDIA GPUs, ARMv8.2 SDOT instruction, ARM GPUs with [cl_arm_integer_dot_product](https://www.khronos.org/registry/OpenCL/extensions/arm/cl_arm_integer_dot_product.txt) extension
- `uint8 X + int8 W`: x86 SSSE3 PMADDUBSW instruction (not exact, can saturate intermediate result), see [Improving the speed of neural networks on CPUs by V. Vanhoucke et al](https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/37631.pdf)

I don't know of any hardware or quantization model that uses signed activations + unsigned weights.
Maratyszcza(2018-07-30 07:25:36):Similarly, I suggest "If the any intermediate computation overflows 32-bit integer, the result is implementation-defined"
Maratyszcza(2018-07-30 07:26:24):Same comment as for `ConvInteger`
Maratyszcza(2018-07-30 07:27:12):`int32` even if input is `uint8`?
Maratyszcza(2018-07-30 07:28:23):Could you add test cases with the PR?
Maratyszcza(2018-07-30 07:29:18):@houseroad could you take a look at this part?
linkerzhang(2018-07-30 08:54:08):Give output data type is 32 bits, we're assuming that production between 2 8-bits should never overflow. Make sense? if there're exceptional cases, the behavior not specified will be undefined by default.
linkerzhang(2018-07-30 08:56:13):zero point is dynamic in our design, thus, it should be flexible although same zero point may be used for same X, but it's not "MUST".
linkerzhang(2018-07-30 08:56:40):make sense, I'll make the change accordingly.
linkerzhang(2018-07-30 08:58:34):thanks a lot for great comments! do we want to filter the int8 X + uint8 W case specifically please? I'd suggest we don't need to filter it out.
linkerzhang(2018-07-30 09:06:14):fixed.
linkerzhang(2018-07-30 09:16:28):Functionality of these ops is the same as their original ones (ones for full-precision). Only differences are data types, which are defined by type constraints and type inference function. besides adding same node test cases, any other test cases suggested to add please?
bddppq(2018-07-31 09:08:31):typo: malmul -> matmul
linkerzhang(2018-07-31 14:58:21):am adding test data for convinteger. cases for the other two will be added separately. For the type checking, it will need some test API change per discussion with @houseroad  offline. will do that separately with changes on test API.
linkerzhang(2018-08-01 03:06:22):fixed.
houseroad(2018-08-01 05:44:40):If shape0.dim_size() == 1... it seems an incorrect case. Either empty or at least rank 2, right?
houseroad(2018-08-01 05:53:16):Nit: commonDimL?
houseroad(2018-08-01 20:25:24):Can use some other case (not symmetric tensor), at least to tell people this is a co-relation, not a real convolution?
linkerzhang(2018-08-01 22:50:24):in this case, a dimension with value "1" was added to ensure its rank is greater than 2. This is the old logic (I didn't touch it) implemented by @anderspapitto 

@anderspapitto  may share more details on why we want to support this case.
Thanks!
linkerzhang(2018-08-01 22:51:53):This is not that common, I think :), so will keep the name. Again, I didn't touch this function, but make it to be used by both matmul and matmulinteger.
linkerzhang(2018-08-02 00:44:22):good point. changed  two cases.
linkerzhang(2018-08-02 07:42:25):fixed.
Maratyszcza(2018-08-05 04:49:03):Ok, I see, it would be better to phrase the first sentence as `The multiplication never overflows`. As for the second part, lets not introduce any undefined behavior, it is a horrible C++ concept that even C++ standardization committee tries to eradicate. It would suggest "If the any intermediate computation overflows 32-bit integer, the result is implementation-defined": this hints that operation must still produce some result (and not e.g. crash the whole system, which would be OK with undefined behavior), but this results depends on the backend implementation and is generally not portable.
Maratyszcza(2018-08-05 04:55:22):@linkerzhang if you want to implement quantized operators/functions on top of ConvInteger/MatMulInteger, it needs to be the same. beware that backends which implement quantized operators directly may use a different internal representation (e.g. store everything as 9-bit signed integers instead of 8-bit unsigned + zero point). using the same `uint8` tensor with different `zero_point` values would make it impossible to use quantized operators with backends using any representation, but canonical `uint8` + `zero_point` + `scale`.
Maratyszcza(2018-08-05 04:57:35):@linkerzhang is there a use-case for it?
Maratyszcza(2018-08-05 05:01:38):Thanks for adding these @linkerzhang
houseroad(2018-07-24 05:38:23):Thanks, @fumihwh !
bddppq(2018-07-30 02:17:40):ping @anderspapitto 
anderspapitto(2018-08-01 16:24:07):actually, I guess I'm not quite comfortable defaulting people in to a lot of new behavior without a good bit more testing - so putting this on hold for now
bddppq(2018-07-24 20:24:11):If we want to encourage for automatically running new passes, probably we should do blacklist here?
anderspapitto(2018-07-24 20:32:44):what I'm going for is that new optimization passes are 

(opt-in for people writing new passes)
 and 
(opt-out for people calling the code as clients)
bddppq(2018-07-25 09:41:16):What's reason to reuse 0x0025 here while skipping 0x0026 and 0x0027?
bddppq(2018-07-25 09:42:01):`ONNXIFI_STATUS_UNSUPPORTED_TAG`?
Maratyszcza(2018-07-25 15:55:09):To add space for `ONNXIFI_STATUS_UNSUPPORTED_something` codes I'll add later
bddppq(2018-07-25 16:58:18):lol are you sure you only need two more? :-)
Maratyszcza(2018-07-25 18:04:55):Good point! Added more space in there
Maratyszcza(2018-07-25 05:07:53):We are going to have another header file for extensions (backend-specific options), but this option is intended to be standard.
bddppq(2018-07-25 09:26:50):Does this line belong to `ONNXIFI_LOG_LEVEL_ERROR `? For `ONNXIFI_LOG_LEVEL_DEBUG` and `ONNXIFI_LOG_LEVEL_INFO ` they don't necessary correspond to some failures.
bddppq(2018-07-25 09:27:06):Add doc for this as well?
Maratyszcza(2018-07-25 17:53:01):Yes, it is for `ONNXIFI_LOG_LEVEL_ERROR`. `ONNXIFI_LOG_LEVEL_INFO` and `ONNXIFI_LOG_LEVEL_DEBUG` also enable logging of errors, but can log many additional events too.
bddppq(2018-07-25 16:59:12):@onnxbot retest this please
bddppq(2018-07-25 17:03:39):@onnxbot retest this please
bddppq(2018-07-25 09:34:59):data data
bddppq(2018-07-25 09:36:55):What does this 'invalidate' mean? Will it cause next `onnxRunGraph` fail or should the backend detect any changes happened to the tensor descriptors across two `onnxRunGraph` calls?
Maratyszcza(2018-07-25 15:53:47):It means other data passed through tensor descriptors can be deallocated/changed/overwritten once `onnxSetGraphIO` returns.
bddppq(2018-07-25 09:32:26):`it indicates` -> `it must indicate`
bddppq(2018-07-25 09:22:09):`default` CI failure unrelated
bddppq(2018-07-26 03:38:56):ir version is not tied to onnxifi nor "backend", so probably better to just call it ONNX_IR_VERSION
bddppq(2018-07-26 03:39:00):ditto
Maratyszcza(2018-07-26 05:20:29):Backend is a combination of software + hardware, so different backends may support different IR versions. This is why the constant has `BACKEND` in its name.
yinghai(2018-07-26 12:44:51):Rebase?
zrphercule(2018-07-26 20:34:51):@yinghai Yes you are right, I am changing it...
yinghai(2018-07-26 20:13:53):`add_msvc_runtime_flag` might be a better name? 
bddppq(2018-07-30 02:14:25):lol I'm wondering whether there is typo checker we can add to the CI :-)
linkerzhang(2018-07-30 23:00:23):Fair point, will add some test cases.
varunjain99(2018-07-31 01:03:25):@onnxbot retest this please
varunjain99(2018-07-31 01:20:50):@onnxbot retest this please
bddppq(2018-07-31 03:18:53):@onnxbot retest this please
bddppq(2018-07-29 17:41:31):why use signed instead of unsigned for i here?
bddppq(2018-07-29 17:43:45):nit: just say the dim value is invalid? because it's not about less or greater than -1
bddppq(2018-07-29 17:48:17):in case the shape is a param, we should still propagate it to the output
bddppq(2018-07-29 17:50:07):you can use `data<int64_t>()` to get the tensor's data pointer and size_from_dim(0) to get the #elements
bddppq(2018-07-29 17:54:01):nit: probably can do 
```
for (const auto& dim : dataInputTensorType.shape.dim()) {
...
}
```
bddppq(2018-07-29 18:03:06):I think we can still infer the negativeOneDim when there is unresolvedZero, e.g.
```
input shape: 1 2 a 3
target shape: 1 1 0 -1
=>
output shape: 1 1 a 6
```
varunjain99(2018-07-29 18:32:30):Isn't TensorProto different from Tensor?
varunjain99(2018-07-29 18:35:04):Protobuf dim_size function returns an int, and I compare i < dataInputTensorType.shape().dim_size(). Also I think the dim function is supposed to take in an int
varunjain99(2018-07-29 19:20:26):Good point, but now I actually use the i
varunjain99(2018-07-29 19:21:38):True. To infer in this case, we need to keep track of which indices are unresolvedZero's. Only when the data input tensor has missing values corresponding to unresolvedZero's can we infer
linkerzhang(2018-07-30 00:53:23):Thanks for adding this! 

Suggestion: name it as getInput, since it does not have to be an initializer. What we want here is to access input tensor data.
linkerzhang(2018-07-30 00:54:01):Shape inference if 2nd input is available.
linkerzhang(2018-07-30 00:57:07):Should we support the case that 2nd input is available and its value is -1? if yes, what's the scenario please?
linkerzhang(2018-07-30 01:00:04):name it as "all_inputs" or "all_input_tensors" may be better since you're putting nullptr for an input if it's not an initializer.
bddppq(2018-07-30 02:01:28):@varunjain99 oh yeah you are right
bddppq(2018-07-30 02:12:13):@linkerzhang Could you clarify by "its value is -1" you mean the second input is a scalar value `-1` or the second input is a 1D tensor which has an item being `-1`? The latter is supported in @varunjain99's current implementation, what is he saying in this comment is the whole inference process has two stages, and this first stage will mark the `-1` axis, and then compute it in the second stage (see negativeOneDim).
varunjain99(2018-07-30 17:33:13):Good point - making it getInputData
varunjain99(2018-07-30 17:35:42):I think allInputData_ should keep it consistent 
linkerzhang(2018-07-30 21:40:14):add "i >= static_cast<int>(unresolvedZeros.size()" here.
linkerzhang(2018-07-30 21:43:40):if (i >= dataInputTensorType.shape().dim_size()), there should be a data error, right?
linkerzhang(2018-07-30 21:44:30):add check to ensure outputProduct is not "0".
linkerzhang(2018-07-30 21:46:54):I mean the latter one.

I was thinking whether we should support the latter case in this shape inference function. It depends on when we want to run this shape inference function,  if it's before the data coming only, then no need to implement this case here I think. If we also run this function after we get the input data, it's fine to have it here.
bddppq(2018-07-30 22:02:19):@linkerzhang Why are there differences between with and without real input?
varunjain99(2018-07-30 23:38:16):The purpose of this function is that if the reshape shape (data of input 2) is present, we try to infer as much of the output shape. When there is a -1 in the 2nd input data, we can try to infer this dimension if we have enough *shape* information from input 1. We don't need the data of input 1, but enough shape information from input 1, to figure out what -1 should actually be.
varunjain99(2018-07-30 23:42:37):@linkerzhang what do you mean by this? We're not in a loop here?
varunjain99(2018-07-30 23:43:52):Yes, good point - I'll add a fail_inference for that 
bddppq(2018-07-31 03:19:18):remove this?
varunjain99(2018-07-31 05:46:06):Oops - removed
bddppq(2018-07-31 09:00:39):should raise error if(negativeOneDim and outputProduct == 0)
bddppq(2018-07-31 09:04:47):nit: maybe also add the invalid value to the error message?
varunjain99(2018-07-31 17:12:33):fixed
varunjain99(2018-07-31 17:12:37):added
snnn(2018-07-30 20:42:46):two cents: We'd better to use strtol instead of stringstream in this case, because:
1. strtol is faster
2. strtol has better error check.  We should never ignore conversion errors, that's too dangerous. 
Maratyszcza(2018-07-31 22:44:15):Why the driver doesn't just dlopen with the default path?
zrphercule(2018-07-31 23:19:38):Marat is right, we can use the default path.
houseroad(2018-08-15 22:37:17):Probably here we should say something like

`In bvlc_alexnet, there are 24 nodes.` Instead list 24 of 24, 100%, it doesn't make too much sense. :-)
houseroad(2018-08-15 22:39:07):Yeah, I mean, change this sentence? it's quite confusing.
houseroad(2018-08-15 22:46:14):Use the ENUM in proto please, 
Ac2zoom(2018-08-16 17:35:46):Was quite confused as to how to do this; will discuss soon
zrphercule(2018-08-24 17:30:50):Wonder if you need to init self.graph as well? In case some people may use self.graph without initializing it.
Ac2zoom(2018-08-24 20:49:56):Absolutely!  I don't use it here, but I'll include it in case anyone does.  Thank you!
houseroad(2018-09-04 20:59:17):why all 0?
houseroad(2018-09-04 23:39:36):Let's lift this method, probably in a different pr.
houseroad(2018-09-04 23:46:30):if model.graph.node == 0, you have some problem
houseroad(2018-09-04 23:48:44):Remove ml in this condition?
bddppq(2018-08-01 17:17:38):@varunjain99 
gramalingam(2018-08-01 23:46:36):I believe that the original check for groups was added because the behavior is different in the case of groups not being 1 (with a different shape inference required) … is this not the case? 
varunjain99(2018-08-01 23:51:49):@onnxbot retest this please
bddppq(2018-08-02 05:35:45):@gramalingam IIUC the group number does not change the output shape
bddppq(2018-08-02 20:56:18):@houseroad will do a follow up PR to update the spec that the size of the kernel's second dimension should be `C / g`
bddppq(2018-08-04 17:57:13):Thanks!
houseroad(2018-08-02 23:22:46):why not get_available_passes?
pliniosilveira(2018-08-03 01:08:02):Good question! I couldn't decide which name is better. Sometimes I picked one, sometimes other. Let's keep with get_available_passes/getAvailablePasses everywhere, then?
bddppq(2018-08-03 03:19:54):No need to define an extra function, just do
```
get_available_passes = C.get_available_passes
```

or do

```
from C import get_available_passes
```

at the top
bddppq(2018-08-03 03:20:56):No need to define an extra function, do ```&optimization::GetAvailablePasses```
bddppq(2018-08-03 03:22:56):no need to allow `tuple`?
yinghai(2018-08-03 16:08:52):Is this required to be implemented? 
Maratyszcza(2018-08-03 20:09:02):@yinghai Yes
bddppq(2018-08-03 20:17:51):@varunjain99 
yinghai(2018-08-03 18:26:48):It's failing because of the dummy backend code. 
houseroad(2018-08-03 20:24:28):indent?
houseroad(2018-08-03 20:26:10):How do we determine the length of auxPropertiesList?
Maratyszcza(2018-08-03 20:28:34):It must be terminated by `ONNXIFI_BACKEND_PROPERTY_NONE`/`ONNXIFI_GRAPH_PROPERTY_NONE`
Maratyszcza(2018-08-03 20:31:14):I decided to not reformat the whole file in this PR, will rebase & reformat after #1259 lands
CLAassistant(2018-08-03 17:32:27):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=1258) <br/>All committers have signed the CLA.
snnn(2018-08-08 23:05:40):"error: Function is missing a type annotation"

I'm fixing it.
houseroad(2018-08-04 04:59:47):newline
zrphercule(2018-08-28 23:43:47):annotations here, describe the data type of `x` and `name`, and also return value of this function.
zrphercule(2018-08-28 23:43:54):and here
zrphercule(2018-08-28 23:49:17):should better use number_info.min here to test extreme case?
I sugget you to use np.iinfo.max/min for all test cases.
snnn(2018-08-29 00:03:04):No. Cannot use min. For example, if it's a 8 bits signed value, the min value is -256, but the abs(-256) is undefined. 
zrphercule(2018-08-31 21:08:25):@snnn Oh I see, yes I forgot this number if for testing abs.
snnn(2018-08-31 21:57:19):Thanks. I'll add the annotations.
houseroad(2018-08-06 16:15:34):Move the introduction of GetAvailablePasses after Optimize?
pliniosilveira(2018-08-06 16:21:31):Sounds more logical to me that one first reads the list of available passes then chose one to use in the `Optimize`.

In the snippet above the `GetAvailablePasses()` comes below because it is really a copy of the code. In the code it is in this order.
houseroad(2018-08-08 02:22:12):Nit: switch the order of `GetAvailablePasses` and `Optimize`
prasanthpul(2019-04-10 20:43:18):superseded by https://github.com/onnx/onnx/pull/1908
fdwr(2018-10-05 21:55:21):Where does the output tensor's scale and zero_point come from? Is output tensor's name listed in the GraphProto.value_info[...].type.tensor_type.quantization (GraphProto::ValueInfoProto::TypeProto::Tensor::QuantizationProto), and that's what must be referenced to access scale and zero_point values? Typically the data type and the shape of output tensors are determined by the operator or shape inference, and it's weird for intermediate tensors between nodes to store this information.
fdwr(2018-10-05 21:59:27):I'm not seeing any additions to TensorProto regarding scale and zero_point for the sake of storing static tensors in initializers. If one is expected to correlate the TensorProto.name to the Graph::value_info's just to extract the scale and zero_point, that's kinda indirect, and it's weird because both TensorProto.dims and TypeProto.Tensor.shape store shape information. Which wins?
Maratyszcza(2018-10-08 18:28:59):`scale` and `zero_point` for convolution output come from either `GraphProto.output.type.tensor_type.quantization` (if operator output is also a graph output) or `GraphProto.value_info.type.tensor_type.quantization` (if operator output is an intermediate output and not a graph output). This does require that the model provide quantization parameters for intermediate outputs in `GraphProto.value_info`: unlike shape and data type, quantization parameters for static quantization schemas can not be inferred at runtime.
Maratyszcza(2018-10-08 18:34:39):The reader of the graph is expected to match `TensorProto.name` to `GraphProto.input.name` to extract `scale` and `zero_point`. I'm open to add `QuantizationParameters` to `TensorProto` as well, but note that `GraphProto.initializer` and `TensorProto` are not always present (e.g. in ONNXIFI weights are commonly passed in memory rather then encoded into protobuf).
jgong5(2018-10-09 12:01:33):How are in-place tensors handled here? Suppose there are tensors sharing the same name, they share the same ValueInfoProto with same TypeProto but the scales are different so could not share QuantizationProto, correct?
Maratyszcza(2018-10-09 18:02:38):`scale` and `zero point` must be unique for a tensor, i.e. the same tensor always use the same `scale` and the same `zero point` values. This is by design, and ensures that a tensor has a unique real-valued representation. 
jgong5(2018-10-09 23:27:15):@Maratyszcza But ValueInfoProto can be shared by multiple tensors with the same name, right? Consider the situation that one ValueInfoProto is shared by multiple ops via the same name, would that conflict with the uniqueness assumption of QuantizationProto?
Maratyszcza(2018-10-09 23:50:07):ONNX is single-static-assignment form, so a name uniquely identifies a tensor.
jgong5(2018-10-10 00:16:25):@Maratyszcza Oh, thanks for pointing that out.
jgong5(2018-10-29 07:34:12):@Maratyszcza Besides the scalar type "scale" and "zero_point", shall we also support the finer-grained quantization, e.g. channel-wise quantization? Is "TensorProto" a more general type for "scale" and "zero_point"? We also need to consider the axis along which to specify axis-wise "scale" and "zero_point". What do you think?
CLAassistant(2018-08-06 18:34:37):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=1267) <br/>All committers have signed the CLA.
linkerzhang(2018-08-13 23:03:35):please follow the same pattern as operators -- putting Version section as the first one.
linkerzhang(2018-08-13 23:03:53):an attribute name missed here?
linkerzhang(2018-08-13 23:05:52):this kind of formatting could be implemented in the gen doc logic, I think. How is it implemented for operator doc ? we may follow the same logic.
linkerzhang(2018-08-13 23:24:42):I got the reason now. thanks!
NiklasGustafsson(2018-08-09 18:09:58):Shouldn't 'NOTSET' be listed among the valid values? It sounds like it is one of the allowed values.
yinghai(2018-08-10 20:12:24):It should be ok. 
houseroad(2018-08-10 21:03:32):@onnxbot retest this please
houseroad(2018-08-10 21:26:38):@onnxbot retest this please
bddppq(2018-08-12 23:21:47):@onnxbot retest this please
yinghai(2018-08-13 04:07:51):@onnxbot retest this please
rdzhabarov(2018-08-10 21:35:31):temporary commented out?
houseroad(2018-08-10 21:36:51):We are checking the problem with caffe2 build
yinghai(2018-08-10 21:36:59):We saw test failures in CI. I comment this out to see if it's my PR that introduced the failure or it's master. Looks like it's in master. :( 
ezyang(2018-08-13 14:24:05):@pytorchbot retest this please
houseroad(2018-08-13 23:35:26):I guess you also need to run `python cmd_tools.py generate-data` to update the test data, and commit them.
zrphercule(2018-08-14 00:11:57):@houseroad Yes sure, I was planning to commit the data once the test code has no problem =)
houseroad(2018-08-14 00:13:18):Awesome, that part is easy to forget. Just wanna confirm :-)
zrphercule(2018-08-14 18:02:12):@houseroad Could you plz give me a stamp and merge it? ty!
houseroad(2018-08-13 22:12:39):Here, you also need to turn them into np.int64. Otherwise, on 32-bit machine, we will generate np.int32 instead.
zrphercule(2018-08-13 22:35:19):yeah you are right
houseroad(2018-08-13 22:36:37):Also do you need to update the generated data files too?
houseroad(2018-08-14 00:17:37):Similar apply here? The explicit type declaration. 
zrphercule(2018-08-14 00:24:04):This one is fine, since it is only a temporary var and will never pass to our model
houseroad(2018-08-22 05:20:47):CI?
houseroad(2018-08-14 17:03:32):Adjust this list accordingly?
houseroad(2018-08-15 22:11:38):Can we remove this?

houseroad(2018-08-15 22:11:55):I mean the comments
houseroad(2018-08-15 22:18:02):Rename c2_broadcastable to onnx_opset1_broadcastable.

And make them as functions not member functions.
houseroad(2018-08-15 22:19:26):even unibroadcastable, you still need two inputs, right? Like B can be broadcast to A
houseroad(2018-08-15 22:20:16):This doesn't seem correct.
houseroad(2018-08-15 22:20:39):This doesn't seem correct.
Ac2zoom(2018-08-15 22:21:39):Hm... the only use case for this is consideration of C, though.
Ac2zoom(2018-08-15 22:47:44):Restoring from https://github.com/onnx/onnx/pull/1184/commits/da507b8a42f68c6bcfc088cd33184d5d05fac154
houseroad(2018-08-16 23:19:35):Change the name to ComptibleAdapter?
houseroad(2018-08-17 00:14:42):wait, this should be opset1_broadcastable, right?
houseroad(2018-08-20 17:14:35):This is incorrect. check [2, 5, 1, 3] vs [2, 1, 5, 3]
houseroad(2018-08-20 17:35:30):Probably add another assertion here: check all the dimensions are dims, not params.

https://github.com/onnx/onnx/blob/36517e6bac9c0f0cbf9390c97c39b8106dd4d33e/onnx/common/ir.h#L75
houseroad(2018-08-20 17:36:52):Instead of using `Reshape`, let's do `Unsqueeze`, which is more flexible.
houseroad(2018-08-20 17:39:32):Remove backward here too
houseroad(2018-08-20 17:41:20):This if-else seems incorrect.
Ac2zoom(2018-08-20 17:43:42):Resolved.  Didn't think of the double-sided broadcasting case.  Sorry about that!
houseroad(2018-08-20 17:46:35):Here also, we probably want to have helper function, to make sure, all the sizes are dims (int), not params (string)
houseroad(2018-08-20 17:48:19):Since Gemm 6==>7, there is no axis, shouldn't we just remove `broadcast` flag?
houseroad(2018-08-20 17:50:37):First make sure A, B, and C's sizes are ints (dims not param).

Then check whether C needs broadcasting or not.
houseroad(2018-08-20 17:52:11):Can we make the unallowed_types and allowed_types as constructor parameters?
Ac2zoom(2018-08-20 20:48:58):How would you recommend naming this function?  It asserts that the op is opset 7 broadcastable, but returns whether or not it requires broadcasting.  Should I call it onnx_opset7_requires_broadcasting?
houseroad(2018-08-22 23:48:56):const reference?
houseroad(2018-08-22 23:49:10):const reference? you won't change the content right?

houseroad(2018-08-22 23:51:46):~~Can you check that all the dimensions use int (dim) instead of param? Otherwise you may have 0 vs 0 cases.~~ Never mind, you have `assertInputsAvailable`. I saw it. :-)

houseroad(2018-08-22 23:56:10):You can return int as result, like 0 is no broadcast needed, 1 is broadcast is needed. -1 is non-broadcastable.

And give better error message in the caller. Or enum may be better.
Ac2zoom(2018-08-23 00:12:43):remove this else case
Ac2zoom(2018-08-23 07:10:43):The reason we don't distinguish between non-broadcastable and no broadcast needed/broadcast needed is that the asserts will interrupt whichever program if the inputs are non-broadcastable, so we'll never reach any such return value.

Where would the enum be useful here?
houseroad(2018-08-23 15:11:47):It's safer to use 3 -> 2, I think.
houseroad(2018-08-23 15:21:26):This is not safe. Instead moving `node`, we should move `n`, insert it before `node`. https://github.com/onnx/onnx/blob/master/onnx/optimizer/passes/fuse_add_bias_into_conv.h#L127
houseroad(2018-08-23 15:26:16):Don't you need to set the sizes for output of node `n`?
houseroad(2018-08-23 15:26:57):Seems to me, input[1]->sizes() should be available...
houseroad(2018-08-23 18:43:38):assert_numpy_unibroadcastable_and_require_broadcast ?
houseroad(2018-08-23 18:43:55):assert_numpy_multibroadcastable_and_require_broadcast ?
Ac2zoom(2018-08-23 18:56:41):Doesn't return anything, so not require broadcast, but will fix
bddppq(2018-08-14 02:50:28):Good catch
houseroad(2018-08-14 17:23:25):Thanks for fixing the doc.
houseroad(2018-08-14 17:22:43):This should be [FILTER_OUT_CHANNEL, FILTER_OUT_CHANNEL/group, FILTER_SPATIAL, FILTER_SPATIAL...]

Also you can say FILTER_OUT_CHANNEL equals to DATA_CHANNEL.

You can change the shape of 2d case from [M, C, kH, kW] ==> [M, C/group, kH, kW]

pranavsharma(2018-08-14 19:13:00):sure, will fix.
houseroad(2018-08-14 20:59:32):In convtranspose case, weight shape should be C/group x M x ...
houseroad(2018-08-14 21:03:21):It should be [FILTER_OUT_CHANNEL, FILTER_IN_CHANNEL, FILTER_SPATIAL, ...]

And FILTER_IN_CHANNEL = DATA_CHANNEL / group

and you don't need the following note I think.
pranavsharma(2018-08-15 00:56:11):I would actually prefer that we rename C and M. The naming is confusing. In the case of conv transpose, num_output_channels = W.shape[1]/group. W.shape[1] is actually a multiplier for num_output_channels. W.shape[0] corresponds to num_input_channels (X.shape[1]). 
If we keep the same names, the spec should be C x M/group.
pranavsharma(2018-08-15 01:03:22):If you look at the caffe2 implementation, X.shape[1] = W.shape[1]*group. In the terminology of the spec, this translates to DATA_CHANNEL = FILTER_OUT_CHANNEL where W.shape[1] = FILTER_OUT_CHANNEL/group. 
houseroad(2018-08-15 20:28:16):What's the meaning of FILTER_OUT_CHANNEL? does it mean the channel of the output?
pranavsharma(2018-08-15 20:52:31):Yes, that's how the implementation interprets it. FILTER_OUT_CHANNEL is number of channels in the output. 
houseroad(2018-08-15 21:01:30):There are two problems in this case, 
1) the denotation filter should be [out, in, spatial, spatial]
2) the w.shape[1] * group == x.shpae[1] == C, so the W's shape should be [M, C/group, kH, kW]
houseroad(2018-08-15 21:48:37):Yes, this should be [C, M/group, spatial, spatial, ...]
houseroad(2018-08-15 21:51:56):I would prefer to keep the names. But I am open to the new names, if you can think of any better name.
pranavsharma(2018-08-16 05:58:33):+1 for #2. 
Regarding #1, are you suggesting that we change the denotation as well in the spec to [FILTER_OUT_CHANNEL, FILTER_IN_CHANNEL, FILTER_SPATIAL, FILTER_SPATIAL ...] ?
pranavsharma(2018-08-16 05:58:55):Sure, lets keep [C, M/group, spatial, spatial, ...]
houseroad(2018-08-24 20:55:44):This is not compatible, because default value of is_test is 0... you have to set it as 1.
houseroad(2018-08-27 17:08:08):Remove this line?
yinghai(2018-08-28 17:32:33):Windows build is failing? 
zrphercule(2018-08-28 21:25:17):@yinghai maybe it is because of the dir operations. Let me check it out.
zrphercule(2018-09-06 00:36:04):@yinghai @Maratyszcza @bddppq @rdzhabarov @houseroad Any more comments? btw I will add support for float16 in a seperate pr immediately after the merge of this pr, and also I will keep maintaining this test driver so please dont worry if there is anything imperfect, just tell me and I will fix it =)
Maratyszcza(2018-08-22 22:23:54):formatting looks off here
Maratyszcza(2018-08-22 22:27:38):Could you just use `0`? It is well-known that `0` means "no error" for `errno`, but `ERRNO_DIR_END` looks like an error code.
Maratyszcza(2018-08-22 22:28:29):Do you use this function only for text files?
zrphercule(2018-08-22 22:29:36):This is actually a protobuf file, but I would like to read it byte by byte
Maratyszcza(2018-08-22 22:30:05):what is the data type of `proto_tensor.dims().data()`? this doesn't look safe
Maratyszcza(2018-08-22 22:31:15):`const bool` please
Maratyszcza(2018-08-22 22:32:03):Tensor with 0 dimensions is a scalar, it has size 1 (in # of elements)
Maratyszcza(2018-08-22 22:32:44):Why `unsigned long long*`? IIRC, it should be `uint64_t`
Maratyszcza(2018-08-22 22:33:41):Memory leak here. Don't use new, instead allocate `std::vector<uint8_t>` of required size
zrphercule(2018-08-22 22:33:54):It is a long long *, so the only difference is the "unsigned". I think this is safe?
bddppq(2018-08-22 22:58:01):you probably don't add the `driver` pattern here, because GLOB_RECURSE `${ONNX_ROOT}/onnx/backend/test/cpp/*.cc` should cover it already
bddppq(2018-08-22 22:58:39):nit: give it more specific name?
bddppq(2018-08-22 23:03:31):why is this needed?
bddppq(2018-08-22 23:07:32):Is this used to distinguish our dummy backend and the other real backends?
zrphercule(2018-08-22 23:16:55):You are right...
zrphercule(2018-08-22 23:19:55):Yes, since we dont have real backend yet, we cannot really load the data into the backend and examine the output, since we dont have any output (although I did tested it without comparing the output)
zrphercule(2018-08-23 00:03:14):By now I only check the model and the tensor as we dont have real backends, since loading dummy backend here is useless.
zrphercule(2018-08-23 00:14:36):Yes it is uint64_t*, but I think there is no difference between these two? But for consistent I will change this to uint64_t*.
bddppq(2018-08-23 00:37:25):Got it. In Python land test cases can throw a special exception to indicate skipping a test at *runtime*. Just found gtest doesn't it yet. There is an open pr https://github.com/google/googletest/pull/1544 to add it to gtest, would be good to use it once it's merged to avoid special logic for the dummy backend.
yinghai(2018-08-23 04:20:07):You can use the ctor initializer to initialize the members. 

https://en.cppreference.com/w/cpp/language/initializer_list
yinghai(2018-08-23 04:20:14):Same here. 
yinghai(2018-08-23 04:21:19):`_target_dir` seems to be an odd naming convention here. 
yinghai(2018-08-23 04:22:35):It would be good to add high level doxygen style comments on what the class and functions are about. 
yinghai(2018-08-23 04:24:52):Do we need to check `/` at the end of `case_dir`?
yinghai(2018-08-23 04:25:14):nit: `FileExists`
yinghai(2018-08-23 04:25:49):Move this into the `for` loop? 
yinghai(2018-08-23 04:26:36):`std::to_string` doesn't work on Android. Check `caffe2::to_string`. 
yinghai(2018-08-23 04:28:34):small optimization would be 
```
input_filenames.emplace_back(std::move(input_name));
```
yinghai(2018-08-23 04:29:25):`emplace_back` here too. 
yinghai(2018-08-23 04:30:26):What if we have input_file but not output_file? 
yinghai(2018-08-23 04:33:13):There seems to be one-liner: https://stackoverflow.com/questions/3203452/how-to-read-entire-stream-into-a-stdstring
yinghai(2018-08-23 04:35:59):We are making copies here. Is this intended? 
yinghai(2018-08-23 04:36:17):Same here and below. 
yinghai(2018-08-23 04:38:12):Doesn't look safe to me, either. 
yinghai(2018-08-23 04:39:14):no namespace? 
yinghai(2018-08-23 04:40:18):remove? 
yinghai(2018-08-23 04:41:31):Is this some quirky requirement from gtest? 
yinghai(2018-08-23 04:42:51):This seems to be a temporary variable to me, because its purpose is to create the `model_`. 
yinghai(2018-08-23 04:44:14):test performance or correctness? 
yinghai(2018-08-23 04:45:12):Why `initializer(0)`? 
yinghai(2018-08-23 04:45:30):`const auto&`
yinghai(2018-08-23 04:48:09):This is not good. We cannot compare them byte by byte because there might be numerical noise. 
yinghai(2018-08-23 04:48:31):Who will release this? 
Maratyszcza(2018-08-23 06:57:05):To be safe, please copy shape values into `std::vector<uint64_t>` as use it from there. You can do it in one line with `std::vector(proto_tensor.dims().cbegin(), proto_tensor.dims().cend())`
Maratyszcza(2018-08-23 06:59:56):Don't read binary files line-by-line! I bet you don't want to debug what happens when some lines end with `\n`, others with `\r\n`.
Maratyszcza(2018-08-23 07:01:49):label is unused. Also, `goto label` doesn't play well with C++ destructors
Maratyszcza(2018-08-23 07:02:51):Since you write in C++, you should prefer `std::cerr` vs `fprintf(stderr, ...`
zrphercule(2018-08-27 21:01:46):any recommendations?... I am lack of imagination now...
zrphercule(2018-08-27 21:50:14):I guess it doesnt make sense if a node has no output? But I will check this at the beginning of this chunk of code.
zrphercule(2018-08-27 22:05:02):It is hard to pass these data from our backend to gtest. Therefore I set this global var for both sides to comunicate. Gtest has similar solution of using GFLAG, and it does the same thing of using a gloabl var.
zrphercule(2018-08-27 22:08:02):you are right....
zrphercule(2018-08-27 22:13:40):since we need the address of the first initializer. Or we should put all initializers in a vector first?
zrphercule(2018-08-27 22:19:15):yes, I agree with you, thats why I set a reminder here... wonder if you have any suggestions?
zrphercule(2018-08-27 23:28:33):@yinghai I added namespace for test driver, but this part I think it is ok?
Maratyszcza(2018-08-28 17:20:43):Could you cache the result of checks? `FileExists` is relatively expensive
Maratyszcza(2018-08-28 17:21:31):A more common (and likely cheaper) way to check if a file exists is through `lstat`
Maratyszcza(2018-08-28 17:22:10):Doesn't seem to be used anymore, remove
Maratyszcza(2018-08-28 17:24:12):You need a `try`/`catch` block here, to close the directory if any of the C++ methods throws an exception
Maratyszcza(2018-08-28 17:24:54):remove?
Maratyszcza(2018-08-28 17:25:48):indentation off
yinghai(2018-08-28 17:28:11):weid indent? 
yinghai(2018-08-28 17:28:42):```
std::string input_data;
```
is good enough. 
yinghai(2018-08-28 17:31:24):nit: we can define this only for Android. 
rdzhabarov(2018-08-29 23:31:56):is this planned to be used, do not see any references.
rdzhabarov(2018-08-29 23:34:45):should probably return 1 on failure here.
rdzhabarov(2018-08-29 23:37:52):why commented out? (seems legit)
rdzhabarov(2018-08-29 23:39:19):since we expect multiple outputs should the name be unique per output?
zrphercule(2018-08-30 00:40:06):It is not yet to be used (including default_dir it self), but I think it might be useful in the future, and unharm to keep it now.
zrphercule(2018-08-30 00:43:40):@rdzhabarov I am not sure about the mechanism of this name, must it be unique? Or it should be unique to distinguish. 
Also, these results are only a temporary vars.
@Maratyszcza What do you think?
rdzhabarov(2018-08-30 21:03:06):nit: if your intention just to rethrow you should `throw` instead of creating a new exception via `throw e`.
rdzhabarov(2018-08-30 21:03:56):nit: FILE *fp could be moved to the inner scope, e.g., FILE *fp = fopen ...
rdzhabarov(2018-08-30 21:05:48):const auto& i, to avoid additional copies.
rdzhabarov(2018-08-30 21:21:10):can you explain in the doxygen comment what is the expected structure of the case_dir.
also, what are the situations when input and output files are missing.
rdzhabarov(2018-08-30 21:24:18):pass by const ref here
zrphercule(2018-08-30 22:25:56):actually I have doxygen comment of this function in test_driver.h, although not for this method. I will update that one. @rdzhabarov 
rdzhabarov(2018-08-30 23:24:56):typo.
rdzhabarov(2018-08-30 23:27:57):what is the return type for?
rdzhabarov(2018-08-30 23:29:19):what the reasoning behind making `testcases_` public, but `default_dir_` private with the setter?
Maratyszcza(2018-08-31 07:23:20):`return EXIT_FAILURE;` would be better
rdzhabarov(2018-08-31 17:05:41):maybe, just ONNXIFI_TEST_DRIVER
zrphercule(2018-08-31 17:10:02):oop, it has no use. I will remove it.
zrphercule(2018-08-31 17:17:11):@rdzhabarov Actually... I dont really want anyone to simply modify default_dir_ by themselves, and in order to do this they need to cal SetDefaultDir. default_dir_ is only useful for this class internally, so no need to expose to others.
Maratyszcza(2018-09-03 05:10:40):Name in tensor descriptors correspond to tensor names in the model, which must be unique
rdzhabarov(2018-09-04 23:29:49):how is it planned to be used in the future?
Say I have an implementation of the ONNXIFI backend, how would I use the driver to run integration test?

If steps are not obvious it would be good to document.
zrphercule(2018-09-04 23:35:11):I think you are right. I would need to document this.
rdzhabarov(2018-09-04 23:39:35):Is the plan to use #ifdefs here for ONNXIFI_BACKEND_USED?
zrphercule(2018-09-05 17:36:05):@rdzhabarov In fact we are doing such things:
1. We dont specify onnxifi backend for test driver, but only use by default backend. That does not mean we only use one backend, but means we do not indicate the target backend in test driver, but indicate it in onnxifi_loader. You can check the logic of onnxifi_loader when given empty backend location for more details.
2. As for ONNXIFI_BACKEND_USED, we will finnally enable it when we have valid by default backend to test. Now since we dont havevalid by default backend, in order to pass gtest, we set this temporary var. We will finnally remove it, or set it to true if by default backend detected.
rdzhabarov(2018-09-05 19:29:16):why not in the inner scope of #else ?
rdzhabarov(2018-09-05 19:56:06):sizeof(char), sizeof(unsigned char), etc would be more error-prone.
houseroad(2018-09-06 00:38:56):Why add this?
zrphercule(2018-09-06 00:41:17):@houseroad Because it is needed... It used some function under onnx_pb.h, therefore if you include proto_utils.h without include onnx_pb.h, there will be a compile error.
houseroad(2018-09-06 00:42:24):Seems this file is pretty useful for our users, shall we move it out of `test/cpp/driver`? to some onnxifi_utils.h/cc?
zrphercule(2018-09-06 00:43:21):@rdzhabarov but I would like to keep the integrity of name, since all other gtest have "_gtest" as the end of their name...
zrphercule(2018-09-06 00:44:46):@houseroad Yes I do agree with you. Just wondering where should it be moved to.
houseroad(2018-09-06 00:47:50):I don't think so... I guess because in some other files, we didn't correctly import #onnx/onnx_pb.h"
houseroad(2018-09-06 00:48:49):How about the folder as onnxifi.h? which is onnx/
zrphercule(2018-09-06 00:51:32):Sounds decent for now. At last I guess most of the onnxifi code in there will need to be migrated to somewhere else, since there are too many onnxifi files already...
zrphercule(2018-09-06 00:55:03):@houseroad "class AttributeProto" is defined under onnx/onnx_pb.h, and used here...  
zrphercule(2018-09-06 00:56:56):yes you are right
rdzhabarov(2018-09-06 01:35:20):clang-format on all changes maybe.
rdzhabarov(2018-09-06 01:36:56):i think we can fix that later, but as of now, numBackends is initialized with whatever is on a stack and call to onnxGetBackendIDs will likely fail.
rdzhabarov(2018-09-06 01:37:14):std::cerr
houseroad(2018-09-06 04:21:17):@zrphercule oh, yeah, it is used there.
bddppq(2018-09-06 06:14:28):@zrphercule I was talking about the "DRIVER_NAME" part, maybe rename it to "ONNXIFI_TEST_DRIVER" as @rdzhabarov suggested.
bddppq(2018-09-06 06:17:59):Better to separate the dependencies of onnxifi test driver from normal unittests (aka create another ${ONNXIFI_TEST_DRIVER}_libs instead of mixing it into ${UT_NAME}_libs.
bddppq(2018-09-06 06:19:13):use include path relative to onnx top directory
bddppq(2018-09-06 06:20:20):I agree with @rdzhabarov to make ONNXIFI_BACKEND_USED as an ifdef so it can be configured at compile time.


nit: I suggest using ONNXIFI_DUMMY_BACKEND (instead of ONNXIFI_BACKEND_USED) and in your code do
```
#ifndef ONNXIFI_DUMMY_BACKEND
  blablablabla(...)
#endif
```
so that normal real backends don't need to add special -D define to build (at the end it's the dummy backend being the special one to the test driver).
bddppq(2018-09-06 06:22:50):this already exist in onnx/string_utils.h
bddppq(2018-09-06 06:40:53):make a wrapper function for this global variable to avoid SIOF issue
bddppq(2018-09-07 00:19:05):1. shouldn't return a copy
2. to avoid SIOF you should make all_test_cases a function static variable inside this wrapper function
rdzhabarov(2018-09-07 03:40:41):someone could start benefit from this driver_test if they have an implementation of the ONNXIFI interface.

If you allow it via the following:
```
#ifndef USE_ONNXIFI_BACKEND
#define USE_ONNXIFI_BACKEND false
#endif
```

If someone wants to enable ONNXIFI_BACKEND they would compile with -DUSE_ONNXIFI_BACKEND=true.
zrphercule(2018-09-07 05:08:46):Oh sure it is...
zrphercule(2018-09-07 05:09:07):Yes I guess that is better
houseroad(2018-09-07 22:09:03):Nit: indent
rdzhabarov(2018-09-07 22:09:05):typo
zrphercule(2018-09-07 22:09:34):oops
rdzhabarov(2018-09-07 22:10:53):I think you could use regular `if` here (not #if)
houseroad(2018-09-07 22:13:58):input_name is empty?
zrphercule(2018-09-07 22:14:34):Sounds decent
houseroad(2018-09-07 22:17:36):Shall we add some error info?
zrphercule(2018-09-07 22:19:13):Oh at first I only used output_name to judge, so I didnt add input_name here. I will fix it.
zrphercule(2018-09-07 22:19:29):OK
houseroad(2018-09-07 22:22:48):Nit: better name ==> UnresolvedTestCase? Feel free to do it in a different pr :-)
houseroad(2018-09-07 22:23:45):Nit: ResolvedTestCase? or LoadedTestCase?
zrphercule(2018-09-07 22:25:39):"Unsolved" is a genius name, really
houseroad(2018-09-07 22:29:07):Shall we add a parameter `eps`, and make ONNX)FI_TESTDATA_EPS as default value?

Also feel this is quite useful, probably lift it util?
zrphercule(2018-09-07 22:32:40):@rdzhabarov Yes you are right, but I wonder what number should be set to this numBackends?

houseroad(2018-09-07 22:32:42):This also seems very general, maybe lift it to the common/utils folder?
zrphercule(2018-09-07 22:38:12):@houseroad For the eps I think you are right, I will do it. As for IsEqual, I think it is fine to leave it here now. If there is other scenario gonna use this in the future, we can do it then. 

In fact, I am thinking of building a whole operator system for onnxifi backend, since I dont think we have now...
houseroad(2018-09-07 22:40:01):I am about to do it in the python world (seriously), are you interested in ?
zrphercule(2018-09-07 22:40:47):Sure, glad if I can help
houseroad(2018-09-07 22:42:00):Nit: indent
houseroad(2018-09-07 22:54:24):We should add some comments to explain how to use the macro :-)
houseroad(2018-08-27 21:29:19):Also need to remove the consumped_inputs
houseroad(2018-08-27 23:19:24):Let's do the indent.
houseroad(2018-08-27 23:23:41):put an assertion here?
houseroad(2018-08-27 23:25:15):assert raw is not zero, and dividable by 8
houseroad(2018-08-27 23:28:18):indent
houseroad(2018-08-28 00:44:28):should be != 1
houseroad(2018-08-28 00:45:21):It has to add is_test = 1
houseroad(2018-08-28 18:34:52):is_test = 1
houseroad(2018-08-28 20:09:38):Let's remove this part.
houseroad(2018-08-28 20:12:22):Good move!
CLAassistant(2018-08-15 19:43:25):[![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/onnx/onnx?pullRequest=1295) <br/>Thank you for your submission! We really appreciate it. Like many open source projects, we ask that you sign our [Contributor License Agreement](https://cla-assistant.io/onnx/onnx?pullRequest=1295) before we can accept your contribution.<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/onnx/onnx?pullRequest=1295) it.</sub>
gramalingam(2022-05-31 22:59:57):I suggest that we close this outdated PR.
HectorSVC(2018-10-26 17:23:26):from my understanding if linear mean bilinear, the result should be : 
[[[
[1.0, 1.5, 2.0, 2.0],
[2.0, 2.5, 3.0, 3.0],
[3.0, 3.5, 4.0, 4.0],
[3.0, 3.5, 4.0, 4.0]
]]]
Jokeren(2018-10-26 17:41:41):This example was copied from torch.

https://pytorch.org/docs/stable/_modules/torch/nn/modules/upsampling.html
HectorSVC(2018-10-26 19:16:25):Then what's the output if the input data is:
[[[
[1, 2, 3],
[4, 5, 6],
]]]
HectorSVC(2018-10-26 21:34:57):We need to make Onnx spec clear, how to deal with the adage data (the last data in the most inner 2 dimension)
gramalingam(2018-08-15 20:59:31):A couple of questions where feedback would help:

(1)	The scan axis is basically the sequence axis. Question: do we need to support sequence-axis other than 1 (with batch-axis being 0)? If sequence axis is other than 1, then some form of copying (transposition) will be required in each iteration. Is it useful enough to add?
(2)	If we want only sequence-axis = 1, we can drop the scan_axes attribute. We would then need some other way then to determine the number of scan inputs and number of state variables. We could either add an attribute (number_of_scan_inputs, may be with a default value of 1) or make directions non-optional.
(3)	For variable length inputs, should the specification require the padded values in the output to be zero? Or, can the padded values be “undefined”?

gramalingam(2018-08-22 23:42:09):I updated the PR as per the discussions above: simplified things by assuming sequence-axis / scan-axis is 1. As Emad says, we can extend this by providing a Boolean flag (for each scan input) in case we want to flip the batch-axis and sequence-axis. We can add this in the future if there is demand.
bddppq(2018-08-25 06:00:55):@gramalingam ping for regenerating the docs for adding new operators
gramalingam(2018-08-25 14:55:08):@bddppq , which docs? I ran "python onnx\defs\gen_doc.py" and added all changed doc files. What am I missing?

bddppq(2018-08-26 05:32:51):@gramalingam If you look at the failed CI, it says
```
+git status
HEAD detached at FETCH_HEAD
Changes not staged for commit:
  (use "git add <file>..." to update what will be committed)
  (use "git checkout -- <file>..." to discard changes in working directory)
    
    modified: docs/TestCoverage.md
...
```
Which means TestCoverage.md file is outdated. Please run `python onnx/backend/test/stat_coverage.py` to re-generate it.
@houseroad Maybe add this command to https://github.com/onnx/onnx/blob/master/docs/CONTRIBUTING.md?

ebarsoum(2018-08-16 01:11:06):Do we need direction per inputs or one global direction?
ebarsoum(2018-08-16 01:14:11):Do we need to support [batch, seq,...] and [seq, batch,...] through flag?
gramalingam(2018-08-16 02:09:45):I think direction per input is better.
skottmckay(2018-08-16 06:02:57):Is the number of output tensors constant across loop iterations, so the body potentially updates none of them rather than returns zero scan_output_element tensors? 
skottmckay(2018-08-16 06:13:22):Are there any known use cases where scan_axes is required or could that be added later if necessary?

If required, the value assumes the first dimension of the input is the batch size right, so 0 would be valid and equate to the first axis of each row in the batch?
dzhulgakov(2018-08-17 07:26:40):does it make sense to also add iteration_number to match Loop's format? It might be useful for doing some masking, but I'm not that sure
ebarsoum(2018-08-17 17:52:21):I am not fan of one loop OP with multiple way to terminate. 
gramalingam(2018-08-17 17:57:00):I am waiting for other people to provide input … I think this is related to the question I asked up at the top also: do we want to allow flexibility with sequence-axis (and/or batch-axis) or just fix them (batch-axis = 0 and sequence-axis = 1) for now? I am fine either way.
gramalingam(2018-08-17 18:02:26):Sorry, I don't understand the question. Yes, the number of outputs produced by the body sub-graph is constant across loop iterations. The values of some of these outputs may not vary from loop-iteration to loop-iteration. I didn't understand the point about zero scan_output_element.
gramalingam(2018-08-17 18:04:59):Yes, I have the same question (your first question) ... and I am hoping somebody will speak up if they think it is needed. Otherwise, let's drop it for now.
gramalingam(2018-08-17 18:15:50):I believe @dzhulgakov is talking about just providing the iteration number to the loop body, not about adding a trip-count … basically a scan/fold with index. We could do this … but if there is no compelling need right now, we can add further such ops (or extend this) later (just my opinion).
ebarsoum(2018-08-17 19:19:59):That's make sense, I am ok with both.
skottmckay(2018-08-17 22:38:50):If there are zero output elements, what is the purpose of the iterations? Don't they need to produce something, or is it valid to just update the state variables?
gramalingam(2018-08-17 22:58:25):We will still return the final-value of state-variables (this will be like the output Y_h of RNN). The scan_output_element tensors are concatenated and returned and that will be like the output Y of RNN.
bddppq(2018-08-24 17:29:44):I think it's better to use boolean or enum/int to indicate the direction rather than string.
bddppq(2018-08-24 17:31:52):nit: you don't need to specify `true` for required attributes.
bddppq(2018-08-24 17:45:19):I would suggest making this attribute required.
bddppq(2018-08-24 17:59:03):IIUC the loop index can be simulated by using a loop state variable. This is not ideal, but if we see this pattern coming up often than we can add it in a backward compatible way.
gramalingam(2018-08-24 18:04:22):I agree … but my goal was to be consistent with the current RNN/LSTM/GRU spec. It might be better to change all of these uniformly, if we want to use an enum/int.
bddppq(2018-08-24 18:12:53):Here there are only two choices, while for RNN there can be potentially many different activation functions.
gramalingam(2018-08-24 18:50:38):I was referring to the "direction: STRING" attribute (Specify if the RNN is forward, reverse, or bidirectional. Must be one of forward (default), reverse, or bidirectional.)
gramalingam(2018-08-24 18:51:51):But I can change it to Boolean … but the inconsistency might cause some confusion too.
ebarsoum(2018-08-16 00:46:44):Does both statements need to return the same Outputs, and types?
ebarsoum(2018-08-16 00:47:20):With Scan being acceptable also, do we still need trip_count?
ebarsoum(2018-08-16 00:53:16):How nested Loop will look like?

for (size_t r=0; r< height; r++) {
    for (size_t c=0; c< width; c++) {
        process(m[r][c])
    }
}
ebarsoum(2018-08-16 00:54:41):Why we have this restriction?
dzhulgakov(2018-08-17 07:22:11):It's a provided number of iterations (as a scalar tensor) vs picking the dim in Scan op

I think fixed trip count is useful - it saves from having to do add-1 manually. Also it'd make it easier if we want to translate ScanOp into generic Loop
dzhulgakov(2018-08-17 07:23:59):Because it's invalid to concatenate non-matching tensor shapes? I think the same is the case for Loop op actually (padding on max seq size is different)
dzhulgakov(2018-08-17 07:28:15):Yep, it sounds as we discussed.
wschin(2018-08-17 17:44:20):@dzhulgakov , is it better to explain how the tensors gets concatenated (and add an example)? For example, if I have 3 iterations and each iteration generates a 7x5 tensor as the only scan_output, is the final scan_output a 3x7x5 tensor? There are multiple ways to concatenate tensors, I believe.
wschin(2018-08-17 17:47:21):I am also wondering how scan_output can be used in nested Loops. Can the inner loop accumulate scan_output variables onto the outer loop's scan_output?
dzhulgakov(2018-08-18 02:52:37):yes, why not. scan_output of the inner loop is a tensor and it can be passed as the output of the outer loop's body and also become the scan output
dzhulgakov(2018-08-18 02:58:23):it's probably more general to concatenate  by the first dimension. Then individual loop iteration can return not only slices of dim 1 (and can insert unsqeeze as needed).

So in your example each iteration would return 1x7x5 to end up with 3x7x5 output. Or each iteration can return 2x7x5, 0x7x5 and 4x7x5 making the output 6x7x5

Then we also need to add which  dimension to concat on similarly to Scan? That one actually has only input axis but hardcoded output axis to be 1.
ebarsoum(2018-08-18 18:48:57):@dzhulgakov do we need the concatenation? Can we pass the needed data as extra parameter?
bddppq(2018-08-24 18:23:44):@ebarsoum Could you elaborate on "pass the needed data as extra parameter"?
bddppq(2018-08-24 20:13:29):Updated the doc
ebarsoum(2018-08-24 21:00:43):I just talked to Rama, can we clarify the doc, is the restriction only for the scan_outputs or the scan_outputs and loop carry dependencies? We don't need restriction for the loop carry dependency, correct?
bddppq(2018-08-25 00:22:35):@ebarsoum Yes this restriction only means to be applied to scan_outputs (for concat). Just updated the doc to clarify.
houseroad(2018-09-07 06:19:56):CI is still red ;-(
houseroad(2018-09-07 17:09:06):Lint the code please. :-)
houseroad(2018-09-05 23:37:54):This can be removed
houseroad(2018-09-05 23:45:41):why do we need this whitelist?
houseroad(2018-09-05 23:54:28):Can we split this part into a separate function? seems the logic is independent, and probably we should add some comments about how to use it?
houseroad(2018-09-06 18:18:41):Add a summary, including the purpose, which env var will be used
CLAassistant(2018-08-17 21:53:51):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=1304) <br/>All committers have signed the CLA.
HectorSVC(2018-08-17 23:43:59):1. it aligns with TF: https://www.tensorflow.org/api_docs/python/tf/gather
2. The reason for not report error directly on GPU is that it's not easy to do this on device normally, it may slow the processing a lot, just set a default value will a lot faster for GPU case
houseroad(2018-08-21 00:31:29):I think probably we should leave the error handling to backends. Especially, in different frameworks, people have different handling. Especially, how to report the error is not well defined in ONNX.
prasanthpul(2019-04-10 20:45:42):CLosing due to lack of activity in a long time. Please reopen if needed
CLAassistant(2018-08-20 20:34:37):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=1307) <br/>All committers have signed the CLA.
houseroad(2018-08-20 21:30:27):Can you also update the doc here? https://github.com/onnx/onnx/blob/master/onnx/examples/optimize_onnx.ipynb
0wu(2018-08-20 23:24:45):@houseroad by doc you mean creating an example showing the before/after optimization?
houseroad(2018-08-20 23:26:17):I mean update the doc (ipynb) to show this optimization pass in the lists
0wu(2018-08-20 23:29:19):@houseroad The list indeed contains the `eliminate_nop_pad` in the list
```
Available optimization passes:
eliminate_identity
eliminate_nop_pad
eliminate_nop_transpose
eliminate_unused_initializer
extract_constant_to_initializer
fuse_add_bias_into_conv
[deleted]
```
 https://github.com/onnx/onnx/blob/a14c0f756b418183d7f347f23417a130fc2508a0/onnx/examples/optimize_onnx.ipynb
houseroad(2018-08-20 23:30:40):@0wu cool, I was blind.
linkerzhang(2018-08-24 03:22:59):doc generation should also be reflected with deprecation flag. 
RyanUnderhill(2018-08-28 02:26:45):Doc generation calls the schema() functions, so deprecating an op leads to errors during document generation. I changed it so that the schema() function will still return a deprecated schema, and the caller has to do the check. This should be easy since a deprecated schema will have no inputs/outputs/type constraints in it's definition.

With the updated doc generation, this is a sample output:

`### <a name="Asin"></a><a name="asin">**Asin** (deprecated)</a>
#### Version
This version of the operator has been deprecated since version 7 of the default ONNX operator set.
#### Inputs
#### Outputs
#### Type Constraints
#### Examples
`

I used 'asin' as an example since deprecating operators that are used in node tests causes them to fail, so this was easier.

linkerzhang(2018-08-28 09:22:30):thank you for updating the doc. The empty sections "Inputs Outputs Type Constraints Examples" should be just removed in the doc in the "deprecated" case.  Please also keep the historical version links in the version section.
RyanUnderhill(2018-08-29 02:46:28):After the latest change, the documentation now looks like this:

`### <a name="Asin"></a><a name="asin">**Asin** (deprecated)</a>

#### Version

This version of the operator has been deprecated since version 7 of the default ONNX operator set.
`
linkerzhang(2018-08-31 02:16:33):@bddppq  any comments please?
RyanUnderhill(2018-09-01 00:35:22):@houseroad : My next step was to deprecate OneHotEncoder from ONNX_ML and add OneHot to the ONNX domain. Would that be a better example than adding back Upsample? (Plus experimental ops are just deleted, they shouldn't be deprecated)
linkerzhang(2018-08-28 09:21:05):why remove this line?
RyanUnderhill(2018-08-29 02:38:55):This case fixes up the pos to be at the right index, the fallthrough case is the same line.

It was more obvious when I had the deprecated check, as it looked very redundant to do it once within the if and once right after it.
bddppq(2018-08-31 03:28:02):nit: Maybe say "Operator xxx has been deprecated since version yyy"
RyanUnderhill(2018-08-31 19:05:36):Done, changed to this and verified:

`fail_check("Operator '", name_, "' has been deprecated since version ", since_version_);
`

snnn(2018-08-23 20:30:42):It's better,  but I think onnx has 3 C/C++ projects and you only changed two of them.
And, if another static library, let's say, lib A, depends on onnx. it still has to manually add "-DONNX_NAMESPACE=.."

daquexian(2018-08-24 00:19:22):@snnn Thanks!

> I think onnx has 3 C/C++ projects and you only changed two of them.

Thanks for pointing it out! I have updated my PR and set flags for `onnx_cpp2py_export`.

> if another static library, let's say, lib A, depends on onnx. it still has to manually add "-DONNX_NAMESPACE=.."

I think it is not the case. It will be handled by CMake. As long as a developer use `add_subdirectory` to add onnx sources into his library, the flag will automatically take effect.

When it comes to the case that onnx is already installed on PC and a developer wants to link to the compiled onnx lib rather than build onnx source from scratch using `add_subdirectory`, a perfect c++ && cmake project should let downstreams directly use `find_package(xxx)` in CMakeLists.txt if the project has been installed, it is a feature of modern(namely 3.0+, I think) cmake (Here is a useful [introduction](https://pabloariasal.github.io/2018/02/19/its-time-to-do-cmake-right/)). For example, if onnx supports this feature, then developers can simply link their lib to installed onnx lib using the following code

```
find_package(onnx)                      # Or another name like "ONNX"
add_library(my_lib my_lib.cpp)
target_link_libraries(my_lib onnx)      # Or another name like "onnx::onnx"
```

on any PC which onnx is installed, the compile flags will automatically take effects too.

Sadly onnx doesn't support it, but my PR also helps when using `add_subdirectory` to directly build onnx sources in other projects rather than link to the installed onnx lib.
snnn(2018-08-24 00:41:05):For a static library, you won't need to call "target_link_libraries(my_lib onnx)". As there is no link step for a static library.
daquexian(2018-08-24 06:34:07):@snnn Yes there is no link for static libraries, but compiler flags will also be spread from onnx to the intermediate static library, and then from the intermediate static library to the executable, as long as the intermediate library and the executable are using cmake properly (e.g. In my project, my static lib A depends on onnx by `add_subdirectory` and then `target_link_libraries`, and my executable B depends on A, `-DONNX_NAMESPACE=...` also takes effect when compiling B). I have verified it in my project, but maybe there is something I'm overlooking. I'm looking forward to your opinion.
snnn(2018-08-24 16:51:14):Sounds good. Would you please also change the definition of "-DONNX_ML=1"?
daquexian(2018-08-25 01:45:49):@snnn @bddppq Thanks for your approving. It's better to replace `add_definition` with modern `target_...`.
 Do I need to make the `ONNX_ML=1` definition public (also take effect on downstream project) or private (only take effect on onnx itself)?
bddppq(2018-08-25 04:25:18):@daquexian Yes please help fixing the ONNX_ML define as well. I think public is more appropriate since we would want to propagate it to the upper lever targets. Thanks
daquexian(2018-08-25 05:40:53):@bddppq Thanks! I have fixed ONNX_ML define, and also used more proper `target_compile_definitions` instead of `target_compile_options` for ONNX_NAMESPACE.
bddppq(2018-08-25 05:45:26):@daquexian Cool Thanks. I will merge it once CI have finished (and passed).
daquexian(2018-08-25 07:17:14):@bddppq Thanks! CI has passed :)
daquexian(2018-08-23 09:40:50):There is also an `add_definitions` here, but I don't know whether it is ok to replace it.
CLAassistant(2018-08-24 02:50:03):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=1320) <br/>All committers have signed the CLA.
houseroad(2018-08-27 21:13:11):@zrphercule this PR won't touch the doc. :-)
rockindy(2018-08-28 06:25:21):@houseroad I have added the checking and wait for CI appveyor.
houseroad(2018-08-27 21:11:54):Please call `hasNInputShapes(ctx, 1)` to make sure we have the shape information of the first input.
bddppq(2018-08-28 16:04:18):Could you add `{}` around the body of if?
bddppq(2018-08-28 16:06:57):You probably only want to copy the input type to output here?
rockindy(2018-08-29 06:45:18):Sure.
rockindy(2018-08-29 07:14:45):My original intention is to prepare an output object for modifying. I will use propagateElemTypeFromInputToOutput() for less code instructions.
Thanks for your advice.
CLAassistant(2018-08-24 09:27:55):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=1321) <br/>All committers have signed the CLA.
CLAassistant(2018-08-24 12:04:56):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=1322) <br/>All committers have signed the CLA.
CLAassistant(2018-08-24 20:54:34):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=1324) <br/>All committers have signed the CLA.
chinhuang007(2018-08-24 22:40:36):Okay, in that case, we might need to look into alternative ways to setuptools + entry_points to set up these console script files.
houseroad(2018-08-24 23:24:49):Actually, you can add instructions about how to install the onnx in the OS level env. By default, we suppose user would like to install that into the users' python env.
houseroad(2018-08-27 21:04:56):@chinhuang007 feel free to reopen the pr if you have new proposal :-)
CLAassistant(2018-08-24 23:52:12):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=1325) <br/>All committers have signed the CLA.
zrphercule(2018-08-31 00:07:19):@onnxbot retest this please
lygstate(2018-09-03 07:34:58):Do not know why it's failed. weird.
lygstate(2018-10-18 19:18:29):@zrphercule Please take a look at this and merge this, 
bddppq(2018-09-06 21:18:03):I think cmake's default build type is not necessary "Release"?
bddppq(2018-09-06 21:18:44):What's the benefit of changing to use generator expression for this? IMO this new code looks more cryptic than the old version.
raymondxyang(2018-09-06 22:00:16):I think what he is trying to do is to get the build_type and add to --build --config, which is a valid change.
bddppq(2018-09-07 00:05:05):@raymondxyang `build_type` will be unconditionally appended to `cmake_args`
lygstate(2018-10-03 10:32:21):https://stackoverflow.com/questions/34490294/what-does-configdebugrelease-mean-in-cmake
lygstate(2018-10-15 04:37:26):Refer to https://stackoverflow.com/questions/24460486/cmake-build-type-not-being-used-in-cmakelists-txt

lygstate(2018-10-15 04:38:52):After https://stackoverflow.com/questions/24460486/cmake-build-type-not-being-used-in-cmakelists-txt, there is no need apped build_type to cmake_args.
yinghai(2018-08-27 19:03:32):What's `protobuf-lite` for? 
bddppq(2018-08-27 19:25:38):@yinghai it's a "lite" version of protobuf :-). From the [official doc](https://developers.google.com/protocol-buffers/docs/proto):
> LITE_RUNTIME: The protocol buffer compiler will generate classes that depend only on the "lite" runtime library (libprotobuf-lite instead of libprotobuf). The lite runtime is much smaller than the full library (around an order of magnitude smaller) but omits certain features like descriptors and reflection. This is particularly useful for apps running on constrained platforms like mobile phones. The compiler will still generate fast implementations of all methods as it does in SPEED mode. Generated classes will only implement the MessageLite interface in each language, which provides only a subset of the methods of the full Message interface.

Maratyszcza(2018-08-28 17:15:49):Thanks a lot, @bddppq!
yinghai(2018-08-28 04:37:03):Can we do `ONNX_ML=1` as well as `LITE=1`? 
bddppq(2018-08-25 07:07:24):Fold into #1326 to avoid waiting since it's small
bddppq(2018-08-25 07:22:09):I personally like Go and I like the idea of enabling the Go community to be able to access ONNX. **But**:

1. I don't think we should put the auto-generated onnx.pb.go file into source control. Similar for Python and C++, we add a step in the build system to auto-generate the language specific api files on the fly https://github.com/onnx/onnx/blob/6146a85d371481222c10ede4430ad5476e60de87/CMakeLists.txt#L234-L239
2. I have to admit at this point we don't have enough resources to maintain one more language interface. I would suggest let's add a separate repo like https://github.com/onnx/onnx-r to host the go interface and grant you write access to the repo (if you are willing to be responsible for the maintenance work for it). @jspisak, @prasanthpul @lupesko

owulveryck(2018-08-25 07:37:21):The separate repo sounds like the best idea indeed. And to be honest, it will be a lot better to have a lighter repo. It will be more efficient for "go-getting" and update it.
jspisak(2018-08-25 14:08:14):Completely agree. A separate repo is definitely the way to go. @owulveryck - thanks for contributing to the project!  cc @houseroad 
owulveryck(2018-08-27 18:40:28):How do we proceed then?
Is there any special Licence required for the repo? Any requirement that needs to be fulfilled? I have read the contribution guide but nothing seems to be related to this.
houseroad(2018-08-27 22:34:52):@owulveryck you may want to take a look at https://github.com/onnx/onnx-r

Since ONNX uses MIT License, you can stick to it you like. Setting up the CI and running tests will be recommended.

If you can create one repo under your user account for pre-review, it will be great.
bddppq(2018-08-28 20:08:31):Closing this one as discussed.
anirudhacharya(2018-08-26 23:58:42):@houseroad I tried running these commands to generate the data and documentation for the changes -
```
python onnx/backend/test/cmd_tools.py generate-data
python onnx/defs/gen_doc.py
python onnx/backend/test/stat_coverage.py
```
The `gen_doc` and `generate-data` tools errored with `ImportError: cannot import name FunctionProto` and 
```
File "onnx/backend/test/cmd_tools.py", line 31, in generate_data
    args.output, case.kind, case.name)
AttributeError: 'TestCase' object has no attribute 'kind'
```
respectively. 

The `stat-coverage` tool changed a lot of unrelated files like `bvlc_alexnet`, `densenet` etc.. Are these tools up-to-date and usable or is there something wrong in my usage?
bddppq(2018-08-27 18:16:54):@anirudhacharya tools are up-to-date, they are run in the CI for every PR and master commits. From the "missing FunctionProto error message" I guess you have invoked the python that installed the onnx release package, you need to instead install your local onnx checkout (that has the changes in the PR).
anirudhacharya(2018-08-27 20:29:35):the CI failed on this test `OnnxBackendNodeModelTest.test_mvn_cpu`. The data file for the above test is badly formed, but my PR does not touch this test.
houseroad(2018-08-27 20:31:07):Let’s don’t commit the mvn tests at this moment
bddppq(2018-08-27 20:47:16):@raymondxyang @linkerzhang for mvn
houseroad(2018-08-27 21:25:10):https://github.com/onnx/onnx/issues/1313, we already reported the issue.
anirudhacharya(2018-08-27 23:55:36):the two `appveyor` tests on the CI are stuck
bddppq(2018-08-27 23:59:26):@anirudhacharya Not stuck, just the queue is super long ... @prasanthpul Maybe upgrade our plan again? :-)
houseroad(2018-08-28 00:08:26):may be from 2 instances to 4 instances :-)
bddppq(2018-08-28 05:25:28):Would be nice to remove the irrelevant data files changes from this PR
bddppq(2018-08-28 17:25:28):Ping @ebarsoum and @linkerzhang for reviewing schema changes
anirudhacharya(2018-08-28 21:55:23):unrelated CI error - `ERROR at setup of onnx/examples/load_model.ipynb::Cell 0`. Have retriggered the CI.
vandanavk(2018-11-05 19:42:21):Is this PR good to go? Would be great if we could have this param and tests for LPNormalization.
Roshrini(2019-01-08 21:36:33):@houseroad @bddppq Due to 'eps' attribute not being present, users wont be able to use LPNormalization operator in mapping framework specific Normalization operators. 

Seems like [onnx-tf](https://github.com/onnx/onnx-tensorflow/blob/master/onnx_tf/handlers/backend/lp_normalization.py ) maps this operator to tf.norm which is same as reduceL1/reduceL2.. 

Pytorch maps norm to reducel1/reducel2 but does not export [normalize](https://pytorch.org/docs/master/nn.html#torch.nn.functional.normalize) operator due to the same restriction.

I see implementation to add eps attribute to LPNormalization in this PR but due to eps usage being different in different frameworks, its kind of stuck. Any idea how we can add this attribute or if we should just remove the operator as it cannot be used in any framework converters? Correct me if I am wrong.
CLAassistant(2019-07-24 00:57:56):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=1330) <br/>All committers have signed the CLA.
ebarsoum(2020-01-09 18:26:10):If this is still important, please bring a proposal to the OP SIG meeting.
bddppq(2018-08-27 18:17:43):Why changing this?
anirudhacharya(2018-08-27 18:52:58):this change was made by the `stat-coverage` tool.
bddppq(2018-08-27 21:43:03):unused import
anirudhacharya(2018-08-27 21:46:57):wrong name. will correct
houseroad(2018-08-28 04:50:58):For the single floating number, 1e-10f is too small. Let's make it 1e-5.
houseroad(2018-08-28 04:51:39):We can have a variable called epsilon, since you use it twice in every cases.
anirudhacharya(2018-08-28 05:10:27):Tensorflow uses 1e-12 - https://www.tensorflow.org/api_docs/python/tf/nn/l2_normalize 
and MXNet uses 1e-10 - https://mxnet.incubator.apache.org/api/python/symbol/symbol.html#mxnet.symbol.L2Normalization. 

IMO 1e-10 is a pretty reasonable default value to have. Let me know if you still want the default value changed.
houseroad(2018-08-28 05:18:22):I checked with the floating converter, it is fine. Let's keep 1e-10.
ebarsoum(2018-08-28 23:11:45):This might make it difficult for converter, due to how each framework add epsilon:

    TF: x / sqrt(max(sum(x**2), epsilon))
    Others: x / sqrt(sum(x**2) + epsilon)
houseroad(2018-08-28 23:44:36):Any suggestion?
ebarsoum(2018-08-29 04:56:08):I don't have a good solution. What Caffe2 and PyTorch use?
bddppq(2018-08-29 05:10:01):caffe2 doesn't have epsilon, pytorch uses `max(sqrt(sum(x**2)), epsilon)`
yep yet another two variants :)

anirudhacharya(2018-08-29 17:46:10):so what is the resolution for this?
anirudhacharya(2018-08-31 01:13:32):@bddppq @ebarsoum ping?
houseroad(2018-08-31 04:22:38):@anirudhacharya we need further discussion on this design.
ebarsoum(2019-01-17 02:07:35):If both TF and PT use max, and based on both equations we can easily convert between them. Then let's use max and not plus.
ebarsoum(2019-01-17 02:33:44):Also, max doesn't add bias like the plus.
bddppq(2018-08-27 20:45:06):@houseroad no doc needs to be updated
houseroad(2018-08-27 20:59:13):@bddppq right, the changes are not part of the expect functions.
snnn(2018-08-27 21:29:27):Thanks! 
raymondxyang(2018-08-27 23:59:15):FYI I ignore some changes caused by generate-data script (parts I didnot touch..)
onnxbot(2018-08-28 00:13:55):Build finished. 

onnxbot(2018-08-28 00:15:29):Build finished. 

chinhuang007(2018-08-31 17:00:41):@houseroad Is it okay to instruct non root users to use a virtual environment to avoid the permissions exception? I believe it is fairly common to run development builds in python virtual envs
chinhuang007(2018-08-31 20:39:51):The situation is that non-root user would get the exception when strictly following the current instructions, not a very good first impression.
chinhuang007(2018-08-31 20:50:28):I believe most developers log in as non-root users. Some python applications do not require adding files to /usr/local/bin. So a little reminder here should help the first time onnx user's experience.
CLAassistant(2018-08-28 02:35:53):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=1338) <br/>All committers have signed the CLA.
houseroad(2018-08-28 02:46:06):Can you sign the CLA?
ArmenAg(2018-08-28 02:55:27):I have signed it.
houseroad(2018-08-28 02:57:02):It shows you have one more to sign :-)
ArmenAg(2018-08-28 03:07:05):Sorry git troubles :)
prasanthpul(2018-08-28 04:33:47):Agree with @bddppq.
ArmenAg(2018-08-28 17:24:40):That makes sense for expand_as, but shouldn't we still have ne/le/ge? That way backends wouldn't need to optimize `not(a==b)` to `a!=b` or `a \leq b` to `(a<b) or (a == b)` for example.

To play devils advocate for expand_as, wouldn't this help in graph optimization since we now understand the intent of the expand is? Otherwise we have to infer that in expand[shape] shape is coming from another tensor to understand the same intent.
prasanthpul(2018-08-28 17:28:30):This seems like a good use for functions (https://github.com/onnx/onnx/blob/master/docs/Functions.md). Functions are basically compound ops and enable optimization in backends that can but still allow others to run the models without implementing lots of ops.
ArmenAg(2018-08-28 22:01:18):In that case should things like sigmoid\tanh be functions and not operators?
snnn(2018-08-29 00:13:14):No test case?
ArmenAg(2018-08-29 01:37:23):Are the only tests I need to implement [here](https://github.com/onnx/onnx/tree/master/onnx/backend/test/case/node)? What's the consensus on which of these ops need to be added?
houseroad(2018-08-29 15:04:15):You also need to generate the data using these tests. `pythno onnx/backend/test/cmd_tools.py generate-data`
ArmenAg(2018-08-29 21:12:35):I wrote all the tests for ne/ge/le/expand_as.
ArmenAg(2018-08-30 01:33:15):For consistency with numpy it makes sense. Logical operators should be defined for all numerical types.
ArmenAg(2018-08-30 17:53:07):@bddppq @snnn  @zrphercule What are your opinions on this PR? What are the next steps?
spandantiwari(2018-09-07 00:44:28):May I suggest that we reconsider the names for these three ops, `NE`, `LE`, `GE`. We currently have op names such as `Less`, `Greater`, `Equal`. It would be more consistent naming if we choose something like `NotEqual`, `LessEqual`, `GreaterEqual` for these new ops. 
ArmenAg(2018-09-10 17:22:46):@spandantiwari Makes sense. But idk if we're adding this ops or not. @houseroad If we're not adding these as op's could we close this pull request?
snnn(2018-08-29 21:46:25):What's the semantic of ‘<=' on bool?
ArmenAg(2018-08-29 22:03:12):This is a good point. I wanted to stay consistent with numpy.less_equal functionality which would return a tensor filled with True. But I see the argument as to not supporting boolean tensors for this op.
ArmenAg(2018-08-29 22:07:49):What we could do is to maintain support <= on boolean tensors (for consistency reasons), but write an optimization pass that would replace this comparison with a Boolean Tensor of True values.
zrphercule(2018-08-30 00:46:07):I think this should be GE?
zrphercule(2018-08-30 00:46:15):And here?
zrphercule(2018-08-30 00:46:29):Should be LE?
raymondxyang(2018-08-29 17:50:15):Friendly ping @snnn as he is working on related build
bddppq(2018-08-29 07:34:41):originated from caffe2 
bddppq(2018-08-29 07:34:47):originated from caffe2 
bddppq(2018-08-29 07:34:52):originated from caffe2 
bddppq(2018-08-29 07:35:17):I'm a fan of using absolute include path
bddppq(2018-08-29 07:36:24):technically it's unnecessary to do such special processing for the include paths of this onnx_cpp2py_export target
bddppq(2018-08-29 07:36:44):redundant
houseroad(2018-08-29 16:42:02):This is pretty useful
yinghai(2018-08-29 16:53:55):Why do we need `PUBLIC` here? 
bddppq(2018-08-29 17:00:42):Initially I just wanted to make it consistent with other targets, and since this target is never meant to be a dependency of others, so PUBLIC or PRIVATE shouldn't matter.
But taken a second thought it's probably better to keep it PRIVATE to follow cmake's rule.
bddppq(2018-08-29 17:39:37):int64_t
bddppq(2018-08-29 17:41:01):if nelem is 0 we should explicitly check num_value_fields is 0
houseroad(2018-08-29 17:43:17):Good catch
yinghai(2018-08-29 22:23:14):Can 8 and 9 swap order? 
Maratyszcza(2018-08-29 22:24:43):It will work, but make the start of inference synchronous
yinghai(2018-08-29 22:27:11):The common trap is inside 8, we will probably call wait on `input event` and on CPU, I've seen implementation of events with conditional_variable. Have 9 after 8 will hang the process. 
Maratyszcza(2018-08-29 22:28:57):9 after 8 with condvar should work fine, so long as condvar wait happens on a different thread
yinghai(2018-08-29 22:29:45):Yeah, this is another common trap... 
rdzhabarov(2018-08-29 22:54:38):@yinghai Currently, 8 and 9 are swapped in the pytorch/C2 implementation, right?
yinghai(2018-08-29 22:58:50):Yes. Because we don't spawn a thread just to communicate to backend. 
rdzhabarov(2018-08-29 23:08:21):how can you run `onnxRunGraph` multiple times without `onnxSetGraphIO`? I assume you can only do that **if** previous run is completed. Can you mention that there is no expected concurrent execution of `onnxRunGraph` with the same `setGraphIO`?
zrphercule(2018-08-30 00:11:57):@rdzhabarov I think onnxSetGraphIO only gives the IO info to backend, therefore it is possible to run onnxRunGraph for multiple times but run onnxSetGraphIO for only one time? Like
onnxSetGraphIO(...);
//first time running
onnxRunGraph(...);
onnxSignalEvent(...);
onnxWaitEvent(...);
... //get the output, maybe even modified the input descriptor...
//then run a second time without changing the shape and address of input&output
onnxRunGraph(...);
...

rdzhabarov(2018-08-30 00:24:33):That makes sense, but that's sequential execution the same thing I mentioned in
> you can only do that if previous run is completed.

So no concurrent execution in this case.
Maratyszcza(2018-08-30 05:02:16):No, the backend should spawn a thread to do the inference, if it does inference on the CPU, and wait until the event is signaled in this thread. If it does inference on something else then the CPU, it instead should to submit a "wait for event" command to the command queue for the async device, e.g. [cudaStreamWaitEvent](https://www.cs.cmu.edu/afs/cs/academic/class/15668-s11/www/cuda-doc/html/group__CUDART__STREAM_gfe68d207dc965685d92d3f03d77b0876.html), [clEnqueueWaitForEvents](https://www.khronos.org/registry/OpenCL/sdk/1.1/docs/man/xhtml/clEnqueueWaitForEvents.html), or [glWaitSync](https://www.khronos.org/registry/OpenGL-Refpages/es3.0/html/glWaitSync.xhtml).
yinghai(2018-08-29 22:13:59):This might be an issue. One use case is we convert C2 op to ONNX node and run `onnxGetBackendCompatibility`. Sometimes one C2 op will map to multiple ONNX nodes. Then this rule will break things. 
Maratyszcza(2018-08-29 22:15:02):@yinghai: in this case we can still check the subgraph nodes one-by-one, right?
Maratyszcza(2018-08-29 22:15:27):Albeit it does create inconvenience on framework side.
yinghai(2018-08-29 22:16:43):That's multiple calls, which isn't really necessary. The rule that this PR proposes looks a bit strict to me. 
rdzhabarov(2018-08-29 22:42:56):@Maratyszcza @yinghai I think it should not be a problem to supply multiple nodes, go for it.
Maratyszcza(2018-08-30 08:17:57):IMO, `if (NOT ANDROID AND NOT IOS)` is clearer, but I don't have a strong opinion
jspisak(2018-08-31 16:20:33):Great job @bddppq!!
linkerzhang(2018-09-01 01:14:47):@houseroad  reformatted. Thanks!
linkerzhang(2018-09-03 03:09:47):@houseroad  this PR is doing the same fix as the fix you mentioned. Abandoning this PR in this way.
houseroad(2018-09-05 17:44:48):Rebase?
CLAassistant(2019-07-24 00:57:38):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=1375) <br/>All committers have signed the CLA.
prasanthpul(2020-02-24 00:51:53):Not sure this is valid anymore since the scoreboard has been revised significantly. Please reopen if necessary
bddppq(2018-09-05 01:00:05):New operator needs to be put into new operator set. Since we just released opset 8 with onnx v1.3 last week, you need to bump the opset to 9.
bddppq(2018-09-05 01:00:44):Is axes optional input? If so you need to annotate the corresponding Input in the schema
ebarsoum(2018-09-05 06:17:17):When we decide to remove it from experimental, it should replace slice, right?
jamesr66a(2018-09-05 18:00:29):Yes, I think that makes sense, unless people want the specialized version with the slices as attributes for optimization
bddppq(2018-09-06 06:05:19):This should be put in OpSet_Onnx_ver9:: ForEachSchema (you need to create the new OpSet_Onnx_ver9 since this is the first change in opset 9).
Mixing schemas in different opset versions will cause issue when __ONNX_DISABLE_STATIC_REGISTRATION is false.
bddppq(2018-09-06 17:51:04):lol good catch
bddppq(2018-09-07 00:34:29):#1364
houseroad(2018-09-07 05:29:26):@zrphercule it's generated from `onnx-operators.proto`
zrphercule(2018-09-07 05:39:52):@houseroad I see... So I think this error is because onnx-operators.pb.h is a part of lib "onnx"?
bddppq(2018-09-07 06:07:01):@houseroad @zrphercule I bet it's cmake treats
```
target_link_libraries(onnx_cpp2py_export
                          PRIVATE "-Wl,--whole-archive" $<TARGET_FILE:onnx>
                                  "-Wl,--no-whole-archive")
```
differently than
```
target_link_libraries(onnx_cpp2py_export PRIVATE onnx)
```
(I have removed the latter in my previous diff) that with the former it doesn't treat onnx as a dependency of onnx_cpp2py_export and so onnx's public includes (and probably also the defines) are not propagated to onnx_cpp2py_export.

Let me remove that cleanup from this PR to fix #1364 first.
spandantiwari(2018-09-07 22:45:56):Maybe we should consider bumping the opset version for those runtimes that support ONNX 1.2 and 1.3 that did not not have the integer types as part of the implementation of these ops. 
houseroad(2018-09-07 22:47:21):@spandantiwari ok, let me update the pr later :-)
houseroad(2018-09-12 21:52:16):@linkerzhang @spandantiwari comments have been addressed, ping for review :-)
linkerzhang(2018-09-11 06:29:13):thank you Lu! shall we add all numeric types please?
houseroad(2018-09-11 17:37:48):Sure, I will do that and bump up the opset version too
rdzhabarov(2018-09-07 23:55:23):can you comment what Unsolved/Resolved actually mean here :) in the comment for the classes?
zrphercule(2018-09-08 02:18:16):Sure!
zrphercule(2018-09-12 22:08:57):@rdzhabarov oh I forgot telling you I updated the doc of "resolved" "unsolved" here =)
rdzhabarov(2018-09-10 22:52:58):Don't you need "***-D***ONNX_DUMMY_BACKEND"? (but i'm not 100% certain)
zrphercule(2018-09-10 22:58:54):Actually whether -D or not it doesnt works...
spandantiwari(2018-09-11 20:12:12):I see you point. Since we have other generator ops such as `RandomNormal` and `RandomNormalLike`, which could also be replaced with `Constant`, but are still there for convenience and clarity, I went with a design similar to those on this. I think this design lends more clarity, but I am OK with something like `ConstantLike` as well. 

It will be great to get some more feedback on this. What do others reviewers think?
spandantiwari(2018-09-11 22:18:00):I have created another PR https://github.com/onnx/onnx/pull/1406 for a different design using `ConstantLike` op. Please review that design as well. If that one is preferable, we can abandon this PR. 

Also, as noted in PR https://github.com/onnx/onnx/pull/1406, there's a experimental op `ConstantFill` which is along similar lines. The new PR PR https://github.com/onnx/onnx/pull/1406 is a streamlined version with more datatype support. 
spandantiwari(2018-09-17 22:05:05):Closing this one out because the other design (https://github.com/onnx/onnx/pull/1406) is now merged. 
spandantiwari(2018-09-18 23:44:36):@houseroad - While `ConstantLike` (https://github.com/onnx/onnx/pull/1406) addressed the requirement for `ZerosLike` and `OnesLike` from this PR, it does not address the need for `EyeLike` (since it is not a constant value). Also, existing `Constant` op is not enough because it requires a tensor of known shape and does not allow for dynamic shape during runtime. I will open a different PR to address requirements for EyeLike. 
spandantiwari(2018-09-11 22:20:19):There's another design for these generator ops (using `OnesLike`, `ZerosLike` etc) in PR https://github.com/onnx/onnx/pull/1405. The idea is only one of these two PRs will be merged based on the review.
spandantiwari(2018-09-12 01:14:37):Not sure what the failure is. @houseroad could you please point to the issue? Is it related to test coverage?
houseroad(2018-09-12 20:08:33):Hi @spandantiwari, you probably need to run `python onnx/backend/test/stat_coverage.py` to update the test coverage page.
spandantiwari(2018-09-13 17:51:54):Hi @houseroad - Thanks for the pointer. I tried running `stat_coverage.py`, but I see a `urllib.error.HTTPError: HTTP Error 403: Forbidden` error. Do I need some permission to download the models? I see that it is trying to access `*opset_9` models, which are not listed on onnx/models repo. 
spandantiwari(2018-09-13 20:29:54):@houseroad - still not getting stat_coverage to run. 
Also, ping for review...
houseroad(2018-09-13 20:43:38):@spandantiwari sorry, the models haven't been updated yet. I will do it probably tomorrow. Could you use opset_8 models to generate the results?
spandantiwari(2018-09-14 00:05:30):@houseroad - thanks for the feedback. I have updated the PR based on your comments. I was also able to use  opset_8 models to generate test coverage results so all the checks are now passing. Could you please review. Thanks.
spandantiwari(2018-09-14 20:07:26):@houseroad - I have updated the branch to remove some conflicts with master (maybe from your latest Less, Greater check-in). All checks are passing. Could you please review and see if this can be approved. 
spandantiwari(2018-09-15 00:51:43):@houseroad Addressed the issue in your latest feedback. 
spandantiwari(2018-09-17 18:51:58):@houseroad - Addressed your comments.
spandantiwari(2018-09-17 18:57:34):@houseroad - Thanks.
houseroad(2018-09-13 20:44:12):we should add it as 9, because it is just added, right?
houseroad(2018-09-13 20:44:33):Same here: opset version 9?
spandantiwari(2018-09-13 22:22:38):Correct. Fixed.
spandantiwari(2018-09-13 22:22:50):Absolutely. Fixed.
houseroad(2018-09-14 22:00:45):Can you directly refer to the Enum, like `https://github.com/onnx/onnx/blob/master/onnx/backend/test/case/node/constant.py#L24`?
spandantiwari(2018-09-15 00:00:32):Good point. Done.
houseroad(2018-09-15 15:24:25):nit: ConstantLike_ver9_doc
spandantiwari(2018-09-17 18:50:47):Fixed.
CLAassistant(2018-09-12 20:56:05):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=1407) <br/>All committers have signed the CLA.
houseroad(2018-09-12 21:57:21):I think you need to remove the condition here: https://github.com/onnx/onnx/blob/6bedd27b0307c9295039bd847895a27275160a98/onnx/onnx_pb.h#L54 to define ONNX_API in all cases.
snnn(2018-09-13 00:01:20):Please wait. 

Please add some document of who will use this macro, at where, for which reason?

Who will build onnx as a dll on Windows?
If that is supported, will that dll statically link to protobuf or dynamically?
Which symbols will be exported?

Generally speaking, it's not good to use C++ types in DLL interface. It means you shouldn't build protobuf library as a DLL.  If you really want to do this, please add some documents, to let users know the possible issues. 



orionr(2018-09-13 00:15:33):Hi @snnn. Happy to talk more here before landing.

ONNX won't actually be built as a DLL in our case, but rather included in another DLL where we want the ONNX interface to be exported. Note that this is really just making the ONNX_API functional where it wasn't before. Per the code we always wanted to support this case, but let me know if that's wrong.

Also FYI these changes simply match what we've done on the PyTorch / Caffe2 side.
houseroad(2018-09-13 01:33:14):@onnxbot retest this please
orionr(2018-09-13 02:48:32):@snnn I just improved the docstring around ONNX_API. Let me know your thoughts and thanks.
yinghai(2018-09-26 01:32:12):Setting this to `PRIVATE` will cause compilation issue for other code which used ONNX and include `onnx.pb.h`. 
CLAassistant(2018-09-12 22:39:17):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=1408) <br/>All committers have signed the CLA.
yuslepukhin(2018-09-12 22:50:07):Version check naturally fails. Will need to learn the due process.
```
 assert reference_output == test_output failed:

  'The ir_versi...is checked!\n' == 'The ir_versio...is checked!\n'
  Skipping 778 identical trailing characters in diff, use -v to show
  - The ir_version in model: 3
  ?                          ^
  + The ir_version in model: 4
  ?                          ^
```
yuslepukhin(2018-09-13 17:50:51):Seems the build errors out on Java exception
yuslepukhin(2018-09-18 17:57:33):@houseroad The Opaque type is meant to serve as placeholder for types defined by ML clients. Our runtime provides C++ definitions for Tensors, Sequences and Maps associated with the corresponding C++ implementation. However, users may want/need to define their own types that are recursively composed of other  other onnx/ml types. To that end we provide an Opaque type with name/domain serving as discriminators and an array of TypeProto params that describe the composition.
gramalingam(2018-09-18 18:11:39):@houseroad , to expand on Dmitri's explanation: opaque types are meant to be an extensibility mechanism, to introduce new types into ONNX. In particular, it can be used with custom ops, as a way to introduce custom types. (It can also be used to introduce experimental types, which we can promote later on as standard types within ONNX if it is considered useful enough.)
yuslepukhin(2018-09-19 23:56:58):@houseroad Checked formatting and it takes the .clang-format in the repo. Could not find any abnormalities.
gramalingam(2018-09-18 16:56:22):Just some minor suggestions: why not "opaque" instead of "opq"? Also, not sure if we really need the designations "d()" and "n()" around the domain and name … we can treat them positionally … except I guess that they can be empty … 
yuanbyu(2018-09-18 17:00:03):Yes "opaque" is better.
yuslepukhin(2018-09-18 17:12:43):I do not mind. Would `sequence` instead of `seq` be a better choice then?
gramalingam(2018-09-18 17:25:56):I'm not sure if changing "seq" would introduce any compatibility issues, since it has been around for a while. "seq" is also a reasonably well-known abbreviation. I personally am fine with either "seq" or "sequence".
houseroad(2018-09-18 18:25:12):Here I am confused. Cannot we just use name to distinguish the special or opaque type? Why do we still need parameters? Could you give some examples here?
gramalingam(2018-09-18 18:51:22):Parametrized types are quite common (in fact, all the existing types, Sequences, Maps, and Tensors are parametrized types). So, allowing parameters adds flexibility. In the special case when the list is empty, we get unparametrized types. If we decide to introduce "sparse tensor" as an experimental type, or a "quantized tensor" as an experimental type (I realize that there is ongoing discussion to introduce a standardized type, but just giving examples here), they will be parametric types … if we want to introduce "Tuple" (or "Pair") as a type constructor, they will require type-parameters.
linkerzhang(2018-09-19 19:26:40):keep "sep".
linkerzhang(2018-09-19 19:27:54):the id should be 6 here (given it's being checked in onnx repo now).
yuslepukhin(2018-09-19 23:46:09):@linkerzhang denotation string is already 6 so we have to put 7
houseroad(2018-09-19 23:59:21):For example here, indent is not right. I guess clang-format won't check proto files.
yuslepukhin(2018-09-20 00:19:21):Will do.
jamesr66a(2018-09-13 16:40:35):@onnxbot retest this please
fumihwh(2018-09-13 23:04:10):@houseroad 
Maybe remove the existing description of default value is a good idea to avoid duplicating?
houseroad(2018-09-13 23:13:09):@fumihwh that will be great, could you do that?
CLAassistant(2018-09-13 18:13:18):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=1413) <br/>All committers have signed the CLA.
fdwr(2018-09-15 00:23:47):Since you're changing this line anyway, mind adding a space between "image.Otherwise"? TY.
BowenBao(2018-09-15 01:23:43):done
spandantiwari(2018-09-13 21:39:32):Sorry - this PR was created erroneously. Please ignore.
ArmenAg(2018-09-14 21:13:22):fixes #1417 
ArmenAg(2018-09-17 18:10:28):I'm not sure why the travis-ci build is failing. @houseroad What's your opinion on this PR?
houseroad(2018-09-18 01:00:20):The problem is the previous file is in dos format, and we have a check on that. I have updated your PR.
ArmenAg(2018-09-19 00:41:18):@houseroad added test + fixed linting\typing issues. How does it look? 

Can we restart the build? It failed without an error: https://api.travis-ci.org/v3/job/430312636/log.txt
ArmenAg(2018-09-19 05:31:31):@houseroad I added the assertions you recommended. 
houseroad(2018-09-18 00:47:22):Here, we should check whether the output of log_node have use or not. If it has, we should not destroy it. We have similar problem in other optimizers. 

Nit: use log_node instead of n.
houseroad(2018-09-18 00:59:30):Here, we should check whether the output of log_node have use or not. If it has, we should not destroy it. We have similar problem in other optimizers.

Nit: use log_node instead of n.
houseroad(2018-09-18 01:03:42):Here, we should replace all the uses of `Softmax` with `LogSoftmax`.
houseroad(2018-09-18 01:07:41):Call `it.destroyCurrent()` twice :-)
houseroad(2018-09-18 01:08:40):I prefer to call `it.destroyCurrent()` twice
houseroad(2018-09-19 04:07:36):Could you also check the len(optimized_model.graph.node) == 1?
houseroad(2018-09-19 04:09:19):Also check len(optimized_model.graph.node) == 2?
houseroad(2018-09-14 21:49:15):`1 to 2 sentences` sounds inappropriate... A clear definition is wanted. I think we should add the steps to generate/update Operators.md and TestCoverage.md.
fdwr(2018-09-14 23:13:03):@houseroad Yes, the description in Operators.md had better be clearly defined. I was referring to the opening comment in the PR, for which people could be a _little_ more brief than what they say in Operators.md, but not so brief as PR 1089 "This PR will add a new operator "expand" to onnx", as it becomes tedious to dig into the raw files across dozens of PR's when trying to get a holistic view of what new operators we need to support during new releases. Maybe people should just copy the description from their Operators.md into the first PR comment? Regarding regenerating Operators.md and TestCoverage.md, I'd support someone adding those details who is familiar with the process (as I don't know them).
fdwr(2018-09-14 23:18:05):e.g. "Operator description - a clear description of what the operator does and how it works, both in Operators.md and the opening PR comment, so that readers can understand a summary even before reviewing the files changed (you can just copy the summary from Operators.md into the PR comment)."
spandantiwari(2018-09-14 23:44:12):@fdwr I agree with your intent of having a smaller description in the opening comment of the PR and full detailed description in Operators.md doc. I think we just need to spell that out in the section above more clearly. 
linkerzhang(2018-09-19 15:22:25):thank you for making this more obvious to onnx customers!
CLAassistant(2018-09-20 18:24:29):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=1418) <br/>All committers have signed the CLA.
houseroad(2018-09-15 15:32:08):Here do we only cover the major version or we also want to cover the minor version? For example, in 1.1.2, opset version is 6.
prasanthpul(2018-09-15 16:19:55):If we actually bumped it, then yes we should include it.
houseroad(2018-09-19 15:24:11):@fdwr could you address my comments?
fdwr(2018-09-20 00:43:39):@houseroad Certainly - I let this PR stall. Updating...
smk2007(2018-09-20 22:38:30):Can we keep this IR Version unless we update it everywhere?
fdwr(2018-09-20 23:38:31):@smk2007 : What do you mean? This PR is just documenting the relationship to existing versions, not changing the file format version.
Maratyszcza(2018-09-19 14:36:44):@prasanthpul, @linkerzhang, @yuanbyu: is this PR good with you?
jspisak(2018-09-19 21:38:37):@prasanth and I connected. They are reviewing. 
jspisak(2018-09-20 00:55:17):@lupesko - Can you guys take a look and approve if you are good?
bddppq(2018-09-25 16:42:41):ping @gramalingam @ebarsoum @linkerzhang @pranavsharma @lupesko
linkerzhang(2018-09-25 16:56:48):LGTM, thank you for adding this! We may also check which operators need to add this type support.
jgong5(2020-02-06 01:32:39):@Maratyszcza Do you have plan to add bfloat16 type support to operators?
gramalingam(2018-09-18 19:58:54):It would be helpful to add a comment/description to FLOAT16 also to clarify the distinction between that and BFLOAT16. 
bddppq(2018-09-18 20:29:16):Actually now this whole `TENSOR_TYPE_TO_STORAGE_TENSOR_TYPE` mapping seems unnecessary to me and I don't remember why I added it :-). I will remove this map in a follow up PR.
Maratyszcza(2018-09-18 20:30:31):I thought an initial type is mapped to storage type, which is then mapped to name of the field
bddppq(2018-09-18 20:40:52):yeah but IIRC we never really used storage type directly (it's always followed by using it to look up in the STORAGE_TENSOR_TYPE_TO_FIELD map).
linkerzhang(2018-09-23 15:57:51):@houseroad any more comment please? thanks!
linkerzhang(2018-09-19 22:32:09):typo.
linkerzhang(2018-09-19 22:35:35):You can not use the same CheckerContext "ctx" for the nodes in this sub-graph. You need to construct a new CheckerContext object. For example, the sub-graph's CheckerContext should have is_main_graph return "false".
linkerzhang(2018-09-19 22:36:30):Same for LexicalScopeContext. You can't use the same LexicalScopeContext object as the main graph's nodes checking.
linkerzhang(2018-09-19 22:40:31):Thinking it twice, it may still be better to use check_graph here. This means the "g" constructed by FunctionExpandHelper should be a complete "graphproto".

btw, Shall we change the "FunctionExpandHelper" to a function as "GraphProto DecomposeFunctionNodeToGraph(node, func)"?
linkerzhang(2018-09-19 23:18:52):else {
   throw error.
}
linkerzhang(2018-09-19 23:22:41):this context can be moved out of the for loop, right?
raymondxyang(2018-09-19 23:26:34):I considered the possibility of changing function into a graph. But we are missing value_info here.
raymondxyang(2018-09-19 23:32:18):We actually need to have a new context for every node. It will pick out the existing shape/type info from the temp_valuetypesbyname in its constructor. 
raymondxyang(2018-09-19 23:34:02):I ll try constructing a new context object here.
linkerzhang(2018-09-20 00:03:47):ok, I think the "node" is new for each loop instance instead of the temp_valuetypesbyname.
raymondxyang(2018-09-20 00:06:58):I thought about it. We should not throw an error since there can be default attribute. ie for mvn, if no attribute is passed into the function node, thus no attribute should be passed to reduced_mean op, and it has its own default values for the attributes
houseroad(2018-09-21 05:04:10):Can we also run the doc comparison in the CI to make sure the doc is up to date? Or it's already checked?
raymondxyang(2018-09-21 06:00:00):We run the doc_gen and git diff in CI but I think the default domain is not checked (we set to build onnx_ml and thus only -ml files are generated). 
I plan to work on a long-pending PR (which is to merge function and operator.md) files after this is merged (since there will be no redundant mvn op both in op and function). I can enable the script then to always build the default docs no matter onnx_ml is set.
linkerzhang(2018-09-21 15:39:09):@houseroad , good point, however, this doc (Function.md) will be merged into Operator.md, so let's keep it as it is right now. sounds good?
houseroad(2018-09-21 17:13:47):Before expand to graph, we should check whether the node's inputs and outputs are the same as defined in the function?
houseroad(2018-09-21 17:14:13):@raymondxyang @linkerzhang that sounds good to me, thanks.
raymondxyang(2018-09-21 17:19:10):The function does not have any class defined with io type constraints. It is only a proto class. So here what I did is to expand the function into a sub-graph while preserving the tensor names in input and output list. Then the actual nodes takes the tensors (partially or wholly) will do the checking job
houseroad(2018-09-21 17:28:25):Here, you access the func inputs without checking whether it is out of boundary. I mean we should check the inputs and outputs of the node match the spec of the function.
houseroad(2018-09-21 17:47:49):Can we add another function called `InferShape(GraphProto g)`, so you just need to expand the node and do the shape inference?
houseroad(2018-09-21 17:48:42):Our API is only for model right now, should be easy to add one for graph: https://github.com/onnx/onnx/blob/master/onnx/shape_inference/implementation.h#L91
raymondxyang(2018-09-21 17:49:46):Sure will do so 
CLAassistant(2018-09-19 13:08:10):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=1426) <br/>All committers have signed the CLA.
houseroad(2018-09-19 16:06:59):I have updated the doc here. `Operators.md` cannot be changed directly. Changing it is a little bit complicated. So I created https://github.com/onnx/onnx/pull/1427 to fix it. 
linkerzhang(2018-09-19 16:15:50):@pengwa
spandantiwari(2018-09-20 18:21:03):All CI checks are passing. Friendly ping to get a review. 
spandantiwari(2018-09-21 21:54:54):Updated the PR based on the feedback. 
spandantiwari(2018-09-23 06:48:19):@houseroad , @anirudhacharya - Thanks for the review.
fdwr(2019-04-30 00:07:39):@spandantiwari : Would it be useful to keep the 2D restriction (I can't think of any use case extending identity matrices to 3D/4D given matmul's are 2D anyway) but treat any higher dimensions as a batch count? I mean, if this operator is useful for processing one image, then it should be useful for processing a *batch* of images in parallel like MatMul and Conv2D (without Concat'enating a bunch of EyeLike outputs). I'm asking because GPU's are better at processing a lot of similar data in parallel than repeating lots of smaller varied executions.
fuzzyBatman(2019-06-19 10:31:08):Hello. I am new to ONNX and I am trying to understand how to add operators.
I am not unable to find the part where the logic for the op is.
Can someone help me understand this?
houseroad(2018-09-21 04:34:09):Nit: add {} even if there is only one stmt.
houseroad(2018-09-21 04:38:25):Do you also want to add `k` parameter to indicate the index of the diagonal: 
0 (the default) refers to the main diagonal, a positive value refers to an upper diagonal, and a negative value to a lower diagonal.
anirudhacharya(2018-09-21 13:16:04):why not have number of rows and columns as parameters rather than infer it from an input tensor? numpy, pytorch and mxnet all have explicit rows and cols as parameters.
houseroad(2018-09-21 17:05:09):I think we can have another operator called `Eye` which takes rows and columns as inputs.
anirudhacharya(2018-09-21 17:40:12):won't that be like duplicating the functionality? are there examples where this Eyelike operator is used?
spandantiwari(2018-09-21 17:42:26):Yes, that's the idea. The `*Like` ops take the properties such as shape and type from the input tensor. This design is similar to Numpy (`ones_like`, `zero_like`) and also other ops in ONNX such as `ConstantLike`, `RandomNormalLike` and `RandonUniformLike`. 

We can have a separate PR for `Eye` if needed.
spandantiwari(2018-09-21 17:42:57):Sure. I will add that. 
spandantiwari(2018-09-21 17:43:06):OK.
spandantiwari(2018-09-21 21:53:46):@anirudhacharya - I don't think this is exactly duplicating functionality. There are several frameworks that subscribe to the design where there are two parallel set of generator ops - one that take tensor as input and one that take explicit shape as input, e.g. (`ones`, `ones_like`), (`zeros`, `zeros_like`). This kind of op is quite useful, e.g. creating epsilon covariance matrices matrices for regularization, and matrix factorization scenarios.

Also, note that I have not added `Eye` in this PR in order avoid adding more ops than necessary, because something like `Eye`, with static shape, can be supported using `Constant` op in current spec.
spandantiwari(2018-09-21 21:53:57):Added.
ArmenAg(2018-09-20 01:58:41):@houseroad How does this PR look? Is there a way I can request you as a reviewer. I can't seem to find a way to do that.
houseroad(2018-09-20 02:44:28):@ArmenAg I will take a look later. I guess you don't have the write access of this repository.
houseroad(2018-09-21 04:40:40):@ArmenAg btw, by default, please make sure fix point mode is off. I have some concern about that some of our optimization passes may be not robust enough. We may get infinite loop there.
ArmenAg(2018-09-21 06:25:08):@houseroad Sounds good. Could we make an optimization room in Gitter or a channel on slack (if we have one). I don't want to bog down _general_ with design ideas/questions.
houseroad(2018-09-20 15:00:17):Please use `Log`, `Exp`, `Sqrt`. `log`, `exp` and `sqrt` are not legal names.
houseroad(2018-09-20 15:02:23):Here we should use kLog, kExp, kSqrt. And remove klog, kexp, ksqrt from intern_strings
houseroad(2018-09-20 15:08:57):Another test you may want to add is two consecutive nop monotone + argmax.
CLAassistant(2018-09-20 17:03:33):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=1433) <br/>All committers have signed the CLA.
bddppq(2018-09-25 16:43:15):what is this for?
sjordan78(2018-09-25 17:48:21):Hi! I'm new github and coding, I was trying to write code, but I seem to have made a mistake.

Get Outlook for Android<https://aka.ms/ghei36>

________________________________
From: bddppq <notifications@github.com>
Sent: Tuesday, September 25, 2018 12:51:06 PM
To: onnx/onnx
Cc: sjordan78; Author
Subject: Re: [onnx/onnx] Update meta.yaml (#1433)


what is this for?

—
You are receiving this because you authored the thread.
Reply to this email directly, view it on GitHub<https://github.com/onnx/onnx/pull/1433#issuecomment-424416881>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AXQxoETQX4YJ4Lj3d3IHkoKdFF5iJNbXks5uel76gaJpZM4WyjDr>.

houseroad(2018-09-25 17:50:18):Seems the PR was created mistakenly. So close.
sjordan78(2018-09-25 17:59:37):Thank you

Get Outlook for Android<https://aka.ms/ghei36>

________________________________
From: Lu Fang <notifications@github.com>
Sent: Tuesday, September 25, 2018 1:50:36 PM
To: onnx/onnx
Cc: sjordan78; Author
Subject: Re: [onnx/onnx] Update meta.yaml (#1433)


Seems the PR was created mistakenly. So close.

—
You are receiving this because you authored the thread.
Reply to this email directly, view it on GitHub<https://github.com/onnx/onnx/pull/1433#issuecomment-424438884>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AXQxoMyEC9kzYB-yefLNQ4_rioCV7IxCks5uemzsgaJpZM4WyjDr>.

spandantiwari(2018-09-20 19:58:29):@houseroad - OK, sounds fine. 
linkerzhang(2018-09-21 15:37:11):@spandantiwari thanks a lot for making the PR!

LGTM, once @houseroad  finishes the migration, it could be merged.
snnn(2018-09-26 18:31:19):Hi,

I feel concerned on it. Converters like onnxmltools, has to support all the recent ONNX versions. One converter version is mapped to multiple onnx versions. If you modified the schema history, you break this. 
spandantiwari(2018-09-26 20:10:30):@snnn - I am not sure I understand the concern, specifically what you mean by 'schema history'. Are you referring to the change in Changelog.md or is it a larger concern about removing this op altogether?
snnn(2018-09-26 21:05:17):Hi @spandantiwari , 

First, if you get a file how could you know if it is a valid ONNX model file? You may run the onnx model checker. But, if the checker reports error, how could you know, it's the model is wrong, or just you are using the wrong ONNX version? It's likely this model is valid in ONNX 1.3 but not ONNX 1.4 because it contains an ConstantFill op. And for a model, there is no easy way to figure out which ONNX version works which doesn't.  

Second, if a backend need to support multiple ONNX versoins, it would be very hard.  If it statically linked to ONNX 1.4, which doesn't contain the ConstantFill OP, but the program got a model from some converter which is using ONNX 1.3, and the model contains a ConstantFill OP. The backend cannot find the schema for that OP.  The same thing also applies to converters. 

snnn(2018-09-26 21:22:46):We identify an OP by a three-tuple: (domain, op_type, and op_version).
If an OP exists in one ONNX version, it must exists in all the future ONNX versions, and without any breaking change. That's my suggestion. 



bddppq(2018-09-26 23:44:36):We do have the mechanism to remove an op with proper opset versioning (thanks to #1317).
But this op was in experimental, so we delete it without dealing with opset.
spandantiwari(2018-09-27 17:51:52):@snnn - I understand you concern now. As @bddppq mentions, this is the reason for having opset versioning. The idea is that when removing an op (and I agree with you that we should not take op removal lightly), we bump up the opset version and that it will be known that the op is no longer part of the updated opset version (which is encoded in the model).

In this particular case, `ConstantFill` is an experimental op, which was not part of the main spec, so we did not bump the opset.
snnn(2018-09-27 20:10:26):In this particular case, for ConstantFill op, I'm fine. I'm not using it and I don't care the other people. 
But please remember ImageScaler is also an experimental op. If somebody created a PR to remove it, with a reason of it's an experimental op,  I wouldn't agree. Because ImageScaler is used in the tiny-yolo model in ONNX model zoo. 
bddppq(2018-09-27 22:04:35):@prasanthpul @lupesko According to @snnn, the tiny-yolo model in model zoo has used an experimental op, could you guys help fixing it?
spandantiwari(2019-01-02 23:58:47):@houseroad - now that `ConstantOfShape` is in ONNX spec, I think we have all the use cases supported. Do you think we can get `ConstantFill` removed before 1.4 release?
linkerzhang(2019-01-04 19:45:52):@houseroad  can you push to fix the internal cases in your side and we can push this PR before 1.4 release? Thanks a lot! We may target to merge this mid next week.
spandantiwari(2019-01-07 20:06:07):Reviving this PR for ONNX 1.4 release. 
@houseroad - With `ConstantAsShape` in the spec now, we can move ahead with removing `ConstantFill` from PyTorch exporter. I can help with that. But I see that the ONNX version that is submoduled to PT is bit older. Is it OK to bring it up to latest ONNX so we can get `ConstantOfShape`?

If this PR breaks any ConstantFill related tests, maybe we can disable them (similar to what we did for `ConstantLike` https://github.com/onnx/onnx/pull/1716)?
houseroad(2019-01-07 20:09:40):Yeah, sure, we can do that, let's see
spandantiwari(2019-01-07 21:19:57):There were test failures for tests that are based on `ConstantFill`. I have updated the tests that were using `ConstantFill`, and removed the shape inference tests for `ConstantFill`.

The following PyTorch backend tests will still fail because of `ConstantFill`:
`OnnxBackendPyTorchOperatorModelTest.test_operator_lstm_cpu
OnnxBackendPyTorchOperatorModelTest.test_operator_rnn_cpu
OnnxBackendPyTorchOperatorModelTest.test_operator_rnn_single_layer_cpu`

Should we disable them for the interim?
spandantiwari(2019-01-10 01:53:58):@houseroad - the PyTorch backend tests listed above are the ones failing. Can we disable them?
houseroad(2019-01-10 22:52:27):@spandantiwari Rui and I are working on this. Hopefully we should be able to migrate the PyTorch exporter and Caffe2 frontend and backend to new ops.
houseroad(2019-01-10 23:03:57):@spandantiwari feel free to review https://github.com/pytorch/pytorch/pull/15938
spandantiwari(2019-01-14 20:04:00):@houseroad - I reviewed the PyTorch PR and left a comment.Overall it looks good. Waiting for it to get merged so that this PR can get unblocked.
houseroad(2019-01-14 20:07:11):@spandantiwari can we also have a whitelist in the checker to make sure the legacy models containing ConstantFill won't be rejected by the checker? We have a lot of models relying on this... I know it's ugly, but I don't want to break anything. We can print some warnings when detect such legacy experimental ops.
spandantiwari(2019-01-17 23:08:02):@houseroad - we can do that. But if it's OK with you, let's do that in a separate PR. It is related but doesn't block this PR, and may need some time. We are hoping to get this PR in for ONNX 1.4.
CLAassistant(2018-09-21 00:51:57):[![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/onnx/onnx?pullRequest=1435) <br/>Thank you for your submission! We really appreciate it. Like many open source projects, we ask that you sign our [Contributor License Agreement](https://cla-assistant.io/onnx/onnx?pullRequest=1435) before we can accept your contribution.<br/><hr/>**qinwen** seems not to be a GitHub user. You need a GitHub account to be able to sign the CLA. If you have already a GitHub account, please [add the email address used for this commit to your account](https://help.github.com/articles/why-are-my-commits-linked-to-the-wrong-user/#commits-are-not-linked-to-any-user).<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/onnx/onnx?pullRequest=1435) it.</sub>
houseroad(2018-09-21 04:47:17):Can we just use Reshape to do the axis merge?
suexu1025(2018-09-21 18:16:30):> Can we just use Reshape to do the axis merge?

Hi, reshape needs to a output shape to be provided while Mergedim will automatically calculated the shape. 
CLAassistant(2018-09-21 08:53:15):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=1437) <br/>All committers have signed the CLA.
shinh(2018-09-21 08:55:43):I failed to get an attention in https://github.com/onnx/onnx/pull/1158 so decided to send a PR to see if my understanding is right. Comments are welcomed, thanks!
shinh(2018-09-22 08:57:52):Thanks for your quick review! I've updated those documents. I think my original commit had updated model.onnx and test_data_set_0 was unchanged since this PR changes inputs/outputs. I've just double-checked there are no update by doing

```
$ cd onnx/backend/test
$ python3 cmd_tools.py generate-data -o /tmp/onnx
$ cp -r /tmp/onnx/node/test_convtranspose* data/node/
$ git diff  # no update
```

shinh(2018-09-22 10:42:55):I'm not sure why stat_coverage.py used vgg16 for me although it was renamed, but anyway, now the tests seem to be passing.
zrphercule(2018-09-24 17:28:40):@shinh Hi shinh, I wonder if this is ready to be merged?
shinh(2018-09-24 22:21:33):Yes, I believe this PR is OK. Thanks again for your review!
houseroad(2019-01-16 06:21:44):Close, since it's able to be covered by Reshape.
bddppq(2018-09-25 23:21:26):wait why do you need MY_ONNX_NAMESPACE?
bddppq(2018-09-25 23:22:38):Maybe simply change this to
```
if(NOT ONNX_NAMESPACE)
  set(ONNX_NAMESPACE "onnx")
endif()
```
?
yinghai(2018-09-25 23:25:14):Crap, we don't. This was old behavior. I didn't notice that things have changed. :)
yinghai(2018-09-25 23:25:30):Yep. 
yinghai(2018-09-26 02:39:58):Actually this is not the correct fix. Downstream project should include `onnx/onnx_pb.h` instead of `onnx/onnx.pb.h` where `ONNX_API` is already defined. 
orionr(2018-09-26 14:48:05):Thanks @yinghai. You are right - all downstream projects should include `onnx/onnx_pb.h` and not onnx.pb.h directly. Having the ONNX_API_DEFINE public would be a problem on Windows.
snnn(2018-09-26 18:23:53):Added onnx-ml.proto3
Maratyszcza(2018-09-26 20:53:49):would be safer to initialize to `0`.
zrphercule(2018-09-26 20:54:14):sure, thanks
houseroad(2018-09-28 05:30:04):@ArmenAg overall, the idea sounds good to me. Some comments:
1) Let's add more comments and examples to explain how we define PassType, PassEfficiency, and PassOptimizationType.
2) Even if we provide onnx fix point optimizer, user can still be able to run customized passes. This is for backward compatibility.
3) Your PredicateBasedPass proposal sounds pretty useful, especially avoid copy-paste code.
ArmenAg(2018-09-28 17:15:58):@houseroad Perfect. I'll start implementing this PR end to end. I'll also rewrite the current optimizations through PredicateBasedPass. PassManagers will also be introduced.
ArmenAg(2018-10-01 07:18:02):@houseroad I've implemented the rewrite. All the optimization passes are now on the new framework for optimization. Could you please review the code. Also I'm still getting a CI error when compiling with MSVC, but I'm unsure what these errors mean. Would you mind helping me out?
CLAassistant(2018-10-02 00:12:44):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=1452) <br/>All committers have signed the CLA.
houseroad(2018-10-02 00:15:02):@ArmenAg Sure, I will take a look tonight or tomorrow
ArmenAg(2018-10-02 01:14:21):@houseroad Sounds good. I also fixed the errors that appeared in MSVC. ~~This PR doesn't implement fixed point optimization. Once this is accepted I can put fixed point optimization into a separate PR.~~
ArmenAg(2018-10-04 18:53:50):@houseroad Fixed the issues mentioned above.
ArmenAg(2018-10-08 03:56:23):@houseroad I added fixed point optimization with deadend elimination. I'm again getting some MSVC error for protobuf. Could you take a look please?
houseroad(2018-10-08 22:06:57):@ArmenAg seems the master is broken.
houseroad(2018-10-09 06:22:01):The windows CI should be fixed in another PR, don't worry about that.
ArmenAg(2018-10-09 06:27:17):@houseroad I can convert all the passes to smart pointers. Should the whole optimization rewrite not use raw pointers or just the pass?

EDIT: For the meanwhile, anything interfacing with stl containers uses smart pointers now.
houseroad(2018-10-09 18:48:46):I prefer to use smarter pointers everywhere, unless you have a strong argument for some cases. If so, I am open to discuss. Otherwise, let's just stick to smart pointer :-)
ArmenAg(2018-10-09 18:52:07):I don't have a good argument :) I can move everything to smart pointers. The old passes all used Node* should I convert all these to std::shared_ptr<Node>?
houseroad(2018-10-09 18:58:08):@ArmenAg sure, if you want, we can use shared_ptr to replace Node* (I also would like to do so). But it may need to change the IR API.
ArmenAg(2018-10-09 19:02:00):I'd prefer not to touch IR code in this PR. Could I keep it Node* for this PR. And sometime in the future we could migrate our IR to shared_ptr?
houseroad(2018-10-09 19:03:38):@ArmenAg sure, just leave the IR API untouched. BTW, I think the pytorch-onnx-caffe2 tests are broken for some reason. Let me check what's going here

ArmenAg(2018-10-09 19:21:58):@houseroad sounds good. I moved as much of the rewrite as possible to use smart pointers. If you have time let me know how everything looks.
ArmenAg(2018-10-11 01:13:15):@houseroad Could you please review.
houseroad(2018-10-11 05:08:06):@ArmenAg I am investigating why with this change, the CI of ONNX becomes really unstable. Very likely, it will hang on the test_pytorch_onnx_caffe2.py... I am not able to reproduce the problem locally.
ArmenAg(2018-10-11 16:25:40):@houseroad I'll investigate the perf regression and memory leaks today. 
ArmenAg(2018-10-11 22:53:12):@houseroad I believe the bug is fixed. It was due to it.destroyCurrent() being called too many times. Running test_pytorch_onnx_caffe2.py:
```
master:
	real    1m27.064s
	user    3m9.906s
	sys     0m27.422s
rewrite:
	real    1m32.443s
	user    3m20.391s
	sys     0m29.016s
```
The extra time is from running the DeadEndElimination pass. Let's see how the CI turns out.
ArmenAg(2018-10-12 01:32:25):@houseroad Seems like the error is fixed. The CI is failing on test/onnx/test_operators.py but it seems like this is happening on master as well, based on your PR (https://github.com/onnx/onnx/pull/1501)
houseroad(2018-10-12 06:01:02):I think i have fixed the error, let's see
ArmenAg(2018-10-12 06:47:16):@houseroad looks like it's fixed!
houseroad(2018-10-12 16:32:08):@onnxbot retest this please
ArmenAg(2018-10-12 17:58:28):@houseroad Seems like there are more ONNX tests that need to be updated in PyTorch. The same error is happening in other PR's as well.
houseroad(2018-10-12 18:01:08):Yeah, I am working on that... annoying, the tests are not triggered in pytorch repo... I need to fix that
ArmenAg(2018-10-13 23:48:24):@houseroad The tests look to be figured out. How is this PR looking?
houseroad(2018-10-15 05:53:56):@onnxbot retest this please
ArmenAg(2018-10-15 22:34:21):@houseroad I removed the need for RTTI following your suggestions.
ArmenAg(2018-10-17 18:04:26):My bad. I added the nop test back. Also all tests have passed disregarding unstable runs. ~~Feel free to rerun the tests.~~ Rerunning them now.
ArmenAg(2018-10-17 18:33:12):@houseroad There we go. All test's pass. Can we merge?
houseroad(2018-10-17 18:50:24):@ArmenAg done, thanks!
houseroad(2018-10-04 06:01:34):We can have a static method to return the _optimizer, make sure _optimizer is already initialized.
houseroad(2018-10-04 06:02:34):Empty line?
houseroad(2018-10-04 06:04:22):removes
houseroad(2018-10-04 06:05:01):To me, unknown sounds better, shall we call it others?
houseroad(2018-10-04 06:11:25):Shall we call it registerPass?
ArmenAg(2018-10-04 18:27:59):I think Other is better. A user will not write a pass that is of unknown type but rather other.
houseroad(2018-10-09 06:15:55):seems, some pass may find more opportunity after other passes, for example, transpose + nop + transpose: first nop elimination, then do transpose fusion. 
houseroad(2018-10-09 06:17:32):can we avoid using raw pointer? use smart pointer? 
ArmenAg(2018-10-09 06:28:42):Determining in what order to do passes is very challenging and is a core problem in optimization. This fixed point manager implements the simplest type of fixed point manager. In the future we can think about adding more types of pass managers.
houseroad(2018-10-15 06:05:04):DestroyOne, DestroyTwo, and so on?
houseroad(2018-10-15 06:10:20):newline
houseroad(2018-10-15 06:13:38):Add some comments why we choose raw pointer here?
houseroad(2018-10-15 06:21:17):Add some comments about what should be put in the `runTransform`? (do everything but not deleting the node?)
houseroad(2018-10-15 06:26:36):override?
houseroad(2018-10-15 06:29:57):operand
houseroad(2018-10-15 06:32:19):Although this one can be considered as a predicatebase pass, however, it's very complicated, I would suggest to make it pass to make sure, the iterator will be valid... 
houseroad(2018-10-15 06:33:36):undefine after use?
houseroad(2018-10-15 06:37:48):Can we reuse some code between test_dataend_elimination_simple and fix?
houseroad(2018-10-15 06:40:05):Can we get ride of `dynamic_pointer_cast`?
houseroad(2018-10-15 06:40:35):Can we get ride of `dynamic_pointer_cast`? Some projects would like to disable RTTI. 
ArmenAg(2018-10-15 21:29:23):I think we have one to one parity with the old implementation when using NodeDestroyType. It would make the code pretty messy especially if we want to use CountBasedAnalysis. Are you sure we should rewrite it using FullGraphBasedPass?
houseroad(2018-10-15 21:37:32):I am fine to keep this as PredicateBasedPass, just want to make sure the logic is correct, since we did pretty complicated manipulations on the graph. I will double check the implementation on this again.
ArmenAg(2018-10-15 21:38:14):How should we circumvent the use of RTTI? We need to know the type of analysis done in order to know whether or not to do fixed point optimization and this cannot be inferred at a compile time right?
houseroad(2018-10-15 21:41:19):Can you add one field in the parent class? For different type of children, you assign with different values, and do the static cast at runtime. Does that sound good to you?
ArmenAg(2018-10-15 21:43:07):Good idea. I'm working on the revisions now.
CLAassistant(2018-09-27 01:25:29):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=1453) <br/>All committers have signed the CLA.
liqunfu(2018-09-27 17:24:42):Compress a tensor that behaves like numpy.compress: https://docs.scipy.org/doc/numpy/reference/generated/numpy.compress.html
This op is needed when processing dynamic data (sequential or spatial) where output are sampled from input according to data outcomes. It cannot be accomplished by using any combination of current ONNX ops.

fdwr(2018-10-02 00:08:45):@liqunfu : I appreciate that you included a summary in the description, justification, and a pointer to existing usage (per the contribution guidance https://github.com/onnx/onnx/blob/master/docs/CONTRIBUTING.md), as these are valuable for implementers.
liqunfu(2018-10-11 17:44:42):Hi @ezyang @bddppq @houseroad , it will be great if any of you can review this PR. Thanks.
liqunfu(2018-10-16 01:02:58):@houseroad Please take a look at this updated PR. Also I am getting ci/circleci failure.   ###1512. Can you please also take a look? Thanks
liqunfu(2018-10-16 18:03:23):@houseroad, thanks for fixing build for this PR. Is there anything I need to do to complete it? Thanks.  
liqunfu(2018-10-17 23:52:31):Hi @houseroad Still need your approval before I can merge. Thanks.
houseroad(2018-10-21 17:29:09):According to the log, seems your local onnx models are out of date. Please remove the cached model in ~/.onnx folder. and rerun https://github.com/onnx/onnx/blob/master/tools/update_doc.sh
liqunfu(2018-10-21 22:00:16):thanks @houseroad , I have updated my change with your comment. May you please approve and merge it if possible? 
houseroad(2018-10-22 00:15:23):Could you specify the behavior if len(condition) < length of the input along the axis? I saw one example in your test.
liqunfu(2018-10-22 16:48:07):@houseroad @bddppq @linkerzhang 
Hi, why is this? Do I need to do anything? I have signed cla and been able to contribute to ONNX before.
Thanks 
license/cla Expected — Waiting for status to be reported

prasanthpul(2018-10-22 16:54:51):@liqunfu - CLA refreshed; you are good to go
liqunfu(2018-10-22 18:02:12):@prasanthpul @houseroad Thank you so much guys!
ebarsoum(2018-10-09 17:58:23):I shouldn't go to the link to understand the behavior. Put the behavior in the description. And the link at the end of the description.
houseroad(2018-10-11 17:48:01):If we want to align with Numpy, we should not set default value as 0
liqunfu(2018-10-11 18:08:50):Gather takes indices. Compress takes condition of bools as input tensor. condition input may naturally come from ops like logical operations. It is not possible to use Gather to achieve what is needed here.
liqunfu(2018-10-11 18:11:27):regarding default axis, I agree we shall follow numpy's default behavior - if not set, assume flatten source.
houseroad(2018-10-11 18:14:39):Make sense. 
houseroad(2018-10-11 18:15:20):How about the default value of axis?
liqunfu(2018-10-15 20:09:05):I am thinking there is no default value for axis attribute. So if the axis attribute is not set, the op will perform as stated: to flatten the input and to select from the flattened tensor according to the condition evaluation. The output, in this case, is a rank 1 tensor. I will add an example for this case.  
houseroad(2018-10-21 17:25:01):compress_ver9_doc
liqunfu(2018-10-21 22:01:01):udpated to ver9
houseroad(2018-10-22 00:10:06):Can you specify whether we allow the len(condition) < length of input along the axis, if we allow, what is the expected behavior?
houseroad(2018-10-22 00:11:52):Can you specify that we allow the len(condition) < length of input along the axis,  and the expected behavior if it happens? I saw one example in your test case.
houseroad(2018-10-22 00:12:06):Can you specify that we allow the len(condition) < length of input along the axis,  and the expected behavior if it happens? I saw one example in your test case.
houseroad(2018-10-22 00:14:20):Could you specify the behavior if len(condition) < length of the input along the axis? I saw one example in your test.
houseroad(2018-10-22 00:14:57):Could you specify the behavior if len(condition) < length of the input along the axis? I saw one example in your test.
houseroad(2018-10-22 01:03:00):one more comment: explicitly specify the behavior if len(condition) < length of the input long the axis. I saw one example in your test cases. Let's make it explicit.
yinghai(2018-09-27 20:33:01):Did I review this and let it slip through? :) 
zrphercule(2018-09-27 20:41:51):(shhhhhh, lets keep it as a secret
yinghai(2018-09-27 23:38:08):Interesting, what's the difference? 
bddppq(2018-09-27 23:50:50):@yinghai `expect` will not immediately exit and subsequent code can cause segfault
houseroad(2018-09-28 05:37:51):I think better to use `ONNX_API_MACROS_H`
in case some day we have onnx/api.h
bddppq(2018-09-28 06:01:34):oops that's because I initially named this file as onnx/api.h
yinghai(2018-09-28 23:32:21):What's the purpose of these build variants? 
zrphercule(2018-09-29 07:57:02):@yinghai See the the followed api_macros.h, they are used there. I think they are used to deal with some ms dll.
raymondxyang(2018-10-03 17:13:00):@RyanUnderhill please help take a look thanks😄 
houseroad(2018-09-29 05:17:03):clean up?
raymondxyang(2018-09-29 19:46:36):This means to leave a example for registering functions. I ll update the marco to be the latest one.
RyanUnderhill(2018-10-03 20:33:22):If you want to remove the dummy class, you can write this:

`static bool functionBuilder_registerer=(RegisterOnnxFunctionBuilder(), false);`
jeffbloo(2018-10-03 22:45:07):The counter parameter can be removed here
spandantiwari(2018-09-28 22:21:53):@houseroad - oh right, there hasn't be a release since this went in. So we're good. Thanks for reminding. :-)
houseroad(2018-09-28 22:48:33):Our CI is broken, let's wait for it, will be back soon.
houseroad(2018-09-28 22:02:43):default type should be the same as the input tensor?
spandantiwari(2018-09-28 22:15:53):Yes, if `dtype` is not specified, then output type should be the same as input tensor. This is the case when even input tensor is not provided (i.e. output created using `shape` attribute).
houseroad(2018-09-28 22:17:18):Yeah, I mean you didn't change the description :-)
spandantiwari(2018-09-28 22:20:40):umm, sorry, I am not sure if the description needs to be changed. The lines you are pointing to (62-63) are for the case when both `dtype` and input tensor are not provided, in which case the output tensor type, by default, should be float. 

Maybe I am misunderstanding your comment?
houseroad(2018-09-28 22:26:36):oh ok, I thought T1 will always have type. Never mind.
spandantiwari(2018-09-28 22:27:43):Cool, thanks.
houseroad(2018-09-28 23:42:05):So if it is not compatible, do we need to record/report something?
zrphercule(2018-09-29 07:52:10):Right now we will fail this test directly. Guess if we should skip this test once we found it is not compatible?
rdzhabarov(2018-10-01 01:29:31):I think it's a valid response if the model cannot be supported by the ONNXIFI implementation.

I'd be in favor to make the test pass if the response is `ONNXIFI_STATUS_UNSUPPORTED_OPERATOR` or `ONNXIFI_STATUS_SUCCESS`.

* In case of `ONNXIFI_STATUS_SUCCESS` do other checks as we do currently.
* In case of `ONNXIFI_STATUS_UNSUPPORTED_OPERATOR` make test pass and report warning for the given test. 
* In case of any other results fail the test immediately. 

rdzhabarov(2018-10-01 01:36:18):how does this work?

```
onnxBackendID backendID = NULL;
...
size_t numBackends = 0;
    lib.onnxGetBackendIDs(&backendID, &numBackends);
```
In case backendID == nullptr, ONNXIFI implementation will just populate numBackends to be equal to number of supported backends.

Don't you need to call `onnxGetBackendIDs` again to actually initialize backendID?

zrphercule(2018-10-01 17:57:10):@houseroad @rdzhabarov Just realized gtest doesnt have skipping... I will let these test passed. 
https://github.com/google/googletest/pull/1544
yinghai(2018-10-01 18:30:49):What @rdzhabarov said. Normally, we need two passes. First to get the number of backends. And second to query. 
rdzhabarov(2018-10-02 00:03:45):do you want to dump graph name every time? (maybe just for soft failures as you do in IsUnsupported).
zrphercule(2018-10-02 01:52:29):Dont worry, I will submit another pr to deal with this.
Right now TEST_P only output a meaningless number of testcase, I just found a way to output the model name instead.
CLAassistant(2018-10-01 08:19:31):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=1463) <br/>All committers have signed the CLA.
houseroad(2018-10-15 06:46:52):This seems a duplicate one... can you close it?
CLAassistant(2018-10-16 18:44:02):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=1464) <br/>All committers have signed the CLA.
houseroad(2018-10-21 17:27:48):Not sure what's the purpose/contribution of this PR. So close.
houseroad(2018-10-15 06:43:13):Can we remove `:` colon at the end of each section title? I didn't see that appears on pages of other projects.
houseroad(2018-10-15 06:46:00):not easy to catch what is the change here? Could you please tell me what we have changed here?
zrphercule(2018-10-16 21:41:53):@houseroad I think in the old version, "frameworks" starts a new line which should not.
houseroad(2018-10-16 23:58:28):In our brower, I think it will output the same results. @zrphercule Or you mean linting the doc? 
houseroad(2018-10-02 01:40:14):indent :-)
houseroad(2018-10-03 06:10:23):Shall we use all_numeric_types instead?
linkerzhang(2018-10-03 15:01:35):no, all_numeric_types contains low precision ones there. To support low-precision, we'd have different definitions. for example, for matmul, the input may be 8 bits integer but the output should be 16-bit or 32-bit (common use) integer to handle overflow. Make sense please?
CLAassistant(2018-10-02 10:49:20):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=1466) <br/>All committers have signed the CLA.
okapies(2018-10-11 06:09:25):I'm sorry for my late response. I signed your CLA. Could you please check it? Thanks. @zrphercule 
linkerzhang(2018-10-02 16:47:43):@BenjaminCoquelle
houseroad(2018-10-04 05:49:56):+flake8
./onnx/backend/test/case/node/upsample.py:29:15: E225 missing whitespace around operator
achernigin1987(2018-10-05 08:55:35):I tried to build your PR locally on my Ubuntu 16.04, with Python 3.6.5 and virtualenv, but it doesn't compiled :(.
```
git clone https://github.com/onnx/onnx.git
cd onnx
git submodule update --init --recursive
git fetch origin pull/1467/head:pull-1467
git checkout pull-1467
python setup.py install
```
[ 97%] Building CXX object CMakeFiles/onnx_cpp2py_export.dir/onnx/cpp2py_export.cc.o
In file included from /home/achernigin/projects/onnx/third_party/pybind11/include/pybind11/pytypes.h:12,
                 from /home/achernigin/projects/onnx/third_party/pybind11/include/pybind11/cast.h:13,
                 from /home/achernigin/projects/onnx/third_party/pybind11/include/pybind11/attr.h:13,
                 from /home/achernigin/projects/onnx/third_party/pybind11/include/pybind11/pybind11.h:43,
                 from /home/achernigin/projects/onnx/onnx/cpp2py_export.cc:1:
/home/achernigin/projects/onnx/third_party/pybind11/include/pybind11/detail/common.h:111:10: fatal error: Python.h: There is no such file or directory
 #include <Python.h>
          ^~~~~~~~~~
compilation terminated.
CMakeFiles/onnx_cpp2py_export.dir/build.make:62: ошибка выполнения рецепта для цели «CMakeFiles/onnx_cpp2py_export.dir/onnx/cpp2py_export.cc.o»
make[2]: *** [CMakeFiles/onnx_cpp2py_export.dir/onnx/cpp2py_export.cc.o] Ошибка 1
CMakeFiles/Makefile2:322: ошибка выполнения рецепта для цели «CMakeFiles/onnx_cpp2py_export.dir/all»
make[1]: *** [CMakeFiles/onnx_cpp2py_export.dir/all] Ошибка 2
Makefile:129: ошибка выполнения рецепта для цели «all»
make: *** [all] Ошибка 2
Traceback (most recent call last):
  File "setup.py", line 328, in <module>
    'backend-test-tools = onnx.backend.test.cmd_tools:main',
  File "/home/achernigin/tfenv/lib/python3.6/site-packages/setuptools/__init__.py", line 129, in setup
    return distutils.core.setup(**attrs)
  File "/usr/lib/python3.6/distutils/core.py", line 148, in setup
    dist.run_commands()
  File "/usr/lib/python3.6/distutils/dist.py", line 955, in run_commands
    self.run_command(cmd)
  File "/usr/lib/python3.6/distutils/dist.py", line 974, in run_command
    cmd_obj.run()
  File "/home/achernigin/tfenv/lib/python3.6/site-packages/setuptools/command/install.py", line 67, in run
    self.do_egg_install()
  File "/home/achernigin/tfenv/lib/python3.6/site-packages/setuptools/command/install.py", line 109, in do_egg_install
    self.run_command('bdist_egg')
  File "/usr/lib/python3.6/distutils/cmd.py", line 313, in run_command
    self.distribution.run_command(command)
  File "/usr/lib/python3.6/distutils/dist.py", line 974, in run_command
    cmd_obj.run()
  File "/home/achernigin/tfenv/lib/python3.6/site-packages/setuptools/command/bdist_egg.py", line 172, in run
    cmd = self.call_command('install_lib', warn_dir=0)
  File "/home/achernigin/tfenv/lib/python3.6/site-packages/setuptools/command/bdist_egg.py", line 158, in call_command
    self.run_command(cmdname)
  File "/usr/lib/python3.6/distutils/cmd.py", line 313, in run_command
    self.distribution.run_command(command)
  File "/usr/lib/python3.6/distutils/dist.py", line 974, in run_command
    cmd_obj.run()
  File "/home/achernigin/tfenv/lib/python3.6/site-packages/setuptools/command/install_lib.py", line 11, in run
    self.build()
  File "/usr/lib/python3.6/distutils/command/install_lib.py", line 107, in build
    self.run_command('build_py')
  File "/usr/lib/python3.6/distutils/cmd.py", line 313, in run_command
    self.distribution.run_command(command)
  File "/usr/lib/python3.6/distutils/dist.py", line 974, in run_command
    cmd_obj.run()
  File "setup.py", line 203, in run
    self.run_command('cmake_build')
  File "/usr/lib/python3.6/distutils/cmd.py", line 313, in run_command
    self.distribution.run_command(command)
  File "/usr/lib/python3.6/distutils/dist.py", line 974, in run_command
    cmd_obj.run()
  File "setup.py", line 197, in run
    subprocess.check_call(build_args)
  File "/usr/lib/python3.6/subprocess.py", line 291, in check_call
    raise CalledProcessError(retcode, cmd)
subprocess.CalledProcessError: Command '['/usr/local/bin/cmake', '--build', '.', '--', '-j', '16']' returned non-zero exit status 2.


houseroad(2018-10-05 20:13:45):But can you build master @achernigin1987? If not, then it's not this PR's problem.
achernigin1987(2018-10-08 09:46:49):Yes, It seems that Linux build is common problem and I successfuly built this PR under Windows. But it seems that [tensorflow-onnx](https://github.com/onnx/tensorflow-onnx) convertor also has to be updated, because I tried to convert our TF model using

`python -m tf2onnx.convert --input D:\denoise\models\2018-07-15.pb --inputs IteratorGetNext --outputs Decoder/LastConvolution/Relu --output D:\denoise\models\2018-07-15.onnx --continue_on_error --opset 9`

and it failed:

```
2018-10-05 19:01:25.652469: I T:\src\github\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:1097] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 6370 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080, pci bus id: 0000:08:00.0, compute capability: 6.1)
['Traceback (most recent call last):\n', '  File "D:\\tools\\windows\\sources\\tensorflow-onnx\\tf2onnx\\tfonnx.py", line 1545, in tensorflow_onnx_mapping\n    onnx_node = func(g, node, node.name, args)\n', '  File "D:\\tools\\windows\\sources\\tensorflow-onnx\\tf2onnx\\tfonnx.py", line 994, in upsample_op7\n    target_shape = node.inputs[1].get_tensor_value()\n', '  File "D:\\tools\\windows\\sources\\tensorflow-onnx\\tf2onnx\\graph.py", line 156, in get_tensor_value\n    raise ValueError("get tensor value: {} must be Const".format(self.name))\n', 'ValueError: get tensor value: Decoder/Upsample1/size__64 must be Const\n']
['Traceback (most recent call last):\n', '  File "D:\\tools\\windows\\sources\\tensorflow-onnx\\tf2onnx\\tfonnx.py", line 1545, in tensorflow_onnx_mapping\n    onnx_node = func(g, node, node.name, args)\n', '  File "D:\\tools\\windows\\sources\\tensorflow-onnx\\tf2onnx\\tfonnx.py", line 994, in upsample_op7\n    target_shape = node.inputs[1].get_tensor_value()\n', '  File "D:\\tools\\windows\\sources\\tensorflow-onnx\\tf2onnx\\graph.py", line 156, in get_tensor_value\n    raise ValueError("get tensor value: {} must be Const".format(self.name))\n', 'ValueError: get tensor value: Decoder/Upsample2/size__51 must be Const\n']
['Traceback (most recent call last):\n', '  File "D:\\tools\\windows\\sources\\tensorflow-onnx\\tf2onnx\\tfonnx.py", line 1545, in tensorflow_onnx_mapping\n    onnx_node = func(g, node, node.name, args)\n', '  File "D:\\tools\\windows\\sources\\tensorflow-onnx\\tf2onnx\\tfonnx.py", line 994, in upsample_op7\n    target_shape = node.inputs[1].get_tensor_value()\n', '  File "D:\\tools\\windows\\sources\\tensorflow-onnx\\tf2onnx\\graph.py", line 156, in get_tensor_value\n    raise ValueError("get tensor value: {} must be Const".format(self.name))\n', 'ValueError: get tensor value: Decoder/Upsample3/size__38 must be Const\n']
['Traceback (most recent call last):\n', '  File "D:\\tools\\windows\\sources\\tensorflow-onnx\\tf2onnx\\tfonnx.py", line 1545, in tensorflow_onnx_mapping\n    onnx_node = func(g, node, node.name, args)\n', '  File "D:\\tools\\windows\\sources\\tensorflow-onnx\\tf2onnx\\tfonnx.py", line 994, in upsample_op7\n    target_shape = node.inputs[1].get_tensor_value()\n', '  File "D:\\tools\\windows\\sources\\tensorflow-onnx\\tf2onnx\\graph.py", line 156, in get_tensor_value\n    raise ValueError("get tensor value: {} must be Const".format(self.name))\n', 'ValueError: get tensor value: Decoder/Upsample4/size__25 must be Const\n']
['Traceback (most recent call last):\n', '  File "D:\\tools\\windows\\sources\\tensorflow-onnx\\tf2onnx\\tfonnx.py", line 1545, in tensorflow_onnx_mapping\n    onnx_node = func(g, node, node.name, args)\n', '  File "D:\\tools\\windows\\sources\\tensorflow-onnx\\tf2onnx\\tfonnx.py", line 994, in upsample_op7\n    target_shape = node.inputs[1].get_tensor_value()\n', '  File "D:\\tools\\windows\\sources\\tensorflow-onnx\\tf2onnx\\graph.py", line 156, in get_tensor_value\n    raise ValueError("get tensor value: {} must be Const".format(self.name))\n', 'ValueError: get tensor value: Decoder/Upsample5/size__12 must be Const\n']
before optimization: ops statistics: Counter({'Transpose': 44, 'Add': 22, 'Mul': 21, 'Max': 21, 'Cast': 20, 'Conv': 17, 'Shape': 10, 'Slice': 10, 'Squeeze': 10, 'Unsqueeze': 10, 'MaxPool': 5, 'Concat': 5, 'Upsample': 5, 'Relu': 1})
INFO:tf2onnx.optimizer.transpose_optimizer:finish after 39 iteration(s)
after optimization: ops statistics: Counter({'Transpose': 49, 'Mul': 21, 'Max': 21, 'Cast': 20, 'Conv': 17, 'Shape': 10, 'Slice': 10, 'Squeeze': 10, 'Unsqueeze': 10, 'Concat': 5, 'MaxPool': 5, 'Upsample': 5, 'Add': 5, 'Relu': 1})

```

linkerzhang(2018-10-08 17:49:40):@achernigin1987 , issue https://github.com/onnx/tensorflow-onnx/issues/151 created in tensorflow-onnx to track your feedback. Thanks!
ahirner(2018-10-14 21:23:24):FWIW it builds on Ubuntu 16.04 for me with one unrelated warning.
```
=============================================================== 731 passed, 477 skipped, 1 warnings in 383.39 seconds ===============================================================
```
linkerzhang(2018-10-18 20:25:45):@houseroad any more needed for merging this PR? Thank you!
houseroad(2018-10-18 20:34:20):@zrphercule is working on updating the pytorch onnx exporter. :-)
linkerzhang(2018-10-22 18:15:49):@zrphercule may I know the status now? Thanks!
zrphercule(2018-10-22 20:16:56):@linkerzhang I just finished my work on hand and started to work on this today. I will update the progress to you if I made any.
linkerzhang(2018-10-23 20:15:48):@zrphercule  Thanks! let's push it a little bit as there're customers depending on this change.
ebarsoum(2018-10-25 16:50:10):LGTM
linkerzhang(2018-10-25 16:54:14):@zrphercule @houseroad Looks like you need more time to do the change in pytorch side. I'd merge this today as customers are waiting for this for a while. 
linkerzhang(2018-10-26 00:49:50):Thank you @houseroad and @zrphercule  for the turning around! I'm merging this PR.
houseroad(2018-10-26 03:12:59):@linkerzhang merging this PR before we land https://github.com/pytorch/pytorch/pull/13135 broke the CI of PyTorch. Next time, please wait more until our fix gets landed (we will stamp this PR to show it's ready to be merged), otherwise, it will affect PyTorch development. Thanks!
achernigin1987(2018-10-03 08:54:00):Inference or Infer
linkerzhang(2018-10-03 17:43:38):fixed.
spandantiwari(2018-10-25 17:44:39):I am wondering if we should include `tensor(bool)` as input here, because it may not be very intuitive how it works for `mode`  option `linear`. Or if we want to support it, then a clarification saying that `linear` is not supported for `tensor(bool)` may be worth adding. 
bddppq(2018-10-03 20:12:06):@rdzhabarov It uses cmake's external project feature which will git clone during build time (I personally prefer submodule though)
fdwr(2018-10-03 23:29:00):#1443 @linkerzhang @prasanthpul 
zrphercule(2018-10-04 21:18:38):After merging of this pr, we would observe more failed, crashed and deadlocked testcases...
houseroad(2018-10-04 21:20:18):Let's kill all the bugs
zrphercule(2018-10-04 22:04:42):mind give a stamp? @houseroad 
houseroad(2018-10-04 00:59:56):So, we have to assume that graph is named graph, hmm...
rdzhabarov(2018-10-04 01:48:03):i'm not sure what's coding standards for ONNX. I'd favor just a simple method instead of #define (easier write/read/debug/... )
rdzhabarov(2018-10-04 01:48:11):why do we specifically care about releasing graph but not onnxReleaseBackend or onnxReleaseBackendID?
rdzhabarov(2018-10-04 01:50:43):I might be missing something, but how does the whole workflow work without actually signaling on inputFence?
zrphercule(2018-10-04 17:25:25):Since this is a macro only used in this unit test, I guess it is okay...
zrphercule(2018-10-04 17:26:21):I saw a lot of using like this in pytorch/caffe2 gtest. I think this actually makes the code more simple...
zrphercule(2018-10-04 17:28:42):In fact we do care about releasing backend, backendID and free memory. However, they are done in TEST_P() instead of in this test class, therefore even the ASSERT failed in class, the release of backend would still be excuted. I have tested this mechanism myself.
zrphercule(2018-10-04 17:36:54):I think you are right about this! I wonder why some of the results are correct...
It might because I didnot initialie the event at all, and therefore both output.event and input.event are set to signaled by default.
Let me fix this, thanks!
rdzhabarov(2018-10-04 17:41:11):People would always argue about things like that :D

http://isocpp.github.io/CppCoreGuidelines/CppCoreGuidelines#es31-dont-use-macros-for-constants-or-functions
zrphercule(2018-10-04 18:32:45):(And also even we dont release&free, OS would always do these things for us. quote: a google testing engineer.... Considering to uninstall all google product in my computer...
houseroad(2018-10-04 19:28:17):Can we pass X and graph? if you the graph is not initialized, we just pass nullptr, does that sound good to you?
zrphercule(2018-10-04 20:17:18):@houseroad I guess it is better.
rdzhabarov(2018-10-04 20:18:58):"And also even we dont release&free, OS would always do these things for us" This is true, but ASAN would often report leaks in this cases and fail.
houseroad(2018-10-05 23:18:14):could you add some comments about the usage of these variables?
yinghai(2018-10-05 23:39:03):Separate member variables from member functions? 
zrphercule(2018-10-05 23:40:58):@yinghai  Yes I think this pool is better to be designed as a member variable of class instead of function, is it?
zrphercule(2018-10-05 23:43:40):@yinghai oops I know what you mean now...... okay I will separate them hhh
raymondxyang(2018-10-09 23:32:14):LGTM.. @snnn do we depend on $MP_FLAG?
snnn(2018-10-10 00:20:01):@raymondxyang no
zrphercule(2018-10-09 18:59:21):@houseroad yes we can, I can have another string store in ResolvedTestCase as its path. But so far the content is same thing, that is the path is the model graph name......
zrphercule(2018-10-09 19:03:15):But I guess you are right, if the model is deprecated, or it has some special name for graph, then it doesnt make sense. let me add it.
houseroad(2018-10-09 19:04:58):Maybe a helper function to extract the test name?
zrphercule(2018-10-09 19:10:23):@houseroad The test nam can be the model.graph.name, also can be the directory name where we load the model. I am going to use the second one as test name now (Although these two names are same in our test data), and I just need to let ResolvedTestCase store where its data loaded from.
houseroad(2018-10-09 19:13:55):@zrphercule we have two reasons to use folder name:
1) easier to parse the string comparing to protobuf
2) name can be empty, but folder won't be empty, so it's safer

houseroad(2018-10-09 19:16:03):BTW, let's add the reasons in the comments :-)
rdzhabarov(2018-10-09 21:25:00):I think this line is confusing...

can you split declarations for entry_dname and full_entry_dname
zrphercule(2018-10-09 21:30:17):sure...

xw285cornell(2018-10-09 21:15:34):Thanks @bddppq! 
houseroad(2018-10-10 18:36:13):newline?
houseroad(2018-10-10 19:39:10):later, argc is used again by init google test, do we want expose the change on argc to initgoogletest?
zrphercule(2018-10-10 20:46:07):@houseroad Yes, because I dont want google test to receive this dir anymore.
rdzhabarov(2018-10-10 22:44:29):Sorry for late comment.

But it's not clear why this `argc--` magic happens here, can you add a comment? (and that we always expect target directory to be the last param).
zrphercule(2018-10-10 22:56:43):oops, sure we need to add comments here, let me do it.
raymondxyang(2018-10-10 23:08:18):Reopen when ready
spandantiwari(2018-10-11 22:13:56):An unrelated test (`test_addmm`) seems to be failing in CI. Not sure how to fix that. Can someone please advise. 

Attached below is a snippet from the error log. 

> =================================== FAILURES ===================================
21:22:03 ___________________________ TestOperators.test_addmm ___________________________
21:22:03 
21:22:03 self = <test_operators.TestOperators testMethod=test_addmm>
21:22:03 
21:22:03     def test_addmm(self):
21:22:03         m1 = Variable(torch.randn(2, 3), requires_grad=True)
21:22:03         m2 = Variable(torch.randn(3, 4), requires_grad=True)
21:22:03         m3 = Variable(torch.randn(4), requires_grad=True)
21:22:03 >       self.assertONNX(lambda x, y, z: torch.addmm(torch.addmm(z, x, y), x, y), (m1, m2, m3))
21:22:03 
21:22:03 test/onnx/test_operators.py:195: 
21:22:03 _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
21:22:03 test/onnx/test_operators.py:67: in assertONNX
21:22:03     self.assertExpected(onnx_model_pbtxt, subname)
21:22:03 test/common.py:552: in assertExpected
21:22:03     self.assertMultiLineEqual(expected, s)
21:22:03 E   AssertionError: 'ir_v[80 chars]n    output: "3"\n    op_type: "Constant"\n   [2060 chars]n}\n' != 'ir_v[80 chars]n    input: "0"\n    input: "1"\n    input: "2[1438 chars]n}\n'
21:22:03 E     ir_version: 3
21:22:03 E     producer_name: "pytorch"
21:22:03 E     producer_version: "0.4"
21:22:03 E     graph {
21:22:03 E       node {
21:22:03 E   +     input: "0"
21:22:03 E   +     input: "1"
21:22:03 E   +     input: "2"
21:22:03 E         output: "3"
21:22:03 E   -     op_type: "Constant"
21:22:03 E   +     op_type: "Gemm"
21:22:03 E         attribute {
21:22:03 E   -       name: "value"
21:22:03 E   ?              -  ^^
21:22:03 E   +       name: "alpha"
21:22:03 E   ?                ^^^
21:22:03 E   +       f: 1
houseroad(2018-10-12 00:48:49):@spandantiwari I am aware of the problem, will fix it soon
spandantiwari(2018-10-12 16:53:47):@houseroad - thanks for fixing this issue. Much appreciated. 

If you could please review this op, that'll be great. The spec for the `MaxUnpool` op is in line with PyTorch's `maxunpool*` ops. 
spandantiwari(2018-10-15 17:21:33):Gentle ping to request your review.
spandantiwari(2018-10-15 23:25:12):@houseroad - thanks for the feedback.

Regarding `storage_order`, I am hoping to start with row_wise only, and add `storage_order` only if needed/requested.

Regarding padding - I have included an attribute `pads` in the spec which should take care of padding. Between `pads` and `output_size` we should be able to handle all situations.
houseroad(2018-10-15 23:39:28):Could you make it clear in the spec: the operator only supports rowwise storage order for the indices?  Thanks
spandantiwari(2018-10-16 20:06:56):@houseroad - that's a good point. I have added a sentence to the spec clarifying that. 
spandantiwari(2018-10-16 21:41:14):@houseroad - Thanks!
houseroad(2018-10-15 23:38:15):Oh, yeah, we have the pads here... sorry I was blind.
spandantiwari(2018-10-16 20:05:12):No worries. :-) 
gramalingam(2018-10-11 01:56:26):"should not contain more than one value field"
gramalingam(2018-10-11 01:56:51):or "contains more than one value field."
linkerzhang(2018-10-11 04:33:48):Fixed.
kit1980(2018-10-11 23:27:55):Unrelated test_addmm failing, same as in another recent PR https://github.com/onnx/onnx/pull/1494
kit1980(2018-10-31 20:54:52):Can someone look at this?
The ops are very similar to the existing ops, and are needed to convert CNTK and TF models.
ankkhedia(2018-10-31 23:08:24):@Roshrini @vandanavk
ebarsoum(2018-11-05 17:07:17):@houseroad  and @bddppq can someone take a look?
houseroad(2018-11-05 19:27:52):Please rebase to master and run `https://github.com/onnx/onnx/blob/master/tools/update_doc.sh` to update the doc.
kit1980(2018-11-07 02:20:02):Rebased and updated the docs.
gramalingam(2018-10-12 16:39:20):With the introduction of control-flow ops, we need to extend the InferenceContext to enable recursive shape-inference for sub-graphs of control-flow ops. @houseroad @bddppq @linkerzhang can you please take a look at the extension to the InferenceContext API in this PR for this purpose? 
gramalingam(2018-10-17 18:24:16):Hi @anderspapitto @linkerzhang : any feedback on the extension to InferenceContext api?
skottmckay(2018-10-28 22:11:27):Updated with changes from PR to separate InferenceContext changes out. https://github.com/onnx/onnx/pull/1548
linkerzhang(2018-10-31 15:40:10):Thank you very much!
gramalingam(2018-10-12 16:48:55):Can we drop this check? It is not really needed here. (It is better if we adopt one uniform scheme to indicate an unnamed, unknown, dimension … I believe that a dimension that has no dim_value and no dim_param is used for this purpose currently. Is there any usage of dim_value == -1 for this purpose also?)
skottmckay(2018-10-12 20:09:23):This is based on the initialization value for the local variable rather than expecting to see -1 as dim_value. 
  int64_t batch_size = -1;
  int64_t sequence_len = -1;

Primary purpose is to know if a real batch size or sequence length value was seen. 
Would current_value < 1 be fine? 
Should I use something other than -1 as the 'not set' value?
gramalingam(2018-10-13 03:15:07):Isn't that the outer check for "current_value" (that you are referring to)? This "value" is dim.dim_value() ...
skottmckay(2018-10-13 03:45:44):Ah yes - sorry - the inner check can go. Will remove.
gramalingam(2018-10-14 17:20:56):I guess we need to check if this has_tensor_type()?
skottmckay(2018-10-14 21:18:20):Would the Scan input validation have already done that? I assumed that would run first, but maybe that's not the case. 
gramalingam(2018-10-15 01:42:23):Do you mean validation based on what is in the input-type-constraints in the opschema? I guess so far the inference functions have not assumed that. (There is some redundancy between the two due to historical reasons … )
skottmckay(2018-10-15 01:48:51):Yes. I can add the extra check if that is more consistent with the other inference functions.
skottmckay(2018-10-15 07:33:17):Should the check be that inferencing fails (call fail_type_inference) if any of the Scan/subgraph inputs/outputs are not tensors, or should it just ignore that input/output? 
zrphercule(2018-10-13 00:20:27):@raymondxyang Could you please review it ASAP? Thanks!
bddppq(2018-10-13 04:19:30):https://stackoverflow.com/a/43714505
pk-g(2018-10-23 21:59:05):@houseroad, thanks for the initial review. Can you please review the updated version so we could move forward with adding this op? thanks.
pk-g(2018-11-14 19:35:38):@houseroad, thanks again for the initial review and feedbacks. I have incorporated the feedback and updated the PR. can you please review the updated PR so we can move forward with adding this op ? thanks!
pk-g(2018-12-05 01:00:46):@houseroad thanks for the review, the CI is now passing, can you please take another look so we can move forward with adding this op ? 
pk-g(2018-12-06 23:00:07):@houseroad sure, updated the PR and applied the feedbacks. can you please take a look so we can move forward ? thanks.
pk-g(2018-12-07 19:35:25):@houseroad just a friendly reminder here, can you please take a look at updated PR so we can move forward with adding this op? thanks.
spandantiwari(2018-10-18 17:31:48):This op should be added to opset_9.
spandantiwari(2018-10-18 17:33:25):Similar to above - this should be added to OpSet_Onnx_ver9.
spandantiwari(2018-10-18 17:33:48):Scatter_ver9_doc 
spandantiwari(2018-10-18 17:53:23):Should be 9.
spandantiwari(2018-10-18 17:56:29):Is this `if` block needed?
spandantiwari(2018-10-18 17:59:53):nit: wording. If any tensor type is allowed then, may be we can do away with "constrain". :-) 
spandantiwari(2018-10-18 18:03:12):I would suggest using descriptive names for these test points. For example, we can eliminate the explicit use of the `axis=0` attribute here, and call this `export_without_axis`. In the next test point, we can have the axis attribute (`axis=1`) and that can be called `export_with_axis`. 
pk-g(2018-10-18 20:42:06):fixed
pk-g(2018-10-18 20:42:16):fixed
pk-g(2018-10-18 20:42:24):fixed
pk-g(2018-10-18 20:42:33):fixed
pk-g(2018-10-18 20:43:05):fixed.
pk-g(2018-10-18 20:43:21):fixed
pk-g(2018-10-18 23:07:16):fixed
houseroad(2018-10-21 17:35:19):please specify the dtype as int64, otherwise, we may generate different type of data on 32bit platform.
houseroad(2018-10-21 17:35:35):ditto
houseroad(2018-10-21 17:40:48):just int64? I think other ops only support int64 as index, not much sense to support int32 here.
houseroad(2018-10-21 17:44:31):Same shape as indices?
pk-g(2018-10-22 18:35:09):since we are already supporting both int32 and int64 in gather(based on https://github.com/onnx/onnx/blob/master/docs/Operators.md#Gather), and given the fact that scatter can conceptually be viewed as opposite of gather, it may make sense for scatter to support what is already being supported by gather ?
pk-g(2018-10-22 18:57:57):sure, resolved. good point!
pk-g(2018-10-22 18:58:02):resolved.
pk-g(2018-10-22 18:58:06):Sure, resolved.
fdwr(2018-10-26 01:35:25):Something that was unclear to me is that `i` and `j` are not loop counters in terms of `data` or `output` but rather `indices` and `updates`. This greatly bewildered me when looking at example 2 because it appeared that "data[i][indices[i][j]] = updates[i][j] if axis = 1" would yield out of bounds array accesses. Propose clarifying that...

```suggestion
  data[indices[i][j]][j] = updates[i][j] if axis = 0, or data[i][indices[i][j]] = updates[i][j] if axis = 1, where i and j are loop counters from 0 up to the respective size in `updates` - 1.
```
pk-g(2018-11-14 19:32:54):sure, thanks for the proposal.
houseroad(2018-12-05 18:26:03):Shouldn't the output have the same shape as the input?

Also please add a test here: https://github.com/onnx/onnx/blob/master/onnx/test/shape_inference_test.py
pk-g(2018-12-06 22:04:19):That's right, fixed and added the shape inference tests.
houseroad(2018-12-07 19:42:37):`hasNInputShapes(ctx, 1)`? Since you just need the shape information of the first input.
pk-g(2018-12-07 19:45:04):the other two inputs, indices and updates are not optional for this op. So we should always have three inputs when calling this op. 
houseroad(2018-12-07 19:52:59):Yeah, they are required inputs. However, their shape information is not required for our shape inference. `hasNInputShapes` checks whether we have necessary information for us to do the shape inference.
pk-g(2018-12-07 19:54:36):makes sense, I'll update this to 1 then.
pk-g(2018-12-07 20:10:14):just updated the PR with this fix and resolved conflicts.
ArmenAg(2018-10-18 00:02:44):@houseroad This is the original optimization passed that triggered the rewrite. We also test the fixed point capability of the new framework. Let me know how it looks.
ArmenAg(2018-10-20 00:24:12):@houseroad could you please review?
houseroad(2018-10-20 01:13:22):Yeah, I will take a look this weekend
ArmenAg(2018-10-25 23:57:39):@houseroad I'm not sure why we should handle the default axis any differently. What I did though was to insure that we have the kaxis using `node->hasAttribute(kaxis)`. Let me know if you had something different in mind.
ArmenAg(2018-10-26 00:13:39):@houseroad It looks like the CI has changed and now I'm getting an error: 
```
/home/travis/build/onnx/onnx/onnx/optimizer/passes/eliminate_nop_monotone_argmax.h:53:8: error: unused parameter ‘graph’ [-Werror=unused-parameter]
   bool runTransform(Node* node, Graph& graph, NodeDestroyType& destroy_current)
```

Unfortunatly since we've taken an OOP approach to building the optimization framework we will have unused variables like the ones above. What do you think we should do about this?
bddppq(2018-10-26 00:17:13):@ArmenAg drop/comment out the parameter name like here https://github.com/onnx/onnx/pull/1547
ArmenAg(2018-10-26 01:55:12):@houseroad @bddppq One of the tests is failing because of an operator check in pytorch unrelated to this PR. Does something need to be fixed on their side?
zrphercule(2018-10-26 02:39:28):@ArmenAg Yes I identified it is because of the recent merge of new upsample_op in onnx. Dont worry, I am going to fix it asap.
houseroad(2018-10-26 04:26:40):@ArmenAg yes, this is what I am looking for. We should check whether the node has axis attribute, then access it.
houseroad(2018-10-21 16:33:36):add newline here please
houseroad(2018-10-21 16:34:03):please restart the kernel and run everything from the beginning
houseroad(2018-10-21 16:36:07):add newline character here, please
houseroad(2018-10-21 17:08:32):if axis is not specified, default value is used. Please check and handle such condition in the code.
houseroad(2018-10-21 17:08:52):Ditto
bddppq(2018-10-18 05:41:26):Technically we need to make sure this field number won't be accidentally reused in the future (otherwise it will break backward compatibility **silently**), so what we used to do is comment out this line (instead of removing) and add a comment saying do not use it.
gramalingam(2018-10-18 18:07:29):The string representation currently looks like "opaque(D,N)" where D/N can be empty. Can we change this to omit the comma if D is empty? So, it would look like "opaque()" if both D and N are empty, "opaque(N)" if D is empty and N is not, and "opaque(D,N)" if D is not empty. (It will still look a bit awkward as "opaque(D,)" if D is non-empty and N is empty … but this is unlikely to be used.) Since opschema specs will need to use this string representation, it will be helpful if it is more readable.
yuslepukhin(2018-10-18 21:26:48):OK. This also means that empty domain and/or name mean there are not present.
gramalingam(2018-10-18 21:32:19):Treating a missing domain/name as if it were the empty-string seems reasonable to me.
houseroad(2018-10-18 05:23:52):@ArmenAg pass order is quite critical for some passes, for example, for us, split should be the last step, can be switched with others.
ArmenAg(2018-10-18 05:43:29):My bad. Yes it should be vector. But if we really want to insure the constraint that split is called last, we should implement it the pass manager.
houseroad(2018-10-18 05:45:02):@ArmenAg yeah, we can add some mechanism to enforce some invariance. 
ArmenAg(2018-10-18 05:52:32):@houseroad Agreed. This can also break the fixed point optimization, since it might call split multiple times. We could make a new type of pass. Maybe call it something like SingularPass that is guaranteed to only run once throughout the lifetime of the optimization. It would also include a field that constrains where in the optimization pass has to run (beginning, end, any).
houseroad(2018-10-18 06:04:00):@ArmenAg You can add this support in another diff. :-)
bddppq(2018-10-18 05:36:54):Maybe add a comment saying the order we add the passes matters, so later people won't accidentally change it back to set. 
houseroad(2018-10-18 05:38:08):Sure, will add that
CLAassistant(2018-10-18 13:37:55):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=1525) <br/>All committers have signed the CLA.
ryotatomioka(2018-10-24 09:58:36):Hi @snnn could you have a final look at the PR?
snnn(2018-10-24 17:02:36):@gramalingam Could you please take a look? Thanks.
ryotatomioka(2018-10-26 08:46:04):Thank you @gramalingam. This PR is ready to be merged.
ryotatomioka(2018-10-29 08:00:29):Could @gramalingam or @houseroad check and merge this PR? It is becoming painful to keeping it up-to-date with the master...
gramalingam(2018-10-29 20:32:46):Close and open to retrigger CI
gramalingam(2018-10-29 20:42:02):The appveyor CI failure looks strange … and I am unable to trigger the CI to happen again.
ryotatomioka(2018-10-30 10:37:55):@gramalingam I've merged master and the CI seems happy.
snnn(2018-10-21 04:59:40):Don't use auto unless it increases readability
snnn(2018-10-21 05:04:25):Is it possible it doesn't have a shape? 
snnn(2018-10-21 05:04:53):Is it possible it doesn't have a dim_value? 
snnn(2018-10-21 05:05:25):Is this cast safe? why?
snnn(2018-10-21 05:08:15):check if r >= 1?
snnn(2018-10-21 05:10:20):check if axis is in [-r, r-1]?
snnn(2018-10-21 05:12:48):If you like, you can use axis+=r;
ryotatomioka(2018-10-23 18:03:21):Fixed.
ryotatomioka(2018-10-23 18:03:55):Done.
ryotatomioka(2018-10-23 18:04:23):I believe this is already checked by `hasNInputShapes(ctx, 2)`
ryotatomioka(2018-10-23 18:04:41):Fixed.
ryotatomioka(2018-10-23 18:04:54):Done.
ryotatomioka(2018-10-23 18:05:25):I don't think so. See my comments in d983cf60598a5a70dbff853e4980637bf1f3af20
ryotatomioka(2018-10-23 18:34:19):The cast is safe because it is from `int64_t` to `int`
snnn(2018-10-23 19:37:03):Is dim_value always available?
ryotatomioka(2018-10-23 19:41:16):Since `i < axis < r` I think it exists, or did I misunderstand?
snnn(2018-10-23 19:50:58):If you open up the protobuf defs. this field is optional. 
ryotatomioka(2018-10-24 00:27:38):I see thanks. Fixed it. I have also added support for denotation.
gramalingam(2018-10-24 22:16:30):Wouldn't "*newdim = dim" have the same effect as these lines of code?
ryotatomioka(2018-10-25 07:23:22):Thank you that's right. I fixed it
snnn(2018-10-21 04:57:05):BTW， why graph is not used ? why this function is not const?
If graph is not used, it should generate a warning?
linkerzhang(2018-10-23 20:14:29):I think this is not you want to change.
pengwa(2018-10-23 23:41:04):yeah that's not my change. seems the result when running gen_doc is dependent on machine accuracy maybe. thanks @linkerzhang !
CLAassistant(2018-10-23 17:29:53):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=1542) <br/>All committers have signed the CLA.
linkerzhang(2018-10-23 20:13:29):This actually tells us MatMul and Gemm are redundant to each other. we may only need one in our op spec? @houseroad @ebarsoum @bddppq @gramalingam  
bddppq(2018-10-25 05:29:45):@linkerzhang MatMul can support "batch" semantic [like numpy](https://docs.scipy.org/doc/numpy-1.15.1/reference/generated/numpy.matmul.html), while Gemm strictly requires 2D inputs.
ArmenAg(2018-10-30 22:11:25):@houseroad LGTM. What do you think?
vloncar(2018-11-02 08:59:08):I believe the latest changes address the issues raised. Is there anything else you would like me to change?

I also have one general question. Should the optimizer defend against invalid models (i.e., check the nodes it works on and bail out if they are invalid, or assume that they are valid and only check what it need to perform the optimization)? For example, how should optimizer handle wrong shapes, missing required attributes, invalid attribute values etc? I am working on another optimizer which, while simple, has many cases where it would fail if the model is invalid.
vloncar(2018-11-06 16:56:05):On second thought (and after noticing that the shape of `MatMul`'s X input may not be available), I have simplified the bias shape check, to do what @houseroad recommended.
ArmenAg(2018-11-06 17:31:32):@houseroad Correct me if I'm wrong but the optimization framework should operate under the assumption that the model is constructed correctly. Otherwise this puts to much responsibility on the optimization framework. Either way we have a model checker that we can run before doing optimization. 
houseroad(2018-11-07 05:52:52):@ArmenAg the principle here is that: having basic check to make sure we are not running into segmentation fault, and leave complete check to the checker. (Even the model is wrong, I think the optimizer should not give segfault)
vloncar(2018-11-09 07:57:50):@houseroad Can you review the latest changes? I believe the current code satisfies the principle you outlined above.
ArmenAg(2018-10-26 07:31:53):Instead of doing a string comparison what you should do is add MatMul to interned_strings.h. Here's an example with the dropout op: https://github.com/onnx/onnx/pull/1555/files
vloncar(2018-10-26 14:59:54):Thanks Armen, I was wondering if I should do it that way, but I wanted to ask first. I'll make the change.
ArmenAg(2018-10-29 18:06:29):Could you remove this print.
ArmenAg(2018-10-29 18:06:56):Could you add tests for some of the conditions that would return false in runTransform?
vloncar(2018-10-29 20:23:09):Sure, I'll add a few tests. Thanks for looking into this.
houseroad(2018-10-31 06:37:10):This line can be removed.
houseroad(2018-10-31 06:38:44):Would suggest use A = Z + Bias, easier to read
houseroad(2018-10-31 06:45:55):This shape of X and Y does not match for matmul...
houseroad(2018-10-31 06:48:40):Here, to be safe, instead of checking num_el, I would suggest checking len(dim) == 1 or 2, and dim[size-1] == M
Otherwise you will get incorrect output shape.
houseroad(2018-11-28 06:41:27):please add two more cases:
1) B ==> 1, 16
2) X = (1, 10), Y = (10, 16), B ==> 16, 16
houseroad(2018-11-28 06:41:32):there is still a problem here. Since in GEMM we only broadcast the bias to the A*B. so we should check here whether bias's second dimension is 1 or the same as A*B.
vloncar(2018-11-28 14:52:49):Done.
vloncar(2018-11-28 14:53:54):I had the stricter check in the previous version, but thought it was too much. I have reintroduced it now.
linkerzhang(2018-10-24 04:26:17):@houseroad  It's a little bit confusing. Let me try to clarify it.

The problem is, in old logic 
"auto typeStr = ToString(type_proto);
  std::lock_guard<std::mutex> lock(GetTypeStrLock());
  if (GetTypeStrToProtoMap().find(typeStr) == GetTypeStrToProtoMap().end()) {
    _**GetTypeStrToProtoMap()[typeStr] = type_proto;**_
  }
  return &(GetTypeStrToProtoMap().find(typeStr)->first);"

The type proto sent via SetType will be stored in the global type map (string->type proto). This is not good. Because, the type proto sent to may carry shape information, while the shape information is not reflected in the type string as part of type system. That means in the map, there may be an element, "tensor(float)->TypeProto (with shape [2,3]".  The correct storage should be "tensor(float)->TypeProto (with no shape information)". Is this clear? :)
houseroad(2018-10-24 02:57:43):indent here, please don't use hard tab.
bddppq(2018-10-25 00:24:05):@zrphercule that is a convention of marking unused argument. But @Flamefire looks to me you should do the same marking for the unused "graph" arguments as well?
Flamefire(2018-10-25 06:35:30):>> Could you please delete some comments you made (like /parent/) 
>
> @zrphercule that is a convention of marking unused argument. But @Flamefire looks to me you should do the same marking for the unused "graph" arguments as well?

Well we are using the convention, that type should be descriptive enough. That is why I removed the (unused) names for `Graph& graph` but not for ` const LexicalScopeContext& /*parent_lex*/`. The difference is that for the former no value is added by using `graph` but for the latter it is essential so people know its the "parent"

I can change this to the comment version, if this is what you prefer, but please consider the above reasoning.
bddppq(2018-10-25 07:50:23):> I can change this to the comment version, if this is what you prefer, but please consider the above reasoning.

@Flamefire fair enough, I don't have strong opinion on this one.
bddppq(2018-10-25 05:23:22):This is not thread-safe. `RegisterOnnxFunctionBuilder` should be wrapped inside the ctor of a registerer class. (or `RegisterOnnxFunctionBuilder` itself can be turned to a registerer class). @raymondxyang @linkerzhang 
Flamefire(2018-10-25 06:32:36):If the static variable is not used, it might get eliminated. I could change the code back to the original but add a runtime check of `functionBuilder_registerer`

Otherwise `std::mutex` and `std::lock` can be used to make it thread safe
bddppq(2018-10-25 07:48:57):No compiler won't eliminate it since it has side effects. static variable itself is thread safe, but now you have changed the "writer" to be out of the static variable's initialization so it's not thread safe anymore. Using lock here should work, but IMO here wrapping/changing RegisterOnnxFunctionBuilder to a class ctor would be simpler. And for the compiler's unused variable warning, you can use the ONNX_UNUSED macro to mark it to silent the compiler.
Flamefire(2018-10-25 12:00:56):To keep this PR simple I changed it back and used `ONNX_UNUSED`
linkerzhang(2018-10-29 20:30:40):@bddppq  @anderspapitto  @houseroad  any more comments on this API change please? this is for the attribute graph type and shape inference.
skottmckay(2018-10-30 21:40:20):It's used by the Scan inferencing in https://github.com/onnx/onnx/pull/1503 so that covers some aspects of it. As we don't use the implementation.cc version directly I wasn't sure how or where to add explicit tests for that. The logic there replicates what we do in our inferencing code, using existing pieces from implementation.cc/h. I didn't see any existing test setup for testing implementation.cc though. Did I miss it?
linkerzhang(2018-10-30 22:02:14):how about we merge this PR to unblock PR #1503 and test could also be added for scan shape inference in #1503. @houseroad 
bddppq(2018-10-25 06:19:26):`std::make_unqiue` is available starting from c++14, while onnx needs to support c++11. Use the backports here instead https://github.com/onnx/onnx/blob/master/onnx/common/stl_backports.h (`ONNX_NAMESPACE::make_unqiue`).
linkerzhang(2018-10-25 15:53:05):Thanks a lot for this change! Please also consider the case of using input data to do shape inference in this interface. 
skottmckay(2018-10-25 23:20:03):Added.
spandantiwari(2018-10-25 17:37:43):An unrelated test (Upsample) seems to be failing. 
spandantiwari(2018-10-25 19:08:37):@bddppq - Thanks!
yuslepukhin(2018-10-25 18:08:57):Cc: @gramalingam 
gramalingam(2018-11-08 22:56:44):I think introducing SparseTensor into ONNX main spec would be useful eventually. This will help us experiment with it first.
gramalingam(2018-11-09 17:12:45):The Travis CI failure appears to be a stalled build. What is the protocol here? Should we try to redo all builds? This seems to have the disadvantage that it will potentially slow down other builds, triggering a cascade of such issues. Or, go ahead and merge it? If so, when is it okay to go ahead and merge it?
ArmenAg(2018-10-26 07:24:26):@houseroad This is a simple pass that removes dropout nop. Even if a backend treats dropout as a nop this pass will still half canonicalize the graph which will help with future passes. 
houseroad(2018-10-29 05:20:20):Even it's not 0, in inference, dropout can be considered as a non-op.
ArmenAg(2018-10-28 02:38:19):Here is a example of the optimization: 

**Before:**
![image](https://user-images.githubusercontent.com/4429794/47611297-a7de3880-da1f-11e8-8c73-73ebffb6c2f7.png)

**After:**
![image](https://user-images.githubusercontent.com/4429794/47611308-cc3a1500-da1f-11e8-86a1-c97557477ed2.png)

ArmenAg(2018-10-29 06:03:26):@houseroad Looks like all the tests passed!
houseroad(2019-01-16 06:19:16):@snnn @raymondxyang could you help review this PR?
CLAassistant(2019-07-24 00:57:53):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=1563) <br/>All committers have signed the CLA.
snnn(2019-01-16 06:27:44):How about keep both of them?
```
cmake_args.append('-DCMAKE_BUILD_TYPE=Debug')
```
and 
```
--config Debug
```

lygstate(2019-01-16 13:24:04):you can not do it both
snnn(2019-01-16 18:46:02):Then please keep CMAKE_BUILD_TYPE there, unless we are pretty sure CMAKE_BUILD_TYPE is not used in any cmake files of this project or its submodules.

Generally speaking, CMAKE_BUILD_TYPE should be specified in Makefiles and ninja generator, it's optional for Visual Studio. And ideally it shouldn't be used in any cmake files. 
lygstate(2019-01-17 06:51:40):@snnn Please take a look at https://stackoverflow.com/questions/24460486/cmake-build-type-not-being-used-in-cmakelists-txt
snnn(2019-01-17 19:03:22):I would suggest taking the changes on cmake/Utils.cmake but leaving setup.py untouched. 
Some third-party modules, they are still using CMAKE_BUILD_TYPE in their cmake files, it's hard to judge if it is safe to build onnx without specify CMAKE_BUILD_TYPE when generating the project files on Windows.  I'd like to treat it conservatively.  
lygstate(2019-01-18 05:00:25):I've tryied to leave setup.py untouched, but the CI failed.
ArmenAg(2018-11-01 21:41:08):@houseroad could you take a look please
ArmenAg(2018-11-03 06:39:57):@houseroad all fixed.
ArmenAg(2018-11-09 04:29:43):@houseroad Looks like some tests are broken on the PyTorch side.
houseroad(2018-11-02 05:50:05):Good catch
houseroad(2018-11-02 05:56:35):why not just use ==?
houseroad(2018-11-02 06:01:20):better to use == 0 instead of != 1
houseroad(2018-11-02 06:14:35):output shape are not valid in some cases
ArmenAg(2018-11-02 06:34:56):Good call
ArmenAg(2018-11-02 06:35:26):Oops. Forgot to remove this. I was using this to debug some stuff. My bad. 
ArmenAg(2018-11-02 06:40:49):fixed.
houseroad(2018-11-06 06:36:32):why do we still keep `axes_match`?
ArmenAg(2018-11-06 18:04:15):Sorry. My bad. Deleted it now.
houseroad(2018-11-07 05:45:12):output_shape seems still incorrect.
For example:
axes1 = [1]
axes2 = [1]
and keepdim = false
outshape should be (5, 1, 9)
ArmenAg(2018-11-07 06:53:46):fixed. I missed a ton of cases. Sorry about that.
houseroad(2018-11-19 13:55:31):Let's also check the output shape please, since we will change it in this opt pass.
ArmenAg(2018-11-19 17:26:40):I added another assertion. Is this assertion what you had in mind?
houseroad(2018-11-20 08:22:39):Yeah, they are what I was looking for
houseroad(2018-11-01 21:11:40):CI failure is unrelated
spandantiwari(2018-11-01 21:35:57):@houseroad, @bddppq - Gentle ping for your review. 
spandantiwari(2018-11-02 23:12:02):@houseroad - Thanks for your feedback. I have overhauled the shape inference code based on your feedback. Also added a couple of test points in `shape_inference_test.py` to test the code.
spandantiwari(2018-11-05 19:31:23):@houseroad - I have updated based on your comments. All the checks are passing. Could you please review? Thanks.
linkerzhang(2018-11-06 17:41:42):Thank you very much! @spandantiwari 
spandantiwari(2018-11-06 18:59:47):@linkerzhang and @houseroad - Thanks!
linkerzhang(2018-10-31 17:33:34):how about we remove attribute "dtype" and have this input as required? looks to me it's more simple.
spandantiwari(2018-10-31 18:16:04):@linkerzhang - Yes, we could do that. A minor reason to keep it this way could be the situation in packages such as TF, when `values` are available in a different datatype (from prior computation) than the type specified by the explicit `dtype` argument. In such a situation the exporter will have to insert an additional `Cast` node before `OneHot`. Having `dtype` in the ONNX op may provide some convenience and eliminate the need for the extra op.
This is a minor point and I am OK either way. What do you think?

Does anyone else have any preference/feedback?
linkerzhang(2018-10-31 22:07:16):Hmmm, agree this is a minor point. but don't quite understand the case you said. "The type of 'values' should be the same as 'dtype', if 'dtype' is specified." this means anyway, the values's type is indeed indicating output's type, if values are always specified, then output type is also specified (no datatype attribute needed). If values are from previous node ('s output) and the type is not expected, then a CAST is anyway needed, am I right?
spandantiwari(2018-11-01 04:29:58):Yes, you are right, and the `Cast` op is needed anyway. I agree that the `dtype` attribute is not really adding much and just making `values` a required input is a cleaner design. Thanks for pointing this out. Will update the signature. 
spandantiwari(2018-11-01 21:34:29):@linkerzhang As per your suggestion, I have removed `dtype` and made `values` a required input.
houseroad(2018-11-02 05:32:57):nit: input2_shape / value_shape
houseroad(2018-11-02 05:35:27):Probably not throwing exception, just return to stop the shape inference.
houseroad(2018-11-02 05:36:00):ditto, if we don't have enough information, just exit the shape inference.
houseroad(2018-11-02 05:39:17):how about i == axis case?
houseroad(2018-11-02 05:40:10):Let's mention axis=-1 means the last dimension.
spandantiwari(2018-11-02 21:29:31):Done.
spandantiwari(2018-11-02 23:07:04):Agreed. I have updated the shape inference at multiple places to return whenever there isn't enough info, instead of throwing. 
spandantiwari(2018-11-02 23:07:13):Updated.
spandantiwari(2018-11-02 23:07:38):Agreed. Updated shape inference code at multiple places to do that.
spandantiwari(2018-11-02 23:09:33):In the `i == axis` case the size of that dimension comes from the `depth` input, which may not be known or inferrable. Therefore, for that case, we just add a dimension but do not add a value. Also, added test points for this scenario.
kit1980(2018-11-03 00:19:38):@zrphercule, there is an issue suggesting using If: https://github.com/onnx/onnx/issues/870
But as I mentioned there, it's not possible to use it for Where because you don't just select one of the output tensors, you select some elements from the first tensor, and some elements from the second tensor. And if there is only one input, Where returns indices of true elements.

I'll update with regenerated docs and test data.
liqunfu(2018-11-06 02:45:02):Please update onnx/defs/operator_sets.h with the new op
kit1980(2018-11-10 03:25:18):I've updated with a test for one input case and regenerated all required files.
Please take a look.
gramalingam(2018-11-20 23:12:59):The two usage scenarios (one where X and Y are specified and one where X and Y are not specified) seem different. Why bundle them into one op? I understand numpy does it, but should we duplicate the same? Why not make these different ops? It is easier to describe/implement/understand when they are separate.
kit1980(2018-11-20 23:46:10):@gramalingam Tensorflow also has the dual-intent Where like numpy. With two different ops, we'll need to invent some non-standard name for one variant.
gramalingam(2018-11-21 05:01:29):Ok. I think it would help to clarify in the documentation what happens when X and Y are not specified: e.g., "returns the indices of positions in the first tensor where the value is 1, as a 2 dimensional tensor where the first index is … and second index is ...".  It looks like the shape you specify for output has the two dimensions in the opposite order to what is in tensorflow. Adding a shape inference function for the op would also be helpful.
kit1980(2019-01-02 12:02:04):Modified the Where op spec to accept only three arguments (the one argument case is to be handled by NonZero). Also added shape and type inference.
kit1980(2019-01-02 12:48:54):A separate PR for NonZero: https://github.com/onnx/onnx/pull/1714
kit1980(2019-01-09 21:34:03):Can someone look at this? The Where Op is much simpler now. 
kit1980(2019-01-15 02:02:30):Rebased on master.
gramalingam(2018-11-05 22:50:03):Hi @bddppq @linkerzhang , any feedback on this update?. Thanks
gramalingam(2018-11-27 22:37:35):Updated PR appears in https://github.com/onnx/onnx/pull/1653 .
skottmckay(2018-11-02 03:55:07):Nice catch. Would be great it some static analysis could catch these things earlier.
rdzhabarov(2018-11-06 00:39:10):We could add "-Wnon-virtual-dtor" compilation flag to detect missing virtual destructor when there is a virtual method declared for the class.
rdzhabarov(2018-11-06 05:27:08):@skottmckay that should make it ^
spandantiwari(2018-11-03 01:09:17):@gramalingam - I have overhauled the shape inference code based on your feedback.
spandantiwari(2018-11-05 18:08:36):@gramalingam - Thanks.
gramalingam(2018-11-02 21:49:08):I think this check-and-fail is not needed … it is valid if the input-dimension is not statically known
gramalingam(2018-11-02 21:50:03):This check is not the same as checking if the optional 3rd input is specified. If the optional 3rd input is specified, we will need to return, I believe.
spandantiwari(2018-11-03 01:07:21):Fixed.
spandantiwari(2018-11-03 01:07:30):Fixed.
linkerzhang(2018-11-08 16:55:26):I agree with @houseroad 

@skottmckay @gramalingam  I believe this is a new topic introduced by control flow ops (Scan, Loop). A scan node can refer outter scope parameters with having them as the node's inputs. Can you guys work on documentation/clarification about parameters' scoping please? Thank you!
gramalingam(2018-11-08 17:05:24):Yes, I agree. We need to clarify this. I can work on it. In fact, we were unclear about one aspect, where we went with what the checker does: I believe we cannot reuse names defined in outer scope to mean something else in an inner scope … so the "static single assignment" naming restriction applies across outer and inner scope. Is this understanding correct?
gramalingam(2018-11-08 17:09:56):However, I think Liqun's PR may be unrelated to the scoping issue. I believe there were some issues caused by a combination of several things: (a) The requirement initializers should be a subset of graph-inputs, (b) The transformation of a name X defined by a constant into an initializer, etc. But I thought they were sorted out, so I don't fully understand this issue yet.
gramalingam(2018-11-08 19:15:08):So, I believe the issue is the one discussed in https://github.com/onnx/onnx/issues/1449 … it will be helpful to eliminate the restriction that every initializer must also appear in the list of inputs. The point of the original design no longer holds, and we find that the implementations are complicated by this requirement. For example, when a constant X is converted to be an initializer, we are forced to add X to the list of graph inputs. In the case of control-flow ops, if we do this to the sub-graph, then it complicates the parameter-matching (because we have extra spurious inputs in the formal parameter list, but not in the actual parameter list). 
snnn(2018-11-12 19:55:40):Every initializer must also appear in the list of inputs. It's in the spec. We shouldn't change it unless necessary, because it's a very big break change. We can talk it later, but at this point in time, please don't change it. 
gramalingam(2019-02-14 20:18:30):Closing this as PR https://github.com/onnx/onnx/pull/1718 covers this
linkerzhang(2018-11-05 18:23:20):Hmmm, don't quite understand, do you mean in an attribute graph (for control flow op/node), its initializer may not be its input? why?
liqunfu(2018-11-05 19:57:56):yes. It fails if a scan op's body subgraph has, for example, an add node with a constant tensor as initializer.
vloncar(2018-11-22 16:05:12):Is there any hope for getting this (and #1542) optimizer into ONNX or should we handle things like this in our downstream tools? Both this PR and #1542 came from working with models exported to ONNX format in a standard way (torch.onnx.export for Pytorch and onnxmltools for Keras, respectively) so they should be useful.
ArmenAg(2018-11-26 18:22:05):@houseroad Could you please review the two passes that @vloncar has written. Both could be very useful to us.
houseroad(2018-11-28 06:58:40):@vloncar yeah, I also reviewed the other one, could you address my comments?
vloncar(2018-11-28 14:56:48):@houseroad Thanks a lot!
zrphercule(2018-11-06 04:32:35):It seems the "graph" here has not been used, and that's why the CI failed.
Wonder if it is designed to be like this?
vloncar(2018-11-06 09:35:57):I noticed and fixed in the latest commit, but I don't understand how is this not an issue in other optimizers, like `fuse_consecutive_squeezes` or `eliminate_nop_pad`?
ArmenAg(2018-11-06 18:02:49):@zrphercule I believe that CI checks for unused parameters.

@vloncar both fuse_consecutive_squeezes and eliminate_nop_pad have definitions which don't use graph:

`bool runTransform(Node* n, Graph&, NodeDestroyType& destroy_current)`

linkerzhang(2018-11-06 17:40:59):Several questions:
1. looks to me the constantlike op does meet all requirements, isn't it? @spandantiwari  may comment more on this.
2. A production operator can't be removed like this. It should be deprecated in next operator set version, since there may be models replying on it.
zrphercule(2018-11-06 18:41:47):@linkerzhang Thanks for your comments!
My comments:
1. ConstantLike has useful functionality, however its design is a little confusion and different from other similar operators. It takes both optional input and optional attribute to indicate the shape of output, which we believe is not a good design.
2. ConstantLike was added a 3~4 weeks ago in version 9, which means it is still not released yet. I believe we may be able to modify it before we release it?
Thanks!
spandantiwari(2018-11-06 19:51:58):@zrphercule 
1) Yes, the design could be more streamlined, but it was created intentionally so as try to support features from the experimental op `ConstantFill`. `ConstantFill` is used in export by PyTorch and requires support. In fact, there's a PR still open (https://github.com/onnx/onnx/pull/1434) for removing `ConstantFill`, but it is on hold till PyTorch migrates away from `ConstantFill`.
Also, the name design of existing `ConstantLike` is largely in line with design of similar generator ops in ONNX, e.g. `RandomUniform` and `RandomUniformLike`, and `RandomNormal` and `RandomNormalLike`, not to mention similar Numpy APIs, e.g. `zeros_like`, `ones_like`. If we want to streamline the signature of `ConstantLike`, we should consider improving the existing op, because the proposed name `ConstantAsShape` is quite different from the existing convention. 

2) Regarding this part, I agree that it isn't the best idea to remove productions ops without deprecation. `ConstantLike` was committed more than six weeks ago. Given the fast release times, this is already part of frameworks' exporters. 
spandantiwari(2018-12-06 18:43:56):Circling back on this. Having looked at `torch.zeros` I understand that there're use cases when the size has to be provided as input on the fly. We should support both use cases:
1) Taking shape (and possibly type) info from the shape of the input tensor. This is because:
a) There are framework APIs that support this directly, so having an analogous op is convenient, because it eliminates the need for inserting multiple nodes such as shape.
b) We already similar generator ops such as `RandomUniformLike` and `RandomUniformLike`.
2) Taking shape directly as an input. This will satisfy the torch.zeros type use case. 

One design is update `ConstantLike` to support both use cases. Another option is to support these two in two separate ops. Both have their pros and cons. Thoughts?


spandantiwari(2018-12-11 21:38:25):@zrphercule - Having looked at this in detail and discussing with others, I am OK with moving forward with this spec. The question is whether should we keep `ConstantLike` also, or should we deprecate it. I can see arguments for both. The arguments for keeping `ConstantLike` are similar to the reasoning in the other open PR for `LessOrEqual` and `GreaterOrEqual`. It is a common op seen in many frameworks such as CNTK and TF, and it lends to better optimization. But at the same time, we are increasing the number of ops by having multiple ops. What do you think? Shall we keep `ConstantLike`? 
spandantiwari(2018-12-13 01:27:25):@zrphercule - could you take a look and address the feedback. We are waiting for this so that some of the our model exports from PyTorch could be unblocked. Let me know if I can help take this forward. Thanks!
zrphercule(2018-12-13 18:22:07):@spandantiwari Sorry I missed you prior message... Yeah the reason I didnt push this spec anymore is I also agree it might be a good thing to keep ConstantLike, while ConstantAsShape should be added or not is still under discussing.
I wonder if you are willing to use ConstantLike or the new ConstantAsShape? Feel free to use ConstantLike since I dont think I will do any breaking commit recently.
spandantiwari(2018-12-13 22:25:33):@zrphercule - thanks for circling back to this. OK, I think we are in agreement so far. We can update this spec to add `ConstantAsShape` as a new op, and after discussion, remove `ConstantLike` op in a separate spec, if needed. Would you be able to update this spec accordingly? It will be great to have this as part of ONNX 1.4. Let me know if I can help. Thanks.
spandantiwari(2018-12-18 18:38:48):@zrphercule @houseroad - can we move to include this op (`ConstantAsShape`) in the spec. I will be happy to help land this. 
houseroad(2018-12-18 19:50:48):@spandantiwari unfortunately, @zrphercule is sick this week, feel free to take over the PR, I can help on the review.
zrphercule(2018-12-19 18:10:41):@spandantiwari Sorry I was having a fever while you ping me. Now I am back, and am working on this. Should be fast, no worries. Thanks!
spandantiwari(2018-12-20 18:14:39):@zrphercule - No worries. Hope you are feeling better. 

You have addressed most of the feedback and the  spec looks good to me. I have added just one comment on the description of the `value` attribute. If you take a look that'll be great. 
zrphercule(2018-12-26 21:29:09):@spandantiwari How do you like it? shall we merge?
spandantiwari(2018-12-27 17:28:52):Yes, it looks good to me.

spandantiwari(2019-01-02 23:42:45):@zrphercule - I may have mentioned this earlier but missed in my later reviews. Could you please add a shape inference also to the `TypeAndShapeInferenceFunction` for `ConstantOfShape`? Right now it is doing type inference only. Thanks!
zrphercule(2019-01-03 18:04:50):@spandantiwari  sure, no problem!
spandantiwari(2019-01-03 20:52:45):@zrphercule - Actually, if you haven't started on it yet, let me do it. I am creating a PR to remove `ConstantLike` anyway and I will add shape inference to `ConstantOfShape` as well. Is that OK?
zrphercule(2019-01-03 21:06:15):@spandantiwari Thanks! If you have any problem, feel free to ping me!
spandantiwari(2019-01-03 22:08:07):@zrphercule - Sure. I have created a PR https://github.com/onnx/onnx/pull/1716 that removes `ConstantLike` and also adds shape inference to `ConstantOfShape`. Please take a look and let me know your feedback. Thanks!
spandantiwari(2018-12-11 21:40:11):nit: spelling "unsigned"
spandantiwari(2018-12-11 21:41:08):Since the shape is an input to this op, may I suggest `ConstantOfShape` as possible name for this op.
spandantiwari(2018-12-11 21:42:13):Did you mean: "The value for this attribute defaults to 0."?
spandantiwari(2018-12-11 21:43:13):Can we clarify if we can create a scalar from this (empty tensor)?
spandantiwari(2018-12-11 21:44:30):Do we need to constrain output types?
zrphercule(2018-12-20 02:13:34):Since we have the same concern in ConstantLike... I am ok with removing this constrain.
zrphercule(2018-12-20 02:14:31):I guess we can? "If empty tensor provided, then output tensor would be a scalar"
spandantiwari(2018-12-20 18:11:31):@zrphercule - Minor point - this probably needs some clarity. I think we want the datatype of the output tensor to be the same as the type of one-element `value` tensor, correct? If yes, then this can be worded better. For one, there's no `dtype` attribute.
zrphercule(2018-12-20 19:17:29):@spandantiwari Oh yes, but the value tensor is optional. Let's say if value is not given, then by default the output dtype is float32.
skottmckay(2018-11-07 00:28:25):Looks good to me. 
houseroad(2018-11-07 07:00:06):docs/TestCoverage.md needs to be updated as well, the script should cover it for you
gramalingam(2018-11-07 19:28:05):I updated test coverage. But I am puzzled that it shows differences (in TestCoverage.md) that have nothing to do with my changes … any idea why?
gramalingam(2018-11-07 19:34:58):A previous merge must have omitted to update test coverage, perhaps.
gramalingam(2018-11-14 01:06:03):@houseroad @linkerzhang please let me know if anything else is required. I have addressed the earlier comments
linkerzhang(2018-11-07 01:28:53):this change is not you want :), I think.
gramalingam(2018-11-07 01:36:17):Any idea what causes this? (I guess something in the document generator pipeline might be using lower precision, but not sure where.)
gramalingam(2018-11-07 00:23:23):Looks good to me
houseroad(2019-01-09 07:55:03):@skottmckay do you mind updating the PR to pass the CI?
CLAassistant(2018-11-07 00:25:20):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=1588) <br/>All committers have signed the CLA.
linkerzhang(2018-11-07 01:55:38):Please make Linux ci happy, btw.
houseroad(2018-11-07 06:03:43):Looks good, can you add/modify one test case to cover this change? Thanks

You can find existing test here: https://github.com/onnx/onnx/blob/master/onnx/test/helper_test.py#L174
houseroad(2018-11-07 06:04:33):If it's none, we will just use default domain (which is empty)
yskim1501(2018-11-07 07:21:47):Thanks! Updated the comment as you suggested.
bddppq(2018-11-07 17:45:18):You need to explicitly check for `None` here, otherwise here will skip setting the domain when it's empty string
houseroad(2018-11-07 07:57:59):@snnn 
snnn(2018-11-07 08:11:51):Thanks.

Could you also add INT16?


houseroad(2018-11-07 19:03:18):Sure, will do it
snnn(2018-11-07 19:26:17):LGTM. 
zrphercule(2018-11-07 22:42:38):Guess the test failed because of they didnt remove the catch check in pytorch.
I saw Peter removed it a few hours ago, let's retest it.
linkerzhang(2018-11-08 16:50:57):looks like I clicked the merge too quickly. @houseroad  you're adding more to support 16 and adding test cases? :) 
gramalingam(2018-11-07 22:14:34):My reading of the op documentation is that the scan-outputs are required to have a shape that is invariant across the loop iterations … but I don't see a similar restriction imposed on the loop-state variables. (We do have such a restriction in the case of "scan"). I would drop the above 3 lines (for loop-state vars) unless the op. spec is changed.
gramalingam(2018-11-07 22:16:01):Likewise the type pushed into subgraph_input_types should be a copy of the type with shape information omitted, for the same reason, since this shape could vary over iterations.
gramalingam(2018-11-07 22:29:16):Adding the expected numeric type would catch a type-error where the subgraph's first parameter is not the right type. (The spec doesn't clarify the type, but int64 seems best, and we could probably clarify the spec too.)
gramalingam(2018-11-07 22:36:46):Wonder if this is meant to be float or int?
skottmckay(2018-11-07 23:02:26):INT64 is more correct. Doesn't play any role in the inferencing though. 
skottmckay(2018-11-07 23:03:03):Should that be handled here or in the checker where it validates types match the constraints?
skottmckay(2018-11-07 23:05:21):Sorry - I see what you mean now. The checker would only validate the max iterations type not this. Will add a check it's int64.
CLAassistant(2018-11-09 06:39:48):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=1594) <br/>All committers have signed the CLA.
ke1337(2018-11-16 22:20:17):@houseroad Could you please take a look at this PR when you have time?
houseroad(2018-11-17 00:46:07):@KeDengMS sure, I will take a look this weekend.
gramalingam(2018-11-09 19:15:59):I think it is possible for one of these to be a symbolic dimension and the other to have a value. I don't think we should throw a dimension-mismatch exception in that case.
gramalingam(2018-11-09 19:22:06):I think it would be useful to extract a utility function "mergeDim" (e.g. https://github.com/onnx/onnx/blob/fa568e4281f5008921e7b677ead901504cab491b/onnx/defs/shape_inference.h#L499 also tries to do the same thing) so that it can be used by multiple shape-inference methods
gramalingam(2018-11-16 22:12:36):Would Sequnce[Union(Text,int)] be simpler? I don't understand TypeVar usage, so I am not sure. Just wondering.
ke1337(2018-11-16 22:15:56):Yes I've switched to Union in latest commit
ke1337(2018-11-16 22:16:11):Thanks for the suggestion. Done
linkerzhang(2018-11-09 18:36:43):don't change the .md directly, change the defs.cc and call defs/gen_doc.py --ml to generate the .md.
wschin(2018-11-10 18:50:57):@linkerzhang, there is no `--ml` and by setting up `ONNX_ML=1`, that script just removes everything in *-ml.md. Any idea?

Ah, I should set `ONNX_ML=1` before installing onnx.
linkerzhang(2018-11-13 12:51:32):Please move the old version (v1) to a separate file, like old.cc.
wschin(2018-11-13 17:17:11):@linkerzhang, no problem. Your comments are addressed.
gramalingam(2018-11-20 21:57:47):Hi, the new specification looks good. However, I wonder if it would be better to introduce this as a new op, rather than replacing the old one op by a breaking change? A breaking change would force all existing exporters that export the older form of the op to update to the new version: do we really want this? 
wschin(2018-11-20 23:45:34):I believe the uses of this op are very limited. We just need to minorly change Onnxmltools and ML.NET.
linkerzhang(2018-11-13 12:50:34):do you want to reuse the default string value as in op set 1? "_Unused". Which looks to me better.
wschin(2018-11-13 17:15:04):Sure.
gramalingam(2018-11-14 00:13:42):look-up mode?
gramalingam(2018-11-14 00:14:37):look-up?
gramalingam(2018-11-14 00:16:33):This is confusing. Do you mean "when a value not present in the classes_* is given"?
gramalingam(2018-11-14 00:19:15):look-up? It looks like look-up and indexing may be mixed up in the above few lines.
gramalingam(2018-11-14 00:22:52):I don't think the other ops say this explicitly. It is always implicit that tensors include scalar-tensors (tensors of rank 0). At least, that is my impression.
gramalingam(2018-11-14 00:26:50):What if someone wants to do lookup mode for mapping int to int? Is that scenario not important? If it is important, adding an attribute "mode" would perhaps help?
wschin(2018-11-14 01:25:09):lookup mode for int-to-int is doable via indexing mode. For example, look-up mode with class_int64s=[2, 4] is equivalent to indexing mode with class_int64s=[default_int64, default_int64, 0, default_int64, 1]. But anyway, we should support it to reduce memory used.
wschin(2018-11-14 01:26:38):Could we make it more clear? It's a standard.
wschin(2018-11-14 01:27:02):Let me break them into two paragraphs. Thanks.
wschin(2018-11-14 01:27:47):I will check all other places. Thanks.
wschin(2018-11-14 01:31:05):Yep. I will improve it.
wschin(2018-11-14 01:31:13):Yes!
wschin(2018-11-18 01:29:40):Spec largely revised. :)
raymondxyang(2018-11-12 00:34:39):Well it actually behave differently once the capacity of the vector is met and the memory spaces got reassigned.. 
houseroad(2018-11-12 00:44:56):@raymondxyang could you give an example to elaborate the problem?
raymondxyang(2018-11-12 00:52:57):Well.. it either caused seg fault or get typeproto with uninitialized fields. The test case is Full Connect function op, which has three inputs and triggers the reassignment of the vector elements addresses. we store them to quickly access its shape/type info within the function.
snnn(2018-11-12 19:58:09):Or you may let temp_valueTypesByName take the ownership of these protobuf objects? Then you can delete temp_types_cache.

raymondxyang(2018-11-12 22:07:42):@snnn its not easy to do it in that way.. The ctx.getInputType will return a const pointer, Which means there has to be a mutable copy of the typeproto object while there should also be a std::unordered_map<std::string, TypeProto*> to pass to the InferenceContextImpl()
snnn(2018-11-13 03:21:59):LGTM.
yuslepukhin(2018-11-13 18:59:44):@gramalingam FYI
spandantiwari(2018-11-13 23:05:07):@linkerzhang and @gramalingam - Thanks!
zrphercule(2018-11-26 18:27:39):@houseroad I think in the discussion of ThresholdedRelu, @dzhulgakov proposed that we could modify relu without adding a new op? Besides, this change of relu does not break old format, since all new attributes are optional.
zrphercule(2018-12-05 00:08:48):@linkerzhang @gramalingam We are planning to enable ThresholdedRelu instead of adding new attrs in Relu, for the consistency of our operator set (Since we have celu, elu, prelu...). What is your opinion? Thanks!
linkerzhang(2019-02-19 17:39:49):@zrphercule let's promote ThresholdedRelu then. Do you want to make a PR to do that please? or I'll do it later.
zrphercule(2019-02-19 19:06:44):@linkerzhang Yeah we would love to promote ThresholdedRelu. I will close this pr for now, and add a new one. If you have any emergency use, feel free to promote it by your self, and I can review it for you :)
prasanthpul(2019-03-11 18:12:10):https://github.com/onnx/onnx/pull/1856 has been created to promote ThresholdedRelu. So we can close this
linkerzhang(2018-11-29 17:52:26):we're having a design issue here.

The attribute "threshold" type should be same as input type, while the attribute "value" type should be the same as output type.

Before we unifying the type system for attribute, input and output, let's set the attribute type as "Tensor", which looks better than current setting, though it's not perfect.


zrphercule(2018-11-29 21:24:58):Yeah I think you are right, thanks!
zrphercule(2018-11-29 21:50:00):@linkerzhang  btw since in relu, the output type should be the same to the input type, that means "x" "y" "value" "threshold" should all share one type.
yinghai(2018-11-14 23:37:50):@onnxbot retest this please
zrphercule(2018-12-05 18:39:54):@houseroad Right now, I have found torch.hardshrink/softshrink and tf.hardshrink/softshrink. And they can all be exported_to / imported_by this shrink op in onnx.
zrphercule(2019-01-09 19:14:56):@linkerzhang @gramalingam @spandantiwari @ebarsoum Could you please take a look at this PR and also 
Celu:https://github.com/onnx/onnx/pull/1676
GE&LE: https://github.com/onnx/onnx/pull/1684

We would be more than happy to see these ops to be merged in upcoming 1.4 released =) Thanks!
wschin(2019-01-10 19:27:33):This op can be composed using primitives. Would it be better to put it into a function?
zrphercule(2019-01-10 19:41:55):@wschin Dont think this can be composed simply. From my knowledge, it is going to be composed as maybe more than 10 ops including a few IF ops. This is not good I guess.
wschin(2019-01-11 01:05:37):@zrphercule, This is IR. You can map a complicated graph to another internal operator implemented in your runtime.
CLAassistant(2018-11-16 00:37:30):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=1623) <br/>All committers have signed the CLA.
JerryShih(2018-11-16 10:37:52):The message is like:

> (op_type:Conv, name:test_layer) :Inferred shape and existing shape differ in rank: (4) vs (5)
> RuntimeError: Inferred shape and existing shape differ in rank: (4) vs (5)
JerryShih(2018-11-19 06:47:22):@zrphercule 
Should I need another reviewer(@bddppq ?) for this pr?
All tests passed.
zrphercule(2018-11-26 18:36:27):@JerryShih yeah I think we can merge it now, it looks good. Sorry didnt reply you last week because of thanks giving...
bddppq(2018-11-16 22:58:01):`ci/circleci: py2-gcc7-ubuntu16.04` failure is known issue, please ignore for now.
bddppq(2018-11-17 00:33:32):@houseroad 

> the value of the elem_type will be changed to 0 if they don't know how to interpret it

yes old runtime still uses enum and unfortunately we can not change them anymore. this change however helps preventing this happening again in the future

> Shall we completely comment out the DataType in proto file

we still need it to define the values as constants

> The typechecker is also unhappy.

oops I will fix it soon
bddppq(2018-11-18 18:12:35):ping @linkerzhang @gramalingam 
gramalingam(2018-11-18 19:09:48):Since the first enum is UNDEFINED, mapping a datatype that the runtime does not know/understand to UNDEFINED seems like reasonable behavior. May be I am missing something, but I don't quite understand the motivation for the change. When a new datatype is added, the ir-version will be changed too, right? So, when a version v1 runtime loads a v2 version model (v2 > v1), it treats any newly added datatypes in v2 as being UNDEFINED. If we change the enum to int, then all the code dealing with datatype need to handle a value outside the legal range (in a given version) in some fashion. 
bddppq(2018-11-18 19:23:45):@gramalingam The problem of blindly treating unknown dtypes as UNKNOWN is the information of the actual type value (although unknown) get lost during deserialization, so a backend doesn't know whether the dtype is really UNKNOWN or is it new dtype newly added. Imagine a framework (tf/pytorch/caffe2/cntk) integrates with an onnxifi backend, in this case the framework is going to delegate the execution to the onnxifi backend, so the framework is fine with new dtypes (as long as the onnxifi backend supports the new dtype, however it's unreasonable to let UNKNOWN dtype passing through the framework or the checker.
I think we should leave backends the flexibility to decide what to do (error out or gracefully pass it to downstream) when an unknown dtype is encountered.
bddppq(2018-11-18 19:35:07):@gramalingam 
> When a new datatype is added, the ir-version will be changed too, right?

It has never been the case, both when we added complex and added bloat16. IMO Bumping ir_version is more intrusive to all existing integrations, I think unless there is a big semantics changes that we can not make it backward compatible, it's better to not bump the ir_version. For adding new dyptes, once we make the change in the PR, we can make it backward compatible at the serialization format level.
linkerzhang(2018-11-19 17:45:40):Data can't be recognized as valid enums is marked as undefined is a by-design behavior, isn't it? (I thought that was why we add undefined). With bfloat, I'm assuming that new runtime (with bfloat support) can run all old models (without bfloat). This is good. Old runtime (without bfloat) can't run new models (with bfloat), which is also good and by design. No?

Changing it to "int" looks to me make the proto definition not good (we defined Enum DataType but we don't use it).
bddppq(2018-11-19 19:26:47):@linkerzhang 
In the example I mentioned above https://github.com/onnx/onnx/pull/1626#issuecomment-439718359, when a framework integrates with onnxifi backends, since the framework delegates the execution to the onnxifi backend, it's actually unnecessary for the framework to be able to handle bfloat16 (as long as the onnxifi backends can handle it). 
The reason that we still keep the enum is to define dtype values as constants, so at python/c++ code level we can refer to these values.
bddppq(2018-11-19 19:29:21):I think we should leave backends the flexibility to decide what to do (error out or gracefully pass it to downstream) when an unknown dtype is encountered.
linkerzhang(2018-11-22 17:13:19):I guess the problem you're facing with here is there's no good way for a backend to distinguish a new model (with bfloat) from an old model. This strongly reminds me of bringing the topic of when and how to bump the ir version. Looks to me, adding bfloat into the standard should bump the ir version to 4.

With version bump, a backend can support ir version <= 3 only is by design not supporting bfloat.

bddppq(2018-11-28 21:45:55):@linkerzhang I think bumping the ir_version or not when adding new dtypes is a little bit orthogonal to the proposed change here. And as I mentioned above, my goal is to let a framework that delegates execution to onnxifi can pass through models with new dtypes, not all frameworks are by design only support a specific ir_version. 
Also I don't see there is any downside of doing this change, backends that want to make stricter assumption can still check whether there are new dtypes that it doesn't recognize and error out.
houseroad(2018-11-30 19:26:33):@linkerzhang any more comment? Can we move forward?
bddppq(2018-12-04 20:23:18):ping @gramalingam @linkerzhang
gramalingam(2018-12-05 19:33:51):@linkerzhang and I talked about this. Summarizing our thoughts: changing the enum to int seems fine. However, when we add new types (like bfloat16), we are extending the set of serialized models (in binary format) that are considered legal. Technically, this requires the ir_version number to be bumped. Does this make sense? It seems like it would make sense for us to increment the ir_version for the upcoming release, with all these changes. Is there a problem with incrementing the ir_version?
bddppq(2018-12-06 17:54:19):@gramalingam Technically any change in the .proto file change the set of legal serialized models, I think we need further discussions about what change should and should not cause an ir_version bump. 

Could you stamp this PR? As mentioned above this change is kinda orthogonal to ir_version bump.
bddppq(2018-12-07 21:08:56):@gramalingam Sure
cc @prasanthpul @jspisak 
gramalingam(2019-01-17 23:14:53):Updating the IR_VERSION in https://github.com/onnx/onnx/pull/1718 .
gramalingam(2018-12-07 20:33:07):For each of these lines, can we add a comment "// This field MUST have a valid TensorProto.DataType value".
This will at least help capture what a valid ONNX model is.
bddppq(2018-12-07 20:46:22):Sure added
CLAassistant(2018-11-18 01:44:20):[![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/onnx/onnx?pullRequest=1630) <br/>Thank you for your submission! We really appreciate it. Like many open source projects, we ask that you sign our [Contributor License Agreement](https://cla-assistant.io/onnx/onnx?pullRequest=1630) before we can accept your contribution.<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/onnx/onnx?pullRequest=1630) it.</sub>
houseroad(2018-11-18 11:57:32):Sign the CLA please
houseroad(2019-01-09 07:57:03):Since the author didn't reply for a long time, close the PR.
CLAassistant(2018-11-19 03:28:25):[![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/onnx/onnx?pullRequest=1631) <br/>Thank you for your submission! We really appreciate it. Like many open source projects, we ask that you all sign our [Contributor License Agreement](https://cla-assistant.io/onnx/onnx?pullRequest=1631) before we can accept your contribution.<br/>**1** out of **2** committers have signed the CLA.<br/><br/>:white_check_mark: raymondxyang<br/>:x: hanzhaogang<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/onnx/onnx?pullRequest=1631) it.</sub>
snnn(2018-11-19 22:14:56):The old code works fine with cmake 3.12.
hanzhaogang(2018-11-20 00:48:56):@snnn see this link: [https://cmake.org/cmake/help/v3.12/module/FindPythonLibs.html?highlight=findpythonlibs](https://cmake.org/cmake/help/v3.12/module/FindPythonLibs.html?highlight=findpythonlibs), it says: FindPythonLibs 

> Deprecated since version 3.12: Use FindPython3, FindPython2 or FindPython instead.


But actually I don't know the reason why you didn't hit this issue  #1625 under cmake 3.12. Maybe there are some other reasons for the issue ? For me I fix this issue by the updated CMakeList.txt (with cmake 3.12).  
snnn(2018-11-20 01:00:36):It's deprecated, not removed. We don't have to do anything for that.
houseroad(2019-01-09 07:58:21):Close the PR since the change is not necessary. Btw, thanks for the PR anyway.
skottmckay(2019-02-20 02:43:48):I think it's necessary. Without the change the cmake generation fails on Windows with CMake > 3.12.

_CMake Error at C:/Program Files/CMake/share/cmake-3.13/Modules/FindPackageHandleStandardArgs.cmake:137 (message):
  Could NOT find PythonLibs (missing: PYTHON_LIBRARIES) (found suitable
  version "3.7.1", minimum required is "3.7")_

If you run the cmake generation again it will work so something gets cached from the first attempt, however it would be better to work the first time wouldn't it?
ayermolo(2019-02-26 01:58:34):Also seeing this issue. Running twice bypasses it, but it's kind of a hack.
snnn(2019-11-14 00:40:17):LGTM.
houseroad(2018-11-19 04:43:11):cc @snnn
snnn(2018-11-19 08:09:41):LGTM.
Maratyszcza(2018-11-19 09:04:43):what is the point of removing `const`? `const` hints the reader that this variable never changes, and unless is breaks compilation / forces less efficient overload I don't see a good reason to remove it.
houseroad(2018-11-19 09:45:58):For the function declaration, const int and int are the same thing, since they are passing value. Meanwhile, in the implementation, we almost never change the value of the parameters, and we would rather to make APIs consistent. In this diff, we also would like to remove the const of the return value, because we may have some post processing afterward.
zrphercule(2018-11-19 18:28:01):@houseroad Since they are the same thing, and also the value never changed, why we dont leave it to be const?...
snnn(2018-11-19 22:12:38):Please make it consistent. Either you always write code like:
```
void func(int c);
```
or 

```
void func(const int c);
```
Most people will choose the first one. 
Most open source code bases don't have the second one.

houseroad(2018-11-20 02:34:35):cc: @pranavsharma
pranavsharma(2018-11-20 03:00:49):LGTM 👍  Btw, we were planning to remove the non-spatial case altogether from the op definition since no other framework (other than CNTK) implements it. @liqunfu will probably send a PR for it.
cloudhan(2020-06-08 11:33:39):> LGTM  Btw, we were planning to remove the non-spatial case altogether from the op definition since no other framework (other than CNTK) implements it. @liqunfu will probably send a PR for it.

Actually mxnet is using non-spatial batchnorm. The doc suggests using flatten to walkaround, but I am frustrating with "unflattening" the tensor...
zrphercule(2018-11-19 23:44:23):@skottmckay @gramalingam Could you please tell me the purpose of using outer_scope_value_types_by_name, and if it is safe to not use it? Thanks! 
skottmckay(2018-11-19 23:51:03):They are required for control flow operators like If and Loop. You need to have that information to do type/shape inferencing for subgraphs within those operators. 

The initial approach was to take a copy of the outer scope values to allow local values to override. 
 
liqunfu(2018-12-04 18:46:06):@houseroad, the purpose of this PR is to simplify BN spec by removing the no-spatial mode. One can view BN in its most generic form as an op to compute statistics across a subset of the input tensor's dimensions. Dimensions not involved in statistics computation are grouped into a single dimension as channel. Therefore, it is unnecessary to call out spatial and non-spatial modes. 
As state in this PR, no-spatial mode can be done by pre reshaping (to group all dimensions other than N into the channel dimension). There is no performance hit. CuDNN per-activation mode can be done similarly in spatial mode.
liqunfu(2018-12-20 00:40:46):@houseroad please any feedback? Thanks 
jspisak(2019-01-16 22:34:17):LGTM!
rdzhabarov(2018-11-21 18:16:55):any reason not to add -Werror flag to the CI?
houseroad(2018-11-22 09:58:33):Even with -Werror, such problem may not be detected. Also if we set -Werror, projects which use onnx may get frustrated.

Some useful blog: https://embeddedartistry.com/blog/2017/5/3/-werror-is-not-your-friend
ArmenAg(2018-11-26 08:26:15):@linkerzhang @houseroad 

Should we change the name from `OpAnnotation` to something else?
ArmenAg(2018-11-28 18:10:26):@houseroad Any updates on this PR?
spandantiwari(2018-12-01 02:06:19):This looks like a better way to manage optimization passes. 

Since it is possible to add multiple annotations to an op, I like the name 'op_annotation', as compared to something else like 'op_category' or 'op_kind', which have connotations of mutual exclusivity.
ArmenAg(2018-12-02 03:20:43):I agree with @spandantiwari  on the naming. We should keep it OpAnnotation. This type of naming is also used in other compilers (e.g. [clang](https://clang-analyzer.llvm.org/annotations.html)).
ArmenAg(2018-12-03 07:00:50):@houseroad Great. I'll polish up my code and add some comments tomorrow. Do we have any other documentation that will need updating with respect to OpSchema?
houseroad(2018-12-03 07:03:25):According to https://github.com/onnx/onnx/tree/master/docs, we probably have no doc describing how our schema works. Feel free to add one to cover general schema, or specific to the op annotation. :-)
ArmenAg(2018-12-04 07:47:23):@houseroad Sorry, this week is incredibly busy for me. I definitely won't be able to work on this today. Hopefully I'll have some time towards the end of the week to finish this.
houseroad(2018-12-04 08:29:31):@ArmenAg no worries at all.
houseroad(2018-12-10 21:25:18):In general, using clang-format (other tools) to format the changes may be a good idea.
ArmenAg(2018-12-10 21:29:05):@houseroad Yea, I'm using the default formatter in vs-code which I think is VS intellisense. I'll set up my environment to use the .clang-format file provided by onnx.
ArmenAg(2018-12-10 23:30:10):> Thanks for the effort. I am wondering whether we should make the relation between different annotations more clear. for example, whether ElementwiseStrictMonotonicIncreasing should be a subclass of ElementwiseWeakMonotonicIncreasing

@houseroad I agree with you that there is a hierarchy in the annotations which is what inspired the OpAnnotation class which contain's this hierarchy. Furthermore this is a good reason to keep 
`OpSchema& OpSchema::AddOpAnnotation(std::shared_ptr<OpAnnotation> annotation)` in case we want to introduce more complicated hierarchies in the future, or allow OpAnnotation to contain values itself (e.g. Sigmoid is InLimitBoundedElementWise(min=0, max=1)).
ArmenAg(2018-12-13 21:39:16):@houseroad Any updates?
houseroad(2018-12-17 18:29:24):Overall looks good to me. The only question is whether we should make ElementwiseStrictMonotonicIncreasingOpAnn a subclass of ElementwiseWeakMonotonicIncreasingOpAnn? Since if it is strict monotonic, it must be weak monotonic, too. Think about how we will use operator annotations.
ArmenAg(2019-01-03 07:56:06):@houseroad

Apologies for not working on this for the last couple of weeks. I've left Microsoft and will be joining Facebook starting next week and in between was vacationing. 

In terms of introducing inheritance between the OpAnnotation classes, I think we should shy away from this. Since it's possible to have multiple super classes per OpAnnotation we would have to rely on multiple inheritance to stay consistent. I believe the way we introduced multiple flags per OpAnnotation is the cleanest. 
houseroad(2019-01-04 06:54:28):@ArmenAg welcome, and no worries, I will do some check later this week. If it's fine, I will merge it.
linkerzhang(2019-01-17 18:32:52):Thank you very much!

I'm suggesting that we'll need a more comprehensive proposal and review meeting on op annotations, and it's also a good timing to discuss moving non-standard related codes/scripts out of onnx repo to make onnx standard development faster.
ArmenAg(2019-02-13 06:07:42):@houseroad @linkerzhang Any updates on this?
houseroad(2019-02-13 06:10:22):Overall I think this is a good move. Let's merge for now. We can discuss for the future later. 
linkerzhang(2019-02-13 17:42:36):Oh no, I was assuming that there should be some overall discussion on how to standardizing the annotation stuff in ONNX. @houseroad , I'll revert this change and let's discuss.
houseroad(2018-12-03 06:39:01):How about enum class?
houseroad(2018-12-03 06:45:33):two return?
houseroad(2018-12-03 06:45:53):Do we really need this?
houseroad(2018-12-03 06:46:39):Add some comments about the purpose of OpAnnotation
houseroad(2018-12-03 06:47:07):Also, could you add comments for each item to explain the meaning?
ArmenAg(2018-12-05 21:16:17):Done.
ArmenAg(2018-12-05 21:17:42):I think we should keep this. What I was envisioning is that if you have a some family of operators that contain a lot of annotations. You would create that a new OpAnnotation class and pass that instance of that class in.
ArmenAg(2018-12-05 21:18:45):Done.
ArmenAg(2018-12-05 21:19:11):What do you mean? What should we do in change in this function?
ArmenAg(2018-12-06 19:47:39):Also in the future I was thinking OpAnnotation could itself contain attributes. For example if the OpAnnotationFlag is Bounded, OpAnnotation would have a min/max attribute.
houseroad(2018-12-10 19:55:25):We can add a link here: https://github.com/onnx/onnx/blob/master/docs/ShapeInference.md
houseroad(2018-12-10 19:56:58):formatting?
houseroad(2018-12-10 19:58:30):Question: is ElementwiseStrictMonotonicIncreasing ElementwiseWeakMonotonicIncreasing?
houseroad(2018-12-10 20:05:45):newline here
houseroad(2018-12-10 20:07:19):The following instruction is also a `return`
ArmenAg(2018-12-10 21:22:59):Oh sorry about that. I didn't see the `return *this` in the github view.
ArmenAg(2018-12-10 21:25:48):They're slightly different:
- ElementwiseStrictMonotonicIncreasing :  ` op(x) > op(y) iff x > y`
- ElementwiseWeakMonotonicIncreasing: `op(x) >= op(y) iff x > y`

The difference is `>` vs `>=`.
houseroad(2018-12-14 05:19:43):Yeah, actually, I would like to ask whether we should make ElementwiseStrictMonotonicIncreasingOpAnn a subclass of ElementwiseWeakMonotonicIncreasingOpAnn? Since if you are strict, you must be weak too.
jspisak(2018-11-27 04:07:40):@houseroad - looks like you need to rebase. I approved if you want to rebase and merge. 
houseroad(2018-11-27 05:56:15):@jspisak, no worries, your approval will work even after the rebase/merge :-)
gramalingam(2018-12-04 19:59:11):The old version is opset 8. (Yes, some of variables in the older version said "ver1" since it was the first version, but it was opset 8.)
houseroad(2018-12-04 20:00:17):@gramalingam yep, do you mind to clean the code (changing variable name) a bit?
gramalingam(2018-12-04 21:02:18):@houseroad, changed it
pranavsharma(2018-11-28 09:12:08):@linkerzhang @gramalingam @wschin can you take a look? thanks!
pranavsharma(2018-11-29 07:00:09):Not obvious why some of the build jobs are failing.
pranavsharma(2018-11-29 22:59:58):> Not obvious why some of the build jobs are failing.

@gramalingam @linkerzhang do any of you know what's wrong with the partial build failures? doesn't seem to be obvious.
gramalingam(2018-11-29 23:03:56):I think you need to regenerate test-coverage data when you add tests. https://github.com/onnx/onnx/blob/master/tools/update_doc.sh is a useful reference to the steps involved.
pranavsharma(2018-11-30 01:27:37):I ran the following command after all my changes and got this result. Not sure why the CI build is still failing.

python setup.py develop && python onnx/backend/test/cmd_tools.py generate-data && python onnx/backend/test/stat_coverage.py && python onnx/defs/gen_doc.py && pytest

============ 777 passed, 496 skipped, 1 warnings in 38.26 seconds =============
gramalingam(2018-11-30 01:36:59):Hi Pranav, I think you are running into the same kind of problems I ran into recently :-) .
(a) After test data generation, just commit the files corresponding to the NaN operator. There are some platform incompatibilities due to which I also see changes in the test data for other ops (e.g., dynamic-slice), but I don't commit them.
(b) I think you might have previously cached model files in your user-root-directory/.onnx/models, which might be affecting the test-coverage results. Deleting those will force it to get the correct current models. (But this download takes several hours! If we know which model has been updated, we might be able to do it more efficiently.)
pranavsharma(2018-12-04 03:50:07):> Hi Pranav, I think you are running into the same kind of problems I ran into recently :-) .
> (a) After test data generation, just commit the files corresponding to the NaN operator. There are some platform incompatibilities due to which I also see changes in the test data for other ops (e.g., dynamic-slice), but I don't commit them.
> (b) I think you might have previously cached model files in your user-root-directory/.onnx/models, which might be affecting the test-coverage results. Deleting those will force it to get the correct current models. (But this download takes several hours! If we know which model has been updated, we might be able to do it more efficiently.)

thanks. let me try one more time. if it doesn't work this time, i'll abandon this and create a new PR.
linkerzhang(2018-12-04 16:55:44):@houseroad @bddppq any comment on this PR please?
pranavsharma(2018-12-05 18:16:57):@gramalingam @linkerzhang this can now be merged.
gramalingam(2018-11-28 20:22:05):Can integral types be NaN ? I thought it was mostly for floats?
pranavsharma(2018-11-28 21:25:16):you're right. fixed.
gramalingam(2018-11-28 21:28:32):The dtype attribute looks like a copy-paste error. "Numeric tensor" also seems inappropriate with the type change.
pranavsharma(2018-11-29 06:59:14):fixed.
linkerzhang(2018-11-29 17:45:20):please add some test cases for this operator. Refer to https://github.com/onnx/onnx/tree/master/onnx/backend/test/case/node for examples.
linkerzhang(2018-11-29 17:45:46):inputs are limited to float/double?
gramalingam(2018-11-29 18:00:34):Because NaN does not apply to integral types. (Or, are you asking about float16?)
pranavsharma(2018-11-29 18:47:12):Thinking more about it. The op returns a boolean tensor if a value is NAN or not. So even if we have integers we'll simply return false for them. So we don't necessarily have to constrain to only floating points. It probably make sense to support all numeric types. Thoughts?
gramalingam(2018-11-29 19:11:50):But why? It seems strange to support it for int, if it is just going to return false always. Is there some use case? Otherwise, it seems to suggest that there is a way to represent NaN in the int type, which is likely to cause confusion. The real issue lies in the original (preceding) int ops that would have produced a NaN (if it could be represented), may be via overflow or divide-by-zero … so a proper solution would require extending the int ops (like multiplication or division) to return a value to indicate that this happened.
pranavsharma(2018-11-29 20:03:49):i had one. forgot to check it in. 
pranavsharma(2018-11-29 20:04:46):Ok, let's keep it float only.
zrphercule(2018-12-04 19:34:55):@linkerzhang Any comments? Thanks!
gramalingam(2018-12-04 19:45:03):I notice that numpy and tensorflow documentation for sign differ in the treatment of NaN. May be we should start clarifying the spec for NaN (for all ops, not just this op … this is a more general issue, especially as I see another PR for adding a IsNaN op).
gramalingam(2018-11-29 18:09:41):The output type should be int32
gramalingam(2018-11-29 18:10:17):Documentation? The documentation needs to be generated also.
zrphercule(2018-11-29 18:38:17):lol yeah, I havent delete all debugging info either, just wanna ask for you and @linkerzhang 's opinion on this Op.
zrphercule(2018-11-29 18:39:31):I think it is not neccesary to be a int32? Or we can set an optional attribute?
houseroad(2018-11-29 18:49:38):I think output tensor should have the same type as the input tensor. At least both tensorflow and pytorch follow this rule.
gramalingam(2018-11-29 19:01:37):Okay, then the shape inference function below is incorrect. I was going by that.
zrphercule(2018-11-29 20:41:46):@gramalingam Wonder why it is incorrect? The output here should use the same shape&type as the input...
gramalingam(2018-11-29 20:49:39):But it says "updateOutputElemType(ctx, 0, TensorProto::INT32);". I think the whole thing can be replaced by .TypeAndShapeInferenceFunction(propagateShapeAndTypeFromFirstInput).
zrphercule(2018-11-29 20:56:40):oh damn I didnt push this. Thanks!
houseroad(2018-12-04 18:19:13):Let's make it clear here: the output has the same shape and type as the input. Although this can be inferred from shape inference function.
zrphercule(2018-12-04 19:04:48):sure
CLAassistant(2018-11-29 06:12:44):[![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/onnx/onnx?pullRequest=1659) <br/>Thank you for your submission! We really appreciate it. Like many open source projects, we ask that you sign our [Contributor License Agreement](https://cla-assistant.io/onnx/onnx?pullRequest=1659) before we can accept your contribution.<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/onnx/onnx?pullRequest=1659) it.</sub>
CLAassistant(2018-11-29 07:07:55):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=1660) <br/>All committers have signed the CLA.
peteryang1(2018-11-29 08:37:43):I solved the shape inference of convtranspose when the node is like bolow:
![1](https://user-images.githubusercontent.com/25981102/49209234-e5eec380-f3f4-11e8-9ffc-2660bec9b419.png)
in this case, the shape of the output is not given, we should determine it from pad and other attributes.

hope to be reviewed, thanks!
peteryang1(2018-12-10 08:34:22):@houseroad I have added tests already.
gramalingam(2018-12-12 00:18:06):The input_shape may have a symbolic dim and not a concrete dim_value. There needs to be a check if input_shape.dim(i+2).has_dim_value() before using that dim_value().
gramalingam(2018-12-12 00:19:27):You can replace these lines with "int64_t group = getAttribute(ctx, "group", 1)".
gramalingam(2018-12-12 00:23:52):There seems to be a similar issue even in the existing code (old line 817, new line 831) above.
gramalingam(2019-01-04 22:04:04):Just "final_output_shape->add_dim()" would be correct. We can't set the dim_param to be the same as input_shape.dim(i + 2).dim_param() (say "N") because that implies that these two dimensions are equal, which is not the case.
gramalingam(2019-01-04 22:06:33):Just omit the if condition "input_shape.dim(i + 2).has_dim_param()" above … the same solution is valid for both cases. If neither dim_value nor dim_param is set, it is treated as an unknown dimension. (A dim_param is useful primarily when there are two unknown dimensions of the same value.)
peteryang1(2019-01-07 05:41:23):I have resolved this problem.
But I found that seems other ops also have this problem, because on one of my model, I set my input's height and width "*", after the shape inference, the height and width of the output is still "*"while it supposed to be different from the input, so I infer that param means dim_value unknown. 
bddppq(2018-11-30 08:14:59):error message:
```
/var/lib/jenkins/workspace/third_party/onnx/onnx/shape_inference/implementation.cc:207:20: error: converting to 'const std::unordered_map<std::basic_string<char>, onnx_c2::TypeProto*>' from initializer list would use explicit constructor 'std::unordered_map<_Key, _Tp, _Hash, _Pred, _Alloc>::unordered_map(std::unordered_map<_Key, _Tp, _Hash, _Pred, _Alloc>::size_type, const hasher&, const key_equal&, const allocator_type&) [with _Key = std::basic_string<char>; _Tp = onnx_c2::TypeProto*; _Hash = std::hash<std::basic_string<char> >; _Pred = std::equal_to<std::basic_string<char> >; _Alloc = std::allocator<std::pair<const std::basic_string<char>, onnx_c2::TypeProto*> >; std::unordered_map<_Key, _Tp, _Hash, _Pred, _Alloc>::size_type = unsigned int; std::unordered_map<_Key, _Tp, _Hash, _Pred, _Alloc>::hasher = std::hash<std::basic_string<char> >; std::unordered_map<_Key, _Tp, _Hash, _Pred, _Alloc>::key_equal = std::equal_to<std::basic_string<char> >; std::unordered_map<_Key, _Tp, _Hash, _Pred, _Alloc>::allocator_type = std::allocator<std::pair<const std::basic_string<char>, onnx_c2::TypeProto*> >]'
```
explanation: https://stackoverflow.com/a/26949099/2143581
bddppq(2018-11-30 08:13:16):I **think** you can instead do `std::unordered_map<std::string, TypeProto*>{}`
bddppq(2018-11-30 08:15:29):const
houseroad(2018-11-30 20:06:58):Please update the doc as well. Thanks
gramalingam(2019-01-03 22:39:22):Hi, I updated the documentation to be consistent with the checker. However, there are some tricky issues (non-uniform handling of name reuse/clash), so I would like to bring them up. Please clarify if the existing implementation is intentional or whether it needs to be fixed.

A name X defined in an outer scope cannot be reused for a different tensor in a subgraph. However, this is permitted for the subgraph inputs. Is this intentional? Why this non-uniform treatment?
gramalingam(2019-01-09 00:14:00):I want to follow up on the earlier discussion. Consider what the current checker does with the following example of reuse of names defined in an outer graph in a subgraph (for a different value):

graph {
   X1 = .. 
   X2 = ..
   … control-flow-op
      sub-graph (X1) { // permitted by checker
         X2 = .. // not permitted by checker
         X3 = .. // permitted by checker
      }
  X3 = ..
}

This looks a bit odd. Why do we not want to permit the redefinition of X2 above? One possible explanation is that we want to avoid the following scenario:

graph () {
   Y = .. // outer scope Y
   .. sub-graph {
         Z = Y; // reference to outer scope Y
         Y = ..; // redefinition of Y
   }
}

It seems a reasonable goal to avoid a case where a name Y in a particular scope S is used to both refer to an outer scope variable (that is, defined outside scope S) as well as a variable define in scope S. (A similar situation is when the reference to the outer scope Y occurs in a nested sub-scope within S.)

Suggested solution: we forbid redefinitions of names defined in an outer scope only in the above situation. In particular, we allow the redefinition of X2 in the original example above. 

Does this seem reasonable? This would also explain why we allow the redefinition of X1 in the first example. 

Note that forbidding the redefinition of X2 has the following consequence: consider a graph G that is completely self-contained (that is, it has no references to an undefined name). Ideally, we should be allowed to use it as a sub-graph anywhere without worrying if it uses a name that conflicts with an outer-scope name. The existing scheme doesn't give us this desirable compositionality property. Fixing it as suggested above will give us this property as well.

Please note that this fix will make the checker more complex. I can look into how to update the checker if this proposal looks reasonable.

linkerzhang(2018-12-04 16:46:44):Confirm: does this mean all node input/output names across main graph and subgraphs have to be unique? if this is the case, then what's the "namespace" here mean please? With this clarification, I assume there's only ONE namespace in a model now. Every name has to be unique in this ONE namespace. Right?
gramalingam(2018-12-04 21:07:49):We have namespaces as described in the table just below (node names are in a different namespace from tensor/value names, for example). We can think of different graphs as a different dimension: let us call them namescopes. So, tensor names must be unique across a graph and a subgraph (one namescope). But a tensor name can be the same as a node name (different namespaces).
houseroad(2018-12-04 19:21:08):Looks good
houseroad(2018-12-05 22:24:18):Add a link of https://en.wikipedia.org/wiki/Error_function here?
zrphercule(2018-12-05 23:19:17):I will update the docs later for not making so many conflicts.
ArmenAg(2018-12-06 01:46:49):This is more of a philosophical question, but at what point should we prefer Functions to adding new Op's? It seems like it's going to become tougher and tougher for back-ends to be ONNX compliant if we have an ever-growing addition of ops.
houseroad(2018-12-06 01:52:16):According to the doc https://github.com/onnx/onnx/blob/master/docs/AddNewOp.md, if CELU can be added as simple combination, we should use function, such as LE => OR(LT, EQ). 
ArmenAg(2018-12-06 01:55:40):So in this case CELU can be implemented using max/min/exp/mul/sub. So should it be a Function?
houseroad(2018-12-06 02:15:15):@ArmenAg in general it depends, if it is (relatively) complicated, and in deep learning frameworks we should implement it as a separate operator. For Celu, personally, I prefer to add it as an operator, not function... because the decomposition is not straightforward. Also, for functions, I think it should not conflict with operator, actually, it can be optional attribute as part of the operator schema, just like the operator attribute.
zrphercule(2018-12-10 19:48:28):@gramalingam @linkerzhang Any comments? Thanks!
zrphercule(2019-01-02 19:12:46):@ArmenAg @spandantiwari @linkerzhang  How do you like if we should merge this op? Thanks!
faxu(2019-04-19 17:48:03):@zrphercule : is this op still needed? ONNX 1.5 will release soon - are you targeting to include this as part of opset10?
zrphercule(2019-04-19 18:00:07):@faxu Thanks for your review. I think it is still needed, I will update it to version 10 next week with the operator LE and GE. Sounds good?
faxu(2019-04-19 18:05:39):@zr

> @faxu Thanks for your review. I think it is still needed, I will update it to version 10 next week with the operator LE and GE. Sounds good?

ONNX 1.5 release branch is planned to be cut by the end of today. If you can only get to it next week, it'll be released as part of opset 11 in ONNX 1.6 most likely. 
zrphercule(2019-04-19 20:19:22):> @zr
> 
> > @faxu Thanks for your review. I think it is still needed, I will update it to version 10 next week with the operator LE and GE. Sounds good?
> 
> ONNX 1.5 release branch is planned to be cut by the end of today. If you can only get to it next week, it'll be released as part of opset 11 in ONNX 1.6 most likely.

Then I am okay with it to leave it to next version. Thanks!
CLAassistant(2019-07-24 00:57:18):[![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/onnx/onnx?pullRequest=1676) <br/>Thank you for your submission! We really appreciate it. Like many open source projects, we ask that you all sign our [Contributor License Agreement](https://cla-assistant.io/onnx/onnx?pullRequest=1676) before we can accept your contribution.<br/>**1** out of **2** committers have signed the CLA.<br/><br/>:white_check_mark: houseroad<br/>:x: zrphercule<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/onnx/onnx?pullRequest=1676) it.</sub>
faxu(2019-08-19 16:54:57):@zrphercule is this change still needed? If so, can you please sign the CLA so it can be included in the 1.6 release?
Data-drone(2019-09-23 03:35:07):was this added in the end?
zrphercule(2019-09-27 22:27:38):> was this added in the end?

Not really since this pr is out-dated. Feel free to add it into modern onnx ;)
spandantiwari(2019-01-10 20:51:30):It will be nice to have a shape inference test also.
faxu(2019-04-18 23:56:37):Is this for opset10? Opset9 released with ONNX 1.4
zrphercule(2018-12-07 22:17:36):I will generate and upload docs once it is approved.
houseroad(2018-12-10 19:44:06):@linkerzhang @ebarsoum @spandantiwari @pk-g do you mind to take a look?
spandantiwari(2018-12-11 07:12:50):@zrphercule @houseroad - The spec looks good to me. 

I do have a high-level question. Do we want to introduce these as separate ops given that we can obtain the effect by simple negation of existing ops. E.g. `LessOrEqual` can be implemented by `Greater` followed by `Not`. I feel that it is OK to include them in the spec because they are common logical operators and have clear design. At the same time, such additions add to the number of supported ops. 

For e.g., we have an `Equal` op and a `Not` op. Does the spec have place for a `NotEqual` op?
shinh(2018-12-11 07:37:07):`Greater` and `Not` is wrong for NaNs?

```
>>> nan = np.array(float('nan'))
>>> nan <= nan
False
>>> not (nan > nan)
True
```
zrphercule(2018-12-11 18:57:50):@spandantiwari Thanks for your review! I have two things would like to clear:

1. For these two ops, we have a need to add them because many backend frameworks have them in their op list, and may have some special optimize for them. Also, as @shinh mentioned, sometimes "not greater" is not fully equal to "less or equal" , e.g. the two numbers are not comparable. As for NotEqual, if it meet the these two conditions, I guess it is also okay to have it in ONNX?

2. I think you mentioned a very good point: when should we add one op as a "function", and when should we add it as a normal operator. @houseroad  @bddppq  and I think it might be better to give both options to frameworks and let themselves decide. 
For example, in <= case, we create a standard op LessOrEqual, meanwhile we add an optional attribute "isFunc" in it.
If isFunc is given, then we will treat LessOrEqual as a function instead of a standard operator, and dismantle it to be "Not Greater" in our exported model. This solution is more flexible, we can have a standard op while have one or even more pre-setting ways of translating it to a function. We can also let backend decide how to dismantle an op on their own by providing a subgraph as optional Attribute. Right now, our function design uses a totally different workflow, and one op can only be a function OR a normal op.

What is your oppinion? Thanks!
spandantiwari(2018-12-11 21:32:10):@zrphercule - I understand and agree with your points about being able to optimize better when `LessOrEqual` is an op in itself and also about it producing different values for edge cases with `NaNs`. Regarding the second point - while I agree that we should let the frameworks, my point was about the number of new ops that this introduces (that's why I included the example for `NotEqual`) in the ONNX spec. Let's say that we let the framework decide which way to export, the ONNX spec still needs to have the `LessOrEqual` op in case the framework wants to export it. And this increases the number of ops. 

In this particular case I am fine with including these ops.But I have heard concerns from others about increasing number ops and that's why I would love to hear their thoughts. 
zrphercule(2018-12-12 00:23:33):@spandantiwari  Thanks for you agree on these ops. As for the concern of number of ops, I think as the coveraged op list of ONNX expanded, it is impossible the number of ops in ONNX not increased. Even if we add op, e.g., NotEqual, as a function, still it is going to show up in our operator list, and have almost all the side effect of adding an operator. 
I do agree we should be very careful and selective when adding new ops, however once we decide to add one, adding it as a function OR op seems not to be better than adding it as an op with fucntion support.

prasanthpul(2018-12-12 00:47:07):One benefit of adding as a function is that runtimes can run models that use it right away, even before they have implemented an optimized implementation of that function
spandantiwari(2018-12-11 06:59:56):Maybe consider adding type and shape inference function as they are increasingly being used by backends for model validation.
spandantiwari(2018-12-11 07:00:37):line break - intentional?
spandantiwari(2018-12-11 07:02:02):The comment does not match the types allowed. Also nit: spelling for "constrains" .
spandantiwari(2018-12-11 07:02:20):Same comment as above. 
spandantiwari(2018-12-11 07:02:34):nit: spelling
spandantiwari(2018-12-11 07:04:38):Great to see both node and model tests for these ops. :-)
spandantiwari(2018-12-11 07:05:44):I suggest slight rephrasing: "Returns tensor resulting from..."
zrphercule(2018-12-11 18:31:43):Yeah, and I just noticed the schema of Greater and Less were also wrong here.
zrphercule(2018-12-11 19:07:18):ok sure, this will update the docs of all logical ops, and I think it is fine =w=
CLAassistant(2018-12-14 19:50:17):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=1694) <br/>All committers have signed the CLA.
linkerzhang(2018-12-19 15:56:10):LGTM. Thank you very much! @HectorSVC

please also kindly add shape inference test. https://github.com/onnx/onnx/blob/master/onnx/test/shape_inference_test.py is the file containing shape inference test for your reference.
houseroad(2018-12-19 21:26:36):@HectorSVC please resubmit the PR, since it requires further review from our side as well, thanks
linkerzhang(2018-12-17 20:08:48):put default value here please.
linkerzhang(2018-12-17 20:10:41):output type.
linkerzhang(2018-12-17 20:10:57):An integer scalar?
linkerzhang(2018-12-17 20:12:06):you may put "tensor(float)" here directly as T1 defined below only has float.
linkerzhang(2018-12-17 20:12:12):Same for the T2 below.
HectorSVC(2018-12-17 22:36:47):I didn't find a overload Attr which can accept the (..., bool required, T defaultvalue)

---
In reply to: [242298590](https://github.com/onnx/onnx/pull/1695#discussion_r242298590) [](ancestors = 242298590)
linkerzhang(2018-12-18 20:16:30):minor: no need inferring output types since the two outputs have only one type specified in their constraints, so this function may only focus on shape inference.
linkerzhang(2018-12-18 20:18:23):we may want to clean the shape before adding a dim.
raymondxyang(2018-12-19 22:53:06):Reopen after fix
bddppq(2018-12-19 21:19:55):cc @linkerzhang @raymondxyang @HectorSVC 
HectorSVC(2018-12-19 22:04:22):@bddppq @linkerzhang 
HectorSVC(2018-12-19 23:07:39):anyway to re-run the CircleCI? I even close and re-open the PR, still can't re-run it. From the log "Connection reset by peer". What's wrong with the CI?
linkerzhang(2018-12-19 23:58:17):@houseroad please help to review this PR. Thanks!
HectorSVC(2019-04-02 23:45:15):I'll update this according our latest changes.
HectorSVC(2019-04-04 18:57:31):@houseroad  Could you review the updated schema? We have a model converted from Keras. Need to get this op in to support that model.
linkerzhang(2019-04-11 23:57:34):let's also create a PR to add the model, so that the usage of this op is clear.
guschmue(2019-04-17 16:28:20):We have a few models working using this op:
From [this list](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md) ssd_mobilenet_v1_coco_2018_01_28 is working (we think a good part of the list will work).
There is a pytorch implementation for the ssd-resnet34 (needed for mlperf) [here](https://github.com/BowenBao/inference/tree/master/cloud/single_stage_detector/pytorch#6-onnx) that is working.
And this [tensorflow version of yolov3](https://github.com/YunYang1994/tensorflow-yolov3)  works as well.

jywu-msft(2019-04-17 19:44:01):@houseroad , given @guschmue 's list, it seems like we can enable many object detection models in onnx with this change. Can you please review and approve if no objections?
HectorSVC(2019-04-18 18:32:04):@houseroad Any further comments?
ebarsoum(2019-04-18 22:04:07):@houseroad  anything else missing? If not can we merge it?
HectorSVC(2019-04-18 22:19:39):Why I still can't merge it? Need approval from @bddppq ?
houseroad(2019-04-18 05:47:40):In ONNX, we use int64 as indices/sizes, shall we align with other ops as well here?
houseroad(2019-04-18 05:47:49):ditto
HectorSVC(2019-04-18 17:19:20):updated

---
In reply to: [276524091](https://github.com/onnx/onnx/pull/1703#discussion_r276524091) [](ancestors = 276524091)
HectorSVC(2019-04-18 17:19:32):updated

---
In reply to: [276524111](https://github.com/onnx/onnx/pull/1703#discussion_r276524111) [](ancestors = 276524111)
raymondxyang(2019-01-09 23:43:39):@houseroad Reverted the format changes in def.cc
houseroad(2019-01-18 19:43:50):Please resolve the conflict.
magneter(2019-10-13 13:25:20):Hi ,a similar problem occured to me .
It tips : Unsupported ONNX ops of type: Cast, onnx version is onnx-1.6.0.
It seems ,1.6.0 did not support Cast op.
How Could I solve this ?
wschin(2019-01-03 00:15:28):Do you have tests for inf and nan? The spec also needs to mention the string literals of them.
linkerzhang(2019-01-08 21:56:42):bool was removed by mistake?
raymondxyang(2019-01-08 22:14:02):oops... fixed
wschin(2019-01-09 01:21:59):There are two `INF`, positive and negative. Which one does `INF` mean? If `INF` stands for positive inf, can we do `-INF` for negative inf? In addition, due to the existence of `-INF`, would it be nice to support `+INF`?
wschin(2019-01-09 01:26:59):What do you mean by `unconvertable`? Is "3.14e23" convertible? It's ok to mention a conversion function suggested to make spec less ambiguous.
wschin(2019-01-09 01:27:36):Please add space to `cast.Strictly`. Maybe
```suggestion
<dd>The data type to which the elements of the input tensor are cast. This field must be one of the types from DataType enum in TensorProto</dd>
```
houseroad(2019-01-09 02:39:34):You should explicitly say when converting `String` tensor to numeric type of tensors. Also you should say which numeric representation supported.
raymondxyang(2019-01-09 21:49:32):Sure. Will clarify that
wschin(2019-01-10 00:06:16):```suggestion
  Casting from string tensor in plain (e.g., "3.14" and "1000") and scientific numeric representations
```
wschin(2019-01-10 00:06:42):```suggestion
  "+INF" (and "INF"), "-INF", and "NaN" are positive infinity, negative infinity, and not-a-number, respectively.
```
wschin(2019-01-10 00:07:35):Let's make it more different than scientific representation.
```suggestion
  to string tensors, plain floating-point representation (such as "314.15926") would be used. Converting non-numerical-literal string such as "Hello World!" is an undefined behavior.
```
wschin(2019-01-10 00:16:59):Conversion from a numerical type to any numerical type is always allowed. User must be aware of precision loss and value change caused by range difference between two types. For example, a 64-bit float 3.1415926459 may be round to a 32-bit float 3.141592. Similarly, converting an integer 36 to Boolean may produce 1 because we truncate bits which can't be stored in the targeted type.
raymondxyang(2019-01-10 00:17:03):ooops.. clicked the auto correction.. should do fix in .cc source
wschin(2019-01-10 04:12:06):This test needs to contain all special literal numbers. You already have "NaN" and "INF" so only "-INF" and "+INF" are missing.
houseroad(2019-01-14 18:32:15):I tried this in numpy, `100.5` to int will cause error. So I would suggest throw the error as well. If user really want to do so, they can convert to float first, then convert to int.
raymondxyang(2019-01-14 18:47:32):+@wschin for discussion
wschin(2019-01-15 23:46:31):I guess users specify float-to-int conversion only when they really want (if not by rare accident).
houseroad(2019-01-16 06:11:07):So to prevent such accident, let's throw error in such cases.
wschin(2019-01-16 17:09:22):No I mean this is an operator, not a programming language. In addition, what's the definition of throwing in an IR? You can have at most an undefined behavior because exception is runtime behavior which can't be controlled by IR.
raymondxyang(2019-01-16 20:21:29):Discussed offline. Define this rare case (string representing 'float' -> int) as undefined behavior. 
bddppq(2018-12-20 01:00:56):@houseroad why? :-) 
houseroad(2018-12-20 01:50:55):There is a tiny possibility, we may get have more than 100 attributes in some message. To me, (1000, max) or (100, 1000) sounds better. Although, there is no big difference.
houseroad(2019-01-19 04:18:13):@linkerzhang any objection?
shinh(2019-01-24 01:52:02):This is great! Any reason ValueInfoProto doesn't have it? For example, if ValueInfoProto is extensible, I'd consider using the room to store information about devices where a value will be placed. Though we can also store such information to NodeProto or TypeProto, I'd imagine there could be some information which ValueInfoProto is the most suitable place.
prasanthpul(2019-01-24 05:58:56):It's not clear how this increases interoperability. The extensions would not be centrally managed so they may conflict and could be interpreted differently by different backends/tools/etc.

Which specific annotations/extensions you are looking to add via these fields? 

@shinh has an interesting case for device info; if that is something of interest to others, we should see how to add a field for that specific annotation.
shinh(2019-01-24 06:25:04):One more possible usecase: If this is merged, I'd use the extension of NodeProto to store debug information. For example, currently, my tool abuse `doc_string` field to store line number and filename of Python code when an ONNX file is exported, and my ONNX runner may show these info to make debug easier. It's fine other ONNX runner ignores it completely.
bddppq(2019-01-24 21:35:56):@prasanthpul 
> The extensions would not be centrally managed so they may conflict and could be interpreted differently by different backends/tools/etc.

I think the policy is the same as custom operator domain. 

> Which specific annotations/extensions you are looking to add via these fields?

My specific use case is similar to @shinh's, i.e. putting some device information into the model.

> if that is something of interest to others, we should see how to add a field for that specific annotation.

I believe runtime is a specific thing that we want to avoid adding to onnx core since the very beginning.

prasanthpul(2019-01-25 00:26:20):@bddppq It's different than custom op domains. With custom op domains, if you use the naming format (reverse dns) correctly, conflicts are minimized. Here in this case, it seems like there could be a field 101 used by both Framework A and B for completely different things and you wouldn't know until you tried to execute a model and it gets interpretted & executed in an unexpected way.
bddppq(2019-01-25 03:26:28):@prasanthpul Same naming mechanism as the ops domain could be used for the naming of the extended fields.
linkerzhang(2019-01-25 17:50:08):I'm thinking in a little bit different way. Looks to me this kind of extension is not part of standard. Shall we make them in each framework locally? This reminds me of reserving ids for ONNX standard for each message actually.
linkerzhang(2019-01-25 18:03:35):@shinh  dumb question please. I don't quite understand why using model file to store debugging information. Are you going to serializing those debugging info into a model file (*.onnx)? Thanks!
shinh(2019-01-27 03:46:25):@linkerzhang Sorry, but I'm not sure if I understand your question, so let me describe my usecase in detail. Let's assume we have the following Python code

```
1: def foo(x, y, z):
2:   a = x + y
3:   return a + z
```

and this code was exported as an ONNX model file, which have two `Add` ops. If a user passes a tensor with wrong shape as `z` to an ONNX importer, the ONNX importer will fail due to dimension mismatch. However, if the user don't know much about ONNX, the user may not see whether line 2 or 3 is wrong. I'm thinking of serializing information like "foo.py:3" to ops so an ONNX importer can show Python-level errors to end-users.

We could put this kind of information in a separate file, though.

linkerzhang(2019-01-28 17:42:31):@shinh Thank you very much for detail clarification. I understand the use case, and I believe this kind of debugging information is very useful, but I personally don't think it should be serialized in model file, but in a separate file (debugging log, say) as you mentioned. 
houseroad(2019-01-28 22:23:09):@linkerzhang I think we should add such supports, so users can easily do experiments with their new ideas. It won't bring us any disadvantage, instead, it will make users' life much easier if the standard onnx cannot meet all their requirements. ONNX standard iterates slowly (at least 3 month per release). When user thinks the design of their new feature is mature enough , they can propose/promote it to an official feature to the core ONNX standard.
bddppq(2019-02-15 01:41:41):@linkerzhang This change is exactly enabling partners to do extension outside of onnx. If we don't provide a way to do extension then people don't have a choice but can only force adding special features into onnx core or hard fork onnx which will be even worse.
faxu(2019-04-19 17:50:22):@bddppq : is this change still needed? ONNX 1.5 will release very soon.
bddppq(2019-04-19 20:04:55):@faxu I still think this is a good change, but it should not block 1.5 release.
gramalingam(2019-01-11 22:17:35):Do you want this for TypeProto or Tensor? The title of the PR says TypeProto, but it is being added to Tensor.
bddppq(2019-01-11 22:20:01):Oh you are right, it should be `TypeProto.Tensor`
CLAassistant(2018-12-21 19:02:04):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=1707) <br/>All committers have signed the CLA.
pranavsharma(2018-12-22 03:17:13):Adding @wschin 
HectorSVC(2019-01-11 17:56:05):@wschin 
wschin(2019-01-18 17:07:28):Could we compose it using a function (and introduce bit-wise operators when needed)?
postrational(2019-08-22 14:06:48):Would it be possible to implement this as an ONNX `Function`? 
If not, then what primitive op are we missing?

houseroad(2019-04-02 16:51:07):At this moment, we can say that string must be encoded using utf-8. Since in ONNX, we only support utf-8.
houseroad(2019-01-23 01:55:59):@MrGeva do you have any concrete example for this op implementation in systems like mxnet, cntk, which use int64? I remember in Caffe2 the operator's implementation (not only the schema) is using floats to store the indices.
CLAassistant(2019-07-24 00:57:18):[![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/onnx/onnx?pullRequest=1713) <br/>Thank you for your submission! We really appreciate it. Like many open source projects, we ask that you sign our [Contributor License Agreement](https://cla-assistant.io/onnx/onnx?pullRequest=1713) before we can accept your contribution.<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/onnx/onnx?pullRequest=1713) it.</sub>
askhade(2021-10-26 21:18:12):Closing this PR since it is pretty old. I dont know of anyone else who has a similar request. Please reopen this PR with proper justification if this is still necessary. Thanks!
houseroad(2019-01-14 23:42:57):@kit1980 could you update the PR using https://github.com/onnx/onnx/blob/master/tools/update_doc.sh?
kit1980(2019-01-15 02:24:52):Fixed a typo that prevented example from showing in the docs.
Rebased on master.
kit1980(2019-01-16 19:00:00):Rebased on master and added mention of always 2D output.
wschin(2019-01-10 04:08:51):Would you mind to add an example here? The code used in numpy could be changed in the future.
kit1980(2019-01-15 02:23:14):Example now can be seen (in Operators.md).
houseroad(2019-01-16 06:09:32):Let's make it clear here the output is always a 2d tensor.
kit1980(2019-01-16 18:59:21):done
spandantiwari(2019-01-03 22:30:19):@houseroad @zrphercule - I see that PyTorch tests for zeros_like, ones_like, and full_like are failing. It is expected because these PT ops are exported using `ConstantLike`, which is being removed here. 

How should we handle this? Is there a way to disable the test so that this PR can make it through? I can then update the export of these ops and enable the tests here. 

Or do you think we should update the PT export first and then get the updated tests here? Not sure how much the lag will be in the latter case to get the updated backend here. We need to be mindful of the ONNX 1.4 release date which may be near.

zrphercule(2019-01-03 22:56:41):@spandantiwari Yes if we can fix pytorch side, the only question is in what level. If we only remove this constantlike op, then we can use constantofshape in oneslike\fulllike\zerolike symbolic; If we are going to remove all these XXXlike ops, it is going to be another story, we need to remove all related symbolic & tests.
spandantiwari(2019-01-04 01:04:32):@zrphercule  - that's a valid point and one that I have discussed with @linkerzhang and @gramalingam. @linkerzhang and @gramalingam can chime in too, but my understanding is that we agree on removing `ConstantLike` now that `ConstantOfShape` has been added. There are no separate ops for OnesLike and ZerosLike in ONNX. It makes sense to overhaul all the `*Like` generator ops, but that will have to be a separate PR, especially because those have been released previously (but not `ConstantLike`) and would need to be deprecated. 

On the PyTorch side, maybe we can start by moving `oneslike\fulllike\zerolike` to using `ConstantOfShape` in export. That way this PR will get unblocked.

It will be great if we can clean up these Constant* ops before the upcoming ONNX 1.4 release. 
houseroad(2019-01-04 18:16:20):@spandantiwari I had a discussion with @zrphercule in person. Yeah, we agree on that after adding ConstantOfShape, we should remove ConstantLike, since the functionality has overlap. Let's disable the ConstantLike test in PyTorch first (Rui is working on that, and will keep you updated), then we can push this PR in. After that, we can update PyTorch ONNX exporter. 

For other *Like ops, let's change them in the same way, but in different PRs.

Thanks for the push on this. :-)
spandantiwari(2019-01-04 18:59:39):@houseroad - that sounds like a good plan. 
@zrphercule - please let me know when the tests are disabled and we are ready to try pushing in this PR. Thanks!
zrphercule(2019-01-04 23:40:26):@spandantiwari https://github.com/pytorch/pytorch/pull/15740 The related pr has been landed. Please try to rerun the test and see if it has been skipped.
spandantiwari(2019-01-07 18:30:25):@zrphercule - thanks! Let me give it a shot.
spandantiwari(2019-01-07 21:45:00):@houseroad - updated based on your feedback. If all goes well, I will merge this.
spandantiwari(2019-01-07 23:12:04):@houseroad - Shall we merge?
houseroad(2019-01-07 23:18:58):Done :-)
spandantiwari(2019-01-07 23:28:51):@houseroad - Thanks!
houseroad(2019-01-07 19:08:51):also add int32 and remove uint8, uint16, and uint32? They are  rarely used in the real systems
houseroad(2019-01-07 19:14:27):The name of the `value` tensor (attribute) is confusing. Can we rename it to `value` or something else?
houseroad(2019-01-07 19:14:35):Same here
houseroad(2019-01-07 19:19:26):Nit: assert targetShapeInitializer's data_type is int64
houseroad(2019-01-07 19:34:40):Or just call the function, not creating the variable newdim to store the return value?
spandantiwari(2019-01-07 21:43:51):Agreed. Done.
spandantiwari(2019-01-07 21:44:03):Fixed.
spandantiwari(2019-01-07 21:44:09):Fixed.
linkerzhang(2019-01-08 02:28:31):please run gen_proto.py to generate onnx.proto, onnx.proto3, onnx-ml.proto, and onnx-ml.proto3 and loop them in your PR.
houseroad(2019-01-17 22:32:13):I will take care of the pytorch ci part.
gramalingam(2019-01-17 23:17:04):@houseroad thanks! I assume you are offering to take care of the CircleCI failure … if there is something that I need to do, please let me know. Thanks again.
houseroad(2019-01-17 23:18:25):Yeah, I mean the CricleCI, I will create a PR to update the expects in pytorch. I will do the review for this PR again soon.
linkerzhang(2019-01-19 02:40:33):@houseroad  ok to merge this one please? this PR also contains the change of bumping IR version. 
houseroad(2019-01-15 23:38:21):We should also tell users which are recommended to appear in the input list, for example, some weights may be overwritten. Some constants won't be changed for sure, they should not be included in the initializers.
gramalingam(2019-01-17 00:12:13):I added this explanation to IR.md, since that seemed like a good place for this.
gramalingam(2019-01-17 00:13:58):@houseroad I am in the middle of updating the IR_VERSION. It looks like I may have to regenerate the python notebooks in the onnx\examples folder since I am running into a failure there during testing. Please let me know how these files are generated or how to update them.
houseroad(2019-01-17 18:17:47):Please record the change in https://github.com/onnx/onnx/pull/1626
yuslepukhin(2019-01-09 19:17:07):Cc: @houseroad @HectorSVC @yuanbyu @gramalingam 
yuslepukhin(2019-01-09 22:24:54):@zrphercule Ngram is provided by [TensorFlow](https://www.tensorflow.org/tfx/transform/api_docs/python/tft/ngrams), used in [sklearn.feature_extraction.text.CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer), [sklearn.feature_extraction.text.TfidfVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer), [Micrsoft ML.NET](https://github.com/dotnet/machinelearning/blob/5e08fa1ea7bfb54f28ed0815cb6413e0068e6dd1/src/Microsoft.ML.Transforms/Text/NgramTransform.cs)
yuslepukhin(2019-01-09 22:25:14):The nature of the most recent travis failure is not apparent to me.
houseroad(2019-01-10 00:19:02):@yuslepukhin I will take a look at the CI problem. no worries
wschin(2019-01-11 17:03:05):Here is a list which summarizes I/O shapes in my mind.
- [C] ---> [max_ngram_index+1]
- [?] ---> [max_ngram_index+1]
- [B, C] ---> [B, max_ngram_index+1]
- [?, C] ---> [?, max_ngram_index+1]

Is it something in your mind too?
yuslepukhin(2019-01-14 17:55:49):> 
> 
> @yuslepukhin I will take a look at the CI problem. no worries

Please, let me know if there are other issues.
yuslepukhin(2019-01-15 18:53:36):Ping
linkerzhang(2019-01-16 18:38:57):@zrphercule any more comments please?
yuslepukhin(2019-01-16 19:21:46):> 
> 
> @zrphercule any more comments please?

I still need to address the shape inference. There seem to be some other comments on our side forthcoming so let's hold off on this for today.
yuslepukhin(2019-01-18 00:38:21):After some discussion, what this operator does is TfIdfVectorization. So the new name is TfIdfVectorizer
linkerzhang(2019-01-22 18:59:01):@houseroad  any more comments about this PR please? or it could be merged for 1.4. Thank you!
wschin(2019-01-10 03:49:20):```suggestion
  Y[ngram_indexes[i]] indicates the times that the i-th n-gram is found. The attribute ngram_indexes is used to determine the mapping 
```
wschin(2019-01-10 03:49:49):```suggestion
  between index i and the corresponding n-gram's output coordinate. If pool_int64s is [94, 17 ,17, 36], ngram_indexes is [1, 0],
```
wschin(2019-01-10 03:58:09):```suggestion
  The output vector (denoted by Y) stores the count of each n-gram; 
```
wschin(2019-01-10 03:59:17):```suggestion
<dd>Maximum number of items (integers/strings) to be skipped when constructing an n-gram from X. If max_skip_count=1, min_gram_length=2, max_gram_length=3, this operator may generate 2-grams with skip_count=0 and skip_count=1, and 3-grams with skip_count=0 and skip_count=1</dd>
```
wschin(2019-01-10 04:00:54):```suggestion
<dd>The weighting criteria. It can be one of "TF" (term frequency),"IDF" (inverse document frequency), and "TFIDF" (the combination of "TF" and "IDF")</dd>
```
wschin(2019-01-10 04:02:05):```suggestion
<dd>The starting indexes of 1-grams, 2-grams, and so on in pool. It is useful when determining the boundary between two consecutive collections of n-grams. For example, if ngram_counts is [0, 17, 36], the first index (zero-based) of 1-gram/2-gram/3-gramin pool are 0/17/36. This format is essentially identical to CSR (or CSC) sparse matrix format, and we choose to use this due to its popularity.</dd>
```
wschin(2019-01-10 04:03:48):```suggestion
<dd>list of int64s (type: AttributeProto::INTS). This list is parallel to the specified 'pool_*' attribute. The i-th element in ngram_indexes indicate the coordinate of the i-th n-gram in the output tensor.</dd>
```
wschin(2019-01-10 04:05:25):```suggestion
<dd>List of int64 n-grams learned from the training set. Either this or pool_strings attributes must be present but not both. It's an 1-D tensor starting with the collections of all 1-grams and ending with the collections of n-grams. The i-th element in pool stores the n-gram that should be mapped to coordinate ngram_indexes[i] in the output vector.</dd>
```
wschin(2019-01-10 04:05:54):```suggestion
<dd>List of strings n-grams learned from the training set. Either this or pool_int64s attributes must be present but not both.It's an 1-D tensor starting with the collections of all 1-grams and ending with the collections of n-grams. The i-th element in pool stores the n-gram that should be mapped to coordinate ngram_indexes[i] in the output vector.</dd>
```
wschin(2019-01-10 19:08:25):Maybe max_last_axis --> ngram_vector_length?
wschin(2019-01-10 19:09:52):Maybe here shouldn't fail because [?, C] can be mapped to [?, max_last_axis].
wschin(2019-01-10 19:11:54):If input is a 1-D tensor, its output shape is always [max_last_axis]. This remains true when input is variable-length.
yuslepukhin(2019-01-11 00:30:03):No it can not be. The input can not be processed if the value for C is not known.
yuslepukhin(2019-01-11 00:30:33):Yes, this is correct.
wschin(2019-01-11 00:57:26):I am not following. In my example, the unknown dimension is batch size, `B_dim`.
gramalingam(2019-01-15 22:36:10):I suggest replacing lines 1558 to 1565 by "*output_shape.add_dim() = B_dim"
gramalingam(2019-01-15 22:36:52):It is compact and does the right thing for all three cases
gramalingam(2019-01-15 22:39:55):May be "Input tensor must have rank 1 or 2" or "Input tensor must have rank <= 2" depending on whether rank 0 (scalar) is permitted.
houseroad(2019-01-10 00:21:28):cc: @BIT-silence
houseroad(2019-01-10 01:16:38):See whether installing package separately solves the problem. If not, we can just go with the first way.
CLAassistant(2019-01-10 01:34:55):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=1726) <br/>All committers have signed the CLA.
skottmckay(2019-01-12 23:24:16):Looks good to me.
yuslepukhin(2019-01-12 00:02:07):Cc: @zrphercule 
houseroad(2019-01-14 18:54:08):Please add some comments to explain the purpose and usage of this class.
houseroad(2019-01-14 20:19:55):Feel free to add it in another PR.
yuslepukhin(2019-01-16 23:46:13):Cc: @houseroad @HectorSVC @yuanbyu @gramalingam @wschin @linkerzhang 
yuslepukhin(2019-01-24 18:19:22):CI fails on Python 2.7 quirks. I have to jump through the hoops bc its inferior Unicode support and it is reaching the end of its life. Should we continue doing CI on it?
wschin(2019-01-25 08:25:44):For this op's doc string, maybe we can change `[optional] Step1: ....... any stop words.` to `StringNormalization performs string operations for basic cleaning. This operator has only one input (denoted by X) and only one output (denoted by Y). This operator first examines the elements in the X, and remove elements specified in "stopwords" attribute. Note that an implementation should sequentially remove "stopwords[0]," then "stopwords[1]," and so on. After removing stop words, the intermediate result can be further lowercased, uppercased, or just returned depending the "casechangeaction" attribute.`
yuslepukhin(2019-01-26 01:40:10):> 
> 
> Guess we should better have a model test as well?

@zrphercule Added model tests
linkerzhang(2019-01-31 01:15:01):@zrphercule  any more comments please? Thank you!
zrphercule(2019-02-05 18:03:48):@houseroad Any comments? Ke is targeting to merge it as soon as possible, could you plz give a verify review of it? Thanks!
linkerzhang(2019-02-08 16:10:52):@houseroad  any comments?
yuslepukhin(2019-02-15 18:25:56):Travis fails on git issue
yuslepukhin(2019-02-19 19:07:45):@houseroad Do you have any outstanding issues?
wschin(2019-01-18 17:21:05):Here is an `optional Step 1` but I don't see a `Step 2`. Because I was involved in its original design, I can say its step 2 is `changing the case according to casechangeaction attribute. After reading this again, I feel it actually consists of some independent operators:
- Stop words remover.
- Lower, Upper

Do you think we should break it down?
linkerzhang(2019-01-23 19:04:05):please run the clang-format. 
linkerzhang(2019-01-23 19:10:46):I think it's better to have the two covered in one op. Covering the two in one op does not add much complexity but help performance.
linkerzhang(2019-01-23 19:15:35):do you want to make them as "optional" attribute with default value? say, casechangeaction with default value "NONE"; and is_case_sensitive with default value "false".
linkerzhang(2019-01-23 19:17:52):In this case, the first dim value has to be "1", right? per the description above. if yes, check it please.
linkerzhang(2019-01-23 19:19:23):this is an invalid case?
linkerzhang(2019-01-23 19:21:15):this should be in opset 10 now. 

We just release onnx 1.4, together with opset 9.
yuslepukhin(2019-01-24 01:24:26):clang-format runs automatically with Studio.
yuslepukhin(2019-01-24 18:21:03):In case of 2-dim the first value can be an int, dim_param or nothing, so I simply copy it.
yuslepukhin(2019-01-24 18:21:50):Ok.
wschin(2019-01-25 16:47:06):```suggestion
            "List of stop words. If not set, no work would be removed from X.",
```
wschin(2019-01-25 16:48:43):Sound reasonable to me. :)
linkerzhang(2019-01-28 18:00:38):This should be changed by mistake?
wschin(2019-01-28 18:01:27):No sure if we want
```suggestion
            "iscasesensitive",
```
@linkerzhang, any preference?
linkerzhang(2019-01-28 18:03:57):"Note that an implementation should sequentially remove "stopwords[0]," then "stopwords[1]," "
why? is this because that the stop word may not be unigram?
linkerzhang(2019-01-28 18:05:35):Change "T" to "tensor(string)" since there's only one option for T, so that you don't need to specify .TypeConstraint(...), which simplifies a little bit.
linkerzhang(2019-01-28 18:07:11):attribute named as "case_change_action" (consistent with the other attribute)? btw, how do you think of my comments of making them as an optional attribute with default value?
linkerzhang(2019-01-28 18:08:26):same comment: I'd suggest to make this attribute as optional with default value.
linkerzhang(2019-01-28 18:09:28):specify the default value via API explicitly please, though I see you specified default value "en-US" in the description.
linkerzhang(2019-01-28 18:11:38):small comment: 

change to if(!hasInputShape(ctx, 0)) {...} to avoid big if block?
linkerzhang(2019-01-28 18:13:06):I suggest the other way, see my comments inline. :)
yuslepukhin(2019-01-30 18:53:51):I have not changed this, only merged the master
yuslepukhin(2019-01-30 18:55:38):The default value is platform dependent and, therefore, can not be specified uniformly.
wschin(2019-01-30 19:02:54):Right. It's for avoiding undefined behavior in that case.
yuslepukhin(2019-01-30 19:03:11):I will make this change
yuslepukhin(2019-01-30 19:04:13):Will change
yuslepukhin(2019-01-30 19:27:54):What is the suggestion here?
linkerzhang(2019-01-30 19:45:38):I discussed with Dmitri offline. Looks to us the order/sequantial does not matter here. Say, input is "x", "y", "z", and stop words are "xy", "yz", then nothing will be removed from input.
wschin(2019-01-30 21:21:45):Ah, yeah. Makes sense. Thank you.
houseroad(2019-02-08 21:29:04):removes
houseroad(2019-02-08 21:34:18):word
houseroad(2019-02-08 21:47:26):I am confused here... What is the expected shape of input?
yuslepukhin(2019-02-11 19:26:19):Yes, this needs to be fixed. B dimension *must* have dim_value() and it has to be 1.
The error message *must* read "Input shape must have either [C] or [1,C] dimension(s) where C > 0"
yuslepukhin(2019-02-12 18:15:21):@houseroad Pls, take another look
houseroad(2019-02-13 05:25:58):Shall we explicitly say that input's shape should [C] and [1, C]? Also, I am wondering why do we need to support [1, C] shape. Sounds a bit redundant to me. Since we always add an unsqueeze before. 
houseroad(2019-02-13 05:28:50):why do we need flatten here? We already handle the np.ndarray case already. (although, we probably should throw exception if it's 3d string array.
yuslepukhin(2019-02-14 00:40:57):@houseroad Current infrastructure does not handle strings input at all. I tried to avoid investing too much time in it. This line was introduced to accommodate another test Cast which I had to change as well.
houseroad(2019-02-14 00:48:34):Since it's not handled appropriately, please add some comments on why flatten is called here, and leave a TODO mark here.
wschin(2019-02-14 01:13:12):In traditional ML pipelines in, for example, scikit-learn and Core ML, batch size 1 is very common. It's ok to remove but I'd like to support it to avoid adding extra ops. In addition, the reason that we can't have batch size > 1 is the lack of sparse tensor. This restriction will be eventually removed, I believe.
houseroad(2019-02-14 01:15:18):Okay, let's keep it as it is now. But please explicitly describe what input shape will be accepted.
CLAassistant(2019-01-17 22:26:21):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=1747) <br/>All committers have signed the CLA.
stevewhims(2019-01-17 22:47:31):Abandoning.
gramalingam(2019-01-18 04:45:30):@houseroad this addresses a couple of comments on PR 1737, to handle negative axes in the shape inference of the scan operator. Sorry I couldn't get it into PR 1737 before it was merged in.
skottmckay(2019-01-18 10:11:02):Looks good to me. 

Also includes a bugfix when setting up the inputs to the subgraph inferencing where it was incorrectly starting at input 1 (which is correct for Scan 8 to skip the sequence_lens input, but not for Scan 9).
linkerzhang(2019-01-18 15:49:25):@houseroad please kindly review, though I'm assuming that this change should be good as it only adds more coverage for shape inference function and adds more test cases accordingly.
houseroad(2019-01-18 18:19:09):The code randomly hangs on some CI instances.
shinh(2019-01-18 10:06:38):I think this improves the readability of error messages:

Before:

```
(op_type:Add, name:): Inferred elem type differs from existing elem type: (7) vs (1)
```

After:

```
(op_type:Add, name:): Inferred elem type differs from existing elem type: (INT64) vs (FLOAT)
```
houseroad(2019-01-18 19:24:03):Great, this would be useful. Could you fix the build issue?
shinh(2019-01-19 15:23:46):The problem was protobuf-lite does not generate the function to stringify enums. Now I've added an ifdef for protobuf-lite and it compiles.
houseroad(2019-01-19 18:27:10):please include "onnx/string_utils.h" and explicitly use ONNX_NAMESPACE::to_string. Thanks.
shinh(2019-01-20 00:14:30):Done, thanks for the suggestion!
Maratyszcza(2019-01-22 07:59:57):@yinghai will be the PoC for these extensions (and likely create a separate PR), as I'm no longer with Facebook.
houseroad(2019-01-24 08:03:25):@prasanthpul could you make rel-1.4.1 branch unprotected temporally? we probably need to rebase and force push to that branch.
chinhuang007(2019-01-24 18:25:14):Thanks for the comments. The script here is to set up the environment, build ONNX, and test ONNX in a Linux node on the ppc64le architecture. It is similar to the script in .travis folder, which is used for Linux on x86 and OSX. Initially we would like to persist and download the script files from Jenkins for CI rather than code it in a Jenkins job. Therefore we created a new folder so we just need to clone ONNX repo to perform the CI. If the general guideline is to keep this kind of script out of the ONNX repo, I will move it somewhere else.
chinhuang007(2019-01-25 02:48:28):Script files are moved to another repo, https://github.com/ppc64le/build-scripts
CLAassistant(2019-01-24 05:50:23):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=1769) <br/>All committers have signed the CLA.
Verma-Rajat(2019-01-28 10:00:12):@houseroad , i am new to onnx, so not aware of merging process. will you please let me know when this will get merged?
houseroad(2019-01-24 23:41:35):@prasanthpul we have merged the PR. So feel free to make them protected again.
prasanthpul(2019-01-24 23:45:18):done
linkerzhang(2019-02-08 16:13:55):@houseroad please comment on this requirement/PR to support down sampling. It's verified that using scales < 1 works good.
linkerzhang(2019-02-19 19:20:01):Something wrong with doc generation, will fix. 
linkerzhang(2019-02-27 23:47:21):@houseroad @zrphercule any comments on this change please?
houseroad(2019-03-05 07:05:30):This PR also needs a corresponding PR in PyTorch, cc: @spandantiwari could you help on this? Thanks.
spandantiwari(2019-03-05 18:58:04):@houseroad @linkerzhang - sure. Let me send a fix to PT. I will let you know
spandantiwari(2019-03-05 21:22:57):I have started a PyTorch PR to disable the failing tests https://github.com/pytorch/pytorch/pull/17696. Once that is merged we can trigger CI build again to move this forward. 
spandantiwari(2019-03-06 21:05:11):@linkerzhang - the PyTorch PR is in. We can try another CI run. 
linkerzhang(2019-03-13 03:39:43):@houseroad  any more comments please? Thank you!
houseroad(2019-03-05 06:49:38):Can we also add some bilinear examples?
linkerzhang(2019-03-12 02:14:22):added.
linkerzhang(2019-03-12 02:14:57):added.
houseroad(2019-03-19 06:04:51):Can we check whether output_shape already has enough information? 

If so, we don't need to erase the data. Instead, we just finish the shape inference here.
linkerzhang(2019-03-19 16:59:50):@houseroad  I'd suggest the check happens outside when running shape inference, otherwise, this kind of check needs happening for every shape inference function implementation. Make sense?
raymondxyang(2019-01-30 00:41:42):I pushed the fix in generating scripts in this PR. 
raymondxyang(2019-01-30 00:43:14):Me and Dimitri were working on the same script during the same time, and I mistakenly implemented the generating function for the string tensor. Just found out and fixed it.
raymondxyang(2019-01-30 00:45:00):oh u mean the second push? I added the wrong generated version in the beginning so yeah it was a wrong submit
rdzhabarov(2019-01-31 20:46:41):cc: @jackm321 who is going to wire that on glow end
jackm321(2019-02-04 23:16:49):Looks good! `inputFence` looks like it's still mentioned in the documentation but should not be.
zrphercule(2019-02-04 23:21:00):> Looks good! `inputFence` looks like it's still mentioned in the documentation but should not be.

Yeah, once CI is passed I will update the documentation. I have been fighting with CI for a while, and finally with help of yinghai this problem is solved I believe.
zrphercule(2019-02-04 23:27:24):@yinghai Could you plz take a final look? This pr is ready to be merged.
Thanks!
rdzhabarov(2019-01-31 20:43:39):#define ONNXIFI_EXT_H

should be enough
rdzhabarov(2019-01-31 20:47:05):I'd mention why we need this to support atomicity
rdzhabarov(2019-01-31 20:49:44):do we need to care about this? or should inputs be available right away on the `onnxSetIOAndRunGraph` call
rdzhabarov(2019-01-31 20:50:44):this looks good to me
yinghai(2019-01-31 20:59:21):You need to add this https://github.com/onnx/onnx/pull/1757/files#diff-8312e1aaff6ed2027647302d102f0d44R54 too. 

The flow is going to be 
1. Query whether we have certain extension function or not. 
2. If we don't have `onnxSetIOAndRunGraph`, we need to go with `onnxSetGraphIO` and `onnxRunGraph` route. 
3. If we have that function, we need to cast `onnxExtensionFunctionPointer` to proper function pointer. 
yinghai(2019-01-31 21:00:03):You need a typedef to define a new function type. Check https://github.com/onnx/onnx/pull/1757/files#diff-8312e1aaff6ed2027647302d102f0d44R271 for an example. 
jackm321(2019-01-31 21:18:10):Yes it would be cool if we could get rid of this also if possible
zrphercule(2019-01-31 21:20:41):Oh yes you are right
zrphercule(2019-01-31 21:22:50):Actually I am not quite sure about what is the meaning of this, just copying codes from Marat. Mind telling me why we should add this macro?
zrphercule(2019-01-31 21:24:52):The I guess InputFence will be defined inside onnxSetIOAndRunGraph, and no need to be exposed to caller?
rdzhabarov(2019-01-31 22:00:46):this is to avoid redefinition in the same translation unit. So when you define those things `ONNXIFI_EXT_H`, header file will be included just once.
rdzhabarov(2019-01-31 22:02:07):or if we have that param for general purpose in this API, we can make C2/Glow to make sure that Glow does not need to wait on that event.
That option also works well.
zrphercule(2019-01-31 22:32:57):Quite a good point. My thought is since this is a general API, we should better give less restrictions or potential bug maker to our backend, so I would like to remove this InputFence from this API for now. If adding this param become very important in the future, I guess we can add it back then. Agree?
rdzhabarov(2019-01-31 22:41:24):That sounds good to me.
zrphercule(2019-02-04 21:50:33):@rdzhabarov Just noticed we have similar macro here https://github.com/onnx/onnx/blob/master/onnx/onnxifi.h#L2
should we align them? like use define onnxifi_ext_h 1 in both side?
rdzhabarov(2019-02-04 22:05:48):do not forget to remove this from the comment, since it was removed from the function declaration.
zrphercule(2019-02-04 22:07:09):Yes!
yinghai(2019-02-04 23:41:39):Add a typedef for this function pointer too? 
zrphercule(2019-02-04 23:45:28):I did, see https://github.com/onnx/onnx/pull/1781/files#diff-8312e1aaff6ed2027647302d102f0d44R68
snnn(2019-02-01 01:36:44):Build error log before the fix:

```
C:\os\onnx>git checkout v1.4.1
Note: checking out 'v1.4.1'.

You are in 'detached HEAD' state. You can look around, make experimental
changes and commit them, and you can discard any commits you make in this
state without impacting any branches by performing another checkout.

If you want to create a new branch to retain commits you create, you may
do so (now or later) by using -b with the checkout command again. Example:

  git checkout -b <new-branch-name>

HEAD is now at 9e55ace5 Bump up version for a patch release (#1766)

C:\os\onnx>pip install -e .
Obtaining file:///C:/os/onnx
  Installing build dependencies ... done
Requirement already satisfied: protobuf in c:\python35\lib\site-packages (from onnx==1.4.1) (3.5.2.post1)
Requirement already satisfied: numpy in c:\python35\lib\site-packages (from onnx==1.4.1) (1.14.3)
Requirement already satisfied: six in c:\python35\lib\site-packages\six-1.11.0-py3.5.egg (from onnx==1.4.1) (1.11.0)
Requirement already satisfied: typing>=3.6.4 in c:\python35\lib\site-packages\typing-3.6.4-py3.5.egg (from onnx==1.4.1) (3.6.4)
Requirement already satisfied: typing-extensions>=3.6.2.1 in c:\python35\lib\site-packages\typing_extensions-3.6.2.1-py3.5.egg (from onnx==1.4.1) (3.6.2.1)
Requirement already satisfied: setuptools in c:\python35\lib\site-packages (from protobuf->onnx==1.4.1) (39.1.0)
Installing collected packages: onnx
  Found existing installation: onnx 1.4.1
    Uninstalling onnx-1.4.1:
      Successfully uninstalled onnx-1.4.1
  Running setup.py develop for onnx
    Complete output from command c:\python35\python.exe -c "import setuptools, tokenize;__file__='C:\\os\\onnx\\setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\r\n', '\n');f.close();exec(compile(code, __file__, 'exec'))" develop --no-deps:
    running develop
    running build_py
    running create_version
    running cmake_build
    -- Building for: Visual Studio 15 2017
    -- Build type not set - defaulting to Release
    -- Selecting Windows SDK version 10.0.17134.0 to target Windows 10.0.18323.
    -- The C compiler identification is MSVC 19.15.26726.0
    -- The CXX compiler identification is MSVC 19.15.26726.0
    -- Check for working C compiler: C:/Program Files (x86)/Microsoft Visual Studio/2017/Enterprise/VC/Tools/MSVC/14.15.26726/bin/Hostx86/x64/cl.exe
    -- Check for working C compiler: C:/Program Files (x86)/Microsoft Visual Studio/2017/Enterprise/VC/Tools/MSVC/14.15.26726/bin/Hostx86/x64/cl.exe -- works
    -- Detecting C compiler ABI info
    -- Detecting C compiler ABI info - done
    -- Detecting C compile features
    -- Detecting C compile features - done
    -- Check for working CXX compiler: C:/Program Files (x86)/Microsoft Visual Studio/2017/Enterprise/VC/Tools/MSVC/14.15.26726/bin/Hostx86/x64/cl.exe
    -- Check for working CXX compiler: C:/Program Files (x86)/Microsoft Visual Studio/2017/Enterprise/VC/Tools/MSVC/14.15.26726/bin/Hostx86/x64/cl.exe -- works
    -- Detecting CXX compiler ABI info
    -- Detecting CXX compiler ABI info - done
    -- Detecting CXX compile features
    -- Detecting CXX compile features - done
    -- Found Protobuf: optimized;C:/portable/protobuf/lib/libprotobuf.lib;debug;C:/portable/protobuf/lib/libprotobufd.lib (found version "3.5.1")
    CMake Warning at CMakeLists.txt:356 (find_package):
      By not providing "Findpybind11.cmake" in CMAKE_MODULE_PATH this project has
      asked CMake to find a package configuration file provided by "pybind11",
      but CMake did not find one.

      Could not find a package configuration file provided by "pybind11"
      (requested version 2.2) with any of the following names:

        pybind11Config.cmake
        pybind11-config.cmake

      Add the installation prefix of "pybind11" to CMAKE_PREFIX_PATH or set
      "pybind11_DIR" to a directory containing one of the above files.  If
      "pybind11" provides a separate development package or SDK, be sure it has
      been installed.


    -- Found PythonInterp: C:/Python35/python.exe (found suitable version "3.5.4", minimum required is "3.5")
    -- Found PythonLibs: optimized;C:/Python35/libs/python35.lib;debug;C:/Python35/libs/python35_d.lib (found suitable version "3.5.4", minimum required is "3.5")
    --
    -- ******** Summary ********
    --   CMake version         : 3.12.1
    --   CMake command         : C:/Program Files/CMake/bin/cmake.exe
    --   System                : Windows
    --   C++ compiler          : C:/Program Files (x86)/Microsoft Visual Studio/2017/Enterprise/VC/Tools/MSVC/14.15.26726/bin/Hostx86/x64/cl.exe
    --   C++ compiler version  : 19.15.26726.0
    --   CXX flags             : /DWIN32 /D_WINDOWS /W3 /GR /EHsc
    --   Build type            : Release
    --   Compile definitions   :
    --   CMAKE_PREFIX_PATH     :
    --   CMAKE_INSTALL_PREFIX  : C:/Program Files (x86)/onnx
    --   CMAKE_MODULE_PATH     :
    --
    --   ONNX version          : 1.4.1
    --   ONNX NAMESPACE        : onnx
    --   ONNX_BUILD_TESTS      : OFF
    --   ONNX_BUILD_BENCHMARKS : OFF
    --   ONNX_USE_LITE_PROTO   : OFF
    --   ONNXIFI_DUMMY_BACKEND : OFF
    --
    --   Protobuf compiler     : C:/portable/protobuf/bin/protoc.exe
    --   Protobuf includes     : C:/portable/protobuf/include
    --   Protobuf libraries    : optimized;C:/portable/protobuf/lib/libprotobuf.lib;debug;C:/portable/protobuf/lib/libprotobufd.lib
    --   BUILD_ONNX_PYTHON     : ON
    --     Python version      : 3.5
    --     Python executable   : C:/Python35/python.exe
    --     Python includes     : C:/Python35/include
    -- Configuring done
    -- Generating done
    CMake Warning:
      Manually-specified variables were not used by the project:

        CMAKE_EXPORT_COMPILE_COMMANDS


    -- Build files have been written to: C:/os/onnx/.setuptools-cmake-build
    Microsoft (R) Build Engine version 15.8.168+ga8fba1ebd7 for .NET Framework
    Copyright (C) Microsoft Corporation. All rights reserved.

    Build started 1/31/2019 5:31:42 PM.
[....]
           Link:
             C:\Program Files (x86)\Microsoft Visual Studio\2017\Enterprise\VC\Tools\MSVC\14.15.26726\bin\HostX86\x64\link.exe /ERRORREPORT:QUEUE /OUT:"C:\os\onnx\.setuptools-cmake-build\Debug\onnx_cpp2py_export.cp35-win_amd64.pyd" /INCREMENTAL /NOLOGO "-WHOLEARCHIVE:C:/os/onnx/.setuptools-cmake-build/Debug/onnx.lib" Debug\onnx.lib C:\Python35\libs\python35_d.lib Debug\onnx_proto.lib C:\portable\protobuf\lib\libprotobufd.lib kernel32.lib user32.lib gdi32.lib winspool.lib shell32.lib ole32.lib oleaut32.lib uuid.lib comdlg32.lib advapi32.lib /MANIFEST /MANIFESTUAC:"level='asInvoker' uiAccess='false'" /manifest:embed /DEBUG /PDB:"C:/os/onnx/.setuptools-cmake-build/Debug/onnx_cpp2py_export.pdb" /SUBSYSTEM:CONSOLE /TLBID:1 /DYNAMICBASE /NXCOMPAT /IMPLIB:"C:/os/onnx/.setuptools-cmake-build/Debug/onnx_cpp2py_export.lib" /MACHINE:X64  /machine:x64 /DLL onnx_cpp2py_export.dir\Debug\cpp2py_export.obj
         7>libprotobufd.lib(coded_stream.obj) : error LNK2038: mismatch detected for '_ITERATOR_DEBUG_LEVEL': value '2' doesn't match value '0' in cpp2py_export.obj [C:\os\onnx\.setuptools-cmake-build\onnx_cpp2py_export.vcxproj]
         7>libprotobufd.lib(coded_stream.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MT_StaticRelease' in cpp2py_export.obj [C:\os\onnx\.setuptools-cmake-build\onnx_cpp2py_export.vcxproj]
         7>libprotobufd.lib(message_lite.obj) : error LNK2038: mismatch detected for '_ITERATOR_DEBUG_LEVEL': value '2' doesn't match value '0' in cpp2py_export.obj [C:\os\onnx\.setuptools-cmake-build\onnx_cpp2py_export.vcxproj]
         7>libprotobufd.lib(message_lite.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MT_StaticRelease' in cpp2py_export.obj [C:\os\onnx\.setuptools-cmake-build\onnx_cpp2py_export.vcxproj]
         7>libprotobufd.lib(zero_copy_stream_impl_lite.obj) : error LNK2038: mismatch detected for '_ITERATOR_DEBUG_LEVEL': value '2' doesn't match value '0' in cpp2py_export.obj [C:\os\onnx\.setuptools-cmake-build\onnx_cpp2py_export.vcxproj]
         7>libprotobufd.lib(zero_copy_stream_impl_lite.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MT_StaticRelease' in cpp2py_export.obj [C:\os\onnx\.setuptools-cmake-build\onnx_cpp2py_export.vcxproj]
         7>libprotobufd.lib(common.obj) : error LNK2038: mismatch detected for '_ITERATOR_DEBUG_LEVEL': value '2' doesn't match value '0' in cpp2py_export.obj [C:\os\onnx\.setuptools-cmake-build\onnx_cpp2py_export.vcxproj]
         7>libprotobufd.lib(common.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MT_StaticRelease' in cpp2py_export.obj [C:\os\onnx\.setuptools-cmake-build\onnx_cpp2py_export.vcxproj]
         7>libprotobufd.lib(arena.obj) : error LNK2038: mismatch detected for '_ITERATOR_DEBUG_LEVEL': value '2' doesn't match value '0' in cpp2py_export.obj [C:\os\onnx\.setuptools-cmake-build\onnx_cpp2py_export.vcxproj]
         7>libprotobufd.lib(arena.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MT_StaticRelease' in cpp2py_export.obj [C:\os\onnx\.setuptools-cmake-build\onnx_cpp2py_export.vcxproj]
         7>libprotobufd.lib(repeated_field.obj) : error LNK2038: mismatch detected for '_ITERATOR_DEBUG_LEVEL': value '2' doesn't match value '0' in cpp2py_export.obj [C:\os\onnx\.setuptools-cmake-build\onnx_cpp2py_export.vcxproj]
         7>libprotobufd.lib(repeated_field.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MT_StaticRelease' in cpp2py_export.obj [C:\os\onnx\.setuptools-cmake-build\onnx_cpp2py_export.vcxproj]
         7>libprotobufd.lib(generated_message_util.obj) : error LNK2038: mismatch detected for '_ITERATOR_DEBUG_LEVEL': value '2' doesn't match value '0' in cpp2py_export.obj [C:\os\onnx\.setuptools-cmake-build\onnx_cpp2py_export.vcxproj]
         7>libprotobufd.lib(generated_message_util.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MT_StaticRelease' in cpp2py_export.obj [C:\os\onnx\.setuptools-cmake-build\onnx_cpp2py_export.vcxproj]
         7>libprotobufd.lib(generated_message_reflection.obj) : error LNK2038: mismatch detected for '_ITERATOR_DEBUG_LEVEL': value '2' doesn't match value '0' in cpp2py_export.obj [C:\os\onnx\.setuptools-cmake-build\onnx_cpp2py_export.vcxproj]
         7>libprotobufd.lib(generated_message_reflection.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MT_StaticRelease' in cpp2py_export.obj [C:\os\onnx\.setuptools-cmake-build\onnx_cpp2py_export.vcxproj]
         7>libprotobufd.lib(text_format.obj) : error LNK2038: mismatch detected for '_ITERATOR_DEBUG_LEVEL': value '2' doesn't match value '0' in cpp2py_export.obj [C:\os\onnx\.setuptools-cmake-build\onnx_cpp2py_export.vcxproj]
         7>libprotobufd.lib(text_format.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MT_StaticRelease' in cpp2py_export.obj [C:\os\onnx\.setuptools-cmake-build\onnx_cpp2py_export.vcxproj]
         7>libprotobufd.lib(once.obj) : error LNK2038: mismatch detected for '_ITERATOR_DEBUG_LEVEL': value '2' doesn't match value '0' in cpp2py_export.obj [C:\os\onnx\.setuptools-cmake-build\onnx_cpp2py_export.vcxproj]
         7>libprotobufd.lib(once.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MT_StaticRelease' in cpp2py_export.obj [C:\os\onnx\.setuptools-cmake-build\onnx_cpp2py_export.vcxproj]
         7>libprotobufd.lib(wire_format_lite.obj) : error LNK2038: mismatch detected for '_ITERATOR_DEBUG_LEVEL': value '2' doesn't match value '0' in cpp2py_export.obj [C:\os\onnx\.setuptools-cmake-build\onnx_cpp2py_export.vcxproj]
         7>libprotobufd.lib(wire_format_lite.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MT_StaticRelease' in cpp2py_export.obj [C:\os\onnx\.setuptools-cmake-build\onnx_cpp2py_export.vcxproj]
         7>libprotobufd.lib(unknown_field_set.obj) : error LNK2038: mismatch detected for '_ITERATOR_DEBUG_LEVEL': value '2' doesn't match value '0' in cpp2py_export.obj [C:\os\onnx\.setuptools-cmake-build\onnx_cpp2py_export.vcxproj]
         7>libprotobufd.lib(unknown_field_set.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MT_StaticRelease' in cpp2py_export.obj [C:\os\onnx\.setuptools-cmake-build\onnx_cpp2py_export.vcxproj]
         7>libprotobufd.lib(descriptor.obj) : error LNK2038: mismatch detected for '_ITERATOR_DEBUG_LEVEL': value '2' doesn't match value '0' in cpp2py_export.obj [C:\os\onnx\.setuptools-cmake-build\onnx_cpp2py_export.vcxproj]
         7>libprotobufd.lib(descriptor.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MT_StaticRelease' in cpp2py_export.obj [C:\os\onnx\.setuptools-cmake-build\onnx_cpp2py_export.vcxproj]
         7>libprotobufd.lib(message.obj) : error LNK2038: mismatch detected for '_ITERATOR_DEBUG_LEVEL': value '2' doesn't match value '0' in cpp2py_export.obj [C:\os\onnx\.setuptools-cmake-build\onnx_cpp2py_export.vcxproj]
         7>libprotobufd.lib(message.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MT_StaticRelease' in cpp2py_export.obj [C:\os\onnx\.setuptools-cmake-build\onnx_cpp2py_export.vcxproj]
         7>libprotobufd.lib(reflection_ops.obj) : error LNK2038: mismatch detected for '_ITERATOR_DEBUG_LEVEL': value '2' doesn't match value '0' in cpp2py_export.obj [C:\os\onnx\.setuptools-cmake-build\onnx_cpp2py_export.vcxproj]
         7>libprotobufd.lib(reflection_ops.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MT_StaticRelease' in cpp2py_export.obj [C:\os\onnx\.setuptools-cmake-build\onnx_cpp2py_export.vcxproj]
         7>libprotobufd.lib(wire_format.obj) : error LNK2038: mismatch detected for '_ITERATOR_DEBUG_LEVEL': value '2' doesn't match value '0' in cpp2py_export.obj [C:\os\onnx\.setuptools-cmake-build\onnx_cpp2py_export.vcxproj]
         7>libprotobufd.lib(wire_format.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MT_StaticRelease' in cpp2py_export.obj [C:\os\onnx\.setuptools-cmake-build\onnx_cpp2py_export.vcxproj]
         7>libprotobufd.lib(zero_copy_stream.obj) : error LNK2038: mismatch detected for '_ITERATOR_DEBUG_LEVEL': value '2' doesn't match value '0' in cpp2py_export.obj [C:\os\onnx\.setuptools-cmake-build\onnx_cpp2py_export.vcxproj]
         7>libprotobufd.lib(zero_copy_stream.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MT_StaticRelease' in cpp2py_export.obj [C:\os\onnx\.setuptools-cmake-build\onnx_cpp2py_export.vcxproj]
         7>libprotobufd.lib(status.obj) : error LNK2038: mismatch detected for '_ITERATOR_DEBUG_LEVEL': value '2' doesn't match value '0' in cpp2py_export.obj [C:\os\onnx\.setuptools-cmake-build\onnx_cpp2py_export.vcxproj]
         7>libprotobufd.lib(status.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MT_StaticRelease' in cpp2py_export.obj [C:\os\onnx\.setuptools-cmake-build\onnx_cpp2py_export.vcxproj]
         7>libprotobufd.lib(int128.obj) : error LNK2038: mismatch detected for '_ITERATOR_DEBUG_LEVEL': value '2' doesn't match value '0' in cpp2py_export.obj [C:\os\onnx\.setuptools-cmake-build\onnx_cpp2py_export.vcxproj]
         7>libprotobufd.lib(int128.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MT_StaticRelease' in cpp2py_export.obj [C:\os\onnx\.setuptools-cmake-build\onnx_cpp2py_export.vcxproj]
         7>libprotobufd.lib(atomicops_internals_x86_msvc.obj) : error LNK2038: mismatch detected for '_ITERATOR_DEBUG_LEVEL': value '2' doesn't match value '0' in cpp2py_export.obj [C:\os\onnx\.setuptools-cmake-build\onnx_cpp2py_export.vcxproj]
         7>libprotobufd.lib(atomicops_internals_x86_msvc.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MT_StaticRelease' in cpp2py_export.obj [C:\os\onnx\.setuptools-cmake-build\onnx_cpp2py_export.vcxproj]
         7>libprotobufd.lib(extension_set.obj) : error LNK2038: mismatch detected for '_ITERATOR_DEBUG_LEVEL': value '2' doesn't match value '0' in cpp2py_export.obj [C:\os\onnx\.setuptools-cmake-build\onnx_cpp2py_export.vcxproj]
         7>libprotobufd.lib(extension_set.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MT_StaticRelease' in cpp2py_export.obj [C:\os\onnx\.setuptools-cmake-build\onnx_cpp2py_export.vcxproj]
         7>libprotobufd.lib(extension_set_heavy.obj) : error LNK2038: mismatch detected for '_ITERATOR_DEBUG_LEVEL': value '2' doesn't match value '0' in cpp2py_export.obj [C:\os\onnx\.setuptools-cmake-build\onnx_cpp2py_export.vcxproj]
         7>libprotobufd.lib(extension_set_heavy.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MT_StaticRelease' in cpp2py_export.obj [C:\os\onnx\.setuptools-cmake-build\onnx_cpp2py_export.vcxproj]
         7>libprotobufd.lib(map_field.obj) : error LNK2038: mismatch detected for '_ITERATOR_DEBUG_LEVEL': value '2' doesn't match value '0' in cpp2py_export.obj [C:\os\onnx\.setuptools-cmake-build\onnx_cpp2py_export.vcxproj]
         7>libprotobufd.lib(map_field.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MT_StaticRelease' in cpp2py_export.obj [C:\os\onnx\.setuptools-cmake-build\onnx_cpp2py_export.vcxproj]
         7>libprotobufd.lib(stringprintf.obj) : error LNK2038: mismatch detected for '_ITERATOR_DEBUG_LEVEL': value '2' doesn't match value '0' in cpp2py_export.obj [C:\os\onnx\.setuptools-cmake-build\onnx_cpp2py_export.vcxproj]
         7>libprotobufd.lib(stringprintf.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MT_StaticRelease' in cpp2py_export.obj [C:\os\onnx\.setuptools-cmake-build\onnx_cpp2py_export.vcxproj]
         7>libprotobufd.lib(any.obj) : error LNK2038: mismatch detected for '_ITERATOR_DEBUG_LEVEL': value '2' doesn't match value '0' in cpp2py_export.obj [C:\os\onnx\.setuptools-cmake-build\onnx_cpp2py_export.vcxproj]
         7>libprotobufd.lib(any.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MT_StaticRelease' in cpp2py_export.obj [C:\os\onnx\.setuptools-cmake-build\onnx_cpp2py_export.vcxproj]
         7>libprotobufd.lib(strtod.obj) : error LNK2038: mismatch detected for '_ITERATOR_DEBUG_LEVEL': value '2' doesn't match value '0' in cpp2py_export.obj [C:\os\onnx\.setuptools-cmake-build\onnx_cpp2py_export.vcxproj]
         7>libprotobufd.lib(strtod.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MT_StaticRelease' in cpp2py_export.obj [C:\os\onnx\.setuptools-cmake-build\onnx_cpp2py_export.vcxproj]
         7>libprotobufd.lib(tokenizer.obj) : error LNK2038: mismatch detected for '_ITERATOR_DEBUG_LEVEL': value '2' doesn't match value '0' in cpp2py_export.obj [C:\os\onnx\.setuptools-cmake-build\onnx_cpp2py_export.vcxproj]
         7>libprotobufd.lib(tokenizer.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MT_StaticRelease' in cpp2py_export.obj [C:\os\onnx\.setuptools-cmake-build\onnx_cpp2py_export.vcxproj]
         7>libprotobufd.lib(dynamic_message.obj) : error LNK2038: mismatch detected for '_ITERATOR_DEBUG_LEVEL': value '2' doesn't match value '0' in cpp2py_export.obj [C:\os\onnx\.setuptools-cmake-build\onnx_cpp2py_export.vcxproj]
         7>libprotobufd.lib(dynamic_message.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MT_StaticRelease' in cpp2py_export.obj [C:\os\onnx\.setuptools-cmake-build\onnx_cpp2py_export.vcxproj]
         7>libprotobufd.lib(strutil.obj) : error LNK2038: mismatch detected for '_ITERATOR_DEBUG_LEVEL': value '2' doesn't match value '0' in cpp2py_export.obj [C:\os\onnx\.setuptools-cmake-build\onnx_cpp2py_export.vcxproj]
         7>libprotobufd.lib(strutil.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MT_StaticRelease' in cpp2py_export.obj [C:\os\onnx\.setuptools-cmake-build\onnx_cpp2py_export.vcxproj]
         7>libprotobufd.lib(structurally_valid.obj) : error LNK2038: mismatch detected for '_ITERATOR_DEBUG_LEVEL': value '2' doesn't match value '0' in cpp2py_export.obj [C:\os\onnx\.setuptools-cmake-build\onnx_cpp2py_export.vcxproj]
         7>libprotobufd.lib(structurally_valid.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MT_StaticRelease' in cpp2py_export.obj [C:\os\onnx\.setuptools-cmake-build\onnx_cpp2py_export.vcxproj]
         7>libprotobufd.lib(stringpiece.obj) : error LNK2038: mismatch detected for '_ITERATOR_DEBUG_LEVEL': value '2' doesn't match value '0' in cpp2py_export.obj [C:\os\onnx\.setuptools-cmake-build\onnx_cpp2py_export.vcxproj]
         7>libprotobufd.lib(stringpiece.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MT_StaticRelease' in cpp2py_export.obj [C:\os\onnx\.setuptools-cmake-build\onnx_cpp2py_export.vcxproj]
         7>libprotobufd.lib(descriptor.pb.obj) : error LNK2038: mismatch detected for '_ITERATOR_DEBUG_LEVEL': value '2' doesn't match value '0' in cpp2py_export.obj [C:\os\onnx\.setuptools-cmake-build\onnx_cpp2py_export.vcxproj]
         7>libprotobufd.lib(descriptor.pb.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MT_StaticRelease' in cpp2py_export.obj [C:\os\onnx\.setuptools-cmake-build\onnx_cpp2py_export.vcxproj]
         7>libprotobufd.lib(descriptor_database.obj) : error LNK2038: mismatch detected for '_ITERATOR_DEBUG_LEVEL': value '2' doesn't match value '0' in cpp2py_export.obj [C:\os\onnx\.setuptools-cmake-build\onnx_cpp2py_export.vcxproj]
         7>libprotobufd.lib(descriptor_database.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MT_StaticRelease' in cpp2py_export.obj [C:\os\onnx\.setuptools-cmake-build\onnx_cpp2py_export.vcxproj]
         7>libprotobufd.lib(substitute.obj) : error LNK2038: mismatch detected for '_ITERATOR_DEBUG_LEVEL': value '2' doesn't match value '0' in cpp2py_export.obj [C:\os\onnx\.setuptools-cmake-build\onnx_cpp2py_export.vcxproj]
         7>libprotobufd.lib(substitute.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MT_StaticRelease' in cpp2py_export.obj [C:\os\onnx\.setuptools-cmake-build\onnx_cpp2py_export.vcxproj]
         7>libprotobufd.lib(zero_copy_stream_impl.obj) : error LNK2038: mismatch detected for '_ITERATOR_DEBUG_LEVEL': value '2' doesn't match value '0' in cpp2py_export.obj [C:\os\onnx\.setuptools-cmake-build\onnx_cpp2py_export.vcxproj]
         7>libprotobufd.lib(zero_copy_stream_impl.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MT_StaticRelease' in cpp2py_export.obj [C:\os\onnx\.setuptools-cmake-build\onnx_cpp2py_export.vcxproj]
         7>libprotobufd.lib(io_win32.obj) : error LNK2038: mismatch detected for '_ITERATOR_DEBUG_LEVEL': value '2' doesn't match value '0' in cpp2py_export.obj [C:\os\onnx\.setuptools-cmake-build\onnx_cpp2py_export.vcxproj]
         7>libprotobufd.lib(io_win32.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MT_StaticRelease' in cpp2py_export.obj [C:\os\onnx\.setuptools-cmake-build\onnx_cpp2py_export.vcxproj]
         7>LINK : fatal error LNK1104: cannot open file 'python35.lib' [C:\os\onnx\.setuptools-cmake-build\onnx_cpp2py_export.vcxproj]
         7>Done Building Project "C:\os\onnx\.setuptools-cmake-build\onnx_cpp2py_export.vcxproj" (default targets) -- FAILED.
         1>Done Building Project "C:\os\onnx\.setuptools-cmake-build\ALL_BUILD.vcxproj" (default targets) -- FAILED.

    Build FAILED.

           "C:\os\onnx\.setuptools-cmake-build\ALL_BUILD.vcxproj" (default target) (1) ->
           "C:\os\onnx\.setuptools-cmake-build\onnxifi_wrapper.vcxproj" (default target) (8) ->
           (Link target) ->
             LINK : warning LNK4098: defaultlib 'LIBCMT' conflicts with use of other libs; use /NODEFAULTLIB:library [C:\os\onnx\.setuptools-cmake-build\onnxifi_wrapper.vcxproj]


           "C:\os\onnx\.setuptools-cmake-build\ALL_BUILD.vcxproj" (default target) (1) ->
           "C:\os\onnx\.setuptools-cmake-build\onnx_cpp2py_export.vcxproj" (default target) (7) ->
           (ClCompile target) ->
             cl : Command line warning D9002: ignoring unknown option '-fvisibility=hidden' [C:\os\onnx\.setuptools-cmake-build\onnx_cpp2py_export.vcxproj]


           "C:\os\onnx\.setuptools-cmake-build\ALL_BUILD.vcxproj" (default target) (1) ->
           "C:\os\onnx\.setuptools-cmake-build\onnx_cpp2py_export.vcxproj" (default target) (7) ->
           (Link target) ->
             libprotobufd.lib(coded_stream.obj) : error LNK2038: mismatch detected for '_ITERATOR_DEBUG_LEVEL': value '2' doesn't match value '0' in cpp2py_export.obj [C:\os\onnx\.setuptools-cmake-build\onnx_cpp2py_export.vcxproj]
             libprotobufd.lib(coded_stream.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MT_StaticRelease' in cpp2py_export.obj [C:\os\onnx\.setuptools-cmake-build\onnx_cpp2py_export.vcxproj]
             libprotobufd.lib(message_lite.obj) : error LNK2038: mismatch detected for '_ITERATOR_DEBUG_LEVEL': value '2' doesn't match value '0' in cpp2py_export.obj [C:\os\onnx\.setuptools-cmake-build\onnx_cpp2py_export.vcxproj]
             libprotobufd.lib(message_lite.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MT_StaticRelease' in cpp2py_export.obj [C:\os\onnx\.setuptools-cmake-build\onnx_cpp2py_export.vcxproj]
             libprotobufd.lib(zero_copy_stream_impl_lite.obj) : error LNK2038: mismatch detected for '_ITERATOR_DEBUG_LEVEL': value '2' doesn't match value '0' in cpp2py_export.obj [C:\os\onnx\.setuptools-cmake-build\onnx_cpp2py_export.vcxproj]
             libprotobufd.lib(zero_copy_stream_impl_lite.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MT_StaticRelease' in cpp2py_export.obj [C:\os\onnx\.setuptools-cmake-build\onnx_cpp2py_export.vcxproj]
             libprotobufd.lib(common.obj) : error LNK2038: mismatch detected for '_ITERATOR_DEBUG_LEVEL': value '2' doesn't match value '0' in cpp2py_export.obj [C:\os\onnx\.setuptools-cmake-build\onnx_cpp2py_export.vcxproj]
             libprotobufd.lib(common.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MT_StaticRelease' in cpp2py_export.obj [C:\os\onnx\.setuptools-cmake-build\onnx_cpp2py_export.vcxproj]
             libprotobufd.lib(arena.obj) : error LNK2038: mismatch detected for '_ITERATOR_DEBUG_LEVEL': value '2' doesn't match value '0' in cpp2py_export.obj [C:\os\onnx\.setuptools-cmake-build\onnx_cpp2py_export.vcxproj]
             libprotobufd.lib(arena.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MT_StaticRelease' in cpp2py_export.obj [C:\os\onnx\.setuptools-cmake-build\onnx_cpp2py_export.vcxproj]
             libprotobufd.lib(repeated_field.obj) : error LNK2038: mismatch detected for '_ITERATOR_DEBUG_LEVEL': value '2' doesn't match value '0' in cpp2py_export.obj [C:\os\onnx\.setuptools-cmake-build\onnx_cpp2py_export.vcxproj]
             libprotobufd.lib(repeated_field.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MT_StaticRelease' in cpp2py_export.obj [C:\os\onnx\.setuptools-cmake-build\onnx_cpp2py_export.vcxproj]
             libprotobufd.lib(generated_message_util.obj) : error LNK2038: mismatch detected for '_ITERATOR_DEBUG_LEVEL': value '2' doesn't match value '0' in cpp2py_export.obj [C:\os\onnx\.setuptools-cmake-build\onnx_cpp2py_export.vcxproj]
             libprotobufd.lib(generated_message_util.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MT_StaticRelease' in cpp2py_export.obj [C:\os\onnx\.setuptools-cmake-build\onnx_cpp2py_export.vcxproj]
             libprotobufd.lib(generated_message_reflection.obj) : error LNK2038: mismatch detected for '_ITERATOR_DEBUG_LEVEL': value '2' doesn't match value '0' in cpp2py_export.obj [C:\os\onnx\.setuptools-cmake-build\onnx_cpp2py_export.vcxproj]
             libprotobufd.lib(generated_message_reflection.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MT_StaticRelease' in cpp2py_export.obj [C:\os\onnx\.setuptools-cmake-build\onnx_cpp2py_export.vcxproj]
             libprotobufd.lib(text_format.obj) : error LNK2038: mismatch detected for '_ITERATOR_DEBUG_LEVEL': value '2' doesn't match value '0' in cpp2py_export.obj [C:\os\onnx\.setuptools-cmake-build\onnx_cpp2py_export.vcxproj]
             libprotobufd.lib(text_format.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MT_StaticRelease' in cpp2py_export.obj [C:\os\onnx\.setuptools-cmake-build\onnx_cpp2py_export.vcxproj]
             libprotobufd.lib(once.obj) : error LNK2038: mismatch detected for '_ITERATOR_DEBUG_LEVEL': value '2' doesn't match value '0' in cpp2py_export.obj [C:\os\onnx\.setuptools-cmake-build\onnx_cpp2py_export.vcxproj]
             libprotobufd.lib(once.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MT_StaticRelease' in cpp2py_export.obj [C:\os\onnx\.setuptools-cmake-build\onnx_cpp2py_export.vcxproj]
             libprotobufd.lib(wire_format_lite.obj) : error LNK2038: mismatch detected for '_ITERATOR_DEBUG_LEVEL': value '2' doesn't match value '0' in cpp2py_export.obj [C:\os\onnx\.setuptools-cmake-build\onnx_cpp2py_export.vcxproj]
             libprotobufd.lib(wire_format_lite.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MT_StaticRelease' in cpp2py_export.obj [C:\os\onnx\.setuptools-cmake-build\onnx_cpp2py_export.vcxproj]
             libprotobufd.lib(unknown_field_set.obj) : error LNK2038: mismatch detected for '_ITERATOR_DEBUG_LEVEL': value '2' doesn't match value '0' in cpp2py_export.obj [C:\os\onnx\.setuptools-cmake-build\onnx_cpp2py_export.vcxproj]
             libprotobufd.lib(unknown_field_set.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MT_StaticRelease' in cpp2py_export.obj [C:\os\onnx\.setuptools-cmake-build\onnx_cpp2py_export.vcxproj]
             libprotobufd.lib(descriptor.obj) : error LNK2038: mismatch detected for '_ITERATOR_DEBUG_LEVEL': value '2' doesn't match value '0' in cpp2py_export.obj [C:\os\onnx\.setuptools-cmake-build\onnx_cpp2py_export.vcxproj]
             libprotobufd.lib(descriptor.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MT_StaticRelease' in cpp2py_export.obj [C:\os\onnx\.setuptools-cmake-build\onnx_cpp2py_export.vcxproj]
             libprotobufd.lib(message.obj) : error LNK2038: mismatch detected for '_ITERATOR_DEBUG_LEVEL': value '2' doesn't match value '0' in cpp2py_export.obj [C:\os\onnx\.setuptools-cmake-build\onnx_cpp2py_export.vcxproj]
             libprotobufd.lib(message.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MT_StaticRelease' in cpp2py_export.obj [C:\os\onnx\.setuptools-cmake-build\onnx_cpp2py_export.vcxproj]
             libprotobufd.lib(reflection_ops.obj) : error LNK2038: mismatch detected for '_ITERATOR_DEBUG_LEVEL': value '2' doesn't match value '0' in cpp2py_export.obj [C:\os\onnx\.setuptools-cmake-build\onnx_cpp2py_export.vcxproj]
             libprotobufd.lib(reflection_ops.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MT_StaticRelease' in cpp2py_export.obj [C:\os\onnx\.setuptools-cmake-build\onnx_cpp2py_export.vcxproj]
             libprotobufd.lib(wire_format.obj) : error LNK2038: mismatch detected for '_ITERATOR_DEBUG_LEVEL': value '2' doesn't match value '0' in cpp2py_export.obj [C:\os\onnx\.setuptools-cmake-build\onnx_cpp2py_export.vcxproj]
             libprotobufd.lib(wire_format.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MT_StaticRelease' in cpp2py_export.obj [C:\os\onnx\.setuptools-cmake-build\onnx_cpp2py_export.vcxproj]
             libprotobufd.lib(zero_copy_stream.obj) : error LNK2038: mismatch detected for '_ITERATOR_DEBUG_LEVEL': value '2' doesn't match value '0' in cpp2py_export.obj [C:\os\onnx\.setuptools-cmake-build\onnx_cpp2py_export.vcxproj]
             libprotobufd.lib(zero_copy_stream.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MT_StaticRelease' in cpp2py_export.obj [C:\os\onnx\.setuptools-cmake-build\onnx_cpp2py_export.vcxproj]
             libprotobufd.lib(status.obj) : error LNK2038: mismatch detected for '_ITERATOR_DEBUG_LEVEL': value '2' doesn't match value '0' in cpp2py_export.obj [C:\os\onnx\.setuptools-cmake-build\onnx_cpp2py_export.vcxproj]
             libprotobufd.lib(status.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MT_StaticRelease' in cpp2py_export.obj [C:\os\onnx\.setuptools-cmake-build\onnx_cpp2py_export.vcxproj]
             libprotobufd.lib(int128.obj) : error LNK2038: mismatch detected for '_ITERATOR_DEBUG_LEVEL': value '2' doesn't match value '0' in cpp2py_export.obj [C:\os\onnx\.setuptools-cmake-build\onnx_cpp2py_export.vcxproj]
             libprotobufd.lib(int128.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MT_StaticRelease' in cpp2py_export.obj [C:\os\onnx\.setuptools-cmake-build\onnx_cpp2py_export.vcxproj]
             libprotobufd.lib(atomicops_internals_x86_msvc.obj) : error LNK2038: mismatch detected for '_ITERATOR_DEBUG_LEVEL': value '2' doesn't match value '0' in cpp2py_export.obj [C:\os\onnx\.setuptools-cmake-build\onnx_cpp2py_export.vcxproj]
             libprotobufd.lib(atomicops_internals_x86_msvc.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MT_StaticRelease' in cpp2py_export.obj [C:\os\onnx\.setuptools-cmake-build\onnx_cpp2py_export.vcxproj]
             libprotobufd.lib(extension_set.obj) : error LNK2038: mismatch detected for '_ITERATOR_DEBUG_LEVEL': value '2' doesn't match value '0' in cpp2py_export.obj [C:\os\onnx\.setuptools-cmake-build\onnx_cpp2py_export.vcxproj]
             libprotobufd.lib(extension_set.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MT_StaticRelease' in cpp2py_export.obj [C:\os\onnx\.setuptools-cmake-build\onnx_cpp2py_export.vcxproj]
             libprotobufd.lib(extension_set_heavy.obj) : error LNK2038: mismatch detected for '_ITERATOR_DEBUG_LEVEL': value '2' doesn't match value '0' in cpp2py_export.obj [C:\os\onnx\.setuptools-cmake-build\onnx_cpp2py_export.vcxproj]
             libprotobufd.lib(extension_set_heavy.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MT_StaticRelease' in cpp2py_export.obj [C:\os\onnx\.setuptools-cmake-build\onnx_cpp2py_export.vcxproj]
             libprotobufd.lib(map_field.obj) : error LNK2038: mismatch detected for '_ITERATOR_DEBUG_LEVEL': value '2' doesn't match value '0' in cpp2py_export.obj [C:\os\onnx\.setuptools-cmake-build\onnx_cpp2py_export.vcxproj]
             libprotobufd.lib(map_field.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MT_StaticRelease' in cpp2py_export.obj [C:\os\onnx\.setuptools-cmake-build\onnx_cpp2py_export.vcxproj]
             libprotobufd.lib(stringprintf.obj) : error LNK2038: mismatch detected for '_ITERATOR_DEBUG_LEVEL': value '2' doesn't match value '0' in cpp2py_export.obj [C:\os\onnx\.setuptools-cmake-build\onnx_cpp2py_export.vcxproj]
             libprotobufd.lib(stringprintf.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MT_StaticRelease' in cpp2py_export.obj [C:\os\onnx\.setuptools-cmake-build\onnx_cpp2py_export.vcxproj]
             libprotobufd.lib(any.obj) : error LNK2038: mismatch detected for '_ITERATOR_DEBUG_LEVEL': value '2' doesn't match value '0' in cpp2py_export.obj [C:\os\onnx\.setuptools-cmake-build\onnx_cpp2py_export.vcxproj]
             libprotobufd.lib(any.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MT_StaticRelease' in cpp2py_export.obj [C:\os\onnx\.setuptools-cmake-build\onnx_cpp2py_export.vcxproj]
             libprotobufd.lib(strtod.obj) : error LNK2038: mismatch detected for '_ITERATOR_DEBUG_LEVEL': value '2' doesn't match value '0' in cpp2py_export.obj [C:\os\onnx\.setuptools-cmake-build\onnx_cpp2py_export.vcxproj]
             libprotobufd.lib(strtod.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MT_StaticRelease' in cpp2py_export.obj [C:\os\onnx\.setuptools-cmake-build\onnx_cpp2py_export.vcxproj]
             libprotobufd.lib(tokenizer.obj) : error LNK2038: mismatch detected for '_ITERATOR_DEBUG_LEVEL': value '2' doesn't match value '0' in cpp2py_export.obj [C:\os\onnx\.setuptools-cmake-build\onnx_cpp2py_export.vcxproj]
             libprotobufd.lib(tokenizer.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MT_StaticRelease' in cpp2py_export.obj [C:\os\onnx\.setuptools-cmake-build\onnx_cpp2py_export.vcxproj]
             libprotobufd.lib(dynamic_message.obj) : error LNK2038: mismatch detected for '_ITERATOR_DEBUG_LEVEL': value '2' doesn't match value '0' in cpp2py_export.obj [C:\os\onnx\.setuptools-cmake-build\onnx_cpp2py_export.vcxproj]
             libprotobufd.lib(dynamic_message.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MT_StaticRelease' in cpp2py_export.obj [C:\os\onnx\.setuptools-cmake-build\onnx_cpp2py_export.vcxproj]
             libprotobufd.lib(strutil.obj) : error LNK2038: mismatch detected for '_ITERATOR_DEBUG_LEVEL': value '2' doesn't match value '0' in cpp2py_export.obj [C:\os\onnx\.setuptools-cmake-build\onnx_cpp2py_export.vcxproj]
             libprotobufd.lib(strutil.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MT_StaticRelease' in cpp2py_export.obj [C:\os\onnx\.setuptools-cmake-build\onnx_cpp2py_export.vcxproj]
             libprotobufd.lib(structurally_valid.obj) : error LNK2038: mismatch detected for '_ITERATOR_DEBUG_LEVEL': value '2' doesn't match value '0' in cpp2py_export.obj [C:\os\onnx\.setuptools-cmake-build\onnx_cpp2py_export.vcxproj]
             libprotobufd.lib(structurally_valid.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MT_StaticRelease' in cpp2py_export.obj [C:\os\onnx\.setuptools-cmake-build\onnx_cpp2py_export.vcxproj]
             libprotobufd.lib(stringpiece.obj) : error LNK2038: mismatch detected for '_ITERATOR_DEBUG_LEVEL': value '2' doesn't match value '0' in cpp2py_export.obj [C:\os\onnx\.setuptools-cmake-build\onnx_cpp2py_export.vcxproj]
             libprotobufd.lib(stringpiece.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MT_StaticRelease' in cpp2py_export.obj [C:\os\onnx\.setuptools-cmake-build\onnx_cpp2py_export.vcxproj]
             libprotobufd.lib(descriptor.pb.obj) : error LNK2038: mismatch detected for '_ITERATOR_DEBUG_LEVEL': value '2' doesn't match value '0' in cpp2py_export.obj [C:\os\onnx\.setuptools-cmake-build\onnx_cpp2py_export.vcxproj]
             libprotobufd.lib(descriptor.pb.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MT_StaticRelease' in cpp2py_export.obj [C:\os\onnx\.setuptools-cmake-build\onnx_cpp2py_export.vcxproj]
             libprotobufd.lib(descriptor_database.obj) : error LNK2038: mismatch detected for '_ITERATOR_DEBUG_LEVEL': value '2' doesn't match value '0' in cpp2py_export.obj [C:\os\onnx\.setuptools-cmake-build\onnx_cpp2py_export.vcxproj]
             libprotobufd.lib(descriptor_database.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MT_StaticRelease' in cpp2py_export.obj [C:\os\onnx\.setuptools-cmake-build\onnx_cpp2py_export.vcxproj]
             libprotobufd.lib(substitute.obj) : error LNK2038: mismatch detected for '_ITERATOR_DEBUG_LEVEL': value '2' doesn't match value '0' in cpp2py_export.obj [C:\os\onnx\.setuptools-cmake-build\onnx_cpp2py_export.vcxproj]
             libprotobufd.lib(substitute.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MT_StaticRelease' in cpp2py_export.obj [C:\os\onnx\.setuptools-cmake-build\onnx_cpp2py_export.vcxproj]
             libprotobufd.lib(zero_copy_stream_impl.obj) : error LNK2038: mismatch detected for '_ITERATOR_DEBUG_LEVEL': value '2' doesn't match value '0' in cpp2py_export.obj [C:\os\onnx\.setuptools-cmake-build\onnx_cpp2py_export.vcxproj]
             libprotobufd.lib(zero_copy_stream_impl.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MT_StaticRelease' in cpp2py_export.obj [C:\os\onnx\.setuptools-cmake-build\onnx_cpp2py_export.vcxproj]
             libprotobufd.lib(io_win32.obj) : error LNK2038: mismatch detected for '_ITERATOR_DEBUG_LEVEL': value '2' doesn't match value '0' in cpp2py_export.obj [C:\os\onnx\.setuptools-cmake-build\onnx_cpp2py_export.vcxproj]
             libprotobufd.lib(io_win32.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MT_StaticRelease' in cpp2py_export.obj [C:\os\onnx\.setuptools-cmake-build\onnx_cpp2py_export.vcxproj]
             LINK : fatal error LNK1104: cannot open file 'python35.lib' [C:\os\onnx\.setuptools-cmake-build\onnx_cpp2py_export.vcxproj]

        2 Warning(s)
        73 Error(s)

    Time Elapsed 00:01:31.64
    Traceback (most recent call last):
      File "<string>", line 1, in <module>
      File "C:\os\onnx\setup.py", line 328, in <module>
        'backend-test-tools = onnx.backend.test.cmd_tools:main',
      File "C:\Users\chasun\AppData\Local\Temp\pip-build-env-pd3ncl01\Lib\site-packages\setuptools\__init__.py", line 145, in setup
        return distutils.core.setup(**attrs)
      File "c:\python35\lib\distutils\core.py", line 148, in setup
        dist.run_commands()
      File "c:\python35\lib\distutils\dist.py", line 955, in run_commands
        self.run_command(cmd)
      File "c:\python35\lib\distutils\dist.py", line 974, in run_command
        cmd_obj.run()
      File "C:\os\onnx\setup.py", line 219, in run
        self.run_command('build_py')
      File "c:\python35\lib\distutils\cmd.py", line 313, in run_command
        self.distribution.run_command(command)
      File "c:\python35\lib\distutils\dist.py", line 974, in run_command
        cmd_obj.run()
      File "C:\os\onnx\setup.py", line 203, in run
        self.run_command('cmake_build')
      File "c:\python35\lib\distutils\cmd.py", line 313, in run_command
        self.distribution.run_command(command)
      File "c:\python35\lib\distutils\dist.py", line 974, in run_command
        cmd_obj.run()
      File "C:\os\onnx\setup.py", line 197, in run
        subprocess.check_call(build_args)
      File "c:\python35\lib\subprocess.py", line 271, in check_call
        raise CalledProcessError(retcode, cmd)
    subprocess.CalledProcessError: Command '['C:\\Program Files\\CMake\\bin\\cmake.exe', '--build', '.', '--', '/maxcpucount:12']' returned non-zero exit status 1

    ----------------------------------------
  Rolling back uninstall of onnx
Command "c:\python35\python.exe -c "import setuptools, tokenize;__file__='C:\\os\\onnx\\setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\r\n', '\n');f.close();exec(compile(code, __file__, 'exec'))" develop --no-deps" failed with error code 1 in C:\os\onnx\
You are using pip version 10.0.1, however version 19.0.1 is available.
You should consider upgrading via the 'python -m pip install --upgrade pip' command.
```
houseroad(2019-02-01 19:14:43):Interesting, i will check the ci.
houseroad(2019-02-01 22:11:35):https://github.com/pytorch/pytorch/pull/16674 Let's see whether this would trigger or fix the problem on PyTorch side.
houseroad(2019-02-01 22:12:08):I have verified this is caused by different versions of numpy
zrphercule(2019-02-04 23:01:01):Bug found. Thanks @yinghai !
CLAassistant(2019-02-06 18:43:47):[![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/onnx/onnx?pullRequest=1794) <br/>Thank you for your submission! We really appreciate it. Like many open source projects, we ask that you sign our [Contributor License Agreement](https://cla-assistant.io/onnx/onnx?pullRequest=1794) before we can accept your contribution.<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/onnx/onnx?pullRequest=1794) it.</sub>
yuslepukhin(2019-11-20 22:08:50):This would conflict with the most recent build instructions bc it would take away package version flexibility. The flexibility is needed for two reasons:

- Some Conda packages versions are built improperly. Some versions are build in a form of a static lib, others are in the form of shared lib. We want to use the versions that work.
- Some users have reported that they want to use ONNX with products that are already using different version of protobuf. For them, rebuilding ONNX with different version of protobuf solves the issue.
prasanthpul(2020-02-24 00:53:00):Closing this old PR. Please reopen if necessary
houseroad(2019-02-07 00:34:18):The version parameter in `find_package` is tricky, it's defined package by package. I am not sure if it is a good idea to use it here. cc: @zrphercule @bddppq @snnn 
zrphercule(2019-02-07 21:50:31):How do we usually do version control like this? I wonder if we can change the source or something better than this.
yuslepukhin(2019-11-19 19:30:57):If you built protobuf from source then 2.6.1 is good. But when it come to packages the version is all over the place. Some Conda packages on Windows are broken, so we recently updated instructions to use 3.9.2 because it works. I think updating instructions is a better way of addressing it.
zrphercule(2019-02-07 19:26:45):cc @jackm321
zrphercule(2019-02-13 19:00:03):@yinghai @rdzhabarov  This is now ready to be merged. Noticing since we disabled onnxifi_dummy_backend in test driver, I will look back to the dummy backend part after this. But for now, this pr is okay to be merged.
yinghai(2019-02-13 22:12:47):CI failure? 
zrphercule(2019-02-15 18:57:09):@rdzhabarov mind if you unblock it and we merge it & update submodule in pytorch? Thanks!
zrphercule(2019-02-15 21:06:11):@onnxbot retest this please
rdzhabarov(2019-02-07 20:10:23):could we test that not only `ONNXIFI_STATUS_SUCCESS ` returned, but actual function will be assigned when name == "onnxSetIOAndRunGraph"?

edit: seem like it's on your `TODO` here
`
2\. write a real onnxGetExtensionFunctionAddressFunction in onnxifi dummy backend with a extension function list.
`
zrphercule(2019-02-07 20:13:17):Yeah, that's exactly I am doing... The main reason why this is "Unfinished" hahah
houseroad(2019-02-07 20:14:19):i guess this is only for temporary test?
zrphercule(2019-02-07 20:54:21):Yeah, I will add this macro in CMAKE, so it is not ready to be merged now.
yinghai(2019-02-08 02:49:11):ENABLE_ONNXIFI_EXT might be more natural? 
zrphercule(2019-02-08 18:43:02):Well, since all onnxifi related macros are start with an "ONNXIFI_...", I wonder if we really want to change this.
yinghai(2019-02-13 22:16:59):Where is `extension_function_number` defined? It is commented out here? 
zrphercule(2019-02-13 22:18:10):oh it's my bad, I thought I have already deleted this.
yinghai(2019-02-13 22:23:52):Why do we need to put these things here? The factor that we include "onnxifi_ext.h" also means we have access to these names, right? Such a table should be kept at the backend side IMO. 
zrphercule(2019-02-13 22:33:12):I also took some consideration when design it like this. My reason of putting these define here is we would like to make these two consts represent the super set of functions we support. Like, a backend may only support a few of functions in it, but still we want to know overall what is supported. Backend can have their own list and number if they need, or just use this two (like in dummy backend), meaning it support all fucntions.
rdzhabarov(2019-02-14 00:21:57):why not test `onnxSetIOAndRunGraph` function extension as part of the driver_test (given that the function is available)
rdzhabarov(2019-02-14 00:26:21):this `else` branch is not needed.
rdzhabarov(2019-02-14 00:27:58):what's the reason to represent all functions in this long char str? and not
char *list[] = {"a", "b"} etc.
^ with this search should be straightforward.
zrphercule(2019-02-14 19:01:11):That's exactly I would like to do. But the thing is since the driver test is down, we cannot test it anyway. Since I dont want to cause some potential bugs here, I will add related test in test driver once we fixed onnxifi driver test.
rdzhabarov(2019-02-14 23:44:11):what's this :P
rdzhabarov(2019-02-14 23:45:02):indentation looks weird.
rdzhabarov(2019-02-14 23:46:13):that's incorrect, we have `i` for this.
zrphercule(2019-02-15 00:18:03):It is because I just realize C lang does not accept "for (int i = 0; ...)".... T^T
jackm321(2019-02-15 00:35:49):What is this? Can you add a comment?
jackm321(2019-02-15 19:00:38):Hmm, what's wrong with that?
jackm321(2019-02-15 19:01:04):please comment these
zrphercule(2019-02-15 19:12:14):ok..
the "for(int i = 0....)" thing is because, this is C, and for (int i = 0; ...) is not allowed in C...
jackm321(2019-02-15 19:42:14):Ohh wow ok got it thanks
CLAassistant(2019-02-07 23:49:38):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=1797) <br/>All committers have signed the CLA.
raymondxyang(2019-02-07 23:56:16):please also push the updated doc, and generated test data; see https://github.com/onnx/onnx/blob/master/docs/AddNewOp.md#step-5-update-the-doc-and-generate-the-test-data for help
RandyShuai(2019-02-08 01:51:03):new test model uploaded.
raymondxyang(2019-02-11 19:06:09):@houseroad Hi lu just want to confirm the addition of testcase wont break things on ur side
houseroad(2019-02-11 19:28:30):I checked, the results I got was:

tensor([[[[21.1887, 56.1887, 13.1887, 16.1887,  2.1887],
          [63.1887, 35.1887, 67.1887, 10.1887, 14.1887],
          [24.1887, 22.1887, 76.1887, 76.1887, 21.1887],
          [ 9.1887,  5.1887, 88.1887, 45.1887, 63.1887],
          [ 3.1887,  2.1887, 33.1887, 18.1887, 54.1887]]]]

Did I miss something here?
RandyShuai(2019-02-11 19:53:15):Mind to share your script here?
houseroad(2019-02-11 20:06:03):Never mind, I forgot to set bias. Apparently, that noise is from bias.

The result look good to me now.
pk-g(2019-03-01 19:41:55):Thanks for the review, updated the PR with addressed feedback.
pengwa(2019-03-06 00:07:30):shall we enhance this op to support reverse sequences of different lengths? in tf, it is called tf.reversesequence().  
pk-g(2019-03-07 19:59:06):@pengwa  If you are referring to the case where sequential data of different lengths has been _padded_, it may be cleaner to have a separate op to cover such scenario.
pk-g(2019-03-15 00:34:43):@linkerzhang, @houseroad: can you please review the updated PR so we can move ahead with this op? thanks.
linkerzhang(2019-03-20 20:25:34):@pk-g can you update the PR by fixing the conflicts?

@houseroad any more comments on this op please?

Thank you!
houseroad(2019-03-21 22:47:28):I will do the review later this week, but please don't rush.

Will revert it here: https://github.com/onnx/onnx/pull/1882

Please resubmit the changes.
gramalingam(2019-02-14 20:14:25):inpu6 => input
linkerzhang(2019-02-18 02:49:20):the operator set should be 10. 

9 was the one released in ONNX 1.4.
houseroad(2019-03-01 17:51:52):Please use 10 instead.
gramalingam(2019-03-01 20:51:00):Is this correct? Shouldn't the axes=0 be passed to np.flip?
pk-g(2019-03-05 17:47:22):you're right, this is an unintended change. will fix.
hariharans29(2019-03-19 18:47:39):Sorry for this clarification so late on - but I assume this operator doesn't support "implicit" copying of Tensors by specifying duplicates in axes ?  For example, would axes of [0,0] mean no reversing but force the tensor's buffer to be copied ? Or simply, don't allow duplicates in axes at all - just like the way tf operates - https://www.tensorflow.org/api_docs/python/tf/reverse ?
pk-g(2019-03-19 19:59:26):Great question, this is more in line with the latter, no duplicate is expected here. 
CLAassistant(2019-02-11 21:25:25):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=1805) <br/>All committers have signed the CLA.
postrational(2019-02-12 21:55:47):@houseroad Let me know what you think.
postrational(2019-02-13 12:07:01):> I wonder what is the purpose to change methods like _prepare_model_data from static to a cls method? Thanks!

@zrphercule: The purpose would be to allow another library to use the ONNX test `Runner` class by inheriting from it, overriding some methods to modify behavior, but keeping the rest as is. 

When the methods are `@staticmethod` and are called from other methods as `Runner.method_name`, then the inheriting class can't override their behavior. They will always be called as `Runner.method_name`, never as `RunnerSubclass.method_name`.

Changing these to `@classmethod` would imply calling them as `cls.method_name`, so a subclass can override them.

I realize these are marked as `_hidden` methods, but it is still very useful to use the same testing logic as the ONNX library does in out customized tests.
houseroad(2019-02-14 19:22:00):@postrational I like this idea. Yes, changing static_method would make extension becomes possible. Feel free to do so.
bddppq(2019-02-15 02:14:20):I suggest let's rename `_prepare_model_data` to `prepare_model_data` to make it more clear that it's ok to override.
postrational(2019-02-18 19:07:37):I addressed the comments - changed static to class methods and renamed `_prepare_model_data` to `prepare_model_data`.

Could someone restart the failed Travis jobs to see if they will pass?
postrational(2019-02-26 13:37:11):This is done, unless you have any other comments.
houseroad(2019-02-20 06:21:39):since we already change this to classmethod, why not also turning it to public method?
houseroad(2019-02-20 06:26:20):same here, if we turn it to classmethod, that means it can be overridden. Let's turn it public
prasanthpul(2019-02-13 18:35:55):yeah CI needs to be passing.

would love to figure out how to merge
prasanthpul(2019-02-13 22:01:36):@houseroad CI is passing now.

let's open a new issue to discuss how/if to merge the profiles.
prasanthpul(2019-02-14 00:02:36):#1813 opened
houseroad(2019-02-14 06:57:43):The file permission has been changed.
shinh(2019-02-14 07:11:04):Done. I guess you'd like to keep 644 because users need to specify python/python3 explicitly? If this is the case, how about removing shebangs from 644 files?

CLAassistant(2019-02-13 07:45:28):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=1811) <br/>All committers have signed the CLA.
houseroad(2019-02-14 08:21:22):Please run https://github.com/onnx/onnx/blob/master/tools/update_doc.sh to update the doc. 
nbcsm(2019-02-14 13:19:21):fixed, please take another look.
zrphercule(2019-02-14 23:13:55):cc @jackm321
yinghai(2019-02-15 00:44:44):> think we should have onnx quantization support done in spec level firstly.

Hey, please feel free to figure out quantization in onnx. Onnxifi is not closely related to onnx spec. We have urgent internal need for this, in addition. 
rdzhabarov(2019-02-15 01:34:00):Irrespective of the outcome of this PR :)

> We should support `onnxCaffe2QTensorDescriptorV1` via Glow's own quantized tensor types but then is there other things we'll need to do? Will there be new c2 ops for these that we'll have to support?

@jackm321 
We already support resnet50_quantized by loading C2 quantized model directly from disc (so have some minimal set of ops supported). We'd need more operators but that part can be figured out down the road (based on the specific models and needs). Also, note, that in Glow we have much more quantized ops supported than number of quantized C2 proto we load now. So for many ops it's just a simple parsing support.

@beicy is working on wider coverage of C2 quantized models, see https://github.com/pytorch/glow/issues/2367.

From Glow perspective we just need to load tensors into Glow internal representation via ONNXIFI to make it work.
rdzhabarov(2019-02-14 23:57:57):does `ONNXIFI_BACKEND_EXTENSIONS` represent just a single key-value?
jackm321(2019-02-15 00:25:40):introduces
jackm321(2019-02-15 00:26:21):also curious about this, would be good to be able to distinguish between supported extensions right? 
jackm321(2019-02-15 00:28:16):too much indentation
jackm321(2019-02-15 00:29:14):too much indentation
jackm321(2019-02-15 00:30:05):should this be indented more?
jackm321(2019-02-15 00:30:18):why must it be 0?
jackm321(2019-02-15 00:31:50):It might be helpful to include somewhere in the comments here the formula that these represent `output = (input - zeroPoint) * scale`
houseroad(2019-02-15 01:10:54):This is very framework specific api... should it appear in ONNX?
Maratyszcza(2019-02-15 06:17:51):`ONNXIFI_BACKEND_EXTENSIONS` is a space-separated list of values
houseroad(2019-02-15 19:41:59):Do we have a plan to replace the whole internal IR structure here and rewrite the whole version converter?
If so, why not change this until then?

I am not sure why we cannot tell the truth here :-)

cc: @jspisak @bddppq 
prasanthpul(2019-02-15 20:04:12):no changes, just streamlining the documentation. it's weird to talk about how it's implemented before talking about what it does or why. while it's relevant to developers of the tool, it's not for users of it. I've added the sentence to the end. what do you think
jspisak(2019-02-16 03:15:34):Looks like you @prasanthpul added back the docs @houseroad mentioned. Should be good..
houseroad(2019-02-19 16:17:01):So if we set ONNX_ML in the parent cmake already, we don't need to check the environment variable any more. 
houseroad(2019-02-20 18:28:40):I think mask tensor should be bool type
gramalingam(2019-02-20 18:43:38):Yes, "Output(1)" type should be changed in the spec (line 1276) as well.
shinh(2019-02-21 05:41:34):Done. As for the spec change, I sent a separate PR: https://github.com/onnx/onnx/pull/1826
CLAassistant(2019-02-20 21:26:28):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=1824) <br/>All committers have signed the CLA.
zrphercule(2019-02-20 21:52:11):@houseroad yes it is......
houseroad(2019-02-20 21:59:28):I think you probably need to update the doc as well. 
zrphercule(2019-02-20 22:14:36):@houseroad It seems the docs remain unchanged, since we dont have model test related information in docs?
zrphercule(2019-02-20 22:54:09):Cause it is a model test...
shinh(2019-02-21 05:42:08):It seems the appveyor failure is not related to my change?
houseroad(2019-02-22 07:15:04):add e.g. before Tensor<float> or turn it to floating tensor?
CLAassistant(2019-02-22 02:35:59):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=1829) <br/>All committers have signed the CLA.
hariharans29(2019-02-22 02:36:07):CC: @linkerzhang 
linkerzhang(2019-02-24 05:11:39):@houseroad  any comments please? thanks a lot!
hariharans29(2019-03-01 20:09:10):@houseroad @linkerzhang - Is this change good to go ? Thanks!
spandantiwari(2019-03-07 00:09:53):@hariharans29 - there was a failing backend test which is why the CI has failed. That test (`test_densenet`) is disabled for now. Could you try running the CI job again to see if this passes?
spandantiwari(2019-03-07 00:14:58):@houseroad  - just FYI - the failing test was `test_densenet`, which we, coincidentally, disabled today (https://github.com/pytorch/pytorch/pull/17696) for the `Upsample` PR (https://github.com/onnx/onnx/pull/1773). 
hariharans29(2019-03-07 00:44:57):> Sorry to block:
> 
> Yes, we do use topk in pytorch-onnx-caffe2
> https://github.com/pytorch/pytorch/blob/master/torch/onnx/symbolic.py#L1305

@houseroad - Sorry I don't quite understand. Could you please elaborate on the changes you are requesting ?

houseroad(2019-03-07 00:49:02):@hariharans29 I don't mean you need to change anything, but we need to prepare for the change. Otherwise, it will break our internal pipeline. I am thinking of setting the pytorch onnx exporter to some stable opset by default. So such changes won't cause problem anymore.
hariharans29(2019-03-07 00:57:50):@houseroad - Got it. I will wait for your go-ahead. Thanks.
hariharans29(2019-03-08 00:15:59):@houseroad - Thanks a lot!
linkerzhang(2019-02-27 23:48:40):@houseroad @zrphercule  any comments on this change? Thanks!
houseroad(2019-03-04 05:14:11):I will look into the CircleCI problem.
gramalingam(2019-03-06 23:48:10):@houseroad : please let us know if you have any comments on this PR. Thanks!
raymondxyang(2019-03-08 22:11:23):> as talked offline, the way to ensure/verify the function body's correctness (compliance) is, when folks adding test case for a function, there should be two models generated, one model is with function node, one model is with function inlined. With same inputs/outputs, the two models should work as same in terms of correctness.

is this verification supposed to be the next PR?
linkerzhang(2019-03-11 18:39:53):Thank you!
gramalingam(2019-02-26 18:31:51):I think we need to specify only a list of nodes here. FunctionProto has extra fields (name, inputs, outputs, attributes) which replicate what is separately specified in an OpSchema. This is an unnecessary duplication (what happens if different values are specified in the two places? There is no check for that.)
raymondxyang(2019-02-26 19:04:01):I checked that in verification.. but its definitely right we need to remove this. Ill change it.
gramalingam(2019-02-27 22:06:21):"return [schema for schema in schemas if schema.has_function_body]" would be more compact.
linkerzhang(2019-02-28 21:58:01):Change this to "Functions".
linkerzhang(2019-02-28 22:01:43):no, this should not be changed, Removing these fields means the function is not complete.
linkerzhang(2019-02-28 22:05:49):a list of nodes are good enough.
linkerzhang(2019-03-05 19:36:52):maybe "IsFunction()"
linkerzhang(2019-03-05 19:42:15):1. GetFunction
2. The function may be owned by the schema.
linkerzhang(2019-03-06 02:09:34):I'm seeing we're also having onnx-operators-ml.proto.  You may either remove this proto to have only one proto file, or handle both cases (ONNX_ML=0 and ONNX_ML=1).
linkerzhang(2019-03-06 02:10:34):a better name  is GetFunciton, as here's not only getting its body (bunch of nodes), but all function related stuff.
linkerzhang(2019-03-06 02:36:24):per the comment, name it as "BuildFunction"?
linkerzhang(2019-03-06 02:36:39):typo: inofrmation ?
raymondxyang(2019-03-06 23:17:49):This file is actually a auto-switch between onnx-operators-ml.pb.h and onnx-operators.pb.h
houseroad(2019-03-07 00:23:02):This is better handling, thanks.
houseroad(2019-03-07 00:23:51):can we call it `has_function` or `contains_function`?
houseroad(2019-03-07 00:25:37):Nit: BuildMVNFunction?
houseroad(2019-03-07 00:28:22):same here... `contains_function` or `has_function` feels more appropriate to me. 
houseroad(2019-03-07 00:32:34):Can we add a counter here, to assert, we at least verify one function?
linkerzhang(2019-03-08 20:47:19):Change this to ""Functions""
linkerzhang(2019-03-08 20:48:46):this file should not be in experiments folder any more, and its file name should not have "experiments_" prefix neither.
linkerzhang(2019-03-08 20:51:38):HasFunction? I'm seeing the other two functions you added with name "FunctionBody" and "GetFunction". Let's keep the name consistent.
houseroad(2019-03-08 21:01:46):Only call it Functions may be misleading (trust me, users may get confused by functions again), I think Operators with function registered is Okay. Because they are operators, right?
linkerzhang(2019-03-11 18:37:14):don't put it under "experiments" folder, I suggest.
skottmckay(2019-02-27 20:13:33):Can use changes from https://github.com/onnx/onnx/commit/873ddbbc33c6e54d90c5628387edd391fb651dfc instead
BowenBao(2019-03-04 19:13:19):@linkerzhang part of the CI is failing on various caffe2 & pytorch test cases & models using the old slice spec(<=opset9). What is the recommended way of working around that?
houseroad(2019-03-05 07:00:30):This change looks good to me. But we should prepare for it.
spandantiwari(2019-03-12 17:01:49):@houseroad - all the tests are passing. Shall we go ahead and merge it now?
houseroad(2019-03-12 17:06:13):Yeah, it should be fine to merge.
ayermolo(2019-05-24 19:17:14):@BowenBao So to clarify. This is for opset 10. For opset < 10 starts/ends is still part of the attributes?
BowenBao(2019-05-24 21:51:05):> @BowenBao So to clarify. This is for opset 10. For opset < 10 starts/ends is still part of the attributes?

yes.
linkerzhang(2019-02-27 19:25:45):so the "steps" is used to specify the axis order/step of slicing? if that's the case, can we just use/follow the "axes" indices order please?
BowenBao(2019-02-27 19:54:24):I'm not sure I understand the question correctly. Is your question suggesting that for a given axis `i = axes[j]`, `output[i]` should be computed based on `starts[j]`, `ends[j]`, `steps[j]`? If that's the case then yes that is the current design, I'll update the description to be more precise.
linkerzhang(2019-02-28 01:43:19):This is solving the issue #1772 
fdwr(2019-09-12 21:16:46):minor typo repeated a few times: "the input.In case"
houseroad(2019-03-07 00:09:56):Seems we need to tweak the script a bit to add the newline character
letmaik(2019-03-07 08:50:36):Sorry, I didn't actually run the script, but made the changes manually as they seemed simple. Feel free to take over this PR.
houseroad(2019-04-02 21:17:45):I created another PR to fix the problem. https://github.com/onnx/onnx/pull/1903/
houseroad(2019-03-12 17:40:35):@Ac2zoom feel free to review if you get time. :-)
CLAassistant(2019-03-14 16:49:27):[![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/onnx/onnx?pullRequest=1847) <br/>Thank you for your submission! We really appreciate it. Like many open source projects, we ask that you all sign our [Contributor License Agreement](https://cla-assistant.io/onnx/onnx?pullRequest=1847) before we can accept your contribution.<br/>**2** out of **3** committers have signed the CLA.<br/><br/>:white_check_mark: prasanthpul<br/>:white_check_mark: houseroad<br/>:x: pk-g<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/onnx/onnx?pullRequest=1847) it.</sub>
prasanthpul(2019-03-26 00:18:56):@Ac2zoom / @houseroad - any feedback on this?
Ac2zoom(2019-04-03 07:12:55):> @Ac2zoom feel free to review if you get time. :-)

Other than @spandantiwari's comments, looks good to me!
spandantiwari(2019-04-01 02:04:33):Since the code here and below strongly assumes that input[1], it may  be useful to have a quick check that input[1] is there, so that just in case there's an invalid graph without it, we throw a clear and meaningful error message.
spandantiwari(2019-04-01 02:21:33):Wold it be possible to use float here instead of double given that spec takes scales as floats? 
pk-g(2019-04-04 16:30:29):It would not. fs is using double in IR. 
pk-g(2019-04-04 16:42:37):Yes, the code strongly assumes there is an input[1] here because in upsample 9 input[1], is "scale", which is required and is not optional. Therefore, if there's no input[1] here we are dealing with an invalid model. From design perspective, model verification in terms of having required inputs and parameters is not part of converters role. so, if an invalid model is passed to converter in first place, there's not much that can be done and issues shall bubble up.
spandantiwari(2019-04-04 17:20:36):Yes, I agree the model is invalid, but if you have an opportunity to throw a meaningful error message with minimal perf impact, it is preferable to have it. 
linkerzhang(2019-03-08 01:52:07):@shinh as a FYI please.
yuslepukhin(2019-03-09 00:18:15):I am reluctant to write code that tests the tester.
houseroad(2019-03-09 00:35:20):So I guess the existing test cases of StringNormalizer should already cover this. Is it true?
yuslepukhin(2019-03-09 00:38:20):> 
> 
> So I guess the existing test cases of StringNormalizer should already cover this. Is it true?

Yes, this is how the issue was discovered.
shinh(2019-03-19 06:47:59):@houseroad : Can I request you to review this PR since you have reviewed a few PRs I made for shape inference?
houseroad(2019-03-22 06:36:35):Also could you please add a test case to https://github.com/onnx/onnx/blob/master/onnx/test/shape_inference_test.py
shinh(2019-03-22 13:48:59):Ah, I thought this op is `np.broadcast_to`. Now it handles mutlidirectional broadcasting. I've added tests for them, too. Thanks for your review!
shinh(2019-03-29 09:08:49):Ping. Could you take another look?
shinh(2019-04-05 03:41:38):> please add a test in which shape's dim is larger than input's dim.

Sorry, I missed this comment. Added a test and fixed the implementation. I hope I'm not missing anything now..
shinh(2019-04-17 01:09:27):Ping? Are there still issues?
shinh(2019-05-01 13:38:18):@houseroad friendly reminder! Is there still any issue?
CLAassistant(2019-07-24 00:57:41):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=1855) <br/>All committers have signed the CLA.
shinh(2019-09-18 00:34:56):It seems this was fixed by https://github.com/onnx/onnx/pull/2041
houseroad(2019-04-03 03:21:00):nit: targetShape => target_shape
houseroad(2019-04-03 03:33:15):Dimensions are right aligned. Please check the case here: https://github.com/onnx/onnx/blob/master/docs/Operators.md#expand
houseroad(2019-04-03 03:34:40):this is an incorrect case.
houseroad(2019-04-03 03:35:02):this should be d1, 2, d3
shinh(2019-04-03 04:29:17):Done
shinh(2019-04-03 04:29:52):Oops, sorry for the incorrect understanding. Done
shinh(2019-04-03 04:30:10):Done
shinh(2019-04-03 04:30:14):Done
CLAassistant(2019-03-11 18:02:51):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=1856) <br/>All committers have signed the CLA.
linkerzhang(2019-03-11 18:10:23):Please also add some test cases for it. Refer to https://github.com/onnx/onnx/tree/master/onnx/backend/test/case/node for examples, and refer to https://github.com/onnx/onnx/blob/master/docs/OnnxBackendTest.md on how to generate test data and update docs accordingly.
linkerzhang(2019-03-11 18:11:37):This is following PR for discussion on https://github.com/onnx/onnx/pull/1616.

@zrphercule  please review. Thanks!
askhade(2019-03-11 18:34:42):This op already has test cases : [https://github.com/onnx/onnx/blob/master/onnx/backend/test/case/node/thresholdedrelu.py](https://github.com/onnx/onnx/blob/master/onnx/backend/test/case/node/thresholdedrelu.py)

---
In reply to: [471657141](https://github.com/onnx/onnx/pull/1856#issuecomment-471657141) [](ancestors = 471657141)
houseroad(2019-03-12 17:39:24):CI is broken, please fix it
CLAassistant(2019-03-12 04:23:30):[![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/onnx/onnx?pullRequest=1858) <br/>Thank you for your submission! We really appreciate it. Like many open source projects, we ask that you sign our [Contributor License Agreement](https://cla-assistant.io/onnx/onnx?pullRequest=1858) before we can accept your contribution.<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/onnx/onnx?pullRequest=1858) it.</sub>
xsacha(2019-03-12 04:56:14):This patch actually isn't required it seems. Sorry about that.
BowenBao(2019-07-02 21:52:17):Closing, replacing with #2148 
houseroad(2019-03-22 17:12:38):I think we should explain more about how we handle dynamic/variable dimension in ONNX. You may link the corresponding doc as well.
CLAassistant(2019-03-12 18:20:10):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=1860) <br/>All committers have signed the CLA.
houseroad(2019-03-13 07:35:36):@lara-hdr could you sign the CLA?
lara-hdr(2019-03-15 18:27:56):@houseroad @zrphercule any comments on this?
linkerzhang(2019-03-13 22:40:02):@wschin 
yuslepukhin(2019-05-10 23:32:27):Upon further discussion and due to high volatility of the spec, we'd like to postpone this operator introduction into ONNX. This PR will be closed until further notice.
houseroad(2019-03-22 16:43:19):Are we sure we intentionally want to add this to ONNX-ML opset version 1, not version 2?
yuslepukhin(2019-03-26 18:24:07):Will bump the version
gramalingam(2019-03-28 20:49:10):Changing "[0]" to "shape [0]" will make this slightly easier to parse/read.
gramalingam(2019-05-07 22:06:23):Copy (local) output_shape to output, or make it a reference. (But the reference would be a problem if neither branch is taken, since it will appear as though output-shape has rank 0.)
gramalingam(2019-05-07 22:14:37):If rank > 2, may be it would be better (for debugging) to throw a shape-inference exception.
raymondxyang(2019-03-13 17:54:48):Added testcase validated with backend implementation 
![image](https://user-images.githubusercontent.com/30183166/54302576-68535b80-457e-11e9-8ae0-72c0ab9f1c56.png)

linkerzhang(2019-03-13 18:55:24):This means, for an op with function body checked in to onnx, for any test case added, there will be two models generated, one with the function, the other with function body inline-ed.  With same inputs, it's expected to have same outputs when running these two models respectively. In this way, we can ensure that the function body attached to this "composed" op is correct.
gramalingam(2019-03-13 20:17:13):Isn't this an error-situation? May be it would be better to return an empty list, instead of continuing, if there is an error like this. Similarly, what if for a function_proto.input there is no matching node.input? In that case, I think we should treat it as an empty string (a missing optional input).
gramalingam(2019-03-13 20:18:04):Same comments as for input.
raymondxyang(2019-03-13 20:25:18):tbh the checker in gtests should catch this later. so however we handle the mismatched # of io tensors is fine here
gramalingam(2019-03-14 03:50:53):Hi, the more important change is: make "input_names_map[function_proto.input[idx]]" the empty string if "idx >= len(node.input)". E.g., iterating over range(len(function_proto.input)) would be fine.
CLAassistant(2019-03-13 07:07:39):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=1863) <br/>All committers have signed the CLA.
maxwillzq(2019-03-13 16:14:56):@houseroad ,

Thanks a lot for your comments.  Could you help take a look at issue #1302 while install does not work to when load defs in python?

maxwillzq(2019-03-15 04:08:05):After clean build, it have no problem. Thanks a lot
houseroad(2019-03-18 19:56:57):@maxwillzq since the problem is solved, can we close the PR?
maxwillzq(2019-03-19 04:15:48):Yes, we can 
CLAassistant(2019-03-14 04:32:42):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=1864) <br/>All committers have signed the CLA.
karljang(2019-03-19 04:29:10):Hi @houseroad, 
I've made it up to date.
I think the MaxPool with dilation is quite useful for speech recognition models.
Can I get this PR reviewed?

karljang(2019-03-19 10:12:57):@houseroad 
I've added a dilation test case for MaxPool operator.
Thanks for your approval~

houseroad(2019-03-22 06:18:16):friendly ping for review @ebarsoum @linkerzhang @raymondxyang 
karljang(2019-03-27 05:42:42):Thank you all~!
linkerzhang(2019-03-14 20:21:17):it's discussed and agreed to remove this op.
zchrissirhcz(2019-11-30 02:34:41):@linkerzhang @askhade Why this op is removed from ONNX, is there any details?
gramalingam(2019-03-18 17:30:13):Please look at the CI error. It seems to be complaining about "onnx/defs/experiments/functions.cc" … not sure what that is.
SherlockNoMad(2019-03-18 19:40:29):Hi @houseroad, could you please have a look on this PR when you got a chance ? 

This PR is intended to simplify the process for defining a Function Body. 
gramalingam(2019-03-18 17:24:33):An example to illustrate might be helpful: e.g., "{ {Z}, "Add", {"X", "Y"} }" represents "Z = Add(X,Y)".
linkerzhang(2019-03-18 22:46:17):this is not related to this PR.

Can you help to move this file out of experiments folder please? since the MVN is not "experimental".

Thank you!
linkerzhang(2019-03-18 23:27:59):looks to me this wrapper is not that useful, but creating one more copy. I may miss sth though.
SherlockNoMad(2019-03-19 01:53:10):Moved to nn/defs.cc
SherlockNoMad(2019-03-19 01:59:52):This class serve as a syntactic sugar for easily define an attribute, so that user simply write following without worrying about how to compose a AttributeProto. 

{"attribute", 1.0f}
{"attribute", int64_t(1)}
{"attribute", "string_value"}
{MakeRefAttribute("axes", AttributeProto::INTS)}}

CLAassistant(2019-03-16 03:14:11):[![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/onnx/onnx?pullRequest=1869) <br/>Thank you for your submission! We really appreciate it. Like many open source projects, we ask that you sign our [Contributor License Agreement](https://cla-assistant.io/onnx/onnx?pullRequest=1869) before we can accept your contribution.<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/onnx/onnx?pullRequest=1869) it.</sub>
sampepose(2019-03-16 20:01:23):cc @rbgirshick for api review
prasanthpul(2019-04-09 05:47:02):@sampepose can you please address the feedback and resolve the conflicts?
sampepose(2019-04-09 05:55:07):Yeah, I should be able to get to it this week.


> On Apr 8, 2019, at 10:47 PM, Prasanth Pulavarthi <notifications@github.com> wrote:
> 
> @sampepose can you please address the feedback and resolve the conflicts?
> 
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub, or mute the thread.

prasanthpul(2019-04-18 01:58:24):@sampepose any update?
jspisak(2019-04-22 20:32:23):@sampepose - let us know if you need help landing this one. Right now it is gating support for detection models for ONNX v1.5 so I'd like to see what the open issues are and try to address them. 

pranavsharma(2019-04-22 21:26:32):Does it make sense to add a 'mode' attribute that determines the pooling method? "avg" or "max"? 
pranavsharma(2019-04-22 23:13:13):I think we should make 2 concrete changes here:
* rois should be specified along with the batch_id (just like pytorch). 
```      .Input(1, "rois", "RoIs (Regions of Interest2) to pool over; rois is 2-D input of shape (num_rois, 5) given as [[batch_id, x1, y1, x2, y2], ...]. The RoIs' coordinates are in the coordinate system of the input image.", "T")```
* Introduce a pooling mode to specify avg/max with default as avg.
```      .Attr(
          "mode",
          "The pooling method. Two modes are supported: 'avg' and 'max'. "
          "Default is 'avg'.",
          AttributeProto::STRING,
          std::string("avg"))
```
cc @spandantiwari @BowenBao 
daquexian(2019-04-23 11:56:26):I noticed @Maratyszcza said "storing batch index in a float tensor is a 100% bad idea." in https://github.com/onnx/onnx/pull/1010#issuecomment-459268400. Should we still follow the current pytorch spec?
spandantiwari(2019-04-23 17:24:47):> I noticed @Maratyszcza said "storing batch index in a float tensor is a 100% bad idea." in [#1010 (comment)](https://github.com/onnx/onnx/pull/1010#issuecomment-459268400). Should we still follow the current pytorch spec?

I agree with @Maratyszcza that batch_index should ideally not be stored in float tensor. How about creating a separate input, with correct type, for batch_index as @BowenBao has suggested above.
pranavsharma(2019-04-23 17:29:37):> I noticed @Maratyszcza said "storing batch index in a float tensor is a 100% bad idea." in [#1010 (comment)](https://github.com/onnx/onnx/pull/1010#issuecomment-459268400). Should we still follow the current pytorch spec?

Yup, this makes sense. Creating a separate input for batch_index is fine.
sampepose(2019-04-23 19:17:32):need to update the changelog, one sec
BowenBao(2019-04-23 20:01:27):Comments on the update
1. The description of input batch_index needs to be updated. It is still the old one for batch_split. 
2. Similarly, the input data for input batch_index in roi_align.py needs to be updated.  
sampepose(2019-04-23 20:32:35):@BowenBao, @pranavsharma , @houseroad: Let me know if you want any additional changes. All comments should be addressed now.
BowenBao(2019-04-23 21:00:00):> @BowenBao, @pranavsharma , @houseroad: Let me know if you want any additional changes. All comments should be addressed now.

@sampepose thank you for the updates!

Two minor changes:
1. Update batch_indices description phrasing
```
          "1-D tensor of shape (num_rois,) with each element denoting "
          "the index of the corresponding image in the batch.",
```
2. In roialign.py
change
```batch_indices = np.array([0, 0, 0], dtype=np.int32)```
to
```batch_indices = np.array([0, 0, 0], dtype=np.int64)```
pranavsharma(2019-04-23 21:45:01):LGTM 👍 (except @BowenBao's roialign.py comment). 
spandantiwari(2019-04-23 22:40:51):Thanks @sampepose . Looks good. 

BowenBao(2019-03-21 22:50:59):This doesn't seem to support all cases in the current api design? Currently (both pytorch and onnx) rois are 2-D input of shape (num_rois, 5) given as [[batch_id, x1, y1, x2, y2], ...], which might not be sorted by batch_id. How about having the input as
```
.Input(
  2,
  "batch_indices",
  "Tensor of shape (num_rois) with each element denoting "
  "the index of the corresponding image in the batch.",
  "T2")
```
BowenBao(2019-03-21 22:51:09):Similar to https://github.com/onnx/onnx/blob/c05f2ae412daf8fd64136ca354b97ccf73e0ea6c/onnx/defs/nn/defs.cc#L683, consider more checks (e.g.: input_shape.dim_size() == 4, rois_shape.dim_size() == 2, etc). 
houseroad(2019-03-22 16:39:24):Do we need to support so many int types for this input?
wenbingl(2019-04-18 17:30:30):Can we seperate batch_split from this op?
pranavsharma(2019-04-22 21:45:58):Or we can just follow the pytorch spec ([[batch_id, x1, y1, x2, y2], ...]) for this? Will probably make conversions easier. 
prasanthpul(2019-04-23 17:04:28):minor: capitalization of I. exiting MaxRoiPool op does not capitalize the I. should keep it consistent
houseroad(2019-04-23 17:27:04):Minimum check/assert is recommended before accessing the shape. It will help us detect the problem early and print out reasonable error message.
pranavsharma(2019-04-23 19:18:18):One minor comment: do the batch indices really need to be so many different types? Shouldn't just int64 suffice to be super conservative?
pranavsharma(2019-04-23 19:22:16):nit: would be good to call out that each of the co-ordinate sets has a 1-1 correspondence with the batch_index tensor. It might be obvious to some and not to others.
pranavsharma(2019-04-23 20:22:55):@sampepose Can you update the documentation for this input? Also, since we'll be storing multiple indices, let's call it batch_indices like @BowenBao mentioned. I think this should be the final change. 
linkerzhang(2019-03-18 18:55:20):Test cases for all ops will be added soon.
linkerzhang(2019-03-29 04:14:52):am going to have all test cases covered in separate PR soon.
linkerzhang(2019-03-29 19:52:18):@raghuramank100 I'm going to check it in today. Please let me know if you have more comments.

@spandantiwari please help to drive the pytorch test failure. I'm not taking the failure as a check-in blocker.

Thank you!
houseroad(2019-04-03 21:54:06):CI side is okay. Waiting @raghuramank100 for the final approval :-)
linkerzhang(2019-04-04 23:32:02):as clarified in mail thread, the "adding axis" comment from @raghuramank100  may be added when we see use cases and want to support per axis quantization for activations. I'm merging this PR now. @raghuramank100  please feel free to share more comments if any. we can keep tuning it.
houseroad(2019-04-05 00:46:30):The merged version is prematured. Please address my comments and resubmit the PR. Thanks
diyessi(2019-03-20 22:50:48):Change to `and a zero point to compute the ...`
diyessi(2019-03-20 22:53:57):Change to `and a zero point to compute the ...`
raghuramank100(2019-03-27 06:26:42):Better to have y_zero_point as optional in the quantizer if it is optional in the dequantizer so that we are consistent.
raghuramank100(2019-03-27 06:29:37):We should explicitly pass the axis as an argument here to be consistent with the definition of per-row/per-column/per-channel quantization.
raghuramank100(2019-03-27 06:32:44):Lets explicitly provide the axis as a parameter here. For example if I have 3x3x3x32 (3 input channels, 3x3 kernel and 32 output channels), I would specify axis=3 and have a vector of length 32 to specify the per-channel scale and zero-point
linkerzhang(2019-03-28 01:30:37):I don't think that we need this flexibility, since for per-channel, it has to be per output channel (for conv) for weights. For matmul, if it's not per tensor, per row for the first and per-column for the 2nd is a good one, but per-column for the 1st input and per-row for 2nd input is not that useful (math is not that straight-forward). So I'd suggest to keep this specification.
linkerzhang(2019-03-28 01:30:49):Sounds good.
linkerzhang(2019-03-28 01:32:49):for conv, the most frequent used one is per-layer (per-tensor), or per output channel (for weights). I don't see much usage about other cases. I'd suggest to keep the spec and extend it when we need the other cases (flexibility). Sounds good?
raghuramank100(2019-03-29 19:58:29):I see what your saying but it might be good to be consistent across the spec. For Quantize_Linear, we specify axis as attribute:
   .Attr(
            "axis",
            "The axis along which same quantization parameters are applied. It's optional. If it's not specified, it means per-tensor quantization and input 'x_scale' and 'x_zero_point' must be scalars. If it's specified, it means per 'axis' quantization and input 'x_scale' and 'x_zero_point' must be 1-D tensors.",
            AttributeProto::INT,
            false)

Might be good to make this consistent across all ops. We could still enforce that the op itself only supports per-row for the first tensor and per column for the second one. 
raghuramank100(2019-03-29 20:00:22):Should we also have an axis argument, like we have with the quantizer?
linkerzhang(2019-03-29 20:28:17):My bad. "axis" for quant should be removed.
linkerzhang(2019-03-29 20:29:05):clarified above, "axis" in quant should be removed. given only "weights" has per output channel quant (which is static).
ke1337(2019-04-01 21:22:30):Please consider support int16 as well. There's cblas_gemm_s16s16s32 for int16 inputs to generate int32 outputs.
linkerzhang(2019-04-02 01:18:29):@darrenscrews is going to make that (more types support) in separate PR.
fdwr(2019-04-04 23:32:07):Guessing this was input from Intel to change from round-evens-away-from-zero (which was our earlier choice, which was TensorFlow's default and C++ std::round's default)?
fdwr(2019-04-04 23:50:44):Well it's certainly simpler to implement DequantizeLinear with `zero-point` and `scale` being scalars. We originally included the `axis` attribute because it was a compromise between space savings and accuracy, and with channel-wise weight packing, @youngkim93 noticed significant accuracy improvements across the various image recognition models. Are you dropping it because QLinearConv doesn't have it? (granted, getting QLinearConv to support it would greatly complicate things) This should be compatible with our current code because the `axis` was optional anyway.
houseroad(2019-04-05 00:20:18):what is this?
houseroad(2019-04-05 00:44:02):output's is non-utf8 character.
CLAassistant(2019-03-19 16:22:50):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=1874) <br/>All committers have signed the CLA.
linkerzhang(2019-03-21 18:29:45):please fix the ci failure - "./onnx/backend/test/case/node/mod.py:39:25: E261 at least two spaces before inline comment"
linkerzhang(2019-03-23 00:40:33):please update test coverage doc. TestCoverage.md.
fdwr(2019-03-26 21:20:51):@jeffsaremi : The spec should be clear regarding signed values, because different languages return different results, some following the numerators sign, others doing some completely different. e.g.:

``` C++
printf("%d %d %d %d\r\n", 8 % 3, 8 % -3, -8 % 3, -8 % -3);
printf("%f %f %f %f\r\n", fmod(8, 3), fmod(8, -3), fmod(-8, 3), fmod(-8, -3));
...
2 2 -2 -2
2.000000 2.000000 -2.000000 -2.000000
```

``` Python
print(8 % 3, 8 % -3, -8 % 3, -8 % -3)

x = np.array([8,  8, -8, -8]).astype(np.float32) 
y = np.array([3, -3,  3, -3]).astype(np.float32) 
z = np.mod(x, y)
print(z)

x = np.array([8,  8, -8, -8]).astype(np.int32) 
y = np.array([3, -3,  3, -3]).astype(np.int32) 
z = np.mod(x, y)
print(z)
...
2 -1 1 -2
[ 2. -1.  1. -2.]
[ 2 -1  1 -2]
```

https://stackoverflow.com/questions/44411646/python-and-c-modulus

I'd include either a signed value in your example, or explicitly state it in the wording.
fdwr(2019-03-26 21:39:13):Also wonder your expectations for division by zero? On the CPU, your application could encounter a fatal application runtime exception (:/) if your tensor happens to have a zero in it somehow, depending on the language and data type (e.g. error in Python for divmod float or int, but NaN in C++ for float and error for integer). On the GPU, you get NaN back. For security and the sake of untrusted data (think of a model you download or dubious inputs), it's good to consider this, as we don't want inputs terminating the application. Granted, the existing Div operator has no notes on this either.
jeffsaremi(2019-03-26 22:53:54):@fdwr 
I will address the points you made.
Regarding the sign treatment I will likely not add any wording but add more descriptive examples.
WRT zero treatment, I scoured the Operators.md and found two similar operators which account for this: `InstanceNormalization `and `BatchNormalization`.
They both have an `Attribute `called 'epsilon' with a description of:
'The epsilon value to use to avoid division by zero.'
i can carry on the same tradition

fdwr(2019-03-26 23:18:43):@jeffsaremi : Example is good for signedness.

As for division by zero, I'd actually keep it consistent with what the most closely related elementwise operator Div does, rather than add epsilon. I mainly wanted to discuss what the desired output should be rather than trying to change your existing operator signature (personally I'm okay with NaN on the GPU - garbage in, garbage out). The approach of adding epsilon may not give you quite what you want because it will change the output result. 8 mod 3 equals exactly 2 currently, but with epsilon it will equal 1.9999.
linkerzhang(2019-03-29 20:57:00):@houseroad any more comments please?
houseroad(2019-04-03 03:40:45):@jeffsaremi could you please address my last comment?
linkerzhang(2019-04-19 06:30:27):@houseroad any more comments please? Thank you!
ebarsoum(2019-04-19 17:59:52):@houseroad any update?
jeffsaremi(2019-04-19 20:09:51):Is there a kind person out there that could merge this? thanks
faxu(2019-04-19 21:51:46):> Is there a kind person out there that could merge this? thanks

@bddppq Can you help review this for inclusion in 1.5? :)
jeffsaremi(2019-04-19 23:04:16):Thanks Emad for merging this.
Thanks everyone for reviewing and correcting. 
houseroad(2019-03-27 05:53:44):if we support both mod for int and fmod for floating numbers, we should make it clear here.
houseroad(2019-03-27 05:54:17):also add test cases for int tensors?
jeffsaremi(2019-03-27 18:30:56):I will add this soon
jeffsaremi(2019-03-27 18:37:22):There is no fmod operator suggested. Mod operator accepts both floating point and integers. The above shortcut points to the following line:
```
    schema.TypeConstraint(
        "T",
        OpSchema::numeric_types_for_math_reduction(),
        "Constrain input and output types to high-precision numeric tensors.");
```
and the numeric_types_for_match_reduction function is defined as below which covers the applicable cases in our scenario:

```
  static const std::vector<std::string>& numeric_types_for_math_reduction() {
    static const std::vector<std::string> numeric_types_for_math_reduction = {
        "tensor(uint32)",
        "tensor(uint64)",
        "tensor(int32)",
        "tensor(int64)",
        "tensor(float16)",
        "tensor(float)",
        "tensor(double)"};
    return numeric_types_for_math_reduction;
  }
```
jeffsaremi(2019-03-27 18:46:41):Actually I just noticed the diffs between fmod and mod. I will make sure Mod operator can emulate both.
jeffsaremi(2019-03-27 21:31:05):updated docs and examples
jeffsaremi(2019-03-27 21:31:11):updated docs and examples
gramalingam(2019-03-28 20:31:41):What is the type of the returned value if "fmod" is specified? The op-schema type indicates that the type is the same, regardless of fmod. In this case, the line "the remainder will be floating point" seems wrong and should be removed.
gramalingam(2019-03-28 20:32:28):"forces" => "forced"
gramalingam(2019-03-28 20:33:39):"dependenant" => "dependent"
jeffsaremi(2019-03-28 20:47:54):corrected. thanks for checking
jeffsaremi(2019-03-28 20:48:05):corrected. thanks for checking
jeffsaremi(2019-03-28 20:48:25):Removed the erroneous documentation line
houseroad(2019-03-29 22:00:48):shall we make it clear that if input type is floating numbers, fmod must be 1?
yuslepukhin(2019-04-19 18:12:32):IMHO, standards exists to make sure there is not a platform dependent behavior.
fdwr(2019-04-19 18:21:31):@yuslepukhin : Generally agreed. Though, even standards don't full specify degenerate situations (e.g. I can point out many lurky places in the OpenType spec), and this case is at least consistent with the corresponding `Div` operator in being unspecified. If we were to mandate a specific result for all CPU/GPU architectures, then it should be consistent between `Div` and `Mod`.
ebarsoum(2019-04-19 18:24:15):It is difficult to mandate that for various HW backend.
jeffsaremi(2019-04-19 18:27:48):This is a good topic for general "error handling" policy for (all?) operators. Currently only a select number of operators allow the user to specify what should be done -- typically in the form of defaults. We need a much more widespread usage of ErrorHandling (or Default) attributes to achieve this form of consistency
jeffsaremi(2019-04-19 20:09:15):@yuslepukhin Let me know if you want me to re-open this. thanks
houseroad(2019-03-22 06:09:18):nit: default_axes sounds too general. how about mvn_default_axes?
gramalingam(2019-03-22 20:04:06):The following documentation line "The default is to reduce over all the dimensions of the input tensor." sounds contradictory. Please delete that line as well.
houseroad(2019-03-21 03:23:43):I remember we agreed on having a better registration mechanism for operators, which should help us avoid such bugs introduced in the previous PRs. cc: @bddppq @linkerzhang 
houseroad(2019-03-21 17:22:33):The check fails on the check here: https://github.com/onnx/onnx/blob/master/onnx/defs/schema.cc#L820

We should have a CI instance which enables the debug mode.
CLAassistant(2019-03-21 22:36:27):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=1881) <br/>All committers have signed the CLA.
wschin(2019-03-22 17:08:33):`Slice` now can have negative steps. Is this op overlapping with `Slice`?
pk-g(2019-03-25 19:04:22):@wschin, thanks for your input. Yes, indeed; with opset 10 recent changes in slice, we can now cover the functionalities of this op as a slice scenario. There will be cons and pros for having/not having a stand-alone `reverse` operator vs. relying on more generic `slice`. However, after considering both sides on cons and pros, I'm leaning more towards relying on `slice` instead due to following reasons: although `reverse` may sound more intuitive wording from clarity standpoint only for simple reverse scenarios, `slice` is more flexible and generic in handling different cases such as greater than one step size, and support for dynamic steps. In addition, from runtime standpoint, reducing amount of overlaps between ops shall help with overall efficiency. 

@houseroad , @linkerzhang , @wschin Therefore, I'd recommend withdrawing/abandoning this PR and using `slice` instead for scenarios addressed by this op . Having said that, please feel free to provide feedback.
houseroad(2019-03-22 16:26:08):please specify the element data type explicitly here. and regenerate the test data to make sure we are using the right type of data.
houseroad(2019-03-22 16:26:25):ditto
houseroad(2019-03-22 16:26:31):ditto
pk-g(2019-03-22 18:13:23):sure.
gramalingam(2019-03-28 20:21:58):Adding a test-case to test this would be helpful.
spandantiwari(2019-04-03 01:03:38):@wschin - This is useful functionality and will be a welcome addition to ONNX.

But have we considered an alternate design where we have a separate op for detecting `Inf` and `-Inf`, possibly named `IsInf`? 

`Inf` and `NaN` are two different floating-point numbers with totally different meaning and IEEE representation. Putting Infs under the `IsNan` op is not the best design. I am aware that we do not want to add too many ops to the spec, but I think this is one of those cases where clarity of design trumps that concern with adding another op. As example, they are separate ops in TF, Numpy, MATLAB, and PyTorch (in PyTorch there is no isinf, but isnan is just for NaNs not Infs).

wschin(2019-04-03 05:06:25):@spandantiwari, sounds good. I will update this PR to add `IsInf` with `detect_positive` and `detect_negative` attributes.
spandantiwari(2019-04-03 19:46:48):@wschin - OK, that sounds good. Thanks for your consideration.
spandantiwari(2019-04-17 18:40:07):> @spandantiwari, sounds good. I will update this PR to add `IsInf` with `detect_positive` and `detect_negative` attributes.

LGTM. 
ebarsoum(2019-03-28 17:57:55):Use the same style for attribute naming as the rest of ONNX. The above should be 'detect_negative_infinity'.
gramalingam(2019-03-28 20:14:30):"positive" => "negative"
gramalingam(2019-03-28 20:18:06):"positive" => "negative"
wschin(2019-04-03 05:53:38):Test files are added and now the strategy becomes
- Not to touch IsNaN
- Add IsInf instead.
wschin(2019-04-03 05:54:17):Ok.
wschin(2019-04-03 05:54:23):Ok.
pranavsharma(2019-03-27 00:00:51):LGTM 👍 
hobei(2019-04-03 18:00:27):Do you see any issue with the 'is_train' input being defined separately on each operation, which could potentially result in some operations in training mode and some in testing mode in the same model?
SherlockNoMad(2019-04-03 18:15:53):@hobei 
Hi Simon, I actually see this as an advantage to have separated control on each op. 

Imagine you have a model with two dropout node, one locates in the first few layer, one in the last few layer. If you wish to only fine tune the last few layers, one can set the second dropout node to train mode while keep the first dropout in the test mode. 


hobei(2019-04-04 08:48:45):@SherlockNoMad 
Hi Sherlock. I agree that this is a potential feature. My concern is how to make it consistent if you would want  apply the same control to operations that do not have the 'is_train' input. How do you identify to fine tune layers which do not contain Dropout and Batchnorm? 
chinhuang007(2019-04-04 19:35:44):@SherlockNoMad I think the proposed solution is sound, handling different behaviors of certain operators between training and inference modes. The question is how do we know we have covered all operators that need to have this optional input? Another question is what do we do with a Dropout/BatchNorm with is_train=true while creating/optimizing the inference graph?
chinhuang007(2019-04-04 19:44:26):@hobei While await for Sherlock's response, I just want to provide my view... The feature of "defining the fine tune layers" seems a separate interesting topic. The is_train input is to determine the expected behavior at the individual operator level. To selectively fine tune certain layers, or some sub-graphs, during training, we might need to introduce something new.
SherlockNoMad(2019-04-05 17:41:48):@chinhuang007 
I have gone through the operator list and only identified these two operators behave differently during inference and training. As we encounter more such operators, we will need to add this optional input for them. Hopefully, there are not many of them. 
Inference graph should have is_train set to false, or left empty. This should be handled by the model constructor/converter/optimizer. 
SherlockNoMad(2019-04-05 17:47:41):@hobei @chinhuang007 
Identifying the layers to fine tune is indeed a separate topic. In my opinion, it should be handle by the training runtime/backend. 
hobei(2019-04-10 09:46:02):@SherlockNoMad  
I have another comment, specifically for batch norm. When the is_test attribute was removed from op set 7 it was replaced (as I understand it) with the number of outputs instead indicating the mode (training/testing).

```
Output case #1: Y, mean, var, saved_mean, saved_var (training mode) Output case #2: Y (test mode)
```

Can you please elaborate on how the value of the new 'is_train' input relates to the number of outputs of batchnorm? As I understand it the number of outputs in a model is fixed, as they may be used as inputs to other operations. 
SherlockNoMad(2019-04-20 00:55:32):@hobei , I have updated the doc to further elaborate it.

Output case #1: Y, mean, var, saved_mean, saved_var (training mode)
Output case #2: Y (test mode, where other outputs will not be populated)

The number of outputs is indeed fixed. During testing, the latter 4 outputs are not populated and should not be consumed by other nodes. 
CLAassistant(2019-07-24 00:56:56):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=1887) <br/>All committers have signed the CLA.
wschin(2020-02-18 19:00:44):Since #2568 is merged, we can close this PR.
spandantiwari(2019-04-08 05:23:03):I think is_train is type tensor(boolean). Maybe we should consider using {true, false} instead. 
edgchen1(2019-11-08 22:13:03):ratio's range should be [0,1)
gramalingam(2019-05-08 18:05:27):@houseroad any feedback on this? We could also do this by adding another value in the enum FormalParameterOption (say "ZeroOrMore"). The current form allows variadic ops that require a minimum number of parameters (other than 0 or 1) as well, in case we need it in the future.
gramalingam(2019-05-09 15:20:27):@houseroad @linkerzhang : any other comments on this PR? Thanks!
houseroad(2019-05-08 18:09:11):This should be opset 11, since we already release 1.5. 10 should be frozen. 
houseroad(2019-05-08 18:09:21):same here.
houseroad(2019-05-08 18:22:41):here, bump up please
gramalingam(2019-05-08 19:39:06):Thanks for catching this. I have updated it. (I created this PR a while back when it was still opset 10 and forgot about it now.)
lara-hdr(2019-03-28 21:30:56):I could modify the operator's name for "UpdateIndex" or "Update" or other
gramalingam(2019-03-28 22:33:17):How does this op related to Scatter (https://github.com/onnx/onnx/blob/master/docs/Operators.md#Scatter ) ? Will Scatter be sufficient for the current purpose?
lara-hdr(2019-03-28 23:44:57):@gramalingam, both this op and scatter would be used to update values in an input tensor.
However, Scatter has more restrictions that make it impossible to cover all cases of this operator.
Scatter follows an equation to update the values (for example, in a 2-D tensor case, data[indices[i][j]][j] = updates[i][j] with an axis of 0); this makes it hard to update some of the values without updating others.

Let's assume data is : 
 [ [ 1, _**2**_, _**3**_],
   [ 4, 5, _**6**_],]
and we want to update the highlighted values.



wschin(2019-04-02 04:47:47):```suggestion
            "Tensor of int32/int64 indices of rank q <= r, can be the indexes of a particular element or a slice (Example 2).",
```
wschin(2019-04-02 04:49:49):I am not sure if `IndexPut` is a good name. How about `Replace`? It's closer to what it's doing in your examples.
lara-hdr(2019-04-02 16:18:21):That's a good name. I'll change the name for Replace
wschin(2019-04-03 05:28:55):Please also rename this file.
wschin(2019-04-03 05:29:38):Test names are not renamed.
lara-hdr(2019-04-03 16:20:36):The file is already renamed, this code is outdated :)
lara-hdr(2019-04-03 16:20:38):The test names are already renamed, this code is outdated :)
spandantiwari(2019-04-03 20:07:57):To give some context, the functionality added in this spec is definitely needed. For example, PyTorch's `torch.gather` op would need this to export correctly and efficiently. However, there are two things that we need to consider:

1) The added functionality is almost like a second "mode" of operation, i.e. the interpretation of indices and the shape of the output is completely different from the existing semantics of `Gather`. We can add this new "mode" under this `Gather` op itself, but since these two modes are different (both modes are needed), we should consider adding a new op for this. My worry is that `Gather` is already a somewhat complicated op, and adding more complexity would confuse users. 

2) This will need a deeper look on my side, but on initial look, I think `Gather` and `Scatter` ops in ONNX can be streamlined more in terms of their semantics. On a quick read of the `Scatter` op spec, this second mode being added to `Gather` in this PR may be more in line with current ONNX `Scatter`. For good design it may be useful to make sure that these ops align with each other. (I can take a deeper look).

@gramalingam @linkerzhang @ebarsoum @houseroad - thoughts?

gramalingam(2019-04-03 22:10:07):Yes, these ops are difficult to understand.
(a) I prefer making this a different op, given that it has a distinct meaning. 
(b) I don't see any example added to illustrate this new op/mode. That would be helpful.
(c) I think that the ops will be easier to understand if we add documentation of the form
    output[i][j][k] = input[.....] (for case 1), etc.
as shown in the TF and Torch documentation at https://www.tensorflow.org/api_docs/python/tf/gather and https://pytorch.org/docs/stable/torch.html#torch.gather)
gramalingam(2019-04-03 22:33:34):Just trying to understand the ops first before considering the name for the op. Is the following description correct? We can consider all the axes other than the indexed-axis as one axis called the non-index axis. (E.g., if it helps, we can visualize all the other axes as reshaped into one axis, just to help understand this.) For example, the non-index axis could be used for, say, batching. In the original gather op, we use the same index-tensor to do indexing for all elements of a batch (thinking of the non-index axis as a batch). In the new gather op, each batch-element has its own index-tensor to do the indexing. However, the original gather op allows the index to be multi-dimensional (e.g., we can think of this as "batching" for the index, which is not correlated to the "batching" for the indexed tensor). 

So, in short: with the original mode, we have "output[data-batch][index-batch] = data[data-batch][indices[index-batch]]", while with the new mode we have "output[data-batch][i] = data[data-batch][indices[data-batch][i]]"

Is that correct? I guess it is more important to explain clearly what the two modes do. I am fine with keeping these as different modes of the same op.
lara-hdr(2019-04-03 23:17:18):@gramalingam, correct, "output[data-batch][index-batch] = data[data-batch][indices[index-batch]]" for the old mode, vs "output[data-batch][i] = data[data-batch][indices[data-batch][i]]" for the new mode, is a good way of putting it.
I will add doc similar to TF and Pytorch to explain it.
spandantiwari(2019-04-03 19:55:49):Maybe we can consider a more evocative name for this attribute. Something like, `GatherElement` or `GatherSlice`, or something else. 
gramalingam(2019-04-03 22:37:42):Should this be the "the output shape is the same as the indices shape"?
gramalingam(2019-04-03 22:40:37):Is this for the new mode? Shouldn't it have "elem_index" attribute?
gramalingam(2019-04-03 22:46:17):I am not sure I understand the description of the attribute below. It sounds like the main distinction is whether the same indices tensor is used for all elements of the non-indexed axis (the original mode) or whether a different indices (sub)tensor is used for each element of the non-indexed axis (the new mode). Is this correct? If so, may be an attribute like "SharedIndices" or "SameIndices" might help.
lara-hdr(2019-04-03 23:17:55):yes I will update this
lara-hdr(2019-04-03 23:18:05):yes I will update this
CLAassistant(2019-03-29 01:26:34):[![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/onnx/onnx?pullRequest=1892) <br/>Thank you for your submission! We really appreciate it. Like many open source projects, we ask that you sign our [Contributor License Agreement](https://cla-assistant.io/onnx/onnx?pullRequest=1892) before we can accept your contribution.<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/onnx/onnx?pullRequest=1892) it.</sub>
askhade(2019-04-18 01:03:39):Static mode involves more inputs right? like the quantization params… can you include an example for static mode too...
askhade(2019-04-23 22:13:02):Does this round half to even?
askhade(2019-04-23 22:18:11):what happens when range does not include 0? For example is the range is 2-10 then in this case we dont have a unique representation for 0. Can you do rmin = min(min(data), 0) and similar for max... 
askhade(2019-04-23 22:25:37):What happens when this input is also being used by another node? This condition should be checked. 
houseroad(2019-04-01 05:05:21):thanks for fixing the problem.
liqunfu(2019-06-12 19:54:20):@houseroad I bumped the version. Please review. Thanks 
CLAassistant(2019-07-24 00:56:56):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=1899) <br/>All committers have signed the CLA.
gramalingam(2021-02-16 20:18:52):Hi @liqunfu : is this PR still relevant? Thanks!
CLAassistant(2019-04-01 17:15:07):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=1900) <br/>All committers have signed the CLA.
CLAassistant(2019-04-01 19:09:24):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=1901) <br/>All committers have signed the CLA.
linkerzhang(2019-04-02 22:36:59):@sonu1-p  please sign the CLA firstly. 
sonu1-p(2019-04-02 23:14:53):Hi Ke,
Thanks for looking into this. I am waiting for the legal team to approve
the CLA before I can accept. Will accept as soon as I hear back from them.

Thanks,
Sonal

On Tue, Apr 2, 2019 at 3:37 PM Ke Zhang <notifications@github.com> wrote:

> @sonu1-p <https://github.com/sonu1-p> please sign the CLA firstly.
>
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/onnx/onnx/pull/1901#issuecomment-479236325>, or mute
> the thread
> <https://github.com/notifications/unsubscribe-auth/ABOupxVaGtb4QjlBhBEeeK9cFrijP5AFks5vc9uUgaJpZM4cWbtw>
> .
>

linkerzhang(2019-05-18 17:30:02):@sonu1-p any update on signing the CLA please? :)
linkerzhang(2019-05-18 17:30:30):and also feel free to offer comments about CLA if any :)
sonu1-p(2019-05-18 21:27:17):Hi Ke, Accepted it now. Thanks! 
askhade(2019-04-04 21:25:25):@houseroad @raghuramank100 Can I get approval for this PR. As Ke already mentioned this PR includes Ke's changes and some typo fixes + doc generation. Tests will be added as part of another PR.

I am hoping I don't need approval from everyone again as this PR does not make any fundamental changes to Ke's original PR. 

Thanks! 
linkerzhang(2019-04-05 00:28:28):This PR is fixing some format issues and also doc generation missed in PR #1872 

Test cases of quant ops will be added in separate PR later.

Thank you @askhade !
askhade(2019-04-08 22:15:59):@raghuramank100, @houseroad  : Waiting for your review :) 
Thanks!
raghuramank100(2019-04-08 23:39:33):I approve of the PR. I am not able to approve it on Github because the PR shows as already merged.

Thanks,

Raghu

________________________________
From: Ashwini Khade <notifications@github.com>
Sent: Monday, April 8, 2019 3:16:10 PM
To: onnx/onnx
Cc: Raghuraman Krishnamoorthi; Mention
Subject: Re: [onnx/onnx] add quantization ops in onnx (#1908)


@raghuramank100<https://github.com/raghuramank100>, @houseroad<https://github.com/houseroad> : Waiting for your review :)
Thanks!

—
You are receiving this because you were mentioned.
Reply to this email directly, view it on GitHub<https://github.com/onnx/onnx/pull/1908#issuecomment-481028430>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AuktTIEctnwpUAvMVe_d7IOqORiZmaxvks5ve7-qgaJpZM4cbwjI>.

askhade(2019-04-09 00:07:14):> I approve of the PR. I am not able to approve it on Github because the PR shows as already merged. Thanks, Raghu
> […](#)
> ________________________________ From: Ashwini Khade <notifications@github.com> Sent: Monday, April 8, 2019 3:16:10 PM To: onnx/onnx Cc: Raghuraman Krishnamoorthi; Mention Subject: Re: [onnx/onnx] add quantization ops in onnx (#1908) @raghuramank100<https://github.com/raghuramank100>, @houseroad<https://github.com/houseroad> : Waiting for your review :) Thanks! — You are receiving this because you were mentioned. Reply to this email directly, view it on GitHub<[#1908 (comment)](https://github.com/onnx/onnx/pull/1908#issuecomment-481028430)>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AuktTIEctnwpUAvMVe_d7IOqORiZmaxvks5ve7-qgaJpZM4cbwjI>.

Are you looking at Ke's older PR? #1908  is the correct PR
prasanthpul(2019-04-09 15:38:42):Thanks all! Appreciate all the work across many people and many months to get this in
askhade(2019-04-09 16:48:22):> I would suggest to add them in the following PR.

@houseroad I will send out a PR soon covering shape inference tests and node tests for these ops

spandantiwari(2019-04-05 05:26:16):Minor suggestion - maybe remove this line altogether since there are no experimental ops now. 
houseroad(2019-04-05 05:27:35):I will create a separate pr to fix it.
houseroad(2019-04-04 20:53:06):seems archinfra is a super set of almost everything else... if we trigger it every time, other list maybe meaningless? Or if it's onnx/onnx/defs, sig-archinfra-approvers won't be trigger, but instead, it triggers sig-operators-approvers.
prasanthpul(2019-04-04 20:59:51):Operators SIG is responsible for operator changes, Currently most of our operator work is in the def files, so changes there should go to the operator sig approvers. I realize some of the tests are outside of that folder, so maybe we need to add more rules.

everything else in the repo should go to archinfra-approvers, until the sig carves out other approver areas



houseroad(2019-04-04 21:39:16):yeah, better put `onnx/backend/test` and `doc/` (maybe specific files in doc folder, TestCoverage.md, Operators.md, and *-ml.md) under @onnx/sig-operators-approvers as well


prasanthpul(2019-04-04 23:42:31):updated with individual entries for now, but suggest we move all the operator related documentation into an operator subdirectory soon.
CLAassistant(2019-04-05 09:15:40):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=1912) <br/>All committers have signed the CLA.
prasanthpul(2019-04-09 15:43:34):Is there a CI for this configuration so we can tell if something ever breaks?
kavanabhat(2019-04-10 05:52:37):Existing CI tests will suffice for AIX too.
prasanthpul(2019-04-11 16:51:04):Hi @kavanabhat , the concern is not about the tests but rather having a pipeline instance that runs on that platform and exercises the code path. Could you setup a CI pipeline like was done with PPC (http://powerci.osuosl.org/job/onnx-ppc64le-nightly-build/)? 
kavanabhat(2019-04-22 05:39:28):We are looking into this and will need more time to have CI pipeline setup on AIX. Let us know if there are any guidelines on setting this up. Till then we plan to internally test new releases and report if any issues to the community. If the changes look fine, we can have the code merged.
kavanabhat(2019-04-29 05:06:59):Please let me know if we can have these changes merged. Thanks.
kavanabhat(2019-05-02 04:57:38):@houseroad and @linkerzhang , let me know if the changes look ok. Thanks.
kavanabhat(2019-05-07 06:00:32):@houseroad, the suggested changes have been incorporated. Please have these changes merged.
houseroad(2019-05-07 06:02:35):@kavanabhat sure, any eta for the AIX CI btw? :-)
kavanabhat(2019-05-13 08:39:40):As mentioned earlier, will need more time to have CI pipeline setup on AIX. Let us know if there are any guidelines on setting this up. Till then we plan to internally test new releases and report if any issues to the community.
CLAassistant(2019-07-24 00:57:20):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=1913) <br/>All committers have signed the CLA.
prasanthpul(2019-08-19 17:11:57):Folks - should we merge this for 1.6?
prasanthpul(2019-09-17 20:54:14):@kavanabhat is the CI now setup?
houseroad(2019-09-18 17:35:53):Yeah, code looks good. Merged :-)
houseroad(2019-05-02 05:00:01):lint the code? space after #
SherlockNoMad(2019-04-09 17:33:44):I think both X and Y should be input. Just like tf.assign(ref, value). Also, maybe let's call the inputs 'ref' and 'value'. 

Currently, the weights are stored as initializers in an onnx model, if we were to use this op for updating the weights, we are performing in-place update for the initializers. And initializers should be feed as inputs. 
gramalingam(2019-04-09 18:13:15):Hi, I don't understand this operator's description at all. All ONNX ops are functional in nature: the output should be computable as a function of the inputs. I would expect an update operator to take the old-value as an input, and the changes/delta as another input.
wschin(2019-04-10 06:08:55):@SherlockNoMad, I am ok with having two inputs because this change doesn't affect this operator's behavior. Just feel `modifying input` is a bit more ambiguous then `modifying output`.

@gramalingam, I will improve its doc and add more context. In addition, `Update` is not necessary to have a delta. Considering gradient descent method with a fixed step length as an example, it first compute gradient, sum up the current model parameters and their gradient, and finally assign the sum back to the model. There are conceptually three steps (and therefore three sub-graphs).

1. Forward graph
2. Backward graph for computing gradient of each adjustable variable
3. Updating graph which combines assign gradient (or so-called `delta` because different training algorithms (e.g., svrg, saga, gauss-newton, adagrad, etc) have different ways to compute update direction) with the current state and assign the combined value back to model.

This operator aims at capturing the `assignment` in step 3. Btw, the existence of three `sequential` steps makes me feel we should have a `Sequential` operator which sequentially evaluates all its sub-graphs.
gramalingam(2019-04-10 09:55:31):I feel I am missing something important regarding the high-level picture. Is each of the forward/backward/update graphs to be represented by a separate ONNX model? Or, is the goal to have a single model that has these different components? I think some fundamental extension to ONNX may be required, not just adding another operator.
gramalingam(2019-04-10 09:59:36):Or, you could have "Train" as an operator which has 3 graph-valued attributes representing the forward/backward/update parts. Within a single graph, the existing ONNX (static single assignment) requirement means that the same Y cannot appear as the output of two different nodes.
gramalingam(2019-04-10 10:18:37):Or, better still: we could use a single model that takes incoming values of (learnable) weights as well as other inputs, and computes the updated values of the weights as outputs and returns them. Thus, the model combines the three graphs into one, but it has different names for the tensors representing the initial value of a weight at the beginning of an iteration and the tensors representing the final value of the weight in the end of that iteration. If we want, we can represent the iterations also using some form of loop.
gramalingam(2019-04-10 10:19:39):So, I don't understand the need for the proposed "Update" operator.
wschin(2019-04-10 15:40:01):> 
> 
> I feel I am missing something important regarding the high-level picture. Is each of the forward/backward/update graphs to be represented by a separate ONNX model? Or, is the goal to have a single model that has these different components? I think some fundamental extension to ONNX may be required, not just adding another operator.

I don't feel `update` and `backward` should be in separated models. They are designed specifically to the associated forward graph. I like to view them as a single model.

I feel introducing `Sequential` operator will be enough. It allows ordered computation of multiple graphs.
wschin(2019-04-10 15:42:30):> 
> 
> Or, you could have "Train" as an operator which has 3 graph-valued attributes representing the forward/backward/update parts. Within a single graph, the existing ONNX (static single assignment) requirement means that the same Y cannot appear as the output of two different nodes.

I am not sure if `Train` is really a three-graph thing. There are tons of heuristics (such as warm-start) in training neural networks and that's why I propose `Sequential` to allow writing graphical code in ONNX. Yes, SSA can be broken across different sub-graphs in `Sequential`. If you feel `Sequential` is not general enough, we can create `World` operator where the current graph decides which graph will be executed next.
wschin(2019-04-10 15:48:43):> 
> 
> Or, better still: we could use a single model that takes incoming values of (learnable) weights as well as other inputs, and computes the updated values of the weights as outputs and returns them. Thus, the model combines the three graphs into one, but it has different names for the tensors representing the initial value of a weight at the beginning of an iteration and the tensors representing the final value of the weight in the end of that iteration. If we want, we can represent the iterations also using some form of loop.

Combining different graphs into one looks super ambiguous. How can user skip steps they don't need? Using `different name` to identify `new value` is a big problem --- how could we make sure all partners follow this rule?
hobei(2019-04-10 16:21:03):@wschin.
I presume that one of the main points of this operation is to update variables based on their gradients. At this stage it is unclear to me how the gradients will be named in the graph and so how this operation is to be used. One proposal is for the backward pass to follow an auto-grad solution, where the backwards operations (and their inputs/outputs) are not explicitly identified, in which case how do we identify the name of the gradient to use in this operation?
wschin(2019-04-10 16:37:51):> 
> 
> @wschin.
> I presume that one of the main points of this operation is to update variables based on their gradients. At this stage it is unclear to me how the gradients will be named in the graph and so how this operation is to be used. One proposal is for the backward pass to follow an auto-grad solution, where the backwards operations (and their inputs/outputs) are not explicitly identified, in which case how do we identify the name of the gradient to use in this operation?

The only person who knows the corresponding between gradient and variable is the auto-grad module (it's true no matter if `Update` exists), so auto-grad will be responsible to produce those `Update` operators. If gradient (and backward operators) are not explicitly specified, how can ONNX user write their own optimization algorithm? It'd not be scalable to implement an operator to each training algorithm.
shinh(2019-04-10 17:45:57):I think I like outputting updated weights/values as mentioned in https://github.com/onnx/onnx/pull/1917#issuecomment-481631741 . To correspond which `input` should be updated by which `output`, I'd propose to add a repeated message like

```
message UpdateValueProto {
  string input = 1;  // must be one of initializer
  string output = 2;  // must be one of output
}
message GraphProto {
   ...
  repeated UpdateValueProto update;
}
```

(this was mentioned in https://docs.google.com/document/d/1_rfkxCZbvoj-0T9KuDuMLecSpsgJHJK6ExOw4aW-DeY/edit#heading=h.2u9zcp1xt95)

This would look like `Update` op, but I think `Update` op has a downside. If `Update` op exists in `Loop`, you should always run the `Loop` op even if the outputs of `Loop` are not used by other ops. I think this would complicate pruning unnecessary nodes.

Just my two cents
gramalingam(2019-04-10 20:40:10):@wschin : is there some context for this, describing the requirements? That would make it easier to understand what is needed. If we assume that the generator of the ONNX model does the necessary (auto) differentiation, then we just need to capture the computations representing the forward pass, the backward pass, and the update. We can generate an ONNX model representing a single iteration of the training loop (which will be a single model combining all the three steps mentioned above) or we could generate an ONNX model that includes even the training loop. None of these require any changes/extensions to ONNX. 
wschin(2019-04-11 04:46:42):> I think I like outputting updated weights/values as mentioned in #1917 (comment) . To correspond which input should be updated by which output, I'd propose to add a repeated message like
> message UpdateValueProto {
>   string input = 1;  // must be one of initializer
>   string output = 2;  // must be one of output
> }
> message GraphProto {
>    ...
>   repeated UpdateValueProto update;
> }
> 
> (this was mentioned in https://docs.google.com/document/d/1_rfkxCZbvoj-0T9KuDuMLecSpsgJHJK6ExOw4aW-DeY/edit#heading=h.2u9zcp1xt95)
> This would look like Update op, but I think Update op has a downside. If Update op exists in Loop, you should always run the Loop op even if the outputs of Loop are not used by other ops. I think this would complicate pruning unnecessary nodes.
> Just my two cents

Right but why would someone add an `Update` to change unused variable? The benefit of having `Update` is not only to make ONNX a formal language but also saving a lot of memory. 
wschin(2019-04-11 04:56:42):> @wschin : is there some context for this, describing the requirements? That would make it easier to understand what is needed. If we assume that the generator of the ONNX model does the necessary (auto) differentiation, then we just need to capture the computations representing the forward pass, the backward pass, and the update. We can generate an ONNX model representing a single iteration of the training loop (which will be a single model combining all the three steps mentioned above) or we could generate an ONNX model that includes even the training loop. None of these require any changes/extensions to ONNX.

My opinion is we need to describe every bit of the training algorithm, so that training algorithm becomes protable.

We can NOT describe the whole training behavior by defining a single iteration. Let's consider warm-start as an example. One first execute ADAGRAD to get an initial result and then switch to Gauss-Newton method for better final accuracy ---- this is not a thing can be described in an iteration.

We should not mix training and prediction graphs together. They are used for different purposes. ONNX will finally become a language (or dead), so I feel it's fine to steal some features from existing languages.
CLAassistant(2019-07-24 00:56:43):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=1917) <br/>All committers have signed the CLA.
wschin(2019-04-11 16:09:36):Remove it.
gramalingam(2019-04-11 16:19:11):Sorry, this example seems to violate several fundamental principles of ONNX. Unclear if 'x' is an input or output to Update (the above seems to conflict with the opschema). In either case, we have problems. May be it will be easier if we discuss this face to face.
linkerzhang(2019-04-12 07:36:02):@houseroad please help to review. Thank you!
nbcsm(2019-04-15 02:49:28):travis failure is due to timeout, does not seem to be code issue.
could anyone help to rerun it?
nbcsm(2019-04-19 10:28:31):@houseroad, PR updated, please help to take another look. 
thanks.
faxu(2019-04-19 21:53:32):> @houseroad, PR updated, please help to take another look.
> thanks.

@houseroad do you mind reviewing this and merge if all is addressed? we'd like for this to be included in 1.5. Thanks!
nbcsm(2019-04-23 07:51:22):@houseroad, thanks for the review.
i have rebased the PR onto latest master.
regarding your comments, i think it is better to have a check to ensure the inputs fit the op spec requirements, but i can remove the check if you have concern.
houseroad(2019-04-18 07:53:17):shall we also support int64 here? since we use int64 as type for indices in most of the cases.
houseroad(2019-04-18 07:54:58):add a test to cover the following code here: https://github.com/onnx/onnx/blob/master/onnx/test/shape_inference_test.py
houseroad(2019-04-22 22:26:59):Is this check really necessary?
houseroad(2019-04-22 22:27:03):Is this check really necessary?
nbcsm(2019-04-23 07:39:03):according the op spec, ```sequence_lens``` is required to be a 1d array, whose shape is [batch_size].
better to have a check.
nbcsm(2019-04-23 07:41:50):this op is designed to deal with ```input``` which is **a batch of sequences**, so dim_size should be >=2.
i feel it is better to have a check.
houseroad(2019-04-23 17:41:30):The check is to make sure our shape inference logic won't crash in the middle, or at least print out useful error message. However, I don't the checked shape information is used. So we should move it to the checker. Minimal necessary check is the principle in ONNX shape inference.
nbcsm(2019-04-24 03:05:40):i got your point, i will take a look at the checker code.
thanks.
askhade(2019-04-11 22:36:23):@houseroad , @linkerzhang : This is the pull request for shape inference tests for quantized ops. Adding node tests for these ops in separate PR as it needs thorough testing.
askhade(2019-04-15 17:08:25):> Looks good. Can you lint the code? One tool I recommend is clang-format, which will only lint your changes.

Thanks @houseroad . I do use that tool not sure why it did not trigger earlier... updated the files with formatting changes. 
pranavsharma(2019-04-15 02:31:45):@linkerzhang @houseroad can this be merged? thanks!
houseroad(2019-04-12 05:37:05):} // namespace ONNX_NAMESPACE
linkerzhang(2019-04-12 07:29:30):run https://github.com/onnx/onnx/blob/master/onnx/backend/test/stat_coverage.py to update the test coverage doc (that's why Linux ci failed). meanwhile, a trick when running this script, you'd have to explicitly set ML=false to update the right doc. https://github.com/onnx/onnx/blob/master/onnx/backend/test/stat_coverage.py#L232

I know this kind of doc update experience is bad in windows, it should be improved (some unified script should be created for it).
wschin(2019-04-17 07:19:30):> run https://github.com/onnx/onnx/blob/master/onnx/backend/test/stat_coverage.py to update the test coverage doc (that's why Linux ci failed). meanwhile, a trick when running this script, you'd have to explicitly set ML=false to update the right doc. https://github.com/onnx/onnx/blob/master/onnx/backend/test/stat_coverage.py#L232
> I know this kind of doc update experience is bad in windows, it should be improved (some unified script should be created for it).

Just tried some combinations of reinstall/ONNX_ML=0 and none of them worked in another PR.

[Update] My problem is resolved by doing `git clean -xdf` before entering `pip install -e onnx` to reinstall ONNX.
linkerzhang(2019-04-12 07:26:40):I think it's better to have on op to current both case (right and left).
linkerzhang(2019-04-12 07:27:11):please also add test case for the op and also shape inference test.
wschin(2019-04-13 07:12:08):Sure.
wschin(2019-04-13 07:12:18):No problem.
daquexian(2019-04-16 03:16:11):Hmm.. CI fails since ONNX_BUILD_MAIN_LIB is set to OFF, and then dllimport is used. I will fix it soon.
daquexian(2019-04-16 05:01:49):I refined the logic of ONNX_EXPORT and ONNX_IMPORT. 

Two new definitions, ONNX_IMPORT_SYMBOLS and ONNX_EXPORT_SYMBOLS are introduced. The documentation is also updated accordingly.
daquexian(2019-04-16 10:19:27):The code is ready to review.  

I feel confused about the BUILD_SHARED_LIBS and ONNX_BUILD_MAIN_LIB so I conclude the logic about them as follows. I think it respects the original behavior, correct me if I am wrong :

 **BUILD_SHARED_LIBS:**
We want to build onnx_proto as dll. In this case, we set ONNX_API=export for onnx_proto itself and set ONNX_API=import for anything depending on onnx_proto. (The original logic is wrong here)

**ONNX_BUILD_MAIN_LIB and BUILD_SHARED_LIBS:**
No difference from above.

**ONNX_BUILD_MAIN_LIB and not BUILD_SHARED_LIBS:**
We use ONNX as a subproject and want to build onnx_proto as a lib and the whole project as a dll. In this case, we want ONNX_API to be "export" for onnx_proto itself to export the symbols to dll, and also for anything depending on onnx_proto to "fix a Windows global-variable-in-dll issue" according to documentation in onnx_pb.h.
snnn(2019-04-24 06:06:29):Well, your logic is correct, but
1. why do people want to build onnx_proto as a separated dll, instead of putting onnx and onnx_proto together?
2. So far, I didn't see onnx or onnx_proto was marked as shared lib. There should be something like:

add_library(onnx_proto **SHARED** ${ONNX_PROTO_SRCS} ${ONNX_PROTO_HDRS})

I didn't see it. 

And what is the "Windows global-variable-in-dll issue"? How did it get fixed?
snnn(2019-04-24 06:11:37):> We use ONNX as a subproject and want to build onnx_proto as a lib and the whole project as a dll

In general. "__declspec(dllexport) " in static lib doesn't work.  "https://devblogs.microsoft.com/oldnewthing/20140321-00/?p=1433"

May I know your use case? 

daquexian(2019-04-24 06:32:37):@snnn Thanks for your review!

> 1. why do people want to build onnx_proto as a separated dll, instead of putting onnx and onnx_proto together?

Frankly speaking, I didn't know when people want to build onnx_proto as a separate dll, either. However, an `onnx_proto.dll` may be generated by `BUILD_SHARED_LIBS` global flag. My edition remains 
the behavior of the original code as far as possible.

> 2. So far, I didn't see onnx or onnx_proto was marked as shared lib. There should be something like:
> 
> add_library(onnx_proto **SHARED** ${ONNX_PROTO_SRCS} ${ONNX_PROTO_HDRS})
> 
> I didn't see it.

Since we didn't add "SHARED" or "STATIC" explicitly here, whether onnx_proto is a shared lib or a static lib is controlled by the global [BUILD_SHARED_LIBS](https://cmake.org/cmake/help/latest/variable/BUILD_SHARED_LIBS.html) flag. So it is possible that onnx_proto is a shared lib.

> And what is the "Windows global-variable-in-dll issue"? How did it get fixed?

It is directly from the comment in onnx_pb.h, which is introduced by @linkerzhang from the result of git blame. Maybe we need to discuss with Ke of this specific question. :)
snnn(2019-04-24 07:12:34):If you want to build the whole project as a dll, I think the more straight forward way is adding a new target.
add_library(onnx_whole_dll SHARED ${ONNX_SRCS} ${ONNX_PROTO_SRCS} ${ONNX_PROTO_HDRS})
As I mentioned, "__declspec(dllexport)" doesn't work in static lib. And also, this dll should dynamically link to libprotobuf. 


daquexian(2019-04-24 07:13:55):> In general. "__declspec(dllexport) " in static lib doesn't work. "https://devblogs.microsoft.com/oldnewthing/20140321-00/?p=1433"
> 
> May I know your use case?

I think what this article points out is that if the obj file is not seen by linker, then the dllexport will not work. 

If the obj file is seen by linker, and the static lib is linked into a shared lib, dllexport will work. Although I have not verified it, exporting symbols of a static onnx lib when linked into a shared lib is exactly what `ONNX_BUILD_MAIN_LIB` wants to do. Reference: https://github.com/onnx/onnx/blob/master/onnx/onnx_pb.h#L34
daquexian(2019-04-24 07:18:13):> If you want to build the whole project as a dll, I think the more straight forward way is adding a new target.
> add_library(onnx_whole_dll SHARED ${ONNX_SRCS} ${ONNX_PROTO_SRCS} ${ONNX_PROTO_HDRS})

I agree. It is unwise to build an onnx_proto.dll. However, it will happen if `BULID_SHARED_LIBS` is set, and windows build will fail in this case.

This PR fixes the error when building with `BUILD_SHARED_LIBS` on Windows. If we want to disallow users to build a separate onnx_proto.dll, maybe we need more discussion about it?
snnn(2019-04-24 07:21:48):In general, DLL shouldn't export a symbol which is provided in another lib. Otherwise the symbol should be handled in a special way.  @RyanUnderhill , any comments on this?
daquexian(2019-04-24 07:25:47):> In general, DLL shouldn't export a symbol which is provided in another lib. Otherwise the symbol should be handled in a special way.

I have no experience in windows programming so I chose to remain the logic of original code as far as possible (however, the original logic is confusing). If you are sure that we shouldn't export symbol in a static onnx lib to a shared lib, I will be glad to remove the confusing `ONNX_BUILD_MAIN_LIB`
RyanUnderhill(2019-04-24 23:43:10):> In general, DLL shouldn't export a symbol which is provided in another lib. Otherwise the symbol should be handled in a special way.
> 
> I have no experience in windows programming so I chose to remain the logic of original code as far as possible (however, the original logic is confusing). If you are sure that we shouldn't export symbol in a static onnx lib to a shared lib, I will be glad to remove the confusing ONNX_BUILD_MAIN_LIB

From the outside it's not visible how the symbol was exported, but it's a rather unexpected to link to a library and cause your DLL to export additional symbols.
daquexian(2019-04-25 01:32:09):Thanks @snnn @RyanUnderhill 

> From the outside it's not visible how the symbol was exported, but it's a rather unexpected to link to a library and cause your DLL to export additional symbols.

Should we remove the `ONNX_BUILD_MAIN_LIB` completely?
snnn(2019-04-25 05:58:41):Please hold on a moment.  We completely understand the current setting first. To me, it's even not clear why this option exists.
linkerzhang(2019-04-26 01:05:50):Aha, I noticed that I added these stuff, but my memory is telling me that they're from some other file and I just did the movement.

How about remove it and see whether there're concerns from other folks? :)
linkerzhang(2019-05-07 13:51:13):@daquexian  please also sign the CLA accordingly.
daquexian(2019-05-11 03:55:10):@linkerzhang @snnn I have removed ONNX_BUILD_MAIN_LIB

Another question is, `ONNX_EXPORT` in the header `onnx_pb.h` is never used now. The reason is that only onnx_proto exports symbols, onnx and libraries/executables depending on onnx import symbols from onnx_proto, and `onnx_pb.h` is only included by the latter ones rather than onnx_proto. However, I'm not sure whether I missed some use cases. Should `ONNX_EXPORT` be removed too?
snnn(2019-05-16 02:22:41):Are you trying to split onnx into two DLLs: onnx.dll and onnx_proto.dll? If not, you don't need to change any code here.

daquexian(2019-05-16 04:09:26):> Are you trying to split onnx into two DLLs: onnx.dll and onnx_proto.dll? If not, you don't need to change any code here.

Thanks for your review! I am not trying to split onnx into two dlls. onnx looks already split into [onnx_proto](https://github.com/onnx/onnx/blob/master/CMakeLists.txt#L284) and [onnx](https://github.com/onnx/onnx/blob/master/CMakeLists.txt#L339). Please correct me if I am wrong.
snnn(2019-05-16 04:13:35):Then you can do this:
1. In your project, add a new dynamic lib target named 'onnx_dll'
2. Let onnx_dll link to onnx and onnx_proto, group them within wholearchive flag
3. define ONNX_BUILD_SHARED_LIBS when building onnx and onnx_proto, compile them as static lib

Would it work?

snnn(2019-05-16 04:19:28):BTW, is there an option for forcing onnx dynamically link to libprotobuf.dll on Windows? If your program will dynamic link to onnx, then it should also dynamic link to protobuf, as onnx's interface contains protobuf symbols. 
daquexian(2019-05-16 04:29:24):> BTW, is there an option for forcing onnx dynamically link to libprotobuf.dll on Windows? If your program will dynamic link to onnx, then it should also dynamic link to protobuf, as onnx's interface contains protobuf symbols.

Yes, I think it is doesn't matter that the cmake target `protobuf::libprotobuf` is a shared library or a static library. It is the [build log in CI](https://ci.appveyor.com/project/daquexian/dnnlibrary/builds/23876935#L13092) of my own project, it compiles and links successfully (line 13092).
daquexian(2019-05-16 04:42:42):> Then you can do this:
> 
> 1. In your project, add a new dynamic lib target named 'onnx_dll'
> 2. Let onnx_dll link to onnx and onnx_proto, group them within wholearchive flag
> 3. define ONNX_BUILD_SHARED_LIBS when building onnx and onnx_proto
> 
> Would it work?

I am not familiar with windows programming. Looks like it will work. It reminds me of the `ONNX_BUILD_MAIN_LIB` :D 

BTW, what do you think of my opinion [here](https://github.com/onnx/onnx/pull/1938#discussion_r284531608)?  :)
snnn(2019-05-16 05:16:01):
> Yes, I think it is doesn't matter that the cmake target `protobuf::libprotobuf` is a shared library or a static library. It is the [build log in CI](https://ci.appveyor.com/project/daquexian/dnnlibrary/builds/23876935#L13092) of my own project, it compiles and links successfully (line 13092).

It matters. You may hit link error at later time when you are using the dll. Not always, but it may happen.
snnn(2019-05-16 05:29:35):Let's say:
If you have a library, foo.dll, which depends on libprotobuf.

Then, let's say, if it doesn't have protobuf symbols on its interface, it's better to let foo.dll static link to protobuf.
otherwise,  if it has, and it choose to static link to protobuf, and you have an app named app.exe, which depends on foo.dll, also, protobuf. Because foo.dll has protobuf in its interface,  app.exe can't use foo.dll without include protobuf headers.
However, app.exe may or may not need to link to protobuf. Because foo.dll will provide some of the protobuf symbols but not all of them. 
If app.exe also link to libprotobuf, whether statically or dynamically, you may have duplicated symbols between app.exe and foo.dll. I mean, two copies of one symbol, and linker won't complain. Because it's not an error, it's the design. If you know ODR, you should also know in DLL world, it's broken. And it would be a disaster if you allocate a protobuf object in one place(e.g. app.exe) but  deallocate it in another place(e.g. foo.dll). Because protobuf has a local memory pool, if you have two copies of the same symbol, you'll have two copy of the pool.  If you allocate it in one pool but deallocate it in another, it will just crash. However, it often happens when you are passing protobuf objects between dlls.

So, as long as you need to pass the ownership of one protobuf from one dll(or exe) to another, you should let them dynamic link to libprotobuf.

It's a not problem specific to Windows. Linux has the similar problem. 















linkerzhang(2019-05-20 18:27:03):@yinghai @houseroad  any concerns from your side on this change please? 
snnn(2019-05-20 18:37:49):Could you please also clean up the comments?
Please describe what's the relationship between

1. ONNX_BUILD_SHARED_LIBS
2. ONNX_BUILD_MAIN_LIB
3. ONNX_IMPORT_SYMBOLS
4. ONNX_API 

When should they get defined.

Thanks.


daquexian(2019-05-21 03:29:31):> Could you please also clean up the comments?
> Please describe what's the relationship between
> 
> 1. ONNX_BUILD_SHARED_LIBS
> 2. ONNX_BUILD_MAIN_LIB
> 3. ONNX_IMPORT_SYMBOLS
> 4. ONNX_API
> 
> When should they get defined.
> 
> Thanks.

All right. I have found the origin of `ONNX_BUILD_SHARED_LIBS` and `ONNX_BUILD_MAIN_LIB`. They are from Caffe2 and especially this commit https://github.com/pytorch/pytorch/commit/91d76f5dbde782396d2777c9d2ae5cde2c83160d. After checking this commit I have figured out these definitions.
daquexian(2019-05-21 04:20:43):The history of these macros:

**ONNX_API**
The usage of _MYDLL_API_ should be like it: https://stackoverflow.com/a/26966255/5283216. When building the dll, _MYDLL_API_ should be dllexport, and when building a program depending on the dll, _MYDLL_API_ should be dllimport.

**ONNX_BUILD_SHARED_LIBS**
However, we cannot always set _ONNX_API_ as "dllimport" when linking onnx_proto, because we need support not only shared lib but also static lib. So a definition _ONNX_BUILD_SHARED_LIBS_ was introduced. However, ONNX_API is set in cmake now, so the _ONNX_BUILD_SHARED_LIBS_ is not used anymore and has been removed.

**ONNX_BUILD_MAIN_LIB**
_XXX_BUILD_MAIN_LIB_ is from Caffe2, and indeed not needed for ONNX.

Let's first learn how it works in Caffe2: In Caffe2, the exported symbols consist of symbols in generated pb.cc/pb.h and also a few symbols in the source code (please check out this commit: https://github.com/pytorch/pytorch/commit/91d76f5dbde782396d2777c9d2ae5cde2c83160d). For the symbols in the source code, _CAFFE2_API_ is set in caffe2/core/macros.h (a common header included by every headers/sources). For the symbols in pb.h/pb.cc, _CAFFE2_API_ is set by cmake since they are generated and do not include any Caffe2 headers. The compiler definition _CAFFE2_BUILD_MAIN_LIB_, which is [always set](https://github.com/pytorch/pytorch/blob/91d76f5dbde782396d2777c9d2ae5cde2c83160d/caffe2/CMakeLists.txt#L102) when compiling Caffe2, is then introduced. As a result, the symbols in the **source code** are exported when building Caffe2 and are imported when linking to Caffe2.

However, in ONNX, only symbols in pb.cc/pb.h are exported (because no _ONNX_API_ is in the source code). So _ONNX_API_ is never to be "export" in onnx/onnx_pb.h, which is only included by the source code. The only use case of _ONNX_API_ in onnx_pb.h is importing the symbols of pb.h/pb.cc. So 1. _ONNX_BULID_MAIN_LIB_ is not needed. 2. _ONNX_API_ is always "import".
daquexian(2019-05-21 09:02:00):Update:

It makes pytorch build [broken](https://circleci.com/gh/onnx/onnx/1941?utm_campaign=vcs-integration-link&utm_medium=referral&utm_source=github-build-link) to set ONNX_API directly in cmake. The problem is not on ONNX side but on PyTorch side. ONNX is linked privately in [caffe2/CMakeLists.txt]( https://github.com/pytorch/pytorch/blob/master/caffe2/CMakeLists.txt#L853) so public compiler definitions of ONNX cannot be spread to the targets depending on caffe2 (e.g., [caffe2_pybind11_state](https://github.com/pytorch/pytorch/blob/master/caffe2/CMakeLists.txt#L1198)) (It not only causes the CI fails but also causes [these hardcoded definitions](https://github.com/pytorch/pytorch/blob/master/cmake/Dependencies.cmake#L1044)). 

To keep PyTorch build from broken, I defined the ONNX_API macro in macros.h now, which is generated by cmake.
snnn(2019-09-10 17:18:40):Don't create a DLL that:
1. has a libprotobuf inside
2. And exported some of the libprotobuf symbols in its interface

You should:
1. build protobuf as a dynamic DLL
2. let your library dynamic link to protobuf 

CLAassistant(2020-09-10 02:00:40):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=1938) <br/>All committers have signed the CLA.
daquexian(2020-12-21 05:11:20):Closing because it is outdated. I don't know whether there is a proper way to solve this issue (or whether this issue has already been fixed)
daquexian(2019-04-16 10:10:37):There are two ONNX_BUILD_MAIN_LIB in the original code, a cmake variable (in here) and a compile definition (in onnx_pb.h). 
PUBLIC here is because that in the original code, cmake variable ONNX_BUILD_MAIN_LIB set the PRIVATE part and the compile definition ONNX_BUILD_MAIN_LIB (in onnx_pb.h) set the INTERFACE part.
snnn(2019-05-16 02:19:07):Why do you remove this line?
What if people want to build a static lib?
What if people want to consume this header and static link to onnx?
daquexian(2019-05-16 04:01:27):Thanks for your review. 

This line is not removed accidentally in the attempt to remove ONNX_BUILD_MAIN_LIB. The dllexport and dllimport are now managed by ONNX_EXPORT_SYMBOLS and ONNX_IMPORT_SYMBOLS, which are set properly in cmake.

The reason to edit this line is to extract the duplicated logic. For example, in the original code, there are two "ONNX_BUILD_MAIN_LIB", one is [a cmake variable](https://github.com/onnx/onnx/blob/master/CMakeLists.txt#L306) and another is [a compiler definition](https://github.com/onnx/onnx/blob/master/onnx/onnx_pb.h#L11). Setting the cmake variable to ON doesn't define its namesake. How can users define this compiler definition? No easy way. I believe it is a bug. This bug clearly shows that it is error-prone to maintain the duplicated logic in two different places. So I extract the logic to cmake, and set the two extra definitions to control the behavior of onnx_pb.h.
snnn(2019-05-16 05:12:19):Yes, it's a bug. It can be easily fixed by adding:
```
if(ONNX_BUILD_MAIN_LIB)
  target_compile_definitions(xxx  PRIVATE ONNX_BUILD_MAIN_LIB)
endif()
```
to cmake files.
linkerzhang(2019-05-20 03:46:04):I personally prefer this idea of removing the duplicated logic in both header and cmake files. :)
linkerzhang(2019-05-20 03:55:41):this is also not needed? Anyway, let's clean up firstly, and we may add it back if needed later :)
CLAassistant(2019-07-24 00:57:23):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=1939) <br/>All committers have signed the CLA.
wschin(2020-02-18 18:55:23):We have a replacement for this PR, so please allow me to close the PR.
https://github.com/onnx/onnx/pull/2573
https://github.com/onnx/onnx/pull/2570
wschin(2019-04-22 15:55:34):Maybe?
```suggestion
            "scores",
```
Prediction sounds a predicted label.
wschin(2019-04-22 15:58:39):Please add equations to define this opertor's behavior. I feel using equation is the only way to eliminate uncertainty in ONNX (a mathematical standard).
wschin(2019-04-22 16:03:12):Please add equations to define this opertor's behavior. I feel using equation is the only way to eliminate uncertainty in ONNX (a mathematical standard). Notice that L2-norm is only formally defined for `vector` (aka 1-D tensor) but it looks like this operator accepts general tensors.
wschin(2019-04-22 16:03:20):```suggestion
        .Input(0, "scores", "The predicted outputs.", "T")
```
CLAassistant(2019-04-16 18:50:05):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=1940) <br/>All committers have signed the CLA.
linkerzhang(2019-04-17 01:37:59):Thank you! Please sign the CLA.
bddppq(2019-04-26 00:34:19):This looks good to me. @calebmadrigal Could you add a test case to make sure support of mixed ints and floats attributes won't be broken accidentally in the future?
(Also we need you to sign the cla :-) )
calebmadrigal(2019-04-26 18:52:00):@bddppq @linkerzhang I added a unit test. CLA signing is still pending on legal team's review (but I don't foresee any problems).
houseroad(2019-05-08 20:40:57):Any update on the CLA part?
askhade(2020-08-14 21:23:32):@calebmadrigal : Looks like this PR is pending for quite a while. let's check this in :) Can you sign the CLA and resolve conflicts with master? Thanks!
calebmadrigal(2020-08-14 21:29:36):> @calebmadrigal : Looks like this PR is pending for quite a while. let's check this in :) Can you sign the CLA and resolve conflicts with master? Thanks!

CLA signed.
askhade(2020-08-17 18:53:40):> > @calebmadrigal : Looks like this PR is pending for quite a while. let's check this in :) Can you sign the CLA and resolve conflicts with master? Thanks!
> 
> CLA signed.

Thanks! Can you also resolve the merge conflict. Thanks!
calebmadrigal(2020-08-17 19:39:17):> > > @calebmadrigal : Looks like this PR is pending for quite a while. let's check this in :) Can you sign the CLA and resolve conflicts with master? Thanks!
> > 
> > 
> > CLA signed.
> 
> Thanks! Can you also resolve the merge conflict. Thanks!

Oh I suppose.
jcwchen(2020-10-07 18:17:10):Hi @calebmadrigal,
We hope to merge this PR before ONNX 1.8 Release.
Could you sign CLA again and also sign-off your commits? Thank you.
calebmadrigal(2020-10-07 18:32:05):> Hi @calebmadrigal,
> We hope to merge this PR before ONNX 1.8 Release.
> Could you sign CLA again and also sign-off your commits? Thank you.

I already signed the CLA.
jcwchen(2020-10-07 19:19:37):> > Hi @calebmadrigal,
> > We hope to merge this PR before ONNX 1.8 Release.
> > Could you sign CLA again and also sign-off your commits? Thank you.
> 
> I already signed the CLA.

Yes I saw you did, but license/cla is still pending... Let's finish DCO first, please check https://github.com/onnx/onnx/pull/1940/checks?check_run_id=1157152262 and sign-off your commits. I suggest you don't use `git rebase HEAD~12 --signoff` directly because it will probably mess the commit log. Sorry for the inconvenience and thank you for your contribution!
faxu(2020-10-13 03:50:09):@calebmadrigal can you confirm the merged PR fixed this issue with the lightgbm model?
calebmadrigal(2020-10-13 13:15:44):@faxu Yes, it does.
askhade(2020-10-16 18:11:54):Closing this PR
jim-meyer(2019-04-16 19:19:16):It would be better to use 'numbers.Integral' instead of just 'int' as is done at line 236 so that it handles 'long', 'np.int64()' and more.
calebmadrigal(2019-04-17 15:30:03):Good idea. And actually, now that you point that out, I can just use `numbers.Real` to cover both ints and floats.
bddppq(2019-04-22 18:47:16):This supersedes the check at line 233 ```instances(v, float)```, so better to directly replace that with your new check instead of adding a new elif branch.
bddppq(2019-04-22 18:53:48):```
if all(isinstance(v, numbers.Integral) for v in value):
...
   INTS
...
elif all(isinstance(v, numbers.Real) for v in value):
  # NB: ints are all instances of numbers.Real,
  # this allows passing mixed of ints and floats values as repeated argument
...
   FLOATS
...
```
calebmadrigal(2019-04-25 14:14:22):Good idea. Done.
xadupre(2020-08-24 10:22:06):Is it possible to add some comments on the documentation of the function to tell users that when there is such a mix of type, type float is chosen?
calebmadrigal(2020-08-27 16:14:27):It's certainly possible. I'm in the middle of moving across the country right now though, so I won't be doing it.
linkerzhang(2019-04-16 22:58:50):Thank you very much!
houseroad(2019-04-17 05:24:50):Thanks for fixing this bug!
CLAassistant(2019-04-17 00:14:01):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=1942) <br/>All committers have signed the CLA.
raymondxyang(2019-04-17 18:42:10):@fs-eire please sign the CLA thx
askhade(2019-04-17 01:46:38):@houseroad , @linkerzhang : This PR is to add node tests for the quantized ops. Please take a look.
hariharans29(2019-04-17 03:06:04):CC: @linkerzhang @houseroad 
linkerzhang(2019-04-18 00:21:46):@houseroad  @spandantiwari this update breaks pytorch test again, I think. Please look into it accordingly. Thank you!
linkerzhang(2019-04-22 03:36:39):@houseroad @bddppq  please kindly review this op update. Thank you!
hariharans29(2019-04-26 03:12:18):> Please bump up to opset 11.

@houseroad - Done
linkerzhang(2019-05-02 01:34:49):@houseroad  any more comments please? Thank you!
hariharans29(2019-05-08 01:03:16):Please hold off merging this until #2005 is merged
hariharans29(2019-05-13 21:22:35):Pinging....

@houseroad @linkerzhang 
hariharans29(2019-05-18 01:03:46):> Can we make the optimizer works for both version of Pad operator?

Hi @houseroad - can we test the same in optimizer_test.py (by creating a Pad v10 node) ? Is it possible to create an older opset node ?
hariharans29(2019-05-18 02:26:46):Closing this in favor of #2031 
linkerzhang(2019-04-18 00:15:36):Add more description or reference to clarify these modes?
linkerzhang(2019-04-18 00:18:07):More clarification about this 1-D tensor please (though the previous one is also not clear enough :)). Its shape should be the same as "pads".


linkerzhang(2019-04-18 00:19:11):padding value's type should be the same as input "data".
linkerzhang(2019-04-18 00:19:57):replace "T1" with "tensor(int64)", as T1 only has one option defined.
hariharans29(2019-04-18 02:01:35):@linkerzhang - Actually the shape need not be the same as 'pads' , this the equivalent of the previous 'value' attribute - it is just a single float value (a scalar essentially) - so it's shape should be either [] or [1] - I ll update it
hariharans29(2019-04-18 05:46:39):Fixed.
hariharans29(2019-04-18 05:46:46):Fixed.
hariharans29(2019-04-18 05:47:10):Updated.
linkerzhang(2019-04-18 06:40:06):I thought you would like to pad different values in each direction. if it's a [] or [1], then let's clarify it as a "Scalar".
hariharans29(2019-04-18 18:18:25):It is explicitly mentioned as a "scalar" in the latest version 
snnn(2019-04-18 20:56:00):LGTM.
linkerzhang(2019-04-19 06:25:48):@houseroad @spandantiwari please take care of the pytorch failure accordingly. Thank you! btw, @hariharans29 the pytorch ci failure is not a blocker for the PR merge. Will merge this later but give pytorch folks some time  to be aware of it.
snnn(2019-04-18 07:02:00):It may have byte-order issue.


hariharans29(2019-04-18 18:28:24):you mean endianness of the machine we are trying to parse the raw data ? That's a good point.  Thanks. I will fix it.
snnn(2019-04-18 19:44:47):You may use [this](https://github.com/Microsoft/onnxruntime/blob/master/onnxruntime/core/framework/tensorprotoutils.cc#L49) as a reference.
hariharans29(2019-04-18 20:55:20):Oh I didn't see your comment - I implemented it already and tested it. You may take a look.
linkerzhang(2019-04-19 02:34:23):maybe it's better to check whether ctx.getInputType(0)->tensor_type().has_shape().

This reminds me that a "Status" is still needed to be returned for shape inference function, so that, frameworks calling shape inference in a graph can do early stop (without having to do that via throwing and catching exceptions).

@gramalingam  thoughts?
gramalingam(2019-04-19 02:38:17):The existing check for “hasNInputShapes(…)” already checks for this. So, I am guessing that the real bug lies elsewhere … it could be in the shape-inference for a preceding node.   

This is somewhat fragile, because of the way “protobuf”’s optional fields work. The “shape” field is optional and not set if it is not known (e.g., if it’s rank is not known). So, every line of code that tries to access “.shape()” should first check if it “.has_shape()”. Otherwise, protobuf accessors will allocate the shape to a default-value when someone accesses it. So, my guess is that some recently added shape-inference code accesses the “.shape()” field without first checking if it “.has_shape()”. 

It is likely to be in the shape-inference for some preceding node that has the same tensor-variable as input or output.

askhade(2019-04-19 02:46:11):As @gramalingam mentioned:  ctx.getInputType(0)->tensor_type().has_shape() is already being checked as part of hasNInputShapes(…)
lucienwang1009(2019-04-19 03:42:55):rank() == 0 may mean the tensor is a scalar. And concatenating a scalar with an array is obviously wrong.
ebarsoum(2019-04-19 22:03:18):Can you fix the failing check.
HectorSVC(2019-04-20 02:56:18):what happened to travis-ci? half done succeed. another half always waiting?
HectorSVC(2019-04-20 04:29:01):could someone help merge it?
HectorSVC(2019-04-20 15:59:51):Build timeout?
linkerzhang(2019-04-22 03:08:43):re-triggered the one pipeline failed. will merge this once all CI passes.
SherlockNoMad(2019-04-22 05:09:34):I echo that having a stateless optimizer is a good design with existing ONNX without breaking SSA assumption. 

Different backends can come up with their own implementation on how to handle the new state and update the weights.
gramalingam(2019-04-24 18:43:21):Hi, Is there a plan to add the complete Adagrad optimization (loop) as another operator? The proposed operator seems only one part of the solution. It would help to understand how the whole thing would work.
wschin(2019-04-25 05:42:18):> Hi, Is there a plan to add the complete Adagrad optimization (loop) as another operator? The proposed operator seems only one part of the solution. It would help to understand how the whole thing would work.

There are two phases of supporting training in ONNX.

1. Training the model with one iteration.
2. Training the model with the whole data set.

Those operators are defined for Phase 1 and will be used in Phase 2 to compose multi-iteration algorithms. Several difficults that we can't go to Phase 2 directly are 

1. Defining a full training algorithm is similar to writing a program.
2. Defining a full training algorithm may put extra constraints on the inference stage. For example, if a written training algorithm wants to run 100 iterations, user will have to feed 100 batches into ONNXRuntime.
3. If we can define one training iteration properly, conceptually users can execute multiple iterations. 
CLAassistant(2019-07-24 00:57:26):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=1955) <br/>All committers have signed the CLA.
SherlockNoMad(2019-04-22 04:31:06):Do we need to output the gradients? 
Isn't it identical to the input gradients ?
SherlockNoMad(2019-04-22 04:32:51):T1 and T2 are both float? Maybe duplicated?
SherlockNoMad(2019-04-22 04:50:41):Just wondering if T_new is the only state variable we need to output? 
SherlockNoMad(2019-04-22 04:53:35):Or should it be "new accumulated squared gradient" ? 
wschin(2019-04-22 05:36:47):Right. It should be called `new accumulated squared gradient`.
wschin(2019-04-22 05:37:02):One for tensor and the other one for scalar.
wschin(2019-04-22 05:38:09):Let me remove it.
postrational(2019-04-22 10:55:18):Perhaps some of these inputs could be static Attributes instead.
wschin(2019-04-24 15:32:20):Gradient and accumulated squared gradient should be optional because a model can come without backward graph.
wschin(2019-04-24 16:16:34):Will do.
wschin(2019-04-24 20:12:21):Maybe I am wrong. It causes ambiguation between [X, G] and [X_1, X_2].
wschin(2019-04-30 06:08:31):```suggestion
    r_ = r / (1 + t * decay_factor)
```
wschin(2019-05-23 20:18:25):```suggestion


```
postrational(2020-03-02 17:57:06):I believe you mean special, not spatial?
```suggestion
    In that reference paper, this operator is a special case of the Figure 1's composite mirror
```
postrational(2020-03-02 18:02:46):It seems that this operator could be decomposed into more primitive operations. Can we add a `FunctionBody` for it as well?
wschin(2020-03-06 19:35:16):Yes... but we need to think about how to process variable-length inputs using a static graph. I will leave this as a future work.
gramalingam(2020-03-07 01:41:49):The comment and pseudo-code above don't seem to match. 
gramalingam(2020-03-07 01:43:10):Above line is unclear. Is the "T" supposed to be "R"? Is the goal to say "If the specified R value is 0, X will never be updated."?
gramalingam(2020-03-07 01:44:33):Nit: "compute square root element-wisely" => "computes square-root element-wise"
gramalingam(2020-03-07 01:47:25):"the current values of optimized tensors, followed by their respective gradients, followed by their respective accumulated squared gradients."
gramalingam(2020-03-07 01:48:05):"gradient" => "squared gradient"?
gramalingam(2020-03-07 01:48:54):T2 => T3?
gramalingam(2020-03-07 01:50:38):Or, say "Specify a T value of 0 if the learning-rate should stay the same."
wschin(2020-03-07 03:05:13):It matches. Let `y=0.5 * coef * x^2`. We have `dy/dx = coef * x`.
gramalingam(2020-03-08 04:31:56):Sorry, I misunderstood the comment. 
wschin(2020-03-10 18:34:17):I will change it to
```
      // Compute a scalar learning-rate factor. At the first update of X, T is generally
      // 0 (0-based update index) or 1 (1-based update index).
```
wschin(2020-03-10 18:40:35):Yes. Many thanks.
gramalingam(2019-04-24 19:02:10):I don't think these ops are control-flow ops … so they should probably go into a more appropriate defs file, instead of the control-flow defs file.
wschin(2019-05-27 01:46:27):@gramalingam, I will move this file to the right place after #1955 is merged.
CLAassistant(2019-07-24 00:57:08):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=1959) <br/>All committers have signed the CLA.
wschin(2019-05-24 06:42:56):Need an attribute to control if Nestrove momentum is used. #Resolved
wschin(2019-05-25 05:26:47):```suggestion
    Pseudo code for SG with Nesterov's Momentum:
``` #Resolved
jeffsaremi(2019-04-25 15:28:32):Could someone review this please? thanks
jeffsaremi(2019-04-25 16:41:43):CI builds are failing for reasons which don't seem related to my changes at all
Is anyone looking into this? 
yuslepukhin(2019-04-25 20:56:18):>        name='test_mod_bcast')

Do we want to call it mod_broadcast? #Closed

---
Refers to: docs/Operators.md:7574 in 812b1d0. [](commit_id = 812b1d08f4101547e79f703f5eec0a2e52a81bfc, deletion_comment = False)
yuslepukhin(2019-04-25 20:57:09):>        name='test_mod_int64_mixed_sign_example')

Why is this an example and others are not? #Closed

---
Refers to: docs/Operators.md:7554 in 812b1d0. [](commit_id = 812b1d08f4101547e79f703f5eec0a2e52a81bfc, deletion_comment = False)
yuslepukhin(2019-04-25 21:00:05):> x = np.array([-4.3, 7.2, 5.0, 4.3, -7.2, 8.0])

Also type as above #Closed

---
Refers to: docs/Operators.md:7509 in 812b1d0. [](commit_id = 812b1d08f4101547e79f703f5eec0a2e52a81bfc, deletion_comment = False)
yuslepukhin(2019-04-25 21:01:57):> <dt><tt>T</tt> : tensor(uint8), tensor(uint16), tensor(uint32), tensor(uint64), tensor(int8), tensor(int16), tensor(int32), tensor(int64), tensor(float16), tensor(float), tensor(double)</dt>

There are no tests for small types here. They are interesting bc in C++ operations as ints so they are upcasted and then the result is downcasted. It would be interesting to see if what numpy does matches my C++ implementation. #Closed

---
Refers to: docs/Operators.md:7470 in 812b1d0. [](commit_id = 812b1d08f4101547e79f703f5eec0a2e52a81bfc, deletion_comment = False)
yuslepukhin(2019-04-25 21:03:34):>         z = np.mod(x, y)

The resulting shape is interesting for broadcasting test. Perhaps, we could mention the shape in the comments. C++ implementation outputs [3, 2, 5] #Closed

---
Refers to: onnx/backend/test/case/node/mod.py:84 in 812b1d0. [](commit_id = 812b1d08f4101547e79f703f5eec0a2e52a81bfc, deletion_comment = False)
jeffsaremi(2019-04-25 21:06:43):> > ```
> >    name='test_mod_bcast')
> > ```
> 
> Do we want to call it mod_broadcast?
> 
> Refers to: docs/Operators.md:7574 in [812b1d0](https://github.com/onnx/onnx/commit/812b1d08f4101547e79f703f5eec0a2e52a81bfc). [](commit_id = [812b1d0](https://github.com/onnx/onnx/commit/812b1d08f4101547e79f703f5eec0a2e52a81bfc), deletion_comment = False)

I just used whatever other ops were using. This template was copied from somewhere else. I think we call know what bcast means
yuslepukhin(2019-04-25 21:07:45):I am just suggesting you use the same name you put in the doc above

---
In reply to: [486838166](https://github.com/onnx/onnx/pull/1962#issuecomment-486838166) [](ancestors = 486838166)
jeffsaremi(2019-04-25 21:08:11):> > ```
> >     z = np.mod(x, y)
> > ```
> 
> The resulting shape is interesting for broadcasting test. Perhaps, we could mention the shape in the comments. C++ implementation outputs [3, 2, 5]
> 
> Refers to: onnx/backend/test/case/node/mod.py:84 in [812b1d0](https://github.com/onnx/onnx/commit/812b1d08f4101547e79f703f5eec0a2e52a81bfc). [](commit_id = [812b1d0](https://github.com/onnx/onnx/commit/812b1d08f4101547e79f703f5eec0a2e52a81bfc), deletion_comment = False)

I think broadcasting is dealt with separately. There's a link to that. Again I followed the same style that other broadcastable ops were using
yuslepukhin(2019-04-25 21:10:10):Not sure what you mean separately. This is the broadcasting test. And when I test in C++ I need to specify the exact shape I am expecting. It is important that numpy result is the same.

---
In reply to: [486838595](https://github.com/onnx/onnx/pull/1962#issuecomment-486838595) [](ancestors = 486838595)
jeffsaremi(2019-04-25 21:11:15):> > T : tensor(uint8), tensor(uint16), tensor(uint32), tensor(uint64), tensor(int8), tensor(int16), tensor(int32), tensor(int64), tensor(float16), tensor(float), tensor(double)
> 
> There are no tests for small types here. They are interesting bc in C++ operations as ints so they are upcasted and then the result is downcasted. It would be interesting to see if what numpy does matches my C++ implementation.
> 
> Refers to: docs/Operators.md:7470 in [812b1d0](https://github.com/onnx/onnx/commit/812b1d08f4101547e79f703f5eec0a2e52a81bfc). [](commit_id = [812b1d0](https://github.com/onnx/onnx/commit/812b1d08f4101547e79f703f5eec0a2e52a81bfc), deletion_comment = False)

The spec should not make any assumptions on what the runtime(s) will do. Upcasting/downcasting is merely a result of the runtime limitations or resource restrictions (developers's time) in implementing them.
yuslepukhin(2019-04-25 21:11:43):I think we just need to restart. It is timing out. Talk to Raymond.

---
In reply to: [486748973](https://github.com/onnx/onnx/pull/1962#issuecomment-486748973) [](ancestors = 486748973)
yuslepukhin(2019-04-25 21:21:12):I think we cleared this during in-person conversation.

---
In reply to: [486839533](https://github.com/onnx/onnx/pull/1962#issuecomment-486839533) [](ancestors = 486839533)
jeffsaremi(2019-04-29 16:42:24):Would someone review and merge this please?
jeffsaremi(2019-04-30 03:28:18):Could someone review and approve this please? thanks
jeffsaremi(2019-04-30 03:54:18):> Looks good, thanks!

thanks a lot!
jeffsaremi(2019-04-30 14:49:20):Could someone merge this please? thanks
yuslepukhin(2019-04-25 20:59:14):>-7.2, 8.0 [](start = 35, length = 9)

Are these floats OR doubles? Can we make tests specify type explicitly? #Closed
yuslepukhin(2019-04-26 19:11:48):Looks like TestCoverage.md was not updated bc the changes you made in python script are not refected. #Closed
yuslepukhin(2019-04-26 19:12:41):Looks like Operators.md were not updated either. #Closed
yuslepukhin(2019-04-26 19:18:36):The Travis builds fail here with the following message:
+flake8
./onnx/backend/test/case/node/mod.py:157:1: W293 blank line contains whitespace #Closed
houseroad(2019-04-29 17:27:46):Can we still call it test_mod_xxx instead of test_fmod?
houseroad(2019-04-29 17:27:51):Can we still call it test_mod_xxx instead of test_fmod?
houseroad(2019-04-29 17:27:57):Can we still call it test_mod_xxx instead of test_fmod?
houseroad(2019-04-25 07:04:22):Circle CI will be fixed here: https://github.com/pytorch/pytorch/pull/19725
kkayri(2019-06-02 16:18:40):Je vous remercie tous d'avoir s'occupés de moi je peux pas faire certains commentaires parce que tout début est difficile
raymondxyang(2019-04-25 20:27:31):looks like i cannot sync with master for the mypy fix (release branch is protected).. @houseroad if it's good to you could you merge this back though the mypy check is failing?
houseroad(2019-04-25 20:58:08):```
Update branch attempt failed
Couldn't update "rel-1.5.0": Required status check "license/cla" is expected. At least 1 approving review is required by reviewers with write access.
```

ping @prasanthpul 
raymondxyang(2019-04-27 01:31:40):Close it right now to fix something.. will reopen soon
pranavsharma(2019-04-30 00:54:02):@raymondxyang have you verified this fix with onnxruntime using the model that had raw_data?
raymondxyang(2019-04-30 01:18:06):Yes. verified with both model with raw or float data
xkszltl(2019-04-27 00:51:02):Trailing space
xkszltl(2019-04-27 00:52:22):Maybe use `static_cast<int>`?
xkszltl(2019-04-27 00:53:04):Maybe use `static_cast<int>`?
xkszltl(2019-04-27 00:54:36):Why do you use insert?
If I understand this correctly, maybe you should `assign(data.begin(), data.end())`, or simply constructor the vector from iterator?
xkszltl(2019-04-27 00:55:17):Same question here of constructor vs. insert
xkszltl(2019-04-27 00:55:54):Same question here for:
1. constructor vs. insert
2. static_cast<int>
raymondxyang(2019-04-27 01:36:18):I was following the same way we did in other ops. see https://github.com/onnx/onnx/blob/master/onnx/defs/tensor/defs.cc#L123 and L551.
Any specific reason to make assign() preferred?
xkszltl(2019-04-27 02:04:51):Still 1 extra trailing space (ーー;)
Feel free to do it in another PR if you prefer.
xkszltl(2019-04-27 02:12:24):Because the vector here is brand new.
If it's not used afterward, you can just have something like
```
if (scales->has_raw_data()) { 
    const auto& data = ParseRawData<float>(scales); 
    vector<float> vec(data.begin(), data.end());
    ...
```
or if you want it later:
```
vector<float> vec;
if (scales->has_raw_data()) { 
    const auto& data = ParseRawData<float>(scales); 
    vec.assign(data.begin(), data.end());
    ...
```

This is a general suggestion, not something serious.
It's just a shorter way.
pranavsharma(2019-04-29 22:28:39):This copy can be avoided.
pranavsharma(2019-04-29 22:28:56):This copy can be avoided.
pranavsharma(2019-04-29 22:33:30):This copy can be avoided.
CLAassistant(2019-07-24 00:57:04):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=1970) <br/>All committers have signed the CLA.
sveta-levitan(2020-03-03 18:37:36):@wschin  Wei-Sheng, please respond to Michal's comments above, plus his comments on Gitter in Operators chat room. Thank you!
chinhuang007(2020-03-05 18:10:51):@wschin Can you please take a look and move this forward? During the TSC meeting today, the members suggest to have at least one optimizer PR merged for release 1.7. Please let us know if more time is needed.
wschin(2020-03-06 19:04:11):@sveta-levitan , @postrational , @chinhuang007, PR is update-to-date again. Please take a look. Thank you. 
chinhuang007(2020-02-18 19:19:28):Maybe I misunderstood, but I thought opset 11 is already part of release 1.6. Should the new operator be in opset 12 for release 1.7?
wschin(2020-02-19 07:47:53):Sorry.. This commit is for merging with master branch. Please only review all changes together. There are some changes

- Domain becomes `ai.onnx.training`
- The c++ code for defining spec is moved to `onnx\defs\training\defs.cc`
- A bug in function expansion is fixed (in function `def function_testcase_helper(node, name)` in a `__init__.py`).
postrational(2020-03-02 18:46:42):Why are these models changed in this PR? 
`onnx/backend/test/data/node/test_add/model.onnx`
`onnx/backend/test/data/node/test_add_bcast/model.onnx`
This may be an artifact.
postrational(2020-03-02 18:46:54):Why are these models changed in this PR? 
`onnx/backend/test/data/node/test_add/model.onnx`
`onnx/backend/test/data/node/test_add_bcast/model.onnx`
This may be an artifact.
wschin(2020-03-06 18:35:42):I will revert it. Thanks.
chinhuang007(2020-03-06 19:29:52):Is this change intentional?
chinhuang007(2020-03-06 19:31:35):This change seems to lead to other changes in docs. Maybe by accident?
wschin(2020-03-06 19:38:44):No. the `-` in the master branch is not ascii code, so my windows anaconda throws when building ONNX.
wschin(2020-03-06 19:39:12):Yes. I changed non-ascii `-` to ascii `-`.
chinhuang007(2020-03-06 20:00:18):Maybe I missed something. Isn't this part of documentation? And I just don't quite understand what "(kernel_size[d] - )" means. But I am certainly okay if you believe this is the right way to describe the calculation. 
wschin(2020-03-06 20:18:38):I will add `1` back. Thanks!
houseroad(2019-04-29 16:16:15):We use camel naming method in docs/: https://github.com/onnx/onnx/tree/master/docs

Shall we name the .md file in the same way?
jspisak(2019-04-30 01:45:14):@prasanthpul - yeah this crossed my mind. I can update the doc and call this out..

houseroad(2019-04-29 16:19:14):Nit: NLPinONNXproposal.md ==> NLPInONNXProposal.md
jspisak(2019-05-03 01:09:18):@prasanthpul 
houseroad(2019-05-03 06:28:31):rebase? there is a conflict :-)
prasanthpul(2019-08-19 17:15:26):@jspisak im not able to merge this due to the conflict. can you resolve?
askhade(2019-04-30 21:12:11):@houseroad : This PR is to fix shape inference for Max Pool and Avg Pool version. Can you please take a look at this PR. 
hariharans29(2019-04-30 21:16:53):Some minor nits that I noticed along the way that can potentially be fixed in the same PR -

1) Line 53: `if (input_shape.dim_size() < 2)` should actually be `if (input_shape.dim_size() < 3)` (I think!) since the first two dimensions are `N` and `C`, and the actual data dimensions start from the third dimension

2) Line 60: Comment is stale. `MaxPool` and `Conv` support dilations

3) Line 118: `second_input_shape.dim_size() < 1` can probably be modified to check if it matches the input shape `second_input_shape.dim_size() !=  input_shape.dim_size()`. This check doesn't seem to be present anywhere.... 
askhade(2019-04-30 22:10:01):> Some minor nits that I noticed along the way that can potentially be fixed in the same PR -
> 
> 1. Line 53: `if (input_shape.dim_size() < 2)` should actually be `if (input_shape.dim_size() < 3)` (I think!) since the first two dimensions are `N` and `C`, and the actual data dimensions start from the third dimension
> 2. Line 60: Comment is stale. `MaxPool` and `Conv` support dilations
> 3. Line 118: `second_input_shape.dim_size() < 1` can probably be modified to check if it matches the input shape `second_input_shape.dim_size() !=  input_shape.dim_size()`. This check doesn't seem to be present anywhere....

For 
1. This shape inference method is common for a lot of ops... Let's figure out how this will affect other operators and test before making this change. 

2. I addressed the comment

3. This is out of scope for MaxPool and AvgPool because these 2 ops only have 1 input. Let's address this as part of a separate PR and add relevant test cases to support these fixes.
CLAassistant(2019-04-30 22:09:50):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=1985) <br/>All committers have signed the CLA.
sfilipi(2019-04-30 23:21:18):> lint?

is that a note for me? 
houseroad(2019-04-30 23:11:54):space before parentheses?
jeffsaremi(2019-05-02 15:41:05):Could someone review this please? thanks
linkerzhang(2019-05-06 20:58:33):@BowenBao 
jeffsaremi(2019-05-07 18:03:40):corrections committed. all checks passing
BowenBao(2019-05-07 20:41:55):Thanks for submitting this PR to make Slice definition more complete and clarified. 

I noticed that the definition of `end_mask` here is not the same as TensorFlow definition. That could cause confusion. In TensorFlow it is defined as 

> begin_mask: a bitmask where a bit i being 1 means to ignore the begin value and instead use the largest interval possible. At runtime begin[i] will be replaced with [0, n-1) if stride[i] > 0 or [-1, n-1] if stride[i] < 0
> end_mask: analogous to begin_mask

It's used in cases like

```a[::-1]```

where the `begin` and `end` are `None`. Because there's no way to express `None` by integer, `begin(end)_mask` is introduced.

Here in Opset 10 Slice, the design was to express `None` by `INT_MAX`. Thus these `a[None:None:-1]` alike cases are actually already supported.

Personally I think your design adds more clarity. It is useful for handling cases of dynamic `start`, `end`, and `length`, where it is only known at runtime if it slices over the last index. 
The attribute should be renamed to something else to avoid confusion with the TF definition. 
Also should we consider adding it as input instead of attribute? Being an attribute assumes a static known input rank.

Let's hear what others have in mind @linkerzhang @houseroad 


jeffsaremi(2019-05-07 20:59:51):@BowenBao great input. So are you saying that currently in ONNX Slice I can do this:
data: [1,2,3,4], 
starts: [3]
ends: [INT_MAX]
steps: [-1]
and actually get [4,3,2,1] as the output?
if so I can close this PR because that's the ultimate goal and the only reason I introduced this was that I did not anything like that in the documentation for this Operator.
On the other points I completely concur if we decide to keep this change. And furthermore we could add a flag like that for start indices as well to be consistent. And make them tensors instead of attributes as well.
BowenBao(2019-05-08 00:43:53):@jeffsaremi yes that's the current design. I agree that the current description needs improvement. 
jeffsaremi(2019-05-08 17:04:05):I'm closing this one. Apparently there is such behavior already in Slice-10.
The only thing I found was this:

> For slicing to the end of a dimension with unknown size, it is recommended to pass in INT_MAX

Some one should look into expanding this and provide some test/examples of the expected results.

fdwr(2019-08-30 00:34:32):@jeffsaremi : You raise a good point and encountered the same difficulty I did. The `Reverse` operator was punted (#1883) in lieu of using `Slice` with negative steps, but using `Slice-10` to implement the missing `Reverse` is truly awkward, needing to reverse the starts/ends, offset them, and then futz with with special sentinel values if they are on the boundary (ick). After playing around with it some, I found it would have been much easier (and clearer) to disentangle `steps` from `start`/`end` attributes and continue thinking of the `start`/`end` as just slices into a subwindow, and that even for negative `steps`, the same rules apply: `start` < `end` keeping the same ordering, `end` is still last-element exclusive, and `end` - `start` is still the correct element distance regardless of step direction (`INT_MAX - 3` on the other hand gives you nonsense). Then `steps` are orthogonal to the subwindow range and just control which direction to sample/step to sample elements within that subwindow. e.g.

    data: [1,2,3,4],
    starts: [0]
    ends: [4]
    steps: [-1]
    output: [4,3,2,1]

    data: [1,2,3,4],
    starts: [0]
    ends: [4]
    steps: [-2]
    output: [4,2]

    data: [1,2,3,4],
    starts: [0]
    ends: [3]
    steps: [-2]
    output: [3,1]
BowenBao(2019-05-07 00:50:41):default is 1 --> default is 0
jeffsaremi(2019-05-07 16:52:05):thanks for catching this.
CLAassistant(2019-05-01 19:32:23):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=1988) <br/>All committers have signed the CLA.
linkerzhang(2019-05-02 01:33:48):Thank you! Can you also add some test cases to cover this change https://github.com/onnx/onnx/blob/master/onnx/test/shape_inference_test.py please?
stevenlix(2019-05-06 02:54:50):> Thank you! Can you also add some test cases to cover this change https://github.com/onnx/onnx/blob/master/onnx/test/shape_inference_test.py please?
Done

hariharans29(2019-05-02 19:26:35):I think this if condition should be (parse the auto_pad attribute prior to this like in line 1266)- 

`(nullptr == auto_pad_attr) || (nullptr != auto_pad_attr && auto_pad_attr->s() == "NOTSET" )`

Probably it is safe to also ensure both `auto_pad` and `pads` attributes are not present in this node - if both are detected fail shape inference with appropriate message 

EDIT: I take the last line back - I think the attributes can exist together - when auto_pad == 'NOTSET' and pads are specified. 
hariharans29(2019-05-02 19:28:12):According to the spec, it is totally possible for pads size to be less than 2 * input dimensions. In this case, default values to 0. Probably instead of failing shape inference, you could do a resize with values set to 0. This is the line in the spec which makes it possible - 

`If not present, the padding defaults to 0 along start and end of each axis.`
 
hariharans29(2019-05-02 19:29:57):Maybe add a few more tests ? 
* NOTSET case
* strides = 1 with either SAME_UPPER or SAME_LOWER (kind of basic, but good to have)
  
stevenlix(2019-05-02 20:06:11):Since auto_pad is deprecated attribute, "pads" has higher priority than auto_pad and should be checked first. If "pads" and auto_pad mode are not present, it means no padding.
stevenlix(2019-05-02 20:12:09):I checked the spec again. My understanding is that if "pads" presents, its size should be n_input_dims * 2. If it doesn't present, its default value is 0, but its size should still be n_input_dims * 2.
stevenlix(2019-05-02 21:14:21):NOTSET is covered by old tests already. I added one more test for strides = 1
CLAassistant(2019-05-02 18:28:12):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=1990) <br/>All committers have signed the CLA.
CLAassistant(2019-05-02 19:39:30):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=1991) <br/>All committers have signed the CLA.
KsenijaS(2019-05-03 21:08:20):@houseroad @bddppq could you please take a look at this? thanks.
prasanthpul(2019-05-07 06:26:57):@KsenijaS can you sign the CLA? you may need to update the email used for the commit with the email used for your GitHub accnt
KsenijaS(2019-05-07 16:43:11):@prasanthpul I have signed the CLA and also my GitHub has an email address. I didn't set up my git email before adding commits. Should I close this PR and try again? 
prasanthpul(2019-05-07 20:05:10):The problem is caused because the commits in the PR are from "Ksenija Stanojevic" while the GitHub account you used is "KsenijaS". You should use git --config user.name to set the right name and you may need to resubmit the commits. https://help.github.com/en/articles/why-are-my-commits-linked-to-the-wrong-user may help
houseroad(2019-05-07 01:38:46):To be honest, I still didn't see a strong motivation to drop this options...

Could you further elaborate?
linkerzhang(2019-05-07 14:00:23):@houseroad  this PR is based on the discussion of #1966, I believe.
ayermolo(2019-05-07 17:51:27):Any way to re-kick CI? Doesn't look like failure in Xcode: xcode9.3 is related. "No output has been received in the last 10m0s, this potentially indicates a stalled build or something wrong with the build itself."
ayermolo(2019-05-07 18:33:27):@houseroad great, thanks!
ayermolo(2019-05-07 21:53:52):@zrphercule All good on your side?
ayermolo(2019-05-09 17:12:17):Awesome, thanks!
houseroad(2019-05-07 01:37:11):Is it only required by CI? or real world case also need it?

CI should just be one instance of real world case in my opinion. 
ayermolo(2019-05-07 17:49:18):Well I added in case few down the road same question is raised.
zrphercule(2019-05-06 01:14:17):@houseroad @linkerzhang @ebarsoum Would you mind take a look again? Thanks!
wschin(2019-05-23 05:40:55):Would you consider making it a FunctionProto? It can be composed using around 6 simple operators. If every such an operator will be added into ONNX, ONNX will end up with infinite amount of signatures.
CLAassistant(2019-07-24 00:56:54):[![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/onnx/onnx?pullRequest=1996) <br/>Thank you for your submission! We really appreciate it. Like many open source projects, we ask that you sign our [Contributor License Agreement](https://cla-assistant.io/onnx/onnx?pullRequest=1996) before we can accept your contribution.<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/onnx/onnx?pullRequest=1996) it.</sub>
postrational(2019-08-22 14:02:38):We should consider adding this as a `Function`.
houseroad(2019-05-06 19:21:25):onnx.TensorProto.BOOL
houseroad(2019-05-06 19:21:49):greater_or_equal
houseroad(2019-05-06 19:21:56):onnx.TensorProto.BOOL
hariharans29(2019-05-09 00:23:03):@linkerzhang - I refined the op description. Please take a look. It aligns more with tensorflow's description. For more details on model usage, @wenbingl @jiafatom maybe can share more details. 

@jeffsaremi - I included the output you requested. Please review to make sure it is indeed what you need.

Thanks!

jeffsaremi(2019-05-09 15:05:31):I only had one comment. Everything else looked good.
hariharans29(2019-05-13 21:01:43):Pinging....

@houseroad @linkerzhang 
linkerzhang(2019-05-15 18:54:12):@houseroad  would you help to review this op please? Thank you!
hariharans29(2019-05-22 01:42:57):Closing in favor of #2042 
linkerzhang(2019-05-07 13:54:41):not sure it's because of my personal understanding of English. Is it better to say dedup? My first thought of unique of input {2, 1, 1, 3, 4, 3} is {2, 4} :)
hariharans29(2019-05-07 21:09:06):This is good feedback. I will try and refine the description - but I was going based on the tf.unique description - 

`This operation returns a tensor y containing all of the unique elements of x sorted in the same order that they occur in x. This operation also returns a tensor idx the same size as x that contains the index of each value of x in the unique output y` from https://www.tensorflow.org/api_docs/python/tf/unique


jiafatom(2019-05-09 18:44:55):tf.unique output idx supports both int64 and int32, but we only support int64. Shall we need support int32 also?
hariharans29(2019-05-09 18:54:52):@linkerzhang - I initially preferred to keep it to just INT64 only, but what's your opinion ?
linkerzhang(2019-05-15 18:50:00):int64 is good enough, I think.
hariharans29(2019-05-08 00:53:04):@raymondxyang (FYI - as the `Upsample` shape inference method has changed due to refactor, could you please review as well ?)

hariharans29(2019-05-08 21:40:12):@linkerzhang  - When you have a chance, please review again.
linkerzhang(2019-05-08 00:07:50):Thank you!

We may do a little bit refactoring to have a function (template) to get data from TensorProto. That function will cover the raw_data check, otherwise, all shape inference functions (using input data / initializers) will have these two "if"s.
hariharans29(2019-05-08 00:51:32):Thanks @linkerzhang. I refactored the function that was previously only parsing raw data to generically handle data parsing (not just raw). So, two "if"s should go away now. 
raymondxyang(2019-05-08 18:29:07):Nice refactor. We can unify the data parsing step after this! Thanks.

btw for testing purpose, did you have the model modified (pass the tensor as initializor instead of input) and tested with the changed code? If you need I can send you the script to do so offline. 
hariharans29(2019-05-08 18:55:37):Thanks @raymondxyang. Not sure if this will cover the case - but I added a shape inference test for upsample that has an initializer with raw data. Will this help ?

As an aside  - I see that the test seems to be for eopset 9, should I add similar tests for opset 10 ? 
hariharans29(2019-05-08 19:00:54):Never mind - I see that the op is deprecated in opset 10. Please just check if he test I added will cover your case. Thanks!
raymondxyang(2019-05-08 21:16:07):Looks it will cover the case. Thanks!
linkerzhang(2019-05-07 21:15:21):this should be covered by https://github.com/onnx/onnx/pull/2002?
jeffsaremi(2019-05-07 22:39:42):@linkerzhang thanks for that
I just spoke to @hariharans29 and he will add the counts as a third output.
I will go ahead and close this then.
spandantiwari(2019-05-07 21:40:53):cc: @houseroad @linkerzhang 
KsenijaS(2019-05-13 20:05:54):cc @bddppq @houseroad Can you please take a look at this PR? Thanks.
houseroad(2019-05-13 20:47:40):Some type annotation is missing. Could you take care of it?
KsenijaS(2019-05-13 20:54:27):@houseroad Can you please elaborate on what type annotations are missing?
KsenijaS(2019-05-13 23:36:45):@houseroad I added type annotations and all checks pass now.
vinitra-zz(2019-05-15 19:21:49):@KsenijaS please merge the master branch into your branch! Otherwise LGTM as well.
KsenijaS(2019-05-16 00:17:24):cc @houseroad @bddppq 
houseroad(2019-05-17 06:50:58):check the node type?
spandantiwari(2019-05-07 21:40:53):cc: @houseroad @linkerzhang 
KsenijaS(2019-05-13 20:05:54):cc @bddppq @houseroad Can you please take a look at this PR? Thanks.
houseroad(2019-05-13 20:47:40):Some type annotation is missing. Could you take care of it?
KsenijaS(2019-05-13 20:54:27):@houseroad Can you please elaborate on what type annotations are missing?
KsenijaS(2019-05-13 23:36:45):@houseroad I added type annotations and all checks pass now.
vinitra-zz(2019-05-15 19:21:49):@KsenijaS please merge the master branch into your branch! Otherwise LGTM as well.
KsenijaS(2019-05-16 00:17:24):cc @houseroad @bddppq 
houseroad(2019-05-17 06:50:58):check the node type?
BowenBao(2019-05-13 17:59:08):cc @ebarsoum @spandantiwari 
gramalingam(2019-05-08 15:01:39):CheckerContext already has the "is_main_graph" attribute … it would be better to set/use that.
gramalingam(2019-05-08 15:04:45):check_value_info checks several other things beyond existence of shape. Why skip all of those checks? It would be better to turn off only the check for existence of shape. 
BowenBao(2019-05-08 20:53:19):Thanks for the comment, update to use CheckerContext. 
BowenBao(2019-05-08 20:55:14):Update to use CheckerContext. check_value_info checks name, type, and shape. Now for subgraph the checker will skip type and shape. 
linkerzhang(2019-05-09 17:29:47):this looks to me very bad. a "const" setter....
gramalingam(2019-05-09 18:54:11):Yes, I agree. I think we should be creating a copy of the CheckerContext to modify it. Furthermore, we can make all components of CheckerContext that do not change "const *" or "const &" so it can be shared (if desired).
gramalingam(2019-05-09 18:54:35):Or, it should be a non-const parameter.
BowenBao(2019-05-09 19:15:25):Agree.. Let's keep the ctx immutable, creating copy for subgraph. 
houseroad(2019-05-09 19:51:25):Define X?
houseroad(2019-05-09 19:51:37):Define the output?
BowenBao(2019-05-09 23:16:56):Added, also added to the original nested_graph test case that I copied from.
BowenBao(2019-05-09 23:17:03):same above
CLAassistant(2019-05-08 14:43:22):[![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/onnx/onnx?pullRequest=2010) <br/>Thank you for your submission! We really appreciate it. Like many open source projects, we ask that you sign our [Contributor License Agreement](https://cla-assistant.io/onnx/onnx?pullRequest=2010) before we can accept your contribution.<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/onnx/onnx?pullRequest=2010) it.</sub>
nandesuka(2019-05-09 15:25:34):Yup will do.
askhade(2021-10-08 17:06:15):Closing this PR as the optimizers are now moved out to ONNX repo
CLAassistant(2019-07-24 00:56:46):[![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/onnx/onnx?pullRequest=2012) <br/>Thank you for your submission! We really appreciate it. Like many open source projects, we ask that you sign our [Contributor License Agreement](https://cla-assistant.io/onnx/onnx?pullRequest=2012) before we can accept your contribution.<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/onnx/onnx?pullRequest=2012) it.</sub>
houseroad(2019-05-17 06:40:56):is the input's rank 1 or 2?
houseroad(2019-05-17 06:42:09):Could you elaborate more on the rank of input and output tensors? in the example, input is rank 2, and output is rank 2, too. 
jeffsaremi(2019-05-20 16:26:54):Thank you. I will revise the description and push again
jeffsaremi(2019-05-20 16:27:12):I will make this clear.
gramalingam(2019-05-13 16:35:12):Thanks Wei-sheng. Looks good to me, with a few minor points I mentioned above.
hobei(2019-07-09 11:12:44):@wschin During reviewing of the TrainingInfoProto structure I find that the TrainingInfoProto in the onnx.in.proto has a 'algorithm' field but the TrainingInfoProto in onnx.proto (and others) have a 'loss' & 'optimizer' field. Are they intentionally different? Thanks
chinhuang007(2019-07-09 17:00:25):The cardinality of TrainingInfoProto in model seems out of sync in different proto files. I believe the latest proposal is to have many (repeated) to support multiple training stages?
wschin(2019-07-09 22:23:47):@hobei, @chinhuang007, That's my bad. `onnx.in.proto` contains the right information.
CLAassistant(2019-07-24 00:57:12):[![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/onnx/onnx?pullRequest=2013) <br/>Thank you for your submission, we really appreciate it. Like many open source projects, we ask that you all sign our [Contributor License Agreement](https://cla-assistant.io/onnx/onnx?pullRequest=2013) before we can accept your contribution.<br/>**26** out of **27** committers have signed the CLA.<br/><br/>:white_check_mark: bddppq<br/>:white_check_mark: gramalingam<br/>:white_check_mark: wschin<br/>:white_check_mark: prasanthpul<br/>:white_check_mark: skottmckay<br/>:white_check_mark: take-cheeze<br/>:white_check_mark: ebarsoum<br/>:white_check_mark: neginraoof<br/>:white_check_mark: BowenBao<br/>:white_check_mark: edgchen1<br/>:white_check_mark: wutiantong<br/>:white_check_mark: askhade<br/>:white_check_mark: daquexian<br/>:white_check_mark: jspisak<br/>:white_check_mark: HectorSVC<br/>:white_check_mark: linkerzhang<br/>:white_check_mark: liqunfu<br/>:white_check_mark: yuslepukhin<br/>:white_check_mark: souptc<br/>:white_check_mark: hariharans29<br/>:white_check_mark: jignparm<br/>:white_check_mark: houseroad<br/>:white_check_mark: lara-hdr<br/>:white_check_mark: lutzroeder<br/>:white_check_mark: ryan-nervana<br/>:white_check_mark: JamesAllingham<br/>:x: LeicongLi<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/onnx/onnx?pullRequest=2013) it.</sub>
wschin(2020-01-24 01:15:30):This PR has been merged into #2314 so we can close it now. 
gramalingam(2019-05-13 16:22:54):If I remember right, we make all fields optional for Proto2 / Proto3 compatibility? And document required fields in the comment only (e.g., see the other fields with the "The <name> field MUST be present" comments.)
gramalingam(2019-05-13 16:29:35):I don't think we need the restrictions below now, do we? If we have pre-defined ONNX ops (for the optimizer), the op spec will indicate the required operand-ordering. We can recommend the following ordering as a preferred practice, but it is  not clear to me why this is a compulsory requirement. (Also: spelling correction "restruction" => "restriction" … but I don't think we need restrictions.)
gramalingam(2019-05-13 16:32:04):Just a minor suggestion: can we swap the order of the optimizer and gradient_bindings? Just to correspond to the order of evaluation (cost function first, then computation of gradients, and then the optimizer step).
wschin(2019-05-13 17:23:41):They are for customized training algorithms. As you mentioned, pre-defined ONNX optimizers have some kind of special schemas for correcting wiring tensors. For the same reason, customized optimizers (defined as FunctionProto in `ModelProto.function`) should use the same schema.
wschin(2019-05-13 17:25:02):Ah. Will change them back.
wschin(2019-05-13 17:25:38):Sure.
gramalingam(2019-05-13 17:30:05):Yes, but the runtime doesn't care really since we already capture the relationship between optimized-tensors and their-gradients separately, and the optimizer NodeProto's input list captures the relationship between the tensors/gradients computed in the preceding steps and the inputs required by the optimizer custom-function. So, I don't understand what extra-restrictions we need to talk about. (There is already an implicit requirement that the inputs listed in the optimizer NodeProto's input should be consistent with the input-ordering in the optimizer custom-function/onnx-op.)
gramalingam(2019-05-13 17:33:22):In other words, there is no requirement that different customized optimization functions should all follow the same pattern/style for ordering inputs/outputs. However, if they all follows the same pattern/style, it may end up being helpful in the future (or to users). So, I can understand the value in a "recommended ordering" (but it doesn't seem like it needs to be a "mandated ordering").
wschin(2019-05-13 17:45:42):Let's consider an example. Can user specify an optimization function which outputs `[Gradient_X1, new_X1, new_X2, Gradient_X2]`? How would runtime know `new_X1` is the new value of input tensor `X`? In addition, there can be other FunctionProto optimizers which produce `[A, B, C, D]`, `[C, A, B, D]`, and so on. Can runtime bind `A`, `B`, `C`, and `D` correctly to updated tensors and updated states for all possible output permutations without those restrictions?
gramalingam(2019-05-13 18:18:58):new_X1 would be in the additional_input list (of TrainingInfoProto), right? It will also appear in the optimizer NodeProto's input list. (The relationship between items in these lists are simply by name.) A corresponding formal_parameter (call it formal_new_X1) will appear in the customized FunctionProto. The correspondence between items in the Optimizer NodeProto input list and the customized FunctionProto input list is by position. So, what else does the runtime need to know?
gramalingam(2019-05-13 18:20:40):Sorry, new_X1 would probably in the inference graph's input list. Other things (like state_X1) may be in the additional_input list. It doesn't matter. But they are referred to by name in the optimizer NodeProto's input list.
gramalingam(2019-05-13 18:22:07):Sorry again (several typos!): I am talking about inputs, but the same thing applies for outputs. new_X1 in optimizer NodeProto's output list is associated with the additional_outputs of training ny name.
gramalingam(2019-05-13 20:48:10):Spoke with Wei-sheng offline. Now I understand his goals. It sounds like this is more a schema restriction for the training-graph (rather than the optimizer function). Can we specify the restriction that the additional_training_outputs must correspond to the concat(graph initializers, training graph additional initializers) one-to-one?
gramalingam(2019-05-13 20:49:06):Otherwise, we may need to add the mapping as another field.
wschin(2019-05-13 21:30:19):One-to-one looks too much because not every tensor is updated in the training phase. Let's add another string-to-string entry list.
wschin(2019-05-21 19:54:32):Please clarify name look-up when referencing `FunctionProto` in `ModelProto.functions` for both of loss and optimizer.
wschin(2019-05-21 19:55:29):Please explain how the gradient tensors are associated with `loss`.
chinhuang007(2019-05-28 17:02:31):Do we really need this binding? Based on the document https://github.com/onnx/onnx/files/3208156/ONNX.Training.Discussion.pptx, the optimizer has W and Gradient W as input. The name of "gradient W", which is output from the backward prop, seems could be derived from the backend and runtime implementation. The current Pytorch and TF APIs do not support user-defined names for the gradients for the optimizers as a reference.
wschin(2019-05-29 17:42:01):We need it. Pytorch creates such a binding in another (but equivalent) way. They have a dictionary where key is parameter and value is that parameter's gradient tensor.
wschin(2019-07-09 22:27:54):[Not directly related to this PR] One important thing in `Gradient`'s spec is to specify the behavior of `non-differentiable` tensors. Say, for example, in
```
X, shape ---> Reshape ---> X_reshaped
```
we can back-propagate `dY/dX_reshaped` to `dY/dX` but `dY/dshape` should be zero (which means no gradient).
CLAassistant(2019-05-12 13:31:58):[![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/onnx/onnx?pullRequest=2014) <br/>Thank you for your submission! We really appreciate it. Like many open source projects, we ask that you all sign our [Contributor License Agreement](https://cla-assistant.io/onnx/onnx?pullRequest=2014) before we can accept your contribution.<br/>**0** out of **2** committers have signed the CLA.<br/><br/>:x: shyu<br/>:x: one-hello<br/><hr/>**shyu** seems not to be a GitHub user. You need a GitHub account to be able to sign the CLA. If you have already a GitHub account, please [add the email address used for this commit to your account](https://help.github.com/articles/why-are-my-commits-linked-to-the-wrong-user/#commits-are-not-linked-to-any-user).<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/onnx/onnx?pullRequest=2014) it.</sub>
linkerzhang(2019-05-13 02:53:02):@one-hello  please sign the CLA accordingly.
linkerzhang(2019-05-14 01:37:35):https://github.com/onnx/onnx/pull/2015 created and merged for same purpose.
CLAassistant(2019-07-24 00:56:50):[![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/onnx/onnx?pullRequest=2016) <br/>Thank you for your submission! We really appreciate it. Like many open source projects, we ask that you sign our [Contributor License Agreement](https://cla-assistant.io/onnx/onnx?pullRequest=2016) before we can accept your contribution.<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/onnx/onnx?pullRequest=2016) it.</sub>
CLAassistant(2019-05-14 20:32:59):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=2018) <br/>All committers have signed the CLA.
prasanthpul(2019-05-17 06:44:05):I think it was a mistake. I had asked offline but didn't hear back. Closing
gramalingam(2019-05-30 18:58:13):Do we need a separate name for the sparse-tensor? Or, is the name in the "values" tensor (which MUST be present) sufficient?
wschin(2019-06-04 05:45:14):> Do we need a separate name for the sparse-tensor? Or, is the name in the "values" tensor (which MUST be present) sufficient?

I think `values` tensor is good enough but we still need to explicitly define this in the spec.
gramalingam(2019-06-12 23:16:08):Hi @wschin , thanks for all your feedback. I believe I have addressed them all. If there's anything more, please do let me know.
CLAassistant(2019-07-24 00:56:45):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=2019) <br/>All committers have signed the CLA.
gramalingam(2019-07-24 18:32:43):@houseroad : I added the helper make_sparse_tensor. However, I have a concern about the python support in protobuf, which makes these kind of utility functions potentially expensive and inefficient. Protobuf, in python, does not let us assign or update field values (except for primitive types). Thus, we end up deep copying in such utility functions, which could be expensive for large tensors. For example, if we create a tensor (using make_tensor) and then create an attribute (using make_attribute), we will likely end up copying the data first in make_tensor (e.g., from a numpy array) and then copying it again in make_attribute. I don't see a easy solution to this. If the user directly does it (without using these utility functions) they can probably eliminate the extra copy. This may not be a big deal for small objects, but could matter for tensors that are 100s of megabytes.
gramalingam(2019-08-12 23:13:10):Hi @linkerzhang : this PR is ready. Please let me know if it is okay to merge it in.
gramalingam(2019-08-12 23:13:57):I can merge in master once we have approval.
gramalingam(2019-08-16 19:09:16):Close and reopen to trigger CI.
gramalingam(2019-05-14 22:09:24):Note that we could generalize this and allow the user to choose to encode the indices in linearized form or directly: e.g., consider a 100x100 sparse matrix. A direct representation would be [ [row-indices], [column-indices] ] e.g., [ [0,1,2], [3,4,5] ] while a linearized form would be more compact as [ 0x100+3, 1x100+4, 2x100+5 ] = [3, 104, 205].
gramalingam(2019-05-14 22:11:17):Should we generalize this and allow values to be a tensor of rank more than 1? That would be useful if we want to capture sparse-tensors with s sparse-dimensions followed by d dense-dimensions where d > 1.
wschin(2019-05-16 00:31:23):I'd like to have higher rank. Examples:
- In multi-class linear classification, we usually store the linear coefficient vectors for all classes as a 2-D tensor.
- In some training algorithms (see truncated Newton methods as an example), a lot of sparse-matrix-vector produces are needed.
- The output of OneHotEncoder/TFIDF should be a sparse tensor and that sparse tensor's rank can be larger than 1.
wschin(2019-05-16 00:32:15):Do we still have experimental ops?
wschin(2019-05-16 00:36:01):Yes, then we need to have checker being aware of this. If linear indexes, the rank of indices should be 2 and otherwise the rank is identical to `dims + 1`.
wschin(2019-05-16 00:39:10):I feel yes for reasons mentioned above.
gramalingam(2019-05-16 15:27:06):In linearized case, the rank is 1 and shape is [NNZ]. In the other case, the rank is 2, and the shape is either [NNZ, dims] or [dims, NNZ] (we need to pick one of these). Note that the values are stored separately (as they may have a different type) from the indices.
gramalingam(2019-05-16 15:35:01):Hi, this question is not about the rank of the sparse-tensor: we do permit sparse-tensors of arbitrary rank. The question is about whether the value stored for a given sparse-index can be, say, N floats instead of a single float. Pytorch, for example, allows this. It allows us to take a d-dimensional tensor, and say that the first s-dimensions are sparse and the remaining (d-s) dimensions are dense. In this case, the values field becomes a (d-s)-dimensional tensor, while the indices becomes a [NNZ, s] dimensional tensor. Representation-wise, this is easy enough to support. However, the implementation of ops will be somewhat more complex to support this generality (vs. treating each value as a scalar).
gramalingam(2019-05-16 15:37:43):I didn't touch this. It was reformatted automatically by VisualStudio using the clang format. (I believe this might be used for backward-compatibility … Ke would know.)
wschin(2019-05-16 16:30:26):I got your point. This might be helpful for scientific simulations where their matrices can have block-diagonal structure. However, I don't feel they will be ONNX's major users at this stage. Let's not to allow the mixture of sparse and dense for now (as your design can be extended to cover that in the future).
wschin(2019-05-16 16:31:02):Ah.. For BC, yes, we need to know those operator names.
wschin(2019-05-16 17:10:15):I'd prefer `[NNZ, dims]`. In general, sparse matrices are used to store `data points` (a collection of sparse feature vectors). I'd imagine that most data transformations would want to access the coordinates of one `non-zero` at the same time.
gramalingam(2019-05-16 17:18:25):Yes, conceptually [NNZ, dims] is more intuitive. I think TF does this. However, the MKL sparse library, for example, requires row-indices as a separate array and column-indices as a separate array (see: https://software.intel.com/en-us/mkl-developer-reference-c-sparse-blas-coordinate-matrix-storage-format ), which can be generated easily if we use [dims, NNZ]. So, it doesn't look like there is one uniform standard anyway.
gramalingam(2019-05-16 17:22:17):PyTorch sparse constructors (at Python level) seem to take a list of lists in [dims,NNZ] ordering: see: https://pytorch.org/docs/stable/sparse.html 
linkerzhang(2019-05-16 17:42:58):this should be just a repeated int64 indices?
wschin(2019-05-16 20:21:12):Let's only consider 2-D tensors for simplicity. Right, there is no uniform standard, but having `[NNZ, dims]` is enough to cover all cases. 

As COO format is not efficient for matrix computation, runtime will have to do a conversion (to, e.g., CSR) by sorting those non-zeros according to either row indexes and column indexes. Because the sorting will be something like `sorting those 1-D index vectors by their first/second value`, I feel using `[NNZ, dims]` may lead to better cache locality.

On the other hand, when producing sparse tensors to COO format using `OneHotEncoder`, we generally produce `[dims]` index vector at one time, so keep appending `[dims]` to `[NNZ, dims]`-output could also lead to a better cache locality.

In other words, I am thinking of minimizing the cache missing rate when

1. producing sparse tensors and
2. converting sparse tensors to other computation-friendly format.
wschin(2019-05-16 20:23:02):No. For a 2-D sparse tensor,
```
[ [0, 0],
  [1, 0] ],
```
the first element in `indices` would be `[1, 0]`.
wschin(2019-05-16 20:31:11):Another behavior I'd like to have is that `indices` should be sorted first by its first element, then by its second element, .., and finally its last element. This is very helpful when searching for a specified value in a sparse tensor.
gramalingam(2019-05-20 22:22:09):@linkerzhang : it is a int64 TensorProto, if that is your point. Using a TensorProto instead of "repeated int64" has a couple of advantages: one, as Wei-sheng mentions, it allows us to capture the "shape" of indices; second, it allows us to use the different formats supported by TensorProto for free.
gramalingam(2019-05-20 22:23:23):Yes, we should require that the indices should appear in sorted order. We need to clarify that in the documentation.
wschin(2019-05-24 07:09:09):Please print out the tensor's name if possible.
wschin(2019-05-24 07:09:17):Please print out the tensor's name if possible.
wschin(2019-05-24 07:09:22):Please print out the tensor's name if possible.
wschin(2019-05-24 07:17:23):Should we throw if sparse tensor has no indices? Also, would you think checking if those indices are properly bound by `dims`  is one thing we need to do here? You might want to check if those indices are sorted.
linkerzhang(2019-05-29 23:04:43):Fair enough.
gramalingam(2019-05-30 18:56:00):On second thoughts, I think it seems simpler to go with @linkerzhang 's suggestion of repeated int64 indices. (Of course, it will contain NNZ * rank entries, in the required order, which the spec should clarify.) It is unclear we will need the extra flexibility provided by the different formats in a TensorProto for a sparse tensor. So, I am inclined to change it as suggested.
wschin(2019-06-04 06:14:33):It seems that this message groups fields by `repeated` and `optional`. Do you feel this line needs to go to line 144?
gramalingam(2019-06-10 18:11:09):After more discussions, we decided to leave it as a TensorProto, given that protobuf repeated fields seem to use int rather than int64 for indexing. TensorProto's flexible options for representing data may end up being useful.
gramalingam(2019-06-10 18:19:29):Added the checks.
wschin(2019-08-16 15:22:23):```suggestion
        ") does not have the right number of values.");
```
Please also print out the right value, `nnz`, and the existing value, `indices.dims(0)`.
wschin(2019-08-16 15:24:01):```suggestion
  // values. The i-th value in index_data is the linear index of the i-th non-zero value.
```
wschin(2019-08-16 15:25:05):Please print out the expected range, `[0, dense_size]`, if possible.
wschin(2019-08-16 15:28:07):What does it mean? An non-zero value can have multiple indices?
wschin(2019-08-16 15:30:13):Is it for sparse block matrix?

[Update] No. It's for COO format.
wschin(2019-08-16 15:32:56):```suggestion
  // corresponding to the j-th index of the i-th value (in the values tensor).
  // That is, the index at the j-th axis in COO format of values[i][j] is indices[i][j].
  // For a 3-D sparse matrix, indices[i] would be a 1-D vector such as (6, 7, 8).
  // The number 6/7/8 is the index of the first/second/last axis. It means
  // sparse_tensor[6][7][8] = value[i].
```
wschin(2019-08-16 15:43:45):[nit] Not sure if we need test for this.
wschin(2019-08-16 15:49:41):`Must`? I saw two formats below. One uses linearized indexing and the other is the same as what described here. Your code supports both cases.
wschin(2019-08-16 15:50:56):```suggestion
      // Check linearized index.
      case 1:
```
wschin(2019-08-16 15:52:00):```suggestion
      // Check COO-style index. Note that, for example, in 3-D tensor, an index means a 3-element vector.
      case 2:
```
gramalingam(2019-08-16 17:30:47):Thanks. I was puzzled about the original message … I guess I meant to write "does not have ", nnz, " values" !
snnn(2019-05-16 04:58:52):The problem is:  
In our Windows CI build, onnx depends on the prebuilt libprotobuf from conda forge.
However, the older version of libprotobuf  was static link to VC Runtime,
But the latest one is dynamic link to VC Runtime.
And, currently, onnx's cmake can only support one of these!

Any suggestion?
hariharans29(2019-05-17 06:50:43):Closing in favor of #2025 
stevenlix(2019-05-17 00:17:52):What if there is the corner case,
input shape: 4, kernel shape: 2, strides: 4
then legacy_target_size = 1, total_pad will be negative value -2. 
BowenBao(2019-05-17 00:20:07):is(was) negative padding supported?
stevenlix(2019-05-17 00:32:56):Not sure if negative padding means anything in real cases. For above corner case, we can safely set total_pad to 0 if it's negative, but there might be more corner cases.
BowenBao(2019-05-17 00:35:30):it seems the check for ```auto_pad_attr->s() != "NOTSET"``` was removed in the previous PR. One edge case slipped through that when auto_pad=NOTSET, and pads are not provided. This doesn't affect the outcome though, as pads values will remain zero for NOTSET.
hariharans29(2019-05-17 00:55:19):I think I did make this comment in the original PR - https://github.com/onnx/onnx/pull/1988. Maybe I should just include my suggestion in the PR, then it might be clear that we are accounting for that case 

EDIT: I think @stevenlix's justification is quite valid. So, I think I ll just leave it like it is now and as you say the outcome should be correct. I don't want to modify any unnecessary code and risk another regression.
hariharans29(2019-05-17 01:21:14):Added a check if negative pads are present when `pads` explicitly provided. Since this is not supported, it will result in shape inference error.
hariharans29(2019-05-17 01:26:35):Thanks @stevenlix. Actually I had a similar concern when I first looked at the ORT shape computation logic. I feel this computation is more complex than it should be. If you substitute `legacy_target_size` from line 108 in the formula for `total_pad` in line 109, and after simplification, it is simply - `kernel_shape[i] - 1` (and this makes sense to me). Pads computation need not involve strides at all as they are not related. Pads are to be computed only so that the filter is able to be fully accommodated for each pixel. So, I have made a change now. Please correct me if I am wrong. 
hariharans29(2019-05-17 01:34:36):@linkerzhang - Please comment on this line - 

`int64_t total_pad = kernel_shape[i] - 1;`


BowenBao(2019-05-17 03:30:49):@hariharans29 I think this will result in different pad number, which changes the kernel center position and leads to different result. 
hariharans29(2019-05-17 04:49:31):@BowenBao - Thank you! But thinking out loud naively here, let's forget about the corner case that @stevenlix  pointed out for this discussion. The current pad computation logic used by ORT is here - 
https://github.com/microsoft/onnxruntime/blob/a7039601c4bf469131569d0c1261f13cd6bd67d9/onnxruntime/core/providers/cpu/nn/conv_base.h#L39

This was the previous logic in this same PR - 
`int64_t legacy_target_size = (dim_value + strides[i] - 1) / strides[i];
int64_t total_pad = (legacy_target_size - 1) * strides[i] + kernel_shape[i] - dim_value;`

Please substitute `legacy_target_size` into `total_pad` and simplify by doing some simple math, the result will be (if my math is correct) `kernel_shape[i] - 1` (which is the new logic I proposed). [Here](https://stackoverflow.com/questions/37674306/what-is-the-difference-between-same-and-valid-padding-in-tf-nn-max-pool-of-t) is another detailed discussion about this and it aligns with the ` kernel_shape[i] - 1`

As long as the total pads computed is the same, it should not affect kernel center position.

hariharans29(2019-05-17 18:24:28):@BowenBao @stevenlix - Any concerns with just relaxing the shape failure ?
stevenlix(2019-05-17 18:39:57):I can't edit the code in your fork, so I created a new PR here https://github.com/onnx/onnx/pull/2028 to fix the bug. Now resnet50 passed with the fix. 
hariharans29(2019-05-17 18:48:49):@stevenlix - sounds good, thanks. Closing this.
linkerzhang(2019-05-17 22:20:34):can you also add a test case to cover this bug case please? Thank you!
hariharans29(2019-05-17 18:52:42):LGTM.

Just 1 naive question - do we really need this mutation ? Even if total pads is negative, in reality, this means just ignoring a few values in the raw data ("unpad" a few values) right ? Also, I think it should not affect the output dim value when we use the formula - floor((in_dim - kernel_shape + total_pad) / s) + 1. Does this seem reasonable ?
stevenlix(2019-05-17 19:03:11):I checked the corner cases and I think probably there is no impact if you leave total_pad as negative value, but for simplicity and clarity, it's better to keep it as non-negative value.
stevenlix(2019-05-17 20:58:36):@linkerzhang Please review this PR
CLAassistant(2019-05-17 20:08:49):[![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/onnx/onnx?pullRequest=2029) <br/>Thank you for your submission! We really appreciate it. Like many open source projects, we ask that you sign our [Contributor License Agreement](https://cla-assistant.io/onnx/onnx?pullRequest=2029) before we can accept your contribution.<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/onnx/onnx?pullRequest=2029) it.</sub>
markbeards(2019-05-17 20:09:20):Accidental PR
liqunfu(2019-05-22 16:53:36):Thanks for this PR.
@linkerzhang @houseroad, this op is also needed to convert pytorch_translate models to ONNX. 
jeffsaremi(2019-06-12 16:33:36):Could someone review/approve/merge this PR? thanks
jeffsaremi(2019-06-13 22:25:50):@ebarsoum , @wschin , @liqunfu 
If there are no more comments could you please provide your approval? thanks
jeffsaremi(2019-07-01 16:14:07):This is ready to be merged
jeffsaremi(2019-07-01 16:42:03):@gramalingam thanks for catching these errors.
I am also pasting here the TF lines to reproduce the correct values:
```

>>> import tensorflow as tf
>>> session = tf.Session()
>>> x = tf.constant([1,2,3])
>>> y = tf.cumsum(x, axis=0)
>>> session.run(y)
array([1, 3, 6])
>>> y = tf.cumsum(x, axis=0, exclusive=True)
>>> session.run(y)
array([0, 1, 3])
>>> y = tf.cumsum(x, axis=0, exclusive=False, reverse=True)
>>> session.run(y)
array([6, 5, 3])
>>> y = tf.cumsum(x, axis=0, exclusive=True, reverse=True)
>>> session.run(y)
array([5, 3, 0])
>>>
```
jeffsaremi(2019-07-01 17:52:49):updated with review feedback
jeffsaremi(2019-07-02 15:05:55):could someone merge this please before it gets stale again? thanks
jcwchen(2021-04-15 22:00:26):Hi @jeffsaremi,
May I ask you do you have any use case for dynamic axis in Cumsum? People is discussing about moving axis from input to attribute:https://github.com/onnx/onnx/issues/3420 It would be great if we can understand your thought about this. Thank you!
ebarsoum(2019-05-22 22:47:13):Why the input is restricted to 1D tensor?
ebarsoum(2019-05-22 22:49:54):Same why 1D? You have axes attribute.
ebarsoum(2019-05-22 22:50:53):How cumsum will work with string (concat) or bool?
wschin(2019-05-23 05:35:02):Would you consider first support only int64 and float version? Other types might be rarely used.
jeffsaremi(2019-05-23 15:29:38):I think I needed float/double immediately.
What if I limit the types to: int32, int64, float and double?
jeffsaremi(2019-05-23 15:30:47):@ebarsoum Thanks for reviewing this! I must have used copy and paste. I will correct this asap.
There should be no limitation on the rank of the input
jeffsaremi(2019-05-23 15:32:06):@ebarsoum Thanks for catching this. I will correct the types. They should only be numeric. No strings. No bools.
Also see @wschin 's comments below on further limiting the types for the initial release
jeffsaremi(2019-05-23 16:07:21):copy and paste! I will correct it soon
jeffsaremi(2019-05-23 16:10:37):mistake. already corrected
jeffsaremi(2019-05-23 16:11:55):i changed it to have (u)int32, (u)int64, float and double only
wschin(2019-05-23 22:53:52):Sounds good.
liqunfu(2019-06-12 18:15:02):need to support int64 too to be compatible with other ops.
jeffsaremi(2019-06-12 18:49:44):@liqunfu Are you talking about axis? Are you saying that we should have axis_int32 and axis_int64?
Could you give me an example of an OP that does this properly? thanks
jeffsaremi(2019-06-12 20:30:58):Ok i think what you're referring to. I found Slice to be a good example:
https://github.com/onnx/onnx/blob/master/docs/Operators.md#Slice
I'll change this spec to match that
jeffsaremi(2019-06-12 20:54:05):I did modify the spec. Please take a look
linkerzhang(2019-06-12 21:08:04):I think having the type changed from “tensor(int32)” to "tensor(int64)" is good enough. don't have to support both. make sense?
gramalingam(2019-07-01 16:27:14):Is this supposed to be [6, 5, 3] ?
gramalingam(2019-07-01 16:27:44):Is this supposed to be [5, 3, 0]?
gramalingam(2019-07-01 16:30:20):How about "If set to 1, the j-th output element is the sum of the first (j-1) elements. Otherwise, it is the sum of the first j elements."?
jeffsaremi(2019-07-01 16:43:24):I'll add your line as well if that makes it more clear
gramalingam(2019-07-01 16:45:12):Yes, that would be helpful (IMO).
gramalingam(2019-07-08 20:36:36):It looks like a "reshape" might be missing here. Where is the shape of x specified?
jeffsaremi(2019-07-08 20:39:48):unit tests are all  one dimensional. 


jeffsaremi(2019-07-08 20:40:39):opps sorry. I just noticed the 2d in the title of the test
Yes definitely missing
jeffsaremi(2019-07-08 21:04:57):I'm going to create an issue and then submit a fix
CLAassistant(2019-07-24 00:56:50):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=2031) <br/>All committers have signed the CLA.
spandantiwari(2019-08-28 18:24:37):Overall a good PR. Thanks for updating the optimizer as well. 
Couple of minor points in line with best practices for op specifications:
1) For posterity, if there's an op in any of the popular frameworks that has similar semantics or which this change is intended to support, could you capture that in the description?
2) We are trying to capture a reference implementation for the ops in the tests. For this we have a np.pad which is close in semantics. For clarity, could you do a simple refactor to capture the implementation in a separate method (say `pad_reference_impl`), that uses np.pad, to be quite clear and readable. 
hariharans29(2019-08-28 19:31:06):> Overall a good PR. Thanks for updating the optimizer as well.
> Couple of minor points in line with best practices for op specifications:
> 
> 1. For posterity, if there's an op in any of the popular frameworks that has similar semantics or which this change is intended to support, could you capture that in the description?
> 2. We are trying to capture a reference implementation for the ops in the tests. For this we have a np.pad which is close in semantics. For clarity, could you do a simple refactor to capture the implementation in a separate method (say `pad_reference_impl`), that uses np.pad, to be quite clear and readable.

Thanks Spandan. Will address your comments soon!
hariharans29(2019-09-11 01:49:58):> > Overall a good PR. Thanks for updating the optimizer as well.
> > Couple of minor points in line with best practices for op specifications:
> > 
> > 1. For posterity, if there's an op in any of the popular frameworks that has similar semantics or which this change is intended to support, could you capture that in the description?
> > 2. We are trying to capture a reference implementation for the ops in the tests. For this we have a np.pad which is close in semantics. For clarity, could you do a simple refactor to capture the implementation in a separate method (say `pad_reference_impl`), that uses np.pad, to be quite clear and readable.
> 
> Thanks Spandan. Will address your comments soon!

Addressed the feedback. Thanks.
wschin(2019-09-11 22:30:46):Please also sync with master. There is a minor conflict. You can just replace your lines with those from master.
hariharans29(2019-09-11 22:40:47):> Just realized that we still have an open item about the allowed tensor shapes for the `pads` input. Could you please address that as well before merging.

Hi Spandan - Sorry, I don't understand this. Should we limit pads input shapes to something specific ?
hariharans29(2019-09-11 22:42:25):> Overall, looks good. Added a few minor comments. Two things to consider:
> 
> 1. Pursuant to previous feedback - if there are any equivalent ops in popular frameworks, consider adding them in the description of the spec for posterity.
> 2. I see that there's limited datatype support (float16, float32, double) for input/output for this op. Padding could be needed for tensors of other numeric types as well. Since we are updating this op, consider extending the datatype to other numeric types as well (ints).

1. The only op that i can think of is the tf.Pad op as this is basically to support a Keras (tf backed) model into ONNX. I have included that in the PR description already. 
2. I will expand type support to all int types.
Exlsunshine(2019-11-21 08:11:11):Hi @hariharans29, glad to see your change has been merged into master, but I still got the following error:
```
OP=Pad
Name=some/path/to/op/Pad
Inputs:
        some/path/to/op/concat:0=Concat, [-1, -1], 1
        some/path/to/op/Pad/paddings_Concat__113:0=Concat, [2, 2], 6
Outpus:
        some/path/to/op/Pad:0=[-1, -1], 1
Traceback (most recent call last):
  File "D:\Python\Python36\lib\site-packages\tf2onnx\tfonnx.py", line 352, in tensorflow_onnx_mapping
    func(g, node, **kwargs)
  File "D:\Python\Python36\lib\site-packages\tf2onnx\onnx_opset\nn.py", line 401, in version_1
    paddings = np.array(node.inputs[1].get_tensor_value()).transpose().flatten()
  File "D:\Python\Python36\lib\site-packages\tf2onnx\graph.py", line 256, in get_tensor_value
    raise ValueError("get tensor value: {} must be Const".format(self.name))
ValueError: get tensor value: some/path/to/op/Pad/paddings_Concat__113 must be Const
2019-11-21 15:52:39,788 - VERBOSE - tf2onnx.tfonnx: Mapping TF node to ONNX node(s)
2019-11-21 15:52:40,098 - ERROR - tf2onnx.tfonnx: Failed to convert node some/path/to/op/Fill


OP=ConstantOfShape
Name=some/path/to/op/Fill
Inputs:
        some/path/to/op/Fill__851:0=Cast, [1], 7
        some/path/to/op/strided_slice_21__842:0=Cast, [], 6
Outpus:
        some/path/to/op/Fill:0=[-1], 6
Traceback (most recent call last):
  File "D:\Python\Python36\lib\site-packages\tf2onnx\tfonnx.py", line 352, in tensorflow_onnx_mapping
    func(g, node, **kwargs)
  File "D:\Python\Python36\lib\site-packages\tf2onnx\onnx_opset\generator.py", line 100, in version_9
    value = np.array([node.inputs[1].get_tensor_value()]).astype(utils.map_onnx_to_numpy_type(dtype))
  File "D:\Python\Python36\lib\site-packages\tf2onnx\graph.py", line 256, in get_tensor_value
    raise ValueError("get tensor value: {} must be Const".format(self.name))
ValueError: get tensor value: some/path/to/op/strided_slice_21__842 must be Const
```

I am using:
```
python -m tf2onnx.convert --input a.pb --inputs in_0:0,in_1:0 --outputs o_0:0,o_1:0 --output b.onnx --opset 11 --fold_const  --continue_on_error --verbose

2019-11-21 15:52:31,785 - INFO - tf2onnx.tfonnx: Using tensorflow=1.13.1, onnx=1.6.0, tf2onnx=1.5.3/7b598d
2019-11-21 15:52:31,785 - INFO - tf2onnx.tfonnx: Using opset <onnx, 11>

```

Do you have any advice about this error? Thanks!
spandantiwari(2019-08-28 18:31:51):I am not sure if we should add support for 2D tensor [1, 2*input_rank] for the pads input. It seems a bit of an unnecessary add-on in the design. Just 1-D tensor makes more sense. If some other op's output feeds into this input, the exporter can adapt that output by adding an `unsqueeze` op. On the other hand if we allow 2D then there may be a possibility for 3D and so on. I think we should consider keeping this streamlined.
spandantiwari(2019-09-11 18:42:12):nit: Consider using static_cast in place of old c-style cast. 
spandantiwari(2019-09-11 18:42:32):Same here. 
hariharans29(2019-09-11 22:41:07):Sure. I will make it only 1D. Thanks.
hariharans29(2019-09-13 01:37:05):Fixed.
hariharans29(2019-09-13 01:37:15):Made it 1D only
hariharans29(2019-09-13 01:37:22):Fixed.
spandantiwari(2019-09-13 16:59:21):Great. Thanks.
wschin(2019-09-13 18:19:19):`pad_impl` is a reference implementation of ONNX `Pad`. It should accept ONNX `pads` directly. We need to modify `pad_impl` to consume `pads` in ONNX's format.
wschin(2019-09-13 18:22:43):I feel `int32` is big enough for `pads`.
wschin(2019-09-13 18:27:22):Why do you need this function? Is it for processing int64 `pads`? Where is the old code that runs before?
wschin(2019-09-13 18:32:51):I guess this `Pad` won't be removed. If yes, could you rename this test to `not_eliminate_pad`?
wschin(2019-09-13 18:34:04):Is it possible to keep the test for old Pad?
hariharans29(2019-09-13 18:43:38):I prefer to keep it `int64` to be consistent with other usage - 

1) Previously the attributes were `int64` type

2) Other very similar semantics use `int64`. For example, ConstantOfShape - the tensor type for the shape is `int64` (where `int32` would actually do)

3) There is not much value add in making it `int32` (not much space savings) when it is safer to keep it `int64`.
hariharans29(2019-09-13 18:48:23):There is no such code needed to run before. Previously, `pads` was an attribute and hence there were semantics already in place to parse the data within the attribute. Now `pads` is an input tensor and we need this code to parse the data within the struct now. Hence, this code is needed and used within the optimizer code. 
EDIT: We have a similar file tensor_proto_util.cc that helps parse data within a Tensorproto
hariharans29(2019-09-13 18:49:15):Good point. will do.
hariharans29(2019-09-13 18:49:39):Sure. Will do.
hariharans29(2019-09-13 18:50:55):I tried. But I am not sure we can create a node for an older opset. I notive that there is something within the `circleci` CI that covers the old optimizer logic because when I removed the old logic, it broke the CI and when the optimizer supported both versions - the CI passed....
hariharans29(2019-09-13 20:30:42):Fixed.
hariharans29(2019-09-13 20:33:11):Done. thanks for catching this.
hariharans29(2019-09-13 20:51:55):by the way - the line your comment is pointing to is the optimizer pass name - that cannot be modified. but the test name can be modified and I modified that.
wschin(2019-09-13 23:03:39):Can they be merged into one function?
wschin(2019-09-13 23:04:10):Make senses. Thanks.
wschin(2019-09-13 23:06:03):Ok. I think old optimization is implicitly tested somewhere. Could you please one an issue to discuss if we should add more tests for old optimizations? Then, we can close this comment.
hariharans29(2019-09-13 23:53:03):I feel this is maybe not a good idea to merge ALL the typed parsing related logic into a single function because -

1) It's really hard to merge them all into one function and honestly it spoils readability of the code as there will be plenty of type checking both while parsing raw data or the parsing the buffer corresponding to the type. This way the code is much cleaner and easily readable as it is templated. 

2) This template will not be instantiated a lot of times -atmost a few times for the types that we need it for. So will not bloat up size too much. (Code readability vs size trade-off)

3) I would like to keep it consistent with tensor_proto_util.cc which has similar cleaner templated implementation 
hariharans29(2019-09-13 23:53:19):sure, will do.
hariharans29(2019-09-14 00:49:56):Raised this issue here - https://github.com/onnx/onnx/issues/2313. Resolving comment as prescribed. Thanks!
ebarsoum(2019-09-15 02:34:09):Add example for other modes.
ebarsoum(2019-09-15 02:35:09):rename it to constant_value
hariharans29(2019-09-15 02:57:13):Makes sense. Will rename. Thanks. 

(Updates PR change description to reflect this)
hariharans29(2019-09-15 02:57:24):Will add
hariharans29(2019-09-16 19:31:33):Add example for 'reflect' and 'edge' modes. Resolved.
hariharans29(2019-09-16 19:31:46):Renamed to 'constant_value'. Resolved.
hariharans29(2019-09-16 23:13:08):Moved down and renamed as "..._opset10"
hariharans29(2019-09-16 23:16:41):I preserved the old Pad tests. Added some infrastructure code for it. Will resolve the issue after PR merge.
hariharans29(2019-09-17 00:05:38):Resolved. (FYI comment)
KsenijaS(2019-05-20 23:19:13):cc @houseroad @linkerzhang Can you please review this PR? thanks.
spandantiwari(2019-05-20 20:25:41):Minor: Maybe add a blank line to have consistent formatting as other test points.
linkerzhang(2019-05-21 22:32:34):@postrational 
linkerzhang(2019-05-23 03:44:43):the pytorch circleci failure should not be related to this PR. @spandantiwari @houseroad  
spandantiwari(2019-05-23 17:51:15):@linkerzhang @askhade Not sure why this test is failing. Will take a look. 
spandantiwari(2019-05-23 17:58:28):I think the test may have been fixed already. @houseroad could you please confirm?

houseroad(2019-05-23 20:16:06):@spandantiwari yes, it is.
askhade(2019-05-23 20:22:20):@linkerzhang , @houseroad : Can one of you merge this PR. Thanks!
houseroad(2019-05-23 20:23:55):Let's wait the CI a bit. I would like to see the result to make sure it's fixed :-)
askhade(2019-05-23 23:20:25):@spandantiwari , @houseroad circleci is failing again. Thanks
linkerzhang(2019-05-24 02:54:01):is this following the same broadcasting as https://github.com/onnx/onnx/blob/master/onnx/defs/shape_inference.h#L435 please?
hariharans29(2019-05-24 18:20:28):@linkerzhang - indeed it is. Thanks for pointing me to these helper functions. Resolved.
hariharans29(2019-06-13 23:15:12):CC: @pranavsharma @jeffsaremi @jiafatom @faxu - Closing this PR as per offline discussion since there is no immediate business requirement. This PR can be re-opened and iterated upon later if need be.
liqunfu(2019-06-19 17:57:09):@hariharans29, there is a pytorch-translate model that need unique op with n-D tensors. Please re-open this PR and make it support n-D cases. Thanks.
hariharans29(2019-06-19 18:49:31):Hi @liqunfu  - Please feel free to iterate on this if you have immediate need. 
liqunfu(2019-06-12 18:43:28):Please support for general n-D tensors as detailed here:
https://pytorch.org/docs/stable/torch.html?highlight=unique#torch.unique

daquexian(2019-05-23 03:38:15):CircleCI fails due to an unrelated error. Retriggering it by force pushing.

> ________________ ERROR collecting test/onnx/test_onnx_opset.py _________________
> ImportError while importing test module '/tmp/pytorch/test/onnx/test_onnx_opset.py'.
> Hint: make sure your test modules/packages have valid Python names.
> Traceback:
> test/onnx/test_onnx_opset.py:11: in <module>
>     from torch.onnx.symbolic_helper import _export_onnx_opset_version
> E   ImportError: No module named symbolic_helper

Update: It looks like CircleCI [fails](https://circleci.com/gh/onnx/onnx) on all builds.
linkerzhang(2019-05-24 17:03:56):@postrational
daquexian(2019-05-28 17:51:28):@houseroad Thanks. BTW, could you validate No.4 of #2049 on your side? It is strange but I have encountered it on my two PCs. Due to it, I cannot run get_doc.py without a hacky workaround.
ebarsoum(2019-05-23 21:24:27):nip: missing end bracket.
ebarsoum(2019-05-23 21:27:19):Why no float16?
ebarsoum(2019-05-23 21:29:12):Add tensor(float16)
jeffsaremi(2019-05-23 21:31:46):sorry. thanks for catching this
jeffsaremi(2019-05-23 21:31:56):will add asap
jeffsaremi(2019-05-23 21:32:06):will do
houseroad(2019-06-05 06:54:27):Nit: specify that the shape and type of the output is the same as input.
jeffsaremi(2019-06-05 15:04:16):I will
bddppq(2019-05-23 23:31:53):https://github.com/pytorch/pytorch/pull/20890
hariharans29(2019-05-24 18:50:47):@linkerzhang and @askhade - could you please review this and give your feedback ? 
gramalingam(2019-05-24 22:42:17):Hi: the logic here is that broadcasting happens only if one of the dims is 1 and the other dim is > 1. So, if you see any of the input-dims has a value > 1, we can be sure that the corresponding dim will have that value (otherwise the broadcast will fail with a runtime error). If the input-dim has a value 1, then the corresponding output-dim may be > 1 due to broadcasting. So, I believe that the existing logic for shape-inference is correct.
hariharans29(2019-05-24 22:46:07):> Hi: the logic here is that broadcasting happens only if one of the dims is 1 and the other dim is > 1. So, if you see any of the input-dims has a value > 1, we can be sure that the corresponding dim will have that value (otherwise the broadcast will fail with a runtime error). If the input-dim has a value 1, then the corresponding output-dim may be > 1 due to broadcasting. So, I believe that the existing logic for shape-inference is correct.

Hi @gramalingam - You are right. Sorry - I didn't mean to say that the broadcasting logic was incorrect. What I meant was there seems to be some arbitrary decision making when the final dim value is assigned to the output shape.

In the existing logic, when computed dim value is greater than 1 (after it passes through the afore-mentioned logic), even if it sees symbolic dims in other inputs, it seems to assign the seen dim value. My argument is that the dim value should not be assigned - rather it should just stop with rank inference.  

Let's take an illustrative example with 2 input shapes for the matmul case - 

(1, 4, 2) 
("a", 2, 5) 

results in

("a", 4, 5)  (by existing logic) 
(None, 4, 5) (my argument)

(2, 4, 2) 
("a", 2, 5) 

results in

(2, 4, 5)  (by existing logic) - notice the change in dim_0
(None, 4, 5) (my argument)

One more example (for the sum operator with 3 inputs)

(2, 1, 1) 
("a", 1, 1)
("b", 1, 1)
 
(2, 1, 1)  (by existing logic)
(None, 1, 1) (my argument)
gramalingam(2019-05-28 02:25:21):Hi @hariharans29 : the idea is to return the most precise information possible (which, in turn, may enable more optimizations). A numeric dimension (e.g., 2) is more precise than a symbolic dimension (e.g., "a"), which is more precise than None.  In the first case, both "a" and None are correct (but 1 would be wrong), and we return "a" as it is more precise. In the second case, 2 and None are both correct, but 2 is the most precise and we return that. Same with the third case. 
hariharans29(2019-05-28 18:57:38):> Hi @hariharans29 : the idea is to return the most precise information possible (which, in turn, may enable more optimizations). A numeric dimension (e.g., 2) is more precise than a symbolic dimension (e.g., "a"), which is more precise than None. In the first case, both "a" and None are correct (but 1 would be wrong), and we return "a" as it is more precise. In the second case, 2 and None are both correct, but 2 is the most precise and we return that. Same with the third case.

Thanks for the detailed explanation @gramalingam. That makes sense. Closing the PR.  
hariharans29(2019-05-24 18:49:47):the output "a" seems arbitrary - the input dims have 1 and "a" - so ideally, we need to stop at incrementing rank and not setting any value (actual value or symbolic). If the input had been 2 instead of 1, by the existing logic, the output dim value would have been 2 and this seems purely arbitrary without any logical basis. 
daquexian(2019-05-27 01:57:09):About the existing test:

The output of the existing test is following the behavior of tensorflow, which has been considered as a bug (https://github.com/tensorflow/tensorflow/issues/6720, https://github.com/chainer/onnx-chainer/issues/147), this legacy behavior is supported by coordinate_transformation_mode=asymmetric in this proposal
daquexian(2019-05-27 07:55:55):I have updated the tests :)
linkerzhang(2019-05-27 21:45:33):@postrational 
linkerzhang(2019-05-30 00:20:11):please also fix the ci failure due to flake8 errors.

like, 
+flake8
./onnx/backend/test/case/node/resize.py:176:14: E201 whitespace after '['
./onnx/backend/test/case/node/resize.py:176:17: E241 multiple spaces after ','
./onnx/backend/test/case/node/resize.py:176:21: E241 multiple spaces after ','
.....
daquexian(2019-05-30 15:43:34):@linkerzhang I have upgraded the op version and fixed flake8 errors.

However, the docs of Resize op ver10 instead of ver11 are generated, while the examples are from ver11. Am I missing something? :)
daquexian(2019-05-31 04:26:16):I have figured out that I should add the upgraded operator in `onnx/defs/operator_sets.h`. Now the docs look right.
daquexian(2019-06-09 14:01:46):I added a new input 'sizes' as requested in https://github.com/onnx/onnx/issues/2062. 

Furthermore, I added two new attrs 'cubic_coeff_a' and 'exclude_outside' for the compatibility with TensorFlow. TensorFlow sets [a=-0.5](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/resize_bicubic_op.cc#L60) (In its legacy version, a=-0.75) and [exclude_outside=True](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/resize_bicubic_op.cc#L95), while PyTorch (and OpenCV) sets a=-0.75 and exclude_outside=False
daquexian(2019-06-10 03:41:42):@linkerzhang @wschin I have added the implementation in the tests. It is ready to review. I'll implement the shape inference after https://github.com/onnx/onnx/pull/2085 is merged.
lara-hdr(2019-07-03 17:54:32):Is this ready? We are also waiting for this PR, @ebarsoum @houseroad
daquexian(2019-07-04 02:27:20):@lara-hdr I'll update the shape inference soon, and then this PR will be ready to review :)
daquexian(2019-07-05 02:45:22):@ebarsoum @linkerzhang @houseroad @wschin I have updated shape inference. This PR is ready to review :)

circleci failed due to an unrelated error
> ERROR: Can not perform a '--user' install. User site-packages are not visible in this virtualenv.
linkerzhang(2019-07-11 16:21:07):@daquexian 

Operator SIG is suggesting to have this resize op extended to also support the "resize" semantics in PR: #2142 , so that the cropandresize op will be able to be defined as a "Function", which is cleaner.

Make sense to you?
daquexian(2019-07-12 02:38:07):@linkerzhang I have checked the proposed RoiCropAndResize op. I found that it is actually unable to compose it by Slice and Resize ops. The coordinates of rois can be non-integer, however, the Slice op can only generate an output tensor with integer height and width. Maybe we can combine RoiCrpAndResize op with Resize op by adding "crop" and "extrapolation_value" attribute?

@jiafatom @ebarsoum @BowenBao What's your opinion? :)
jiafatom(2019-07-12 04:52:44):> @linkerzhang I have checked the proposed RoiCropAndResize op. I found that it is actually unable to compose it by Slice and Resize ops. The coordinates of rois can be non-integer, however, the Slice op can only generate an output tensor with integer height and width. Maybe we can combine RoiCrpAndResize op with Resize op by adding "crop" and "extrapolation_value" attribute?
> 
> @jiafatom @ebarsoum @BowenBao What's your opinion? :)


What do you mean "combine RoiCrpAndResize op with Resize op by adding "crop" and "extrapolation_value" attribute?" 
RoiCropAndResize op has extrapolation_value attribute. Where do you want to add "crop" and "extrapolation_value"? To Resize op?

daquexian(2019-07-12 06:32:05):> > @linkerzhang I have checked the proposed RoiCropAndResize op. I found that it is actually unable to compose it by Slice and Resize ops. The coordinates of rois can be non-integer, however, the Slice op can only generate an output tensor with integer height and width. Maybe we can combine RoiCrpAndResize op with Resize op by adding "crop" and "extrapolation_value" attribute?
> > @jiafatom @ebarsoum @BowenBao What's your opinion? :)
> 
> What do you mean "combine RoiCrpAndResize op with Resize op by adding "crop" and "extrapolation_value" attribute?"
> RoiCropAndResize op has extrapolation_value attribute. Where do you want to add "crop" and "extrapolation_value"? To Resize op?

Yes, add crop and extrapolation_value attributes to resize op.
jiafatom(2019-07-12 15:00:09):Resize is a special case of RoiCropAndResize, where the roi for Resize is [0, 0, 1, 1] (normalized coordinates). On the other hand, the current RoiCropAndResize is used only for 2d case, there is no use case to extend it to n-d.

So I feel it not good to add crop and extrapolation_value on resize op, because these two are only used in RoiCropAndResize.
daquexian(2019-07-12 15:16:50):@jiafatom thanks for your reply. Do you have any ideas on how to define roicropandresize as a function? I think the non-integer rois is a problem (as said in https://github.com/onnx/onnx/pull/2057#issuecomment-510722131)
jiafatom(2019-07-12 17:47:50):To define roicropandresize as a function, we need enhance resize op to make it like a crop_and_resize for a single ROI. Then like you said, we need add one input as crop, the other attributes as extrapolation_value. This way we make resize op as a general roicropandresize. Actually I have concerns about this because there is no use case for n-d roicropandresize other than 2d, it is over-killing the problem.

Another thing: Why non-integer roi is a problem? The input roi range is from 0 to 1 because it is normalized. if it is [0, 0, 1, 1], then it is actually original resize. No matter it is an integer or not, we need find the corresponding pixel in the input tensor (which is also a floating number), and then apply specific modes such as bilinear, nearest (it will use the integer coordinates in the original input). 
daquexian(2019-07-15 02:37:15):> Why non-integer roi is a problem? The input roi range is from 0 to 1 because it is normalized. if it is [0, 0, 1, 1], then it is actually original resize. No matter it is an integer or not, we need find the corresponding pixel in the input tensor (which is also a floating number), and then apply specific modes such as bilinear, nearest (it will use the integer coordinates in the original input).

@jiafatom You are right :) What I said is about https://github.com/onnx/onnx/pull/2142#issuecomment-507898382, which defines cropandresize based on slice op and resize op. The output tensor of slice has integer size but the roi can have float size, so that we cannot use slice op to get the rois and we have to add a bulit-in crop input in resize op instead. I think we both have the same opinion on it so it is not a problem now.
daquexian(2019-07-15 02:56:18):> Actually I have concerns about this because there is no use case for n-d roicropandresize other than 2d, it is over-killing the problem.

@jiafatom I think it is necessary to add this "crop" input. There is no other way to handle the float roi coordinates. Please correct me if I am wrong.
jiafatom(2019-07-15 22:42:58):> @jiafatom I think it is necessary to add this "crop" input. There is no other way to handle the float roi coordinates. Please correct me if I am wrong.

I agree that we need add "crop" input to support roi coordinates :)

winnietsang(2019-07-18 01:14:28):@daquexian which Tensorflow operator will create the same result as your unit test. There are tf.image.resize_images and tf.compat.v2.image.resize.
tf.image.resize_images has align_corners as one of the args but tf.compat.v2.image.resize don't.
In your earlier comment you mention that you have try this on Tensorflow, may you please share with me which tf operator are you using? Thanks
daquexian(2019-07-18 06:50:32):> @daquexian which Tensorflow operator will create the same result as your unit test. There are tf.image.resize_images and tf.compat.v2.image.resize.
> tf.image.resize_images has align_corners as one of the args but tf.compat.v2.image.resize don't.
> In your earlier comment you mention that you have try this on Tensorflow, may you please share with me which tf operator are you using? Thanks

<del>I tested with tf.compat.v2.image.resize. I'm not familiar with tensorflow. It seems that the result of tf.image.resize_images and tf.compat.v2.image.resize are different.</del>

tf.compat.v2.image.resize is equivalent to tf.image.resize_images with half_pixel_centers=True. The attribute `half_pixel_centers` is introduced in tf v1.14, which fixes [the bug](https://github.com/tensorflow/tensorflow/issues/6720) mentioned in https://github.com/onnx/onnx/pull/2057#issuecomment-496053050 and https://github.com/chainer/onnx-chainer/issues/147. However, the default value of half_pixel_centers in tf v1 is still "False", so I'll add a corresponding attribute to ONNX resize op to support this case.
spandantiwari(2019-07-20 00:38:58):@daquexian - could you please confirm if all the feedback has been addressed. This is an important change and we  are waiting for this PR to merge in ONNX.
daquexian(2019-07-20 07:48:53):@spandantiwari I'll update this PR soon. Thanks :)
spandantiwari(2019-07-20 19:57:51):@daquexian - thanks a lot!
daquexian(2019-07-21 01:30:47):@winnietsang tf.v1 with half_pixel_centers=False is supported by setting the attr "scaler" to "tf_legacy"
daquexian(2019-07-21 07:14:22):@jiafatom I have updated this PR and added input "roi" and attr "extrapolation_value". I'll check whether the output is consistent with your PR tomorrow. 
CLAassistant(2019-07-22 06:52:25):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=2057) <br/>All committers have signed the CLA.
daquexian(2019-07-22 06:58:23):@jiafatom I have updated this PR. The output of my implementation is the same as yours. :)
daquexian(2019-07-22 07:08:29):@ebarsoum @linkerzhang @houseroad @wschin @jiafatom @BowenBao @spandantiwari This PR is ready to review :)
winnietsang(2019-07-23 00:05:17):@daquexian I'm in the process to verify all of your test cases using Tensorflow ops, I will update you shortly once I have some preliminary result.
daquexian(2019-07-23 00:33:28):> @daquexian I'm in the process to verify all of your test cases using Tensorflow ops, I will update you shortly once I have some preliminary result.

Thanks! Please set scaler="tf_legacy" for the case half_pixel_centers=align_corners=False, and set scaler="half_pixel", cubic_coeff_a=-0.5, exclude_outside=True for the case half_pixel_center=True
winnietsang(2019-07-24 00:44:10):> > @daquexian I'm in the process to verify all of your test cases using Tensorflow ops, I will update you shortly once I have some preliminary result.
> 
> Thanks! Please set scaler="tf_legacy" for the case half_pixel_centers=align_corners=False, and set scaler="half_pixel", cubic_coeff_a=-0.5, exclude_outside=True for the case half_pixel_center=True

@daquexian I try the 4 testcases for mode=nearest and here are my findings:

1. upsample_scales_nearest: able to get the same values as yours with 
tf.compat.v2.image.resize, 
tf.compat.v1.image.resize_nearest_neighbor(half_pixel_centers=False) and tf.compat.v1.image.resize_nearest_neighbor(half_pixel_centers=True)
 
2. downsample_scales_nearest: can only get the same values as yours with     tf.compat.v1.image.resize_nearest_neighbor(half_pixel_centers=False)
but got different values with 
tf.compat.v2.image.resize and    
tf.compat.v1.image.resize_nearest_neighbor(half_pixel_centers=True)

3. upsample_size_nearest: can only get the same values as yours with 
tf.compat.v1.image.resize_nearest_neighbor(half_pixel_centers=False)  
but got different values with 
tf.compat.v2.image.resize and    
tf.compat.v1.image.resize_nearest_neighbor(half_pixel_centers=True)

4. downsample_size_nearest: got different values with 
tf.compat.v2.image.resize, 
tf.compat.v1.image.resize_nearest_neighbor(half_pixel_centers=False) and tf.compat.v1.image.resize_nearest_neighbor(half_pixel_centers=True)

Based on above results, I think the implementations of half_pixel_centers in onnx and tensorflow are different. Can you possibly look into this?

I also changed the initial values for case 1, scale = [1.0, 1.0, 2.5, 3.0], and saw the results not match with the expected values in tf.compat.v2.image.resize or tf.compat.v1.image.resize_nearest_neighbor(half_pixel_centers=True). So this seems to again indicate the half_pixel_centers has significant impact to the expected results. 


daquexian(2019-07-24 03:25:21):> > > @daquexian I'm in the process to verify all of your test cases using Tensorflow ops, I will update you shortly once I have some preliminary result.
> > 
> > 
> > Thanks! Please set scaler="tf_legacy" for the case half_pixel_centers=align_corners=False, and set scaler="half_pixel", cubic_coeff_a=-0.5, exclude_outside=True for the case half_pixel_center=True
> 
> @daquexian I try the 4 testcases for mode=nearest and here are my findings:
> 
> 1. upsample_scales_nearest: able to get the same values as yours with
>    tf.compat.v2.image.resize,
>    tf.compat.v1.image.resize_nearest_neighbor(half_pixel_centers=False) and tf.compat.v1.image.resize_nearest_neighbor(half_pixel_centers=True)
> 2. downsample_scales_nearest: can only get the same values as yours with     tf.compat.v1.image.resize_nearest_neighbor(half_pixel_centers=False)
>    but got different values with
>    tf.compat.v2.image.resize and
>    tf.compat.v1.image.resize_nearest_neighbor(half_pixel_centers=True)
> 3. upsample_size_nearest: can only get the same values as yours with
>    tf.compat.v1.image.resize_nearest_neighbor(half_pixel_centers=False)
>    but got different values with
>    tf.compat.v2.image.resize and
>    tf.compat.v1.image.resize_nearest_neighbor(half_pixel_centers=True)
> 4. downsample_size_nearest: got different values with
>    tf.compat.v2.image.resize,
>    tf.compat.v1.image.resize_nearest_neighbor(half_pixel_centers=False) and tf.compat.v1.image.resize_nearest_neighbor(half_pixel_centers=True)
> 
> Based on above results, I think the implementations of half_pixel_centers in onnx and tensorflow are different. Can you possibly look into this?
> 
> I also changed the initial values for case 1, scale = [1.0, 1.0, 2.5, 3.0], and saw the results not match with the expected values in tf.compat.v2.image.resize or tf.compat.v1.image.resize_nearest_neighbor(half_pixel_centers=True). So this seems to again indicate the half_pixel_centers has significant impact to the expected results.

Thanks! I found there are some differences in the implementation of nearest resize. I'll add a new attribute for this case soon.
daquexian(2019-07-24 03:38:08):@winnietsang I have checked the result of this PR vs tensorflow on linear and cubic interpolation.

Here is the full code:

https://gist.github.com/daquexian/4bab98a7e706ad95616567433c1234b8

 The resize implementation of tf.v2 are the same with those of tf.v1 with half_pixel_centers=True (thus covered by 'half_pixel' scaler of this PR), please check out [tf.v1](https://github.com/tensorflow/tensorflow/blob/v1.14.0/tensorflow/python/ops/image_ops_impl.py#L1102) and [tf.v2](https://github.com/tensorflow/tensorflow/blob/v1.14.0/tensorflow/python/ops/image_ops_impl.py#L1186) to validate it.
winnietsang(2019-07-25 01:42:14):@daquexian I try to verify the testcase for linear and cubic today, I got a lot of cases that the expected values doesn't match with the values returned from Tensorflow. 
Then I switch to look at the test code you refer https://gist.github.com/daquexian/4bab98a7e706ad95616567433c1234b8 yesterday. I try to run downsample_scales_linear in that code by changing h=1, w=2 and resize='linear' and I got matching result from expected and tensorflow values. I also try to run upsample_sizes_cubic and downsample_sizes_cubic cases in your test code, both get matching values. 
Then I go back to the onnx unit test for resize in this PR, I notice that not all cubic testcase with scaler=half_pixel have the exclude_outside attribute set. And the ﻿coeffs value are different in the onnx unit test and your test code. May you please look into this and validate the unit test in this PR match with the test code you provide yesterday? Thanks
daquexian(2019-07-25 01:54:10):> @daquexian I try to verify the testcase for linear and cubic today, I got a lot of cases that the expected values doesn't match with the values returned from Tensorflow.
> Then I switch to look at the test code you refer https://gist.github.com/daquexian/4bab98a7e706ad95616567433c1234b8 yesterday. I try to run downsample_scales_linear in that code by changing h=1, w=2 and resize='linear' and I got matching result from expected and tensorflow values. I also try to run upsample_sizes_cubic and downsample_sizes_cubic cases in your test code, both get matching values.
> Then I go back to the onnx unit test for resize in this PR, I notice that not all cubic testcase with scaler=half_pixel have the exclude_outside attribute set. And the ﻿coeffs value are different in the onnx unit test and your test code. May you please look into this and validate the unit test in this PR match with the test code you provide yesterday? Thanks

Sorry for not making myself clear. 

There are many frameworks and they are different implementations on resize op. To be compatible with every framework, some attributes like "exclude_outside" are introduced. TensorFlow expects exclude_outside to be True while PyTorch expects it to be false (when scaler='half_pixel'). I have to cover all these cases in testcases. So only some of the testcases are for TensorFlow, while others are for other frameworks like PyTorch. As a result, when testing this PR against TensorFlow, not every testcase can be used directly, since only a part of testcases are for TensorFlow
daquexian(2019-07-25 10:13:28):@winnietsang I have updated this PR and also my gist https://gist.github.com/daquexian/4bab98a7e706ad95616567433c1234b8, now it is fully compatible with TensorFlow (and also PyTorch)
winnietsang(2019-07-27 01:19:59):> @winnietsang I have updated this PR and also my gist https://gist.github.com/daquexian/4bab98a7e706ad95616567433c1234b8, now it is fully compatible with TensorFlow (and also PyTorch)

@daquexian, thanks for the explanation of the unit test, and your test code is very helpful too.
while I ran your unit test, I notice there are some minor update are required.
In export_resize_tf_crop_and_resize_extrapolation_value(), extrapolation_value is not applied in your expected output.
export_resize_downsample_scales_linear(), align_corner is applied in your expected_output
daquexian(2019-07-29 03:08:37):@winnietsang Thanks for your careful check! I have updated them
daquexian(2019-07-30 11:26:52):@houseroad @linkerzhang @wschin Is this PR ok to merge? I think I have addressed all comments :)
daquexian(2019-08-02 03:50:24):@linkerzhang Thanks. It has been updated
daquexian(2019-08-07 07:08:38):@wschin thanks for your valuable comments. I have updated this pr accordingly

I will update soon to pass the ci
lara-hdr(2019-08-07 19:39:48):Thank you @daquexian for the updates.
The CI is failing with the error "Unrecognized attribute: scaler for operator Resize" when checking the models, since the attribute was renamed.
Could you please regenerate them to fix the issue?
daquexian(2019-08-08 08:48:31):@wschin @lara-hdr I have updated this PR :) Travis CI failed due to network error
linkerzhang(2019-05-27 21:46:52):please bump the op version. You may refer to https://github.com/onnx/onnx/blob/master/docs/Versioning.md#operator-versioning to check onnx versioning design.
wschin(2019-05-28 05:56:31):Where do these numbers come from? It's not straightforward to write code to implement this behavior. Would you consider adding more `math` details and a lightweight numpy implementation here?
daquexian(2019-05-30 14:45:16):@wschin 

These numbers come from pytorch and opencv. The result of opencv is the same as pytorch with align_corners=False.

I have implemented linear and cubic interpolation [here](https://gist.github.com/daquexian/18b3fda2c7d2a6ad236a0a5ded4fe845) :) Should I include this implementation in ONNX docs?
wschin(2019-05-31 16:44:29):Could you please have a Python version of your implementation (only depending on numpy) plus some documentations and compute the test outputs using that Python implementation? This operator is important and also very sophisticated, so a reference implementation is desired. Please take a look at #2066 for more reasons. A PR complying #2066 is #1959, which includes verification code, reference implementation, and math equations.
daquexian(2019-05-31 18:05:57):All right, thanks for your suggestion :) I'll do it soon.
BowenBao(2019-07-16 20:55:57):add "if input 'sizes' is not specified"
BowenBao(2019-07-16 20:57:21):> "The \"linear\" mode includes linear interpolation for 1D tensor and N-D linear interpolation for N-D tensor (For example, trilinear interpolation for 2D tensor). "

should it be "_bilinear_ interpolation for 2D tensor" ?
BowenBao(2019-07-16 20:57:36):same above, bicubic?
daquexian(2019-07-20 09:22:34):Thanks! I'll edit them soon.
daquexian(2019-07-21 07:23:51):Thanks! It is added.
daquexian(2019-07-21 07:24:06):Thanks! It is fixed.
daquexian(2019-07-21 07:24:27):Thanks! It is fixed.
jiafatom(2019-07-22 00:57:19):add roi here?
jiafatom(2019-07-22 00:58:32):add roi in all the test case?
jiafatom(2019-07-22 00:59:05):roi missing?
daquexian(2019-07-22 01:41:39):Thanks! I have not re-generate the doc yet.
daquexian(2019-07-22 01:46:04):Thanks! Will add.
daquexian(2019-07-22 06:54:18):Added. Thanks!
daquexian(2019-07-22 06:54:24):Added. Thanks!
daquexian(2019-07-22 06:54:39):Done. Thanks!
linkerzhang(2019-08-02 00:08:05):put "tensor(float)" here directly.
linkerzhang(2019-08-02 00:08:27):put "tensor(int64)" here directly.
daquexian(2019-08-02 03:49:57):Done :)
daquexian(2019-08-02 03:50:04):Done :)
wschin(2019-08-02 04:34:52):Do you mean `a` in the Equation (4) in the reference paper?
wschin(2019-08-02 04:42:26):I can't figure out how to implement this operator after reading this spec. Are you using this equation (copied from your reference paper)?
![image](https://user-images.githubusercontent.com/3524474/62344992-3eef8980-b4a5-11e9-8ac7-fff5cb24ca52.png)
The paper only provides coefficients up to 2-D. How do you compute the kernel's coefficient for N-D?
wschin(2019-08-02 04:49:00):What is `weight`? Without an equation like 
![image](https://user-images.githubusercontent.com/3524474/62345182-1320d380-b4a6-11e9-84f4-6ea5d0db0768.png)
, it's not easy to explain those attributes in a way that the reader can implement this operator.
wschin(2019-08-02 04:51:13):length_original as the length of the original tensor in the `specific dimension`? If it's for a specific dimension, would it be clearer to use length_original_0 for axis 0 and so on? I feel a bit ambigous if we use length_original, a single symbole, to denote all the original dimensions along all axes.
daquexian(2019-08-02 04:52:10):Yes :)
daquexian(2019-08-02 05:13:37):No, the paper is only to describe the parameter `a`. A common (but not most efficient) way to implement N-D (2-D is a special case where N=2) interpolation is performing 1-D interpolation recursively.

For N=2, assume we want to get the value at `(1.6, 2.8)` in resized 2-D HxW tensor, we firstly calculate H values at `(1.6)` by performing 1-D interpolation on H rows (a.k.a. H 1-D tensors) respectively, and then calculate the value at `(2.8)` of the generate line (a.k.a. 1-D tensor) consisting of H above values by one more 1-D interpolation.

Similarly, 3-D interpolation is 2-D interpolation on every layer (a.k.a. every 2-D tensor) plus 1-D interpolation on the generated line (a.k.a. 1-D tensor), and every N-D interpolation is always (N-1)-D interpolation plus 1-D interpolation.

My numpy implementation in this PR has implemented it.
wschin(2019-08-02 05:17:17):Could we break this long sentence into smaller ones?
wschin(2019-08-02 05:19:19):```suggestion
        .Attr("coordinate_transformation_mode",
```
maybe?
daquexian(2019-08-02 05:23:59):Given N-D interpolation is a combination of 1-D interpolation, symbols here are for 1-D interpolation. I'll try to make this paragraph more clear.
wschin(2019-08-02 05:24:13):```suggestion
            " be the same as the rank of input 'X'. Only one of 'scales' and 'sizes' can be specified. If 'size' is needed, the user can use an empty string as the name of 'scales' in this operator's input list.",
```
wschin(2019-08-02 05:24:36):```suggestion
            " rank of input 'X'. Only one of 'scales' and 'sizes' can be specified.",
```
daquexian(2019-08-02 05:36:46):Thanks! I will do it
wschin(2019-08-02 05:38:41):```suggestion
        Return the n nearest indexes to x among [0, limit), prefer the indexes smaller than x.
```
daquexian(2019-08-02 05:38:42):Looks much better than the old one. I'll update it.
wschin(2019-08-02 05:40:52):```suggestion
        :param n: the number of neighbors.
```
wschin(2019-08-02 05:41:08):```suggestion
        :param x: center index (in the unpadded coordinate system) of the found nearest elements.
```
wschin(2019-08-02 05:46:12):What if `x > len(padded)` or `x + n/2 > len(padded)`?
wschin(2019-08-02 05:53:27):Your code looks very inspiring. It basically decomposes Resize into several abstractive steps:

- roi selection
- coordinate transformation
- neighbor selection
- dot product (interpolation)

It's not a blocking comment but I am wondering if we can further decompose Resize in the future.

@ebarsoum, @gramalingam, @linkerzhang for visibility. 
wschin(2019-08-02 06:07:02):Sounds good. Let's be more specific,
```suggestion
            " (in PyTorch). Check out Equation (4) in https://ieeexplore.ieee.org/document/1163711 for the details.",
```
daquexian(2019-08-07 06:33:08):It will not happen. After `x += pad_width`, x is in [n/2, len(data) - 1 + n/2], while len(padded) = len(data) + n
daquexian(2019-08-07 07:07:39):I have added some description in 1c87d37. Is it enough? Thanks!
daquexian(2019-08-07 07:08:02):I have added some description in ff74643
wschin(2019-08-07 16:01:22):```suggestion
if coordinate_transformation_mode is "half_pixel", <br/>
```
and also for the following places.
wschin(2019-08-07 16:05:51):```suggestion
The coordinate of each dimension is transformed individually. Let's describe a case using axis x as an example. 
Denote x_resized as the coordinate of a specific dimension in the resized tensor, x_original as the coordinate of this dimension in the original tensor, length_original as the length of the original tensor in this dimension, length_resized as the length of the resized tensor in this dimension, roi_x = (startx, endx) of the this dimension in input "roi", scale = length_resized / length_original, <br/>
```
wschin(2019-08-07 16:10:55):```suggestion
x_original = length_resized > 1 ? roi_x[0] * (length_original - 1) + x_resized * (endx[1] - startx[0]) * (length_original - 1) / (length_resized - 1) : 0.5 * (endx[0] + startx[1]) * (length_original - 1).</dd>
```
wschin(2019-08-07 16:12:55):```suggestion
x_original = length_resized > 1 ? startx * (length_original - 1) + x_resized * (endx - startx) * (length_original - 1) / (length_resized - 1) : 0.5 * (endx + startx) * (length_original - 1).)DOC";
```
It'd be nicer to align with the `roi` input's document.
wschin(2019-08-07 16:14:39):```suggestion
Resize the input tensor. In general, it calculates every value in the output tensor as a weighted average of neighborhood (a.k.a. sampled locations) in the input tensor.
```
wschin(2019-08-07 16:19:18):I really like your detailed documentation. It truly standardize something.
wschin(2019-08-07 16:20:24):```suggestion
            "The \"linear\" mode includes linear interpolation for 1D tensor and N-linear interpolation for N-D tensor (for example, bilinear interpolation for 2D tensor). "
```
wschin(2019-08-07 16:21:16):```suggestion
            "The \"cubic\" mode includes cubic interpolation for 1D tensor and N-D cubic interpolation for N-D tensor (for example, bicubic interpolation for 2D tensor)."
```
wschin(2019-08-07 16:23:47):Looks good to me. Thank you.
wschin(2019-08-07 16:25:38):```suggestion
            "When scaler is \"tf_crop_and_resize\" and x_original is outside the range [0, length_original - 1], this value is used as the corresponding output value. Default is 0.0f. This attribute is valid only if \"exclude_outside\" is 0.",
```
Is my understanding correct? If so, could you please add a check in your shape inference code? Thank you.
wschin(2019-08-07 16:27:28):```suggestion
            "Four modes: round_prefer_floor (default), round_prefer_ceil, floor, ceil. Only used by nearest interpolation. It indicates how to get \"nearest\" pixel in input tensor from x_original, so this attribute is valid only if \"mode\" is \"nearest\".",
```
wschin(2019-08-07 16:28:41):```suggestion
            "When \"coordinate_transform_mode\" is \"tf_crop_and_resize\" and x_original is outside the range [0, length_original - 1], this value is used as the corresponding output value. Default is 0.0f.",
```
Could you please do a proof-reading?
daquexian(2019-08-08 02:47:07):Great! your help is precious!
daquexian(2019-08-08 02:52:24):"sampling locations" is from https://github.com/tensorflow/tensorflow/blob/3ae2c6691b7c6e0986d97b150c9283e5cc52c15f/tensorflow/core/kernels/resize_bicubic_op.cc#L98. Do you think "sampled locations" is more natural?
daquexian(2019-08-08 03:29:03):It is not so. "exclude_outside" is about the neighbors outside of the input tensor in the weighted average, "extrapolation_value" is about the "x_original".
daquexian(2019-08-08 03:30:33):Thanks! I have updated it.
daquexian(2019-08-08 05:13:58):Thanks! They are now replaced in 66d2003
daquexian(2019-08-08 05:14:32):Thanks! It is now updated in b067855
daquexian(2019-08-08 05:14:49):Updated in b067855, thanks!
CLAassistant(2019-05-28 20:10:53):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=2060) <br/>All committers have signed the CLA.
linkerzhang(2019-05-30 00:18:06):@JamesAllingham welcome and thank you for the contribution to ONNX. It's ok to remove some test cases as long as the scenarios/cases are also covered by new ones.

Please fix the CI failures:

+flake8
./onnx/backend/test/case/node/gemm.py:24:24: E231 missing whitespace after ','
./onnx/backend/test/case/node/gemm.py:72:29: E261 at least two spaces before inline comment
./onnx/backend/test/case/node/gemm.py:81:24: E231 missing whitespace after ','
./onnx/backend/test/case/node/gemm.py:87:29: E261 at least two spaces before inline comment
./onnx/backend/test/case/node/gemm.py:96:24: E231 missing whitespace after ','
./onnx/backend/test/case/node/gemm.py:111:24: E231 missing whitespace after ','
JamesAllingham(2019-05-30 07:29:17):@linkerzhang Thanks for the welcome. I'm very keen to be getting involved with this project. And thanks for the feedback. 

What my test cases don't cover (that the old ones did) is whether the `transA`, `transB`, `alpha` and `beta` parameters work as expected when used together. However, that seems like a bit of an edge case, and to properly test all interactions like that for every op would require a huge number of tests. That said, I suppose there is nothing wrong with having a few of these sorts of test cases. Let me know what you think. 
JamesAllingham(2019-06-17 14:32:15):Any feedback on this? Would be great to get this closed so that I can go ahead with some more PRs (would be nice to know I'm doing things correctly before doing more 😃)
JamesAllingham(2019-07-10 14:48:13):Hey @linkerzhang any feedback on this? I also have another [PR](https://github.com/onnx/onnx/pull/2127) which could use some love. I have quite a few more tests I'd like to submit PRs for but I really would like to get some feedback on what I'm doing before submitting! 
JamesAllingham(2019-09-02 13:00:27):@postrational I've updated the branch. Let me know if I need to do anything else to get this merged.
JamesAllingham(2019-09-17 08:27:11):> 
> 
> This PR looks very good. Could you please take a look at my minor comments and see if we can address them? Thank you! To me more clear, I think this PR is almost ready to be merged.

Cool, I think I've addressed all of your comments, please let me know if I misunderstood anything. Also, let me know what you think about allowing the bias input (`'C'`) to be optional with a default of 0.
JamesAllingham(2019-09-17 21:37:57):> 
> 
> Overall looks good and it improves the document considerablely. Just last few comments to address.

One unfortunate thing about the documentation is that the reference implementation is not shown in `Operators.md`, I don't suppose there is an easy fix for this?
wschin(2019-09-16 05:09:27):[nit] This is good enough for tests. On the other hand, it'd be nice if you can create a reference implementation of Gemm using numpy and use it in all tests. For example,
```
y = your_gemm_reference_impl(a, b, c, ...)
```
wschin(2019-09-16 05:11:53):No bias? Do you mean that `c` will be removed?
wschin(2019-09-16 05:17:22):`[1]` is not a scalar. It's a 1-element vector. Maybe we can use `np.array(1)` whose shape is `()`. As mentioned in my comment above, it'd even nicer if we have a reference implementation.
wschin(2019-09-16 05:19:20):May we keep this one? It represents a case where all attributes are non-default.
JamesAllingham(2019-09-17 07:37:55):What I meant was that the bias term has no impact on the result. I think this is a useful test case because in many frameworks the bias term of the fully-connected layer is optional with a default of 0. I'll rename the test to make this more clear. 

With that in mind, do you think it is worth changing the spec for `Gemm` so that the bias is optional and has a default of 0? I'm happy to do so, and I don't think it is too much work or too big a change. (Then I can add a test which legitimately has no bias term).
JamesAllingham(2019-09-17 07:51:06):Agreed and done 👍 
JamesAllingham(2019-09-17 07:51:41):You are totally correct. I've fixed this. And I am going to do a reference implementation.
JamesAllingham(2019-09-17 07:52:15):I agree, it will be good to have a reference implementation, I'll add one. 👍 
JamesAllingham(2019-09-17 08:24:41):(I renamed this test, it is now called 'test_gemm_all_attributes')
wschin(2019-09-17 20:23:40):It totally makes sense to me. Could we do it in another PR? I will try to get it reviewed (and then hopefully merged) before the deadline (9/19 is the targeted release date)? I remember that I had to insert zero initializers when converting cases without bais. The case with `C=0` is also a common case in practices, so we should have an one-to-one mapping in the IR.
wschin(2019-09-17 20:30:25):This is still not a scalar. 
```
>>> np.random.ranf(1).shape
(1,)
```
As you already have a reference implementation, could we do
```suggestion
c = 3.14 or c=np.array(3.14)
```
?
wschin(2019-09-17 20:35:24):Nice!
JamesAllingham(2019-09-17 20:45:07):Sure, I'll make another PR for it. I'm afraid it will have to be about 11 hours from now though!
JamesAllingham(2019-09-17 20:47:54):Whoops! Let me fix that!
JamesAllingham(2019-09-17 21:21:46):Okay, I think I've fixed this for real now, sorry about that :)
CLAassistant(2019-05-30 04:59:21):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=2069) <br/>All committers have signed the CLA.
linkerzhang(2019-06-21 18:51:09):The pytorch failure should not be related with this change. Will merge this PR end of day today if no concern. @houseroad @spandantiwari  Thanks!
hariharans29(2019-06-04 00:00:04):Actual change begins here
hariharans29(2019-06-04 00:00:19):Actual change ends here
gramalingam(2019-06-05 20:49:26):This may be a subjective/personal preference, but I feel the following lines could be replaced as below. I perfonally find this a bit easier to read and check:
```cpp
    TensorShapeProto::Dimension batch_size_dim, class_count_dim;
    class_count_dim->set_dim_value(class_count);
    if (hasNInputShapes(ctx, 1)) {
        const auto& input_shape = ctx.getInputType(0)->tensor_type().shape();
        const auto input_rank = input_shape.dim_size();
        if (input_rank == 1) {
          // if input_rank is 1, batch_size is interpreted to be 1
          batch_size_dim->set_dim_value(1);
        } else if (input_rank == 2) {
          batch_size_dim = input_shape.dim((int)0);
        } else {
          fail_shape_inference("Input's shape should be 1D or 2D");
        }
    }
    updateOutputShape(ctx, 0, {batch_size_dim});
    updateOutputShape(ctx, 1, {batch_size_dim, class_count});
}
```
hariharans29(2019-06-05 22:09:14):Hi @gramalingam - that does look better. Thanks for your feedback. I will incorporate it.
hariharans29(2019-06-06 03:04:19):is this logic to determine whether to run the test or not okay with you @linkerzhang @houseroad @gramalingam ?

(We need to skip this test for ONNX_ML=0 builds as this test is for a traditional ML op)
houseroad(2019-06-06 04:20:50):A better way to check it is creating a helper method using the #ifdef ONNX_ML to generate the body of the helper function, which returns whether it's build with ONNX_ML or not.
hariharans29(2019-06-06 04:47:39):Can you please point me to which file you think this helper can go into ? A similar example method will be useful...
hariharans29(2019-06-07 00:50:31):For now, I prefer to keep this logic simple and as it is. This is not too dissimilar to the logic in `gen_doc.py` where the environment variable is queried to determine which file to generate he documentation for. If this is okay for now, can I merge this PR ? @linkerzhang @houseroad 
linkerzhang(2019-06-20 22:26:15):this is weird format. did you run "clang-format ..."
hariharans29(2019-06-20 22:30:18):Yes, I did. That's why we see a lot of formatting changes in this file. 
CLAassistant(2019-06-05 12:17:51):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=2080) <br/>All committers have signed the CLA.
wschin(2019-06-05 15:56:57):Maybe we can check if `lambda >= 0` in its shape inference function.
linkerzhang(2019-06-05 21:39:39):Thank you! This is a nice clarification!

Please don't edit the Operators.md directly. This file is auto-generated. See, https://github.com/onnx/onnx/blob/master/docs/CONTRIBUTING.md#generated-operator-documentation.

To update the doc, the related defs.cc should be changed and doc-generation should be run to refresh the docs.

Here's a doc on how to add a new op, https://github.com/onnx/onnx/blob/master/docs/AddNewOp.md, but it also tells how to update the doc, I think.
tomdol(2019-06-06 13:50:41):Thanks, I will revert this commit and add the changes it in the proper location.
wschin(2019-08-20 20:53:00):@tomdol, could you please sync this PR with the master branch? 
tomdol(2019-08-21 06:55:58):@wschin since I introduced changes in a totally wrong location, I think I would rather close this PR and open a new one when I have changes ready. Unfortunately I didn't have time to take care of it before.

Would this be ok?
wschin(2019-08-21 16:12:06):It's ok. You can do it in this branch and then do `git push -f`. Totally up to you.
tomdol(2019-08-21 16:25:37):Closing for now. Will get back to this later and implement it properly.
hariharans29(2019-06-06 23:33:29):CC: @raymondxyang who was also working on this

@lara-hdr - Thanks for this fix. Can you add corresponding shape inference tests to have this change covered please ? 


raymondxyang(2019-06-07 00:10:50):Yes I am fixing/unifying the shape inference for upsample/resize.. theres also another problem with the dim_value when handling symbolic shapes. will send a PR likely by today. Thanks @hariharans29 for the help!
hariharans29(2019-06-06 23:43:39):Hi @lara-hdr - can you please wait for @raymondxyang 's fix for this ? We have some other fixes like handling symbolic dims (this code doesn't have a check `has_dim_value()`) and while we are at it - we can fix `Upsample` as well which is very similar to this.... 
lara-hdr(2019-06-06 23:45:42):Sounds good. Thanks.
raymondxyang(2019-06-07 03:23:03):@lara-hdr @hariharans29 would you mind helping review it since you are also working on this part? Thanks!
hariharans29(2019-06-07 18:48:12):CC: @linkerzhang - we need this fix to unblock a few failing models.
take-cheeze(2019-06-11 07:39:28):Great PR!
I needed this workaround to make it infer: https://github.com/chainer/onnx-chainer/blob/d428f4def37e32ac4af4213c2fc69e72fa75f87d/onnx_chainer/export.py#L48-L52
hariharans29(2019-06-07 18:26:35):I think we need to only check for the first input shape. Even if the second input shape is missing, we can still proceed further and atleast trigger rank inference section. What do you think about this ?
hariharans29(2019-06-07 18:36:40):same comment as previously - I think first input shape is good enough to proceed further....
hariharans29(2019-06-07 18:41:47):If this is more or less just a copy of the above method - maybe we can just shift it to a util.h under`..defs\tensor\` and reference that method in all places. Any concerns with that approach ? 
hariharans29(2019-06-07 18:45:45):Since we support opset 7 too - I guess this fix needs to be applied to `Upsample (7)` as well. Since Upsample (7) uses attributes to store 'scales ' (scales is not an input), I would make the function handle attribute 'scales' as well and just reference that function in all 4 places - Upsample (7), Upsample (9), Upsample (10), Resize (10).

  
hariharans29(2019-06-07 18:46:53):I would add a few simple tests for Upsample (7), Upsample (9) as well.
raymondxyang(2019-06-07 20:55:52):Good point. Switch to do so
hariharans29(2019-06-10 21:30:41):nit: const auto&
hariharans29(2019-06-10 21:30:52):nit: const auto*
hariharans29(2019-06-10 21:31:08):same nits as above (optional)
linkerzhang(2019-06-11 17:59:53):is this one "upsampleShapeInferenceV7" also needed to be generalized and put in this common file? I'm seeing it's only used once. if it's not needed, let's not put it here,
jiafatom(2019-06-07 21:39:55):Looks good to me.
wschin(2019-06-07 23:50:07):At the first glance of seeing the proposed signature, the first thing comes to my mind is
```c
    vector<int> indexes;
    int start = 1;
    int end = 8;
    int step = 2;
    for (int i = start; i < end; i += step)
        indexes.push_back(i);
```
Is it something we can compose using `Loop`?
hariharans29(2019-06-08 00:23:26):> At the first glance of seeing the proposed signature, the first thing comes to my mind is
> 
> ```c
>     vector<int> indexes;
>     int start = 1;
>     int end = 8;
>     int step = 2;
>     for (int i = start; i < end; i += step)
>         indexes.push_back(i);
> ```
> 
> Is it something we can compose using `Loop`?

@jiafatom - do you think this can be constructed using Loop and will there be perf implications if so ?

jiafatom(2019-06-08 22:41:24):> At the first glance of seeing the proposed signature, the first thing comes to my mind is
> 
> ```c
>     vector<int> indexes;
>     int start = 1;
>     int end = 8;
>     int step = 2;
>     for (int i = start; i < end; i += step)
>         indexes.push_back(i);
> ```
> 
> Is it something we can compose using `Loop`?

@wschin It can be composed using `Loop`, but feels not convenient, because we need construct the body graph for `Loop` operator using multiple nodes and logic. This `Range` is already a contrib op in onnxruntime, it is straightforward picking it up in onnx.
jiafatom(2019-06-09 01:15:01):@wschin I also feel that if it is not easily readable if we use `Loop` to compose `Range`. If we just use `Range`, it is easy to understand its meaning. But if we use `Loop`, the actual function is encapsulated in the graph structure of the `Loop`, which is hard to understand if we look at the model graph.
wschin(2019-06-10 23:44:09):@jeffbloo, for both of convince and readability, I prefer to have `Function` (see [MVN](https://github.com/onnx/onnx/blob/master/docs/Operators.md#meanvariancenormalization
) as an example of ONNX function) instead of an new operator. This is very important to avoid a exponential grow of ONNX operator set. In an ONNX model, we can just use that `Function` as if we have a `Range` operator but runtime can decide if that `Function` will be executed as a sub-graph or an internal magic implementation of that `Function`.

@hariharans29, as runtimes can have their own magic implementations for each function, there should not have performance issue.
hariharans29(2019-06-24 21:27:54):CC: @linkerzhang @houseroad for feedback
liqunfu(2019-06-28 16:07:11):Thanks for this new op and for making it a function op. Range op is needed by PyTorch as well.
CLAassistant(2019-07-24 00:56:52):[![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/onnx/onnx?pullRequest=2087) <br/>Thank you for your submission, we really appreciate it. Like many open source projects, we ask that you all sign our [Contributor License Agreement](https://cla-assistant.io/onnx/onnx?pullRequest=2087) before we can accept your contribution.<br/>**19** out of **21** committers have signed the CLA.<br/><br/>:white_check_mark: hariharans29<br/>:white_check_mark: gramalingam<br/>:white_check_mark: wschin<br/>:white_check_mark: wutiantong<br/>:white_check_mark: prasanthpul<br/>:white_check_mark: neginraoof<br/>:white_check_mark: take-cheeze<br/>:white_check_mark: ebarsoum<br/>:white_check_mark: askhade<br/>:white_check_mark: linkerzhang<br/>:white_check_mark: bddppq<br/>:white_check_mark: edgchen1<br/>:white_check_mark: HectorSVC<br/>:white_check_mark: souptc<br/>:white_check_mark: lara-hdr<br/>:white_check_mark: skottmckay<br/>:white_check_mark: BowenBao<br/>:white_check_mark: daquexian<br/>:white_check_mark: liqunfu<br/>:x: LeicongLi<br/>:x: jeffsaremi<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/onnx/onnx?pullRequest=2087) it.</sub>
hariharans29(2019-08-02 18:34:52):@gramalingam - Just to confirm, I got notified about this comment but don't find it here - 

"I think that the "7" denotes the repeated ints type, and should be "2" here?"

I assume you deleted it, correct ? I think "7" does correspond to int64. So it should be right. But just confirming that this needs no action.
gramalingam(2019-08-02 18:38:50):@hariharans29 : yes, I deleted that comment. My mistake, I was looking at the wrong enumeration-type.
hariharans29(2019-08-02 18:44:25):> @hariharans29 : yes, I deleted that comment. My mistake, I was looking at the wrong enumeration-type.

Thanks for confirming.
hariharans29(2019-08-02 19:33:24):@linkerzhang @liqunfu @wschin @gramalingam @ebarsoum  - All comments should now be addressed. Can you guys please do a final check of the main op part (composed using a function) in the defs.cc and this should be ready for merge? Thanks a lot!
hariharans29(2019-08-15 20:38:46):Closed in favor of #2042 
hariharans29(2019-06-21 00:49:34):@wschin - any thoughts on this please ? 
hariharans29(2019-06-21 23:03:24):The only issue is "delta" is an optional input. But I don't know how to deal with optional inputs in the 'Functions' world. Any pointers @linkerzhang please ? 

It seems like there are only two options:

1)  Make Functions capable of handling optional inputs (I don't know if this functionality already exists)
2) Make the input `delta` non-optional. This is ugly but it will solve this issue.


wschin(2019-07-02 04:49:52):Scalar in ONNX has empty shape. This basically means that we support both of scalar and tensor with a single element (shape can be `[1]`, `[1, 1, 1, 1, 1]`, and so on. Is this semantic expected?
wschin(2019-07-02 04:51:35):We can make it non-optional.
wschin(2019-07-02 05:00:47):Not sure what do you mean. If you are talking about computing iteration count before the `Loop`, I feel your strategy is great. Pre-computing iteration count means CPU doesn't need to predict if a loop will terminate or continue.
wschin(2019-07-02 05:13:40):Is this `Identity` necessary? In the spec, `cond` is a loop-carried dependency so we can assign a new value to `cond` in each iteration. On the other hand, if there is no assignment, `cond`'s value should remain unchanged.
wschin(2019-07-02 05:23:59):If `Ceil` is used, could the `output` contain elements larger than `limit`?
wschin(2019-07-02 05:46:26):Is it `current` or `prev`?
gramalingam(2019-07-02 15:55:08):I recommend using the builder convenience functions to keep this compact: e.g., see: https://github.com/onnx/onnx/blob/806aa863020fa180e57f576cb032ec44ce8ddcca/onnx/defs/nn/defs.cc#L2157 or https://github.com/onnx/onnx/blob/b29e78a4efb8e5d8995f576bbf19a846807829b6/onnx/defs/function.h#L69 .
For example, something like:
```
    // current = Add(prev, delta)
    { {"current"}, "Add", {"prev", "delta"}} 
```
hariharans29(2019-07-02 18:20:43):Will do. Thanks for the suggestion.

EDIT: @gramalingam - I am quite unsure how to use the node builder to build a graph proto. As you can see, I use the node builder helper to compose the `Range` operator itself. The `Range` operator requires a `Loop` node which needs a `Graph` attribute and I am not quite sure how we can use the node builder helpers there.
hariharans29(2019-07-02 18:22:03):That looks a bit ugly as usually delta is 1 and thus would be cleaner to make it optional. If the way to support optional inputs in functions is cumbersome, then we can make it optional. Any thoughts ?
hariharans29(2019-07-02 18:26:35):I don't think so. 

The number of elements for range is computed as: 

Max(ceil((limit - start) / delta), 0)

like here:

https://github.com/microsoft/onnxruntime/blob/bf6a9f9c27e14cf334e6cf30e8a91c39f2456728/onnxruntime/contrib_ops/cpu/range.cc#L38
hariharans29(2019-07-02 18:27:34):I was only asking for your review :) Thanks!
hariharans29(2019-07-02 18:30:59):Not sure what you mean. 

In this `if` condition, I only check to make sure the initializers have a single elements (i.e.) scalar - I have in fact not checked the shape and I believe this loose check is fine.  
hariharans29(2019-07-02 18:36:16):If my understanding of the `Loop` operator is correct,I believe this has to be `current`. `current` is then mapped to the loop carried dependency `prev` below.  
hariharans29(2019-07-02 18:38:08):I believe this `Identity` is necessary. This `Idenetity` maps `cond` to `cond_out` which in turn becomes `cond` in the next iteration.

EDIT: I simplified the subgraph a bit in my commit - https://github.com/onnx/onnx/pull/2087/commits/596c025976b9fc69017e5a420dc3c125d8df2640. Maybe this is a better way to go ?  

EDIT 2:  My previous commit broke the model validator, so I made an update. Please let me know if it can be improved.
wschin(2019-07-31 15:52:30):I'd prefer strongly-typed setting. If one variable is a scalar, we should use the scalar definition in ONNX. Could you please throw if the shape is not an empty list?
wschin(2019-07-31 16:01:45):Please use int64, the largest allowed type in the spec below. Also, can we access the data during shape inference? I through this shape inference happens before we load the data.
wschin(2019-07-31 16:05:18):@gramalingam, do we need to connect `cond` and `cond_out`? I thought `cout_out` is just an alias of `cond` so `cond_out` automatically store the latest value of `cond`.
gramalingam(2019-07-31 17:17:20):Please use a name like loop_body_attribute (a valid C identifier). See: https://github.com/onnx/onnx/blob/master/docs/IR.md#names-within-a-graph … while this check is probably not done (because of some models that violate this), it will help us in the future when we start enforcing these requirements.
gramalingam(2019-07-31 17:26:14):The identity assignment is needed if they have different names. (I wish I could see the loop-body compactly ... our existing syntax for building this is so verbose I can't see it all in one place.) 
wschin(2019-07-31 17:27:08):For optional inputs, we use `empty string` as their names.
wschin(2019-07-31 17:32:33):I just some manual calculation. You're right. Thanks.
wschin(2019-07-31 17:51:08):You're probably right. My question comes from an statement in Loop's spec.

- Any variables which you wish to make available in the enclosing scope (i.e. the variables b and keepgoing) must be declared as either loop-carried dependencies (**_both_** at the op inputs and output and at the body net input and output) or scan_outputs.

, which makes me wonder if we can replace `current` with `prev` in your code below
```cpp
    auto* output_value_info_proto_1 = loop_sub_graph.add_output(); 
    output_value_info_proto_1->set_name("current");
```
. 

For loop-carried dependencies, there are two possible patterns. 

- First, Loop's input and output can have the same name (which somehow breaks the SSA of the entire graph)
- The second case is that we use different names for the same loop-carried variable. We implicitly uses the loop-carried outputs as the loop-carried inputs in the next iteration. Those parameters are matched by their positions.

I believe the second is more desirable and your implementation is that case.
wschin(2019-07-31 18:33:17):Aren't those names always different? If the same name can be used in both of input and output lists, SSA would be broken. It means we always need to link loop-carried dependencies via Identity.

Btw, I understand the need of Identity now. Thanks a lot.
hariharans29(2019-07-31 19:04:45):I did try that, but the function validator failed. I need to check if the validator has a bug.

Also, "delta" is needed inside the subgraph and so it does get trickier. Maybe we should just make it non-optional.
hariharans29(2019-07-31 19:30:00):Done
hariharans29(2019-07-31 19:32:07):Yes, we can access data at shape inference if it is available as an initializer. There is a check on line 678 to make sure the necessary data is available as an initializer before invoking this method.
hariharans29(2019-07-31 19:34:17):And I don't see why it should be made int64. The value that is being returned is of `int` type (this is a dim value which is int type). So after applying the formula - ceil(....), it needs to be casted back to 'int' as per requirement. This looks valid to me.
hariharans29(2019-07-31 19:38:13):Ok, done.
hariharans29(2019-08-01 01:14:00):Actually "current" has to remain as "current" because it will be fed in as "prev" in the next iteration. 
wschin(2019-08-01 15:41:33):Yes. Let's me add more details to make sure we are on the same page. The loop-carried dependencies are matched and assigned by position in the body graph's input and output lists. If we have iteration count `T`, loop-terminating condition `cond`, and loop-carried dependencies `[X, Y, Z]`, the input lists of the body graph would be `[T, cond, X, Y, Z]`, and its output list would be `[cond_new, X_new, Y_new, Z_new]`. The variable `cond_new`/`X_new`/`Y_new`/`Z_new` would be assigned to `cond`/`X`/`Y`/`Z` at the beginning of the next iteration.
wschin(2019-08-01 15:44:58):Nice comment.
wschin(2019-08-01 15:51:49):Now, I think we should use non-optional. Here is the reason.
The invoker of function needs to be aware of optional inputs of functions. If an optional input is empty, the runtime has to look into its spec and generate the right optional value. It means --- runtime will have an implementation for each function with some optional inputs, and therefore breaks the idea of having function.
wschin(2019-08-01 15:56:33):Can't `limit_data` be int64? I thought it can according to the allowed types of `limit_data`. If the dim type must be int32, please throw if overflow happens. You can do something like
```
int64_t n = ...
if (n > INT_MAX)
  throw ...
```
wschin(2019-08-01 15:58:35):```suggestion
            // Explicitly compute the output dimension if Range's inputs are stored in initializer list.
            if (start_initializer->data_type() == TensorProto::FLOAT) {
```
wschin(2019-08-01 16:00:23):```suggestion
            // If any of Range's inputs are not initializers, the output dimension would be unknown. 
            int output_dim_value = -1;
```
wschin(2019-08-01 16:08:17):I think those inputs should just be scalars as described in your def.cc.
hariharans29(2019-08-02 01:33:11):That is what I get from the spec too. So, "current" will be fed in as "prev" in the second iteration onwards. For the initial iteration, "start" will be fed in as "prev". Based, on "cond" in the first iteration, "start" will be included in the result (cond is true) or may not be (cond is false)
hariharans29(2019-08-02 01:34:29):Add the comment - thanks.
hariharans29(2019-08-02 01:37:47):Thanks for this comment. But I felt this comment is more suitable at the place we do the check in line 680 (the if... there does this check). The comment is added there. let me know what you think of this.
hariharans29(2019-08-02 01:38:36):Right - I made the change in the def but didn't generate the doc and check-it in. Did that now. #Resolved.
hariharans29(2019-08-02 01:40:59):ell I think 'int' may be either int64_t or int32_t based on the platform. So dim type can be either int32 or int64. Correct ? 

But I will add this check just to be safe when it is running on 32-bit platform...

EDIT: int is mostly 32 bit on most platforms. So it is safer to add the check.
hariharans29(2019-08-02 01:41:37):Ok - making it non-optional is ugly, but I see no other choice. making it non-optional.
wschin(2019-08-02 02:37:08):Both work to me. If you move it to line 680, please do
```suggestion
            // If any of Range's inputs are not initializers, the output dimension would be unknown (denoted by -1 below). 
            int output_dim_value = -1;
```
hariharans29(2019-08-02 02:42:49):Moved already. Resolved.
wschin(2019-08-02 03:37:47):Need another test for `int` type.
gramalingam(2019-08-02 05:43:35):This is essentially a limitation of the function construct. We have been considering extending the function construct so that, instead of registering a fixed constant graph as its body, we register a lambda, similar to the type-and-shape-inference lambda, which takes a "context" (which will give us access to the types of inputs, information about attribute values, and which inputs/outputs are present/required), and returns a subgraph. With such an extension, it should be easy to support optional inputs like delta.
gramalingam(2019-08-02 05:56:20):Just to make sure: why initialize this to -1 here? Please do not set_dim_value to -1: if it is unknown, please leave dim_value not set. It doesn't look like this -1 will reach the set_dim_value, but I just want to make sure. This is a slight problem because ONNX uses a convention slightly different from several frameworks where a single integer is used to represent both known and unknown dimensions, and this seems to confuse people.
gramalingam(2019-08-02 06:02:57):Dim value is int64 (see: https://github.com/onnx/onnx/blob/3ba2e31a4954b4c151dfcf7deb141f244a43fecf/onnx/onnx.in.proto#L451 ). I think we should use int64 here.
gramalingam(2019-08-02 06:04:55):I mean the return value, and the corresponding variable in the calling function should all be int64.
hariharans29(2019-08-02 06:11:08):Thanks. That would be great. Looking forward to it. 

For now, resolving his comment as there is no action needed.
wschin(2019-08-02 06:14:14):Cool, so the only change we need is to use int64 in the entire line.
gramalingam(2019-08-02 06:32:31):If I remember right, "Div" expects both inputs to be of same type. So, this would require a cast of "sub_result" also.
hariharans29(2019-08-02 18:28:59):Fixed, thanks a lot!
hariharans29(2019-08-02 18:39:00):This test is for shape inference. I assume you wanted another test for the actual op in `range.py` ? Anyway, I made one of the 2 tests in shape inference pertaining to INT as well.
hariharans29(2019-08-02 19:06:19):Added a test in 'range.py' pertaining to a node test for int32 and one for shape inference for int32. Resoving this.
ebarsoum(2019-08-14 00:40:21):Can you add a pseudocode or a simple example in the doc?
hariharans29(2019-08-14 04:37:37):Added pseudocode and some examples.
gramalingam(2019-06-14 16:54:48):Hi: In general, I see that existing shape-inference code can be simplified quite a bit if (a) We lift common patterns into utility functions so it can be shared, and (b) We use these utility functions appropriately. In the context of the current PR, it would help to have some "model shape inference functions" that we can point to so that the new code follows recommended styles. But may be that requires some work first, so perhaps we can do it as a separate PR. I am happy to help with that. (For example, the documentation here points to TopK. I think it is valuable to extract lines 967 to 976 as a utility function "getScalarConstant(...)", which is likely to be needed for all ops which take a scalar input which may be a constant.)
hariharans29(2019-06-14 18:26:15):> Hi: In general, I see that existing shape-inference code can be simplified quite a bit if (a) We lift common patterns into utility functions so it can be shared, and (b) We use these utility functions appropriately. In the context of the current PR, it would help to have some "model shape inference functions" that we can point to so that the new code follows recommended styles. But may be that requires some work first, so perhaps we can do it as a separate PR. I am happy to help with that. (For example, the documentation here points to TopK. I think it is valuable to extract lines 967 to 976 as a utility function "getScalarConstant(...)", which is likely to be needed for all ops which take a scalar input which may be a constant.)

This is a valuable suggestion @gramalingam. As you said, let's do this across multiple small PR(s) that are easy to review.   
pranavsharma(2019-06-08 00:36:09):Can we also add that if the shape cannot be determined for whatever reasons, we shouldn’t throw an exception? 
pranavsharma(2019-06-08 00:48:24):Do we want to enforce this? If yes, can we enforce this in the code rather than documentation (can happen in a separate PR)?
hariharans29(2019-06-08 00:53:15):As discussed offline - (FYI for others) - I would refrain from discouraging contributors to not throw exceptions in case of bad inputs/attribute data. For legitimate cases, exceptions should be thrown, isn't it ?

Also this seems like a detail - something that should be caught in PR reviews (should a user throw an exception here (is this bad input?) or should the user just return back gracefully with a no-op (not enough data to proceed?). Such things can be clarified at code-review time.) 
hariharans29(2019-06-08 00:55:26):For now - I vote that the key reviewers for ONNX ops (not a lot of people) have a common agenda - refrain from checking-in ops without shape inference and encourage contributors to implement them as part of code review process.

If we need to enforce such a thing, that obviously means we need to plug gaps (missing shape inference functions) along with the PR that enforces it...
houseroad(2019-06-10 22:59:27):Asking shape inference for all the nodes, is it too much?

Also mention where to add the tests for shape inference? And provide a good example for shape inference?
pranavsharma(2019-06-10 23:02:38):Sure. 
linkerzhang(2019-06-11 01:01:58):@pranavsharma this is a document on how to add a new op, so that I think it's ok to ask contributors to add shape inference function for new ops.

@houseroad @hariharans29 we may rephrase it a little bit to not a "MUST", but provide shape inference function when it's applicable and meaningful.
hariharans29(2019-06-14 00:51:10):Updated
linkerzhang(2019-06-14 16:11:06):great. the updated one look good to me. @houseroad  sounds good to you?
CLAassistant(2019-06-08 02:15:05):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=2089) <br/>All committers have signed the CLA.
gramalingam(2019-06-17 15:48:37):Hi, it might be worth double-checking if this is a clarification or change in spec: the previous spec left it ambiguous whether the values beyond the sequence-length will be zero or potentially undefined/garbage. There could be some implementations that choose the latter interpretation.
gramalingam(2019-06-17 17:09:37):Just to clarify (@edgchen1 and I talked about it offline): I was wondering whether the version-number of the op needs to be bumped or not? @Ke, @ebarsoum, any thoughts?
edgchen1(2019-06-17 18:42:04):@linkerzhang, @ebarsoum, @gramalingam - another question: in the (nonsensical?) case where sequence_lens specifies a zero-length sequence, with this change, Y should contain zeros for that sequence. What about Y_h and Y_c (given possibly non-zero initial_h and initial_c)?
gramalingam(2019-08-23 21:27:59):I am okay with this change if we need it: but I am just curious why we need to specify that the outputs will be zeroed out beyond the sequence length (given that in many cases they may never be used). This may have a performance impact (e.g., in a cpu setting as opposed to gpu). Is it really needed?
edgchen1(2019-08-23 21:50:22):@gramalingam One motivation for avoiding possibly uninitialized memory in the output is to allow consumers of the output to freely read it without risking undefined behavior (admittedly a C/C++-specific concern). Alternatively, the consumers need to be aware that they had better not try to access the possibly uninitialized segments of the output.
gramalingam(2019-08-23 22:04:50):What is the risk? It is not as if we access an invalid memory location. These are valid memory locations with an undefined value. Is this zeroing standard in other existing frameworks? If so, that is a good reason to adopt it.
ebarsoum(2019-09-13 22:35:03):The end of the sequence shouldn't be set to zero.
ebarsoum(2019-09-13 22:35:47):Even in C++, you have the length of the sequence, so the remaining part shouldn't be read.
edgchen1(2019-10-09 21:24:44):Ok. That was the main point of this PR, so I'll just close it.
CLAassistant(2019-06-10 02:24:57):[![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/onnx/onnx?pullRequest=2091) <br/>Thank you for your submission! We really appreciate it. Like many open source projects, we ask that you sign our [Contributor License Agreement](https://cla-assistant.io/onnx/onnx?pullRequest=2091) before we can accept your contribution.<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/onnx/onnx?pullRequest=2091) it.</sub>
CLAassistant(2019-06-11 03:30:21):[![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/onnx/onnx?pullRequest=2093) <br/>Thank you for your submission! We really appreciate it. Like many open source projects, we ask that you sign our [Contributor License Agreement](https://cla-assistant.io/onnx/onnx?pullRequest=2093) before we can accept your contribution.<br/><hr/>**Sam Sun** seems not to be a GitHub user. You need a GitHub account to be able to sign the CLA. If you have already a GitHub account, please [add the email address used for this commit to your account](https://help.github.com/articles/why-are-my-commits-linked-to-the-wrong-user/#commits-are-not-linked-to-any-user).<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/onnx/onnx?pullRequest=2093) it.</sub>
wschin(2019-08-05 16:37:17):Why does this PR change so many model files? Could you please sign the CLA and synchronize your PR with the master branch? Thank you!
gramalingam(2022-05-31 23:03:39):Equals still does not support string inputs. It makes sense to make this extension, but it seems better to close this PR, and open a new PR if someone wants to.
CLAassistant(2019-06-11 15:22:13):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=2095) <br/>All committers have signed the CLA.
CLAassistant(2019-07-23 22:37:31):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=2096) <br/>All committers have signed the CLA.
BowenBao(2019-08-05 19:58:46):@linkerzhang could you take a look at the updates to the requested changes?
spandantiwari(2019-08-07 23:26:47):Thanks @ebarsoum. 
linkerzhang(2019-06-13 17:39:18):Thanks for the refactoring! The 3 inputs (input, min, max) should have same type. Let's use same "T" for them then.
linkerzhang(2019-06-13 17:41:51):this min should be a scaler. please make it clear in the description.
linkerzhang(2019-06-13 17:42:02):same for max, I think.
linkerzhang(2019-06-13 17:44:14):this one should be a scaler, instead of an array. min_val = np.float32(-1)?
BowenBao(2019-06-13 18:08:37):Thanks for reviewing. The reason I separated out is to try mention the shape constraint on min and max in TypeConstraint, I know this also brings in the side effect that `T` and `M` now can actually be different float types. I'll use the same "T" and move the shape constraint to input description.
BowenBao(2019-06-13 21:57:08):I think we should support both scalar and 1-d tensor. That seems to be the consensus for the current onnx spec of the definition scalar. 

For example
https://github.com/onnx/onnx/blob/master/docs/Operators.md#size

> Size
Takes a tensor as input and outputs a int64 **scalar** that equals to the total number of elements of the input tensor.
...
```
node = onnx.helper.make_node(
    'Size',
    inputs=['x'],
    outputs=['y'],
)
x = np.array([
    [1, 2, 3],
    [4, 5, 6],
]).astype(np.float32)
y = np.array(6).astype(np.int64)
expect(node, inputs=[x], outputs=[y],
       name='test_size_example')
```

https://github.com/onnx/onnx/blob/master/docs/Operators.md#nonmaxsuppression

> score_threshold (optional) : tensor(float)
Float representing the threshold for deciding when to remove boxes based on score. It is a scalar
```
iou_threshold = np.array([0.5]).astype(np.float32)
score_threshold = np.array([0.0]).astype(np.float32)
selected_indices = np.array([[0, 0, 3], [0, 0, 0], [0, 0, 5]]).astype(np.int64)
```
wschin(2019-07-20 17:56:11):```suggestion
specified by the inputs 'min' and 'max'. They default to
```
wschin(2019-07-20 17:56:53):```suggestion
numeric_limits::lowest() and numeric_limits::max(), respectively.
```
wschin(2019-07-20 17:58:40):```suggestion
            "T",
```
Could we make it type-safer?
wschin(2019-07-20 18:02:55):It should be just a scalar.
wschin(2019-07-20 18:09:58):If you allow tensors, it means you need to check the shape in runtime because ONNX can have dynamic shapes. Whenever possiblre, I prefer strongly-typed way, so I'd suggest to follow Ke's comment. 

Previously, we didn't do a good job to emphasize the difference between scalar and 1-element tensor, but it doesn't mean we can't starting improving it from now on.

:)

I have also seen that you use scalar instead of tensor, so we should be good.
wschin(2019-07-20 18:12:43):Could you add expected output in a comment (just like what you did right above)? User needs to see the values as an example.
wschin(2019-07-20 18:16:22):Could you print out the expected output in a comment? For example,
```
# expected output of print(y)
#  [0.2, 0.2]
```
Please also checke other similar places.
wschin(2019-07-20 18:17:42):```suggestion
        expect(node, inputs=[x, "", max_val], outputs=[y],
```
maybe?
BowenBao(2019-07-23 21:35:52):good point for known inputs, but how to add expected output for inputs that are randomly generated?
BowenBao(2019-07-23 21:54:03):This is correct. Please check the implementation of `expect`
https://github.com/onnx/onnx/blob/47acb06a969c3fd899f7a13270588ddb0cccd1c1/onnx/backend/test/case/node/__init__.py#L107
and another use case of optional input with operator `Scan`
https://github.com/onnx/onnx/blob/47acb06a969c3fd899f7a13270588ddb0cccd1c1/onnx/backend/test/case/node/scan.py#L43
BowenBao(2019-07-23 22:34:09):I agree that runtime will need to check the shape, but that is introduced by changing min/max from attributes to tensors. It has nothing to do with whether it is a scalar(0-rank tensor) or a 1-element tensor.

scalar vs 1-elem tensor is a bigger issue and should be discussed separately. It is not immediately clear to me which one is more preferable. That being said, I think we should use 'tensor' as description in the spec, avoid using the ambiguous 'scalar'. 
BowenBao(2019-07-23 22:38:00):thanks, updated.
BowenBao(2019-07-23 22:38:08):thanks, updated.
wschin(2019-07-29 21:28:22):In ONNX, the definition of scalar is a tensor with empty shape. Since `min` and `max` attributes are scalars (they are float attribute instead of floats attribute), I don't feel we should use 1-element tensor to store them.
wschin(2019-07-29 22:16:16):Write out values explicitly.
```
3.343 x 1e304 (for example, numeric_limits::max() in c++)
```
gramalingam(2019-07-29 22:39:31):I agree with @wschin and @linkerzhang : it is better to consistently use a tensor with an empty shape as a scalar, instead of a 1D tensor with 1 element. We have not been doing this consistently, but this will potentially create problems later on. In this case, since we are specifying a unit-test-case, we should use zero-dimensional tensors to represent scalars in the test-case. Otherwise, implementations will start supporting both zero-dimensional and 1D tensors (to get past the test-case) and then when somebody wants to do code-generation they will be unable to represent the value using a primitive value in the generated-code … they will be forced to support a boxed representation, making things inefficient.
gramalingam(2019-07-29 22:41:48):Note that people can always use a Reshape to coerce any tensor with 1 element into a scalar. The idea is that people should pay the cost of this kind of representation-conversion only when needed. Strong typing will enable zero-cost when typing guarantees the representation is already the correct one.
gramalingam(2019-07-29 22:46:35):As long as the test-cases here represent the corresponding scalars with empty shape (is this correct?), we should be fine here.
BowenBao(2019-07-31 19:57:38):Thanks for the review @wschin @gramalingam @linkerzhang ! I agree we should have a clear and strict definition of scalar.  
zhaozhixu(2019-06-12 01:56:24):Can't pass the tests with ONNX_ML=0.
![2019-06-12 09-49-57屏幕截图](https://user-images.githubusercontent.com/12733773/59317927-c3205e80-8cf7-11e9-8b41-3585b6866e1e.png)

Travls-cl generated changes in Changelog.md and Operators.md with `python onnx/defs/gen_doc.py`.
However,
* my python3.5 generates no changes to those files 
* my python2.7 returned with the following error

```
➜  onnx git:(master) ✗ python2 onnx/defs/gen_doc.py       
Traceback (most recent call last):
  File "onnx/defs/gen_doc.py", line 21, in <module>
    SNIPPETS = collect_snippets()
  File "/home/zhixu/source/onnx/onnx/backend/test/case/__init__.py", line 14, in collect_snippets
    import_recursive(sys.modules[__name__])
  File "/home/zhixu/source/onnx/onnx/backend/test/case/utils.py", line 22, in import_recursive
    import_recursive(module)
  File "/home/zhixu/source/onnx/onnx/backend/test/case/utils.py", line 20, in import_recursive
    module = importlib.import_module(module_name)
  File "/usr/lib/python2.7/importlib/__init__.py", line 37, in import_module
    __import__(name)
  File "/home/zhixu/source/onnx/onnx/backend/test/case/node/meanvariancenormalization.py", line 13, in <module>
    class MeanVarianceNormalization(Base):
  File "/home/zhixu/source/onnx/onnx/backend/test/case/base.py", line 42, in __init__
    export()
  File "/home/zhixu/source/onnx/onnx/backend/test/case/node/meanvariancenormalization.py", line 42, in export
    name='test_mvn')
  File "/home/zhixu/source/onnx/onnx/backend/test/case/node/__init__.py", line 130, in expect
    expanded_function_nodes = function_testcase_helper(node, name)
  File "/home/zhixu/source/onnx/onnx/backend/test/case/node/__init__.py", line 87, in function_testcase_helper
    node_list = function_expand_helper(node, function_proto, op_prefix)
  File "/home/zhixu/source/onnx/onnx/backend/test/case/node/__init__.py", line 45, in function_expand_helper
    new_node.ClearField("input")
TypeError: Error when calling the metaclass bases
    field name must be a string
```
BowenBao(2019-06-12 02:21:59):hi @zhaozhixu, you need to re-generate the documents and include them in the PR as well. https://github.com/onnx/onnx/blob/master/docs/AddNewOp.md#step-5-update-the-doc-and-generate-the-test-data. Also remember to call `export ONNX_ML=0` before calling the script.
zhaozhixu(2019-06-12 03:07:41):> hi @zhaozhixu, you need to re-generate the documents and include them in the PR as well. https://github.com/onnx/onnx/blob/master/docs/AddNewOp.md#step-5-update-the-doc-and-generate-the-test-data. Also remember to call `export ONNX_ML=0` before calling the script.

hi @BowenBao ,  `onnx/tools/update_doc.sh ` returned with this error, is there something incompatible with python2?

```
===> regenerate test data from node test
Traceback (most recent call last):
  File "onnx/backend/test/cmd_tools.py", line 80, in <module>
    main()
  File "onnx/backend/test/cmd_tools.py", line 76, in main
    args.func(args)
  File "onnx/backend/test/cmd_tools.py", line 28, in generate_data
    cases = model_test.collect_testcases() + node_test.collect_testcases()
  File "/home/zhixu/source/onnx/onnx/backend/test/case/node/__init__.py", line 156, in collect_testcases
    import_recursive(sys.modules[__name__])
  File "/home/zhixu/source/onnx/onnx/backend/test/case/utils.py", line 20, in import_recursive
    module = importlib.import_module(module_name)
  File "/usr/lib/python2.7/importlib/__init__.py", line 37, in import_module
    __import__(name)
  File "/home/zhixu/source/onnx/onnx/backend/test/case/node/meanvariancenormalization.py", line 13, in <module>
    class MeanVarianceNormalization(Base):
  File "/home/zhixu/source/onnx/onnx/backend/test/case/base.py", line 42, in __init__
    export()
  File "/home/zhixu/source/onnx/onnx/backend/test/case/node/meanvariancenormalization.py", line 42, in export
    name='test_mvn')
  File "/home/zhixu/source/onnx/onnx/backend/test/case/node/__init__.py", line 130, in expect
    expanded_function_nodes = function_testcase_helper(node, name)
  File "/home/zhixu/source/onnx/onnx/backend/test/case/node/__init__.py", line 87, in function_testcase_helper
    node_list = function_expand_helper(node, function_proto, op_prefix)
  File "/home/zhixu/source/onnx/onnx/backend/test/case/node/__init__.py", line 45, in function_expand_helper
    new_node.ClearField("input")
TypeError: Error when calling the metaclass bases
    field name must be a string
```
BowenBao(2019-06-12 20:49:25):@zhaozhixu maybe try a clean rebuild
```
git clean -xdf
export ONNX_ML=0
sh tools/update_doc.sh
```
CLAassistant(2019-07-24 00:56:46):[![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/onnx/onnx?pullRequest=2097) <br/>Thank you for your submission! We really appreciate it. Like many open source projects, we ask that you sign our [Contributor License Agreement](https://cla-assistant.io/onnx/onnx?pullRequest=2097) before we can accept your contribution.<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/onnx/onnx?pullRequest=2097) it.</sub>
jcwchen(2021-10-06 17:39:30):This one has been fixed by https://github.com/onnx/onnx/pull/3440. Thanks!
gramalingam(2019-06-13 02:49:47):Looks good to me, thanks.
linkerzhang(2019-06-25 13:54:47):@houseroad do you have any more comments/concern please? Thank you!
gramalingam(2019-06-12 23:22:28):I think "lex_ctx.this_or_parent_graph_has(output)" would preserve the existing behavior, since output_names at this point, includes outer-scope names.
gramalingam(2019-06-12 23:40:54):minor nit: perhaps "this_or_ancestor_graph_has" would be more accurate.
houseroad(2019-06-14 07:36:49):check line 405, we don't allow shadowing
skottmckay(2019-06-14 08:08:51):Doesn't that happens after the graph (or subgraph) inputs are checked? So in the case of a subgraph, a value in the parent graph could be shadowed by a subgraph input. 
gramalingam(2019-06-14 15:35:56):@houseroad : taking a step back: the existing version of the checker allows shadowing in the case of graph-inputs (as Scott mentions). However, it does look slightly odd, considering that shadowing is not allowed for node outputs inside the subgraph. I believe Scott's current version preserves the existing behavior, but if it seems appropriate we could change the behavior to forbid shadowing for the subgraph inputs also. (The concern is whether it will break some existing model; @skottmckay : I assume that the related bug in ORT implies that no model currently using ORT could have used this feature successfully?)
skottmckay(2019-06-16 21:25:58):Regarding the bug in ORT, the model could have run but it would not have been correct as it would have been using the incorrect value (outer scope value instead of the subgraph input value).
linkerzhang(2019-06-21 16:42:29):who owns this pointer? It smells not good to me that it asks caller to have a very clear understanding about this design of ownership.
skottmckay(2019-06-25 05:12:33):It's a raw pointer so the ownership is outside of this class. As the parent context would be null for the top level graph a reference can't be used. 

The LexicalScopeContext instance is just a temporary local variable when the checker is traversing the graph so the parent instance should always be available. Added comments to clarify.
KsenijaS(2019-06-14 02:26:44):This PR fixes the issue:
https://github.com/onnx/onnx/issues/1986
spandantiwari(2019-06-18 21:16:47):@KsenijaS - Thanks for this bug fix. Will you be able to look at this further, maybe rebase to the latest master? Or if you like, we could help. Thanks.
KsenijaS(2019-06-27 06:06:28):I will continue working on this
CLAassistant(2019-07-24 00:56:42):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=2101) <br/>All committers have signed the CLA.
askhade(2019-08-10 00:53:50):Your CIs are failing with error :  AttributeError: type object 'Callable' has no attribute '_abc_registry'
This was a CI pipeline issue which was fixed a few days back. Please merge with master and this will be resolved. 
KsenijaS(2019-09-13 22:33:32):PR is ready to be merged, just update it to master branch first.
jcwchen(2022-06-09 21:30:29):This should be covered in the main branch now: https://github.com/onnx/onnx/blob/a92870cdf359297495a118184dca2eaecee4b717/onnx/version_converter/convert.h#L265. Close this PR. Thanks for the contribution!
spandantiwari(2019-09-16 18:18:49):nit: extra line.
spandantiwari(2019-09-16 18:19:15):nit: extra line
spandantiwari(2019-09-16 18:20:57):nit: can we enclose this single line (return) in braces, just to keep the style of the rest of the code.
wschin(2019-09-16 18:35:54):```suggestion
```
houseroad(2019-09-16 19:59:52):Nit: Could you briefly document what's the different between 8 and 9?
houseroad(2019-09-16 20:03:13):We should clearly declare what the casting rules we use here at the beginning of this file.
houseroad(2019-09-16 20:04:53):Can we also add tests for UINT64, and FLOAT/DOUBLE cases
KsenijaS(2019-09-23 18:39:08):We have a test for UINT64.
CLAassistant(2019-06-14 22:51:20):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=2102) <br/>All committers have signed the CLA.
jiafatom(2019-06-17 17:28:47):Does anyone agree (or not) the interface change? @wschin 
linkerzhang(2019-06-19 16:18:18):please refer to https://github.com/onnx/onnx/blob/master/docs/AddNewOp.md to update an operator.
wschin(2019-06-20 00:15:58):LGTM. Thanks.
jiafatom(2019-06-20 00:19:00):@ebarsoum @linkerzhang  do you agree the interface? thanks.
jiafatom(2019-06-28 17:56:13):We decide to add an op rather than adding a mode, so I am going to use [PR](https://github.com/onnx/onnx/pull/2142) instead. Close this one.
ebarsoum(2019-06-20 00:19:36):Most other OP use `mode` instead of `method`
jiafatom(2019-06-20 00:23:06):> Most other OP use mode instead of method

We already have an attribute called `mode`, used to distinguish the mode in `roi_align`. If we change `method` to `mode`, then what should the old `mode` be called?
CLAassistant(2019-07-22 20:05:02):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=2103) <br/>All committers have signed the CLA.
harryskim(2019-07-22 19:52:46):Could be good to add the gitter link so users can click on the hyperlink in the "joining a SIC" section ( https://gitter.im/onnx)

harryskim(2019-07-22 19:53:28):Just need to captalize P in this sentence: "Providing a comprehensive collection..."
gramalingam(2019-06-25 22:47:16):TF's gather-nd seems to have an extra batch_dims attribute for batched gather-nd … is that required or useful?
hariharans29(2019-06-26 23:11:56):> Also could you provide some brief summary about the PR in the PR's description?

Well sure - but the description is same as the title - to support GatherND op in ONNX. Do you mean you want a justification to include this by any chance ?
hariharans29(2019-06-26 23:12:58):> TF's gather-nd seems to have an extra batch_dims attribute for batched gather-nd … is that required or useful?

Not sure about this - let me take a look. Thanks!
hariharans29(2019-07-02 20:45:39):> > TF's gather-nd seems to have an extra batch_dims attribute for batched gather-nd … is that required or useful?
> 
> Not sure about this - let me take a look. Thanks!

@gramalingam - I took a look at the `batch_dims` concept in the tf Gather_nd op and like you said, it is to support batched inputs ('params' and 'indices' in tf op) and does seem like a good extension to the op's abstraction. However, I see no immediate use-case for us to justify including that. If need be, we can version this op later to support `batch_dims`. Or I can make the spec adjustment now and modify the contrib implementation of this op in ORT. Any thoughts?

CC: @wschin @linkerzhang @spandantiwari  
gramalingam(2019-07-02 22:04:14):I personally think it is okay to leave it for future extension if we don't see a use-case for it. 
hariharans29(2019-07-03 00:48:22):> I personally think it is okay to leave it for future extension if we don't see a use-case for it.

I think this is a good idea too. We can always version the op later.
CLAassistant(2019-07-24 00:56:42):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=2106) <br/>All committers have signed the CLA.
spandantiwari(2019-08-05 21:41:36):@hariharans29 - Overall, this looks OK. I don't have any specific blocking issues on this. Could you please take a look at the recently approved guideline document for adding new ops and update (if needed) based on that. 
https://github.com/onnx/onnx/blob/master/docs/AddNewOp.md 
Some suggestions in line with the guidelines:
1. Add more details in the op definition; could we include some equations?
2. Add more test examples.
3. Add a reference implementation in the test script. 
hariharans29(2019-08-19 23:16:19):@wschin @spandantiwari @BowenBao - Thanks for all the comments. I have addressed them all. Can you please take a look again ? Thanks!
hariharans29(2019-08-22 03:09:49):Let me also add - This operator is the inverse of ScatterND ,since ScatterND has a similar line in it
gramalingam(2019-08-22 03:12:37):By the way: there was a discussion about supporting negative indices in Gather (since pytorch supports that). Do we want to extend this to allow negative indices?
hariharans29(2019-08-22 03:18:43):> By the way: there was a discussion about supporting negative indices in Gather (since pytorch supports that). Do we want to extend this to allow negative indices?

This is a good question - in this spec version, I explicitly did not allow it and pretty much followed the spec clarification in `Gather`. Any comments @spandantiwari @BowenBao  ? Btw - Does `GatherElements`, `ScatterElements`, and `ScatterND` support negative indices ?
 
hariharans29(2019-08-23 01:52:37):Spoke offline to @spandantiwari and @BowenBao - We will support negative indexing in this op after a consensus on it is reached and we will add negative indexing support in a more streamlined manner across similar ops.

For now, it makes sense to stay consistent with `Gather` which doesn't support negative indexing.
spandantiwari(2019-08-05 21:39:17):nit: Based on the code format style followed in this file (at least) I would suggest putting the body for for/if in braces, even for single-statement body. Similar to what you are doing above on line 1836.
spandantiwari(2019-08-05 21:39:38):nit: same, braces.
spandantiwari(2019-08-05 21:40:59):Some people might see the name `last_indice_dimension` as a typo. Maybe rename this to `last_index`dimension`
BowenBao(2019-08-05 22:42:44):nit: indent seems off starting from this line
wschin(2019-08-14 07:08:18):Please describe the indexing mechanism to compute output values. It'd be very helpful if we have something like
```
output[i][j] = data[indices[i]][indices[j], :]
```
wschin(2019-08-14 07:10:20):Could you compute output using a reference implementation? The actual value can be kept in a comment such as
```
output = your_implementation(data, index) # output = [[2, 3], [4, 5]]
wschin(2019-08-14 07:11:56):Is it possible to have `q>r`? Are there any constrains (e.g., max & min) to the values in `indices`?
wschin(2019-08-14 07:15:21):```suggestion
        "Both `data` and `indices` tensors in GatherND need to have rank larger than 0.");
```
wschin(2019-08-14 07:19:04):Could you please explain why this check is required?
wschin(2019-08-14 07:19:27):```suggestion
        "Last dimension of `indices` tensor in GatherND must not be larger than the rank of `data` tensor");
```
hariharans29(2019-08-16 01:11:20):I don't understand why this is needed, sorry. To implement this in Python might not be needed in the context of this file. We can add equations - I feel that is good enough. This file is mostly to generate the node test case that will be tested by a backend - not for human understanding....

EDIT: I see that the newly added Scatter ops have such a reference implementation -  so I will add a similar simpler implementation too...
hariharans29(2019-08-16 01:16:12):Just as the comment above it says - if the indices_shape is missing a value in the last dimension - it is not possible to validate the tensor shape requirement for this op. So we stop at this point with the shape inferencing as we are not able to validate the input shapes....
wschin(2019-08-16 17:49:27):Yes, reference implementation is a requirement for new operators per Op WG's conclusion. :)
wschin(2019-08-16 17:52:01):I understnad every word in this sentence, but why do you choose `!indices_shape.dim(indices_rank - 1).has_dim_value()`? Can I do `!indices_shape.dim(i).has_dim_value()` for `i=0,...,rank-1`?
hariharans29(2019-08-17 23:54:40):Fixed, thanks
hariharans29(2019-08-17 23:54:48):Fixed
hariharans29(2019-08-17 23:55:44):Should be okay now
hariharans29(2019-08-17 23:57:29):It is very hard to write a single equation for this op - in fact the equation written by tf gather_nd is not even clear to me. Instead, I have a detailed section on how to compute the output values in my latest commit and I have linked the explanation to an illustrative example. I believe this drives home the usage of the operator better than some equation that may be hard to parse. 
hariharans29(2019-08-17 23:58:02):This is a very good question - thanks. I have addressed all this in the doc for this op in my latest commits.
hariharans29(2019-08-17 23:58:19):Fixed
hariharans29(2019-08-19 19:43:34):No we don't have to check for every dimension. Please look at the next check in the next line or two. We need to perform a validation against `data_rank` for which we need this dim_value. Without this dim_value, we cannot perform this validation and hence we stop at that point. 
hariharans29(2019-08-19 22:53:58):Added reference implementation as suggested.
wschin(2019-08-21 00:11:35):>1-D tensors [](start = 16, length = 11)

1-D tensors?
hariharans29(2019-08-21 00:19:51):yes - plural. 1-D tensors => as there are many 1-D tensors within the (q-1)-dimensional tensor
gramalingam(2019-08-22 03:02:57):Replace "indices[:,:,i]" by "indices[…,i]" ?
gramalingam(2019-08-22 03:07:57):May be change "tensor of indices" to "tensor of index-tuples"?
gramalingam(2019-08-22 03:08:16):Change to "The output tensor is obtained by mapping each index-tuple in the  `indices` tensor to the corresponding slice of the input `data`"
hariharans29(2019-08-22 03:08:56):sure

CLAassistant(2019-07-23 17:24:13):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=2108) <br/>All committers have signed the CLA.
edgchen1(2019-06-19 00:00:00):**each** operator set version representing the combination of the **most recent version** of each operator was confusing to me. hopefully this matches the intended meaning.
edgchen1(2019-06-19 00:00:37):the original looks like a copy-paste error. need some help with the actual wording
linkerzhang(2019-06-19 16:12:47):I'm not a good English editor/writer, so am not sure whether this editing change improves the readability and clarifies more on semantics of operator set versioning. @prasanthpul  @gramalingam @houseroad @ebarsoum 

The operator set version may be considered as a time point (normally one per release if and only if there's any semantic change for any operator in that release). Each operator may have its specific semantics since one time point (one operator set version). That's why operator does not have "version" itself, but "sinceversion".
gramalingam(2019-06-19 17:47:44):I think "Each operator set version represents a combination of a specific version of some set of operators." would be simpler. I think adding "The operator set version may considered as a time point. Each version of an operator belongs to some contiguous set of operator set versions. Normally, there is one operator set version per release of ONNX (if and only if there is a change in the set of operators or the specification of any existing operator)." would also be helpful.
gramalingam(2019-06-19 17:54:41):The build metadata of this version of the operator set?
edgchen1(2019-06-21 16:40:38):thanks for the suggestion.
since the field is named `ir_build_metadata`, is it the metadata of the operator set or the IR?
if the former, would a better name for this field be `opset_build_metadata`?
edgchen1(2019-06-21 22:23:39):updated and added an example to versioning.md.
gramalingam(2019-06-21 22:37:16):Changing field names is tough, right? May be @linkerzhang can comment on it, but it may not be worth changing field names. (My suggestion was based on the documentation in the proto file … I don't really know what this is used for.)
linkerzhang(2019-07-01 04:25:41):“ For graph inputs, those values are provided by a same-named value in GraphProto. initializer.", it's better to keep this. This is telling "how to specify the default value of a graph input".
linkerzhang(2019-07-01 04:27:15):Thank you so much for adding this table to show a very clear view of operator set version evolvement.
gramalingam(2019-07-01 15:36:55):Hmmm … this (the original version itself, I don't mean this PR's change) is inaccurate. There were proposals for both these extensions, but I don't believe either is supported. I believe the statement should be entirely dropped.
gramalingam(2019-07-01 15:39:57):Even the preceding paragraph seems inaccurate. This could be a subject for design discussion, but I believe currently the dimension variables are globally scoped.
edgchen1(2019-07-02 17:29:25):that's also specified here, right?
https://github.com/edgchen1/onnx/blame/edgchen1/ir_doc_fixes/docs/IR.md#L159

would it be better to specify it once? that information also seemed out of place here.
edgchen1(2019-07-02 17:32:46):ok, i can remove it
edgchen1(2019-07-02 17:34:05):how should we follow up?
gramalingam(2019-07-23 05:24:15):Let's merge in what you have. I can take a shot at clarifying this.
gramalingam(2019-07-23 05:28:56):Well, no harm in reminding the reader that the default value is specified by the initializer. Most people are familiar with the term "initializer", but may not immediately correlate that to a "default value". How about adding back "Recall that default-values for inputs are specified in the initializer list."
linkerzhang(2019-07-23 17:28:43):I agree to add some words back for more clarification.
CLAassistant(2019-07-24 00:56:46):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=2110) <br/>All committers have signed the CLA.
wschin(2019-09-11 01:13:24):@take-cheeze, could you please sync your PR with the master branch? Thanks.
CLAassistant(2019-06-19 21:47:31):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=2115) <br/>All committers have signed the CLA.
xykong58(2019-06-20 19:56:20):Do I need to do anything to push the change to the master ?
CLAassistant(2019-07-24 00:56:42):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=2117) <br/>All committers have signed the CLA.
fdwr(2020-02-07 00:01:04):@prasanthpul : Can we get an approving review for these small typos? TY.
postrational(2020-02-07 10:25:23):This is good to merge, but we need to rerun the failing CI check.
fdwr(2020-02-07 22:23:07):@postrational : Is there a magic incantation to do so? (short of a dummy push) It's not evident in the GitHub UI how to, and the circleci.com webpage has a "Rerun workflow" button that is grayed out. The error `test/onnx/test_utility_funs.py::TestUtilityFuns::test_constant_fold_reshape Fatal Python error: Segmentation fault` isn't even related to this typo fix.
postrational(2020-02-11 17:04:06):@fdwr A dummy push, or an update of the branch will trigger a full round of CI for you. I'm not sure if you can manually restart only Circle CI of you don't have permissions (I currently don't).
prasanthpul(2020-02-12 01:56:59):@houseroad can you take a look at the Circle CI failure?


linkerzhang(2019-06-21 16:34:27):@houseroad @spandantiwari the pytorch ci failed. please help to check it, and I'll merge this PR end of day today if no concern. Thanks!
spandantiwari(2019-06-21 17:07:30):@linkerzhang - Yes, this does look unrelated.
@houseroad - I am not sure why though. Doesn't look like any issue with the new ORT test in PT CI. Any idea?
raymondxyang(2019-06-25 17:43:54):I added this into windows CI already. Any changes breaks the protoc compilation will be blocked
CLAassistant(2019-07-24 00:56:42):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=2122) <br/>All committers have signed the CLA.
houseroad(2019-09-18 12:34:18):@raymondxyang could you sign the CLA?
gramalingam(2019-09-18 19:58:26):Raymond left Microsoft. So, we may need to duplicate this, if we want to merge this in.
houseroad(2019-09-24 01:08:28):Thanks a lot @raymondxyang 
liqunfu(2019-06-27 19:06:16):@houseroad @linkerzhang 
This PR updates TopK to take optional attributes for direction and output sorting. It aligns with TF and PyTorch better than the current version.
wschin(2019-07-01 18:11:25):`docs/TestCoverage.md` got removed? Is it expected?
CLAassistant(2019-07-24 00:56:46):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=2126) <br/>All committers have signed the CLA.
liqunfu(2019-07-24 14:48:38):@gramalingam , @linkerzhang, @ebarsoum please confirm if this PR can be merged.
ebarsoum(2019-07-30 17:18:28):Can you resolve the conflict?
wschin(2019-07-01 18:02:44):Adding `sorted` implies the need of another `Sort` operator. Would you consider split the new top-k into two operators?
wschin(2019-07-01 18:10:19):I am not sure if `largest` is necessary. If a user wants to have a graph:
```
X ---> TopK(largest=0) ---> Y, IndexY
```
He can do this
```
X ---> Neg ---> NegX ---> TopK ---> NegY, IndexY
NegY ---> Neg ---> Y
```
Another possibility is to propose `BottomK` as a `FunctionProto` which mimics the graph right above.
liqunfu(2019-07-02 18:42:35):good question on Sort operator! Actually sort op could be defined as a function op using TopK with 'sorted' attribute. This is one of reasons of this PR.  Hope it clears the question of "split the new top-k into two operators".
liqunfu(2019-07-02 19:00:48):I think it is better to avoid extra cost of Negs. 
Regarding BottomK, I think "TopK" is a more acceptable term. It aligns with PyTorch spec.
wschin(2019-07-02 21:51:01):I feel we start preferring smaller primitives when possible, so I still likely weight `Sort+TopK` more over modifying `TopK`.
wschin(2019-07-02 21:52:40):This is not extra cost. How a FunctionProto is evaluated is totally up to runtime. Even if you have a very complicated FunctionProto, a runtime can still map that FunctionProto to a single custom operator.
wschin(2019-07-16 23:20:13):I'd suggest to use `order` as the attribute name and that attribute is an enum of `"ascent"`, `"descent"`, and `"none"`.
wschin(2019-07-16 23:34:14):Do you have shape inference tests?
gramalingam(2019-07-16 23:44:50):I'd prefer changing the attribute-name to "largest" (or something like that). "Order/descent/ascent" is still slightly ambiguous whether it is talking about which elements are selected or about the order in which the selected elements appear in the output. 
gramalingam(2019-07-16 23:48:44):It may also be useful to say that the attribute value must be 0 or 1 (to avoid dealing with ambiguous cases like a value like 2).
gramalingam(2019-07-16 23:59:57):One comment/question about the shape-inference logic (I understand it was pre-existing shape-inference logic, not part of this PR): this checks that the "k" input is a 1-dimensional tensor. However, it is usually recommended that scalars be represented as 0-dimensional tensor. This will likely cause some unexpected error later on. I think it is better that we allow 0-dimensional tensors also here.
wschin(2019-07-17 03:21:16):@gramalingam, this operator can return the largest `K` or the smallest `K` elements depending on the attribute `mode`, so `largest` looks ambiguous.
gramalingam(2019-07-17 16:34:47):"largest" is a Boolean flag. So, if "largest=1" return K largest. If "largest=0" return K smallest. Or, use a string-valued "mode" with possible values as "largest" or "smallest" … but I think Boolean-flags are simpler than string-valued flags.
wschin(2019-07-17 16:56:06):Your `largest` attribute is the `mode` attribute in this PR. This PR's `sorted` attribute indicates if the returned values should be sorted.
wschin(2019-07-17 16:57:42):This place needs an reference implementation and the output should be computed using the reference implementation.
wschin(2019-07-17 17:02:11):Is the sorting algorithm stable?
gramalingam(2019-07-17 17:02:25):Okay, it looks like we were talking about two different attributes. I was suggesting renaming the "mode" attribute to "largest" (because mode=1 or 0 doesn't tell me what it means). It looks like you are talking about changing the "sorted" attribute. Sorry about the confusion.
liqunfu(2019-07-18 22:40:04):yes it is stable sort, i.e. to use axis indices as a tiebreaker for equal values.
liqunfu(2019-07-19 16:34:46):Yes it is the preexisting shape inference code. Spec update in this PR does not change shape logic.
Regarding axis type, because it is an attribute instead of an input, it shall be a int64_t.
liqunfu(2019-07-19 16:41:11):I changed attribute name back from 'mode' to 'largest'. 
Regarding attribute name 'sort', it is redundant to name it with ascending or descending because order is already implied by the 'largest' attribute. I updated description for sort to make it clearer:
'If "sorted" is 0, order of returned 'Values' and 'Indices' are undefined.'
gramalingam(2019-07-25 17:28:16):How about extending "resulting k elements will be sorted." to "resulting k elements will be sorted, in descending order if largest=1 and in ascending order if largest=0."? 
JamesAllingham(2019-06-28 08:08:00):Hmmm, I can't seem to figure out why travis-ci is failing. There doesn't seem to be a reason given. Just `The command "./.travis/script.sh" exited with 1.`. Does anyone have any idea what the problem might be?
JamesAllingham(2019-07-10 21:05:46):@linkerzhang any idea why the travis-ci build is failing here? Also, the circlecle build has started to fail but I don't think it is a problem on my side since it used to pass and I haven't made any changes since it last passed. The error message suggests a problem with the venv: 

```
ERROR: Can not perform a '--user' install. User site-packages are not visible in this virtualenv.
```
CLAassistant(2019-07-24 00:56:41):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=2127) <br/>All committers have signed the CLA.
postrational(2019-08-22 13:13:45):@JamesAllingham I think Travis CI is failing, because your PR ends up modifying these files:
```
	modified:   docs/Operators.md
	modified:   docs/TestCoverage.md
```
These files are generated from other sources.
If you want to change them, you need to change their sources, then re-generate them and add to your PR.
JamesAllingham(2019-09-02 14:03:04):@postrational Thanks, re-generating the files has fixed the issue.
wschin(2019-09-17 16:46:13):It seems that we should still have a test to test if the first dimension can be changed correctly. It doesn't sound right if we change all tests not to touch the first dimension. Those are tests for ONNX operators, not specific framework. What do you think?
wschin(2019-09-17 16:47:07):It means that we should have a reference implementation of `ONNX Reshape`. I am thinking something like
```
reshaped = onnx_reshape_impl(…)
```
JamesAllingham(2019-09-17 17:01:14):Totally agree, I did try to make sure that there were still tests touching the first dim, but I didn't think to specifically test reordering of the first dim with another. I'll add one back in.
JamesAllingham(2019-09-17 17:01:37):Sure, I'll  do that 👍 
CLAassistant(2019-06-25 09:34:09):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=2128) <br/>All committers have signed the CLA.
CLAassistant(2019-06-26 09:14:16):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=2129) <br/>All committers have signed the CLA.
lucienwang1009(2019-07-09 06:53:05):Thanks for fix. Empty list also falls into this issue.
hariharans29(2019-06-26 21:32:06):CC: @linkerzhang @houseroad 
CLAassistant(2019-07-24 00:56:41):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=2133) <br/>All committers have signed the CLA.
hariharans29(2019-09-17 18:46:08):Changed title to reflect the actual changeset now.
CLAassistant(2019-09-10 20:56:20):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=2135) <br/>All committers have signed the CLA.
gramalingam(2019-07-01 21:48:40):What happens in the "axis" case when the number of unique elements is not the same at every position? I guess the dimension will be the maximum, but what happens to the undefined/padding positions? How will the users know which are padding values?
wschin(2019-07-17 03:27:22):What are the targeted frameworks of this operator? TF, numpy, and pytorch?
CLAassistant(2019-07-24 00:56:35):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=2141) <br/>All committers have signed the CLA.
gramalingam(2019-07-30 18:03:32):A general comment: it looks like this op will not work well in the batched case. Batching requires a different treatment of axes. For example, if the outermost axis is a batch axis, then with an input of [ [0,0,1], [1, 2, 2] ], the user would want an output of [ [0,1], [1,2] ]. The current definition does not allow us to get this with or without the axis attribute. (I think we should think about what ONNX's philosophy and approach is for batch-support in general. But this op may be a good example to drive the general approach we want, so mentioning it here.)
liqunfu(2019-07-31 18:18:24):@hariharans29 please update your cla 
gramalingam(2019-07-01 21:36:14):According to the description earlier, indices has same shape as input.
gramalingam(2019-07-01 21:37:19):This doesn't seem consistent with the description … shouldn't this have the same number of elements as "uniques"?
gramalingam(2019-07-01 21:44:21):Please add description of "inverse_indices" output also.
gramalingam(2019-07-01 21:50:32):Similar comments for the other outputs as well. I assume that they will not be flattened if axis is specified?
liqunfu(2019-07-02 18:00:56):Updated the spec to take output shapes of numpy. indices, inverse_indices, and counts are all 1-D tensors.
liqunfu(2019-07-02 18:03:56):spec updated. when axis is not provided, input is flattened. output are all 1-D tensors. When axis is provided, output Y has the same shape of "X" except in dimension "axis" which is unknown.
liqunfu(2019-07-02 18:04:26):updated comment to fix this.
gramalingam(2019-07-02 22:15:20):How about "The second optional output tensor 'indices': indices[j] contains the index of the first occurrence of Y[j] in input X."
gramalingam(2019-07-02 22:16:15):The third optional output tensor 'inverse_indices': inverted_indices[j] contains the index of the occurrence of X[j] in the output Y.
gramalingam(2019-07-02 22:24:30):When I read the scipy documentation, it says that all the other dimensions (other than axis) are flattened ... is that correct, or does that flattening not happen for the output?
liqunfu(2019-07-03 17:20:53):I think it means that the rest dimensions are flattened after the 'axis' dimension is moved to the front for sorting. However the final output is transposed back to the original input form. Test cases also conform this. 
liqunfu(2019-07-08 18:16:01):Good one! Updated the doc.
liqunfu(2019-07-08 18:16:22):updated the doc according to the comments.
gramalingam(2019-07-17 00:32:53):"inverted_indices[j]" => "inverse_indices[j]"
gramalingam(2019-07-17 00:33:12):"forth" => "fourth"
gramalingam(2019-07-17 00:37:08):Can replace these 4 lines by "*dim = input_shape.dim(I)", I think.
gramalingam(2019-07-17 00:37:56):That is a lower-case eye ... auto-correct capitalized it :-( 
wschin(2019-07-17 03:29:41):This is optional, so what's the default value? I don't feel `None` is a good value because in protobuf format, default value can be stored as `None`.
wschin(2019-07-17 03:32:08):Do you mean linear index? In general, index of a tensor element is a tuple such as `(i,j,k)` when the tensor is 3-D. Could you also explain this attribute when the input is 3-D and the axis is 1?
wschin(2019-07-17 03:32:42):Can this signature also handle TF's unique?
wschin(2019-07-17 03:34:46):Is `X[j]` also a correct expression when outputing sub-tensors? I feel we need examples for both of 1-D and N-D tensors.
wschin(2019-07-17 03:39:58):I saw you mentioned that output 1 to output 3 are all optional above. Could you also add `Optional` into their doc strings here?
wschin(2019-07-17 03:42:27):It'd be nice if you can add more complicated cases (such as 2x3x2 tensor) and some edge cases such as scalar, one-element tensor, etc.
hobei(2019-07-19 09:14:24):Should we have examples in the description and in the 'examples' section?
liqunfu(2019-07-19 18:25:29):it is to support both TF and PyTorch.
gramalingam(2019-07-30 17:53:01):Whether the output is 1-D depends on whether the axis attribute is specified.  I suggest dropping the "1-D" part above.
gramalingam(2019-07-30 18:13:54):"in the order they occur in the input" => "in the order of the first occurrence of the values in the input" ?
gramalingam(2019-07-30 18:32:55):Is it possible to add a python implementation for this case? E.g., can we use some sort-routine on inverse-indices to permute all the outputs to the required order?
gramalingam(2019-07-30 19:01:40):From what I observed when experimenting with np.unique, it looks like the result in 1D. When axis is not specified, it looks like a linear index. Actually, the whole thing (that is, the behavior of inverted indices as well) looks like the input is flattened, and then the basic "unique" is invoked on the flattened tensor. When axis is specified, index is naturally 1D since it is an index into only one axis.
liqunfu(2019-07-31 17:00:51):Indices are always 1D of int64_t. It either points to a flattened input, when axis is not provided, or a slice along the given axis. Updated the comments to clear this confusion. 
wschin(2019-07-31 23:51:49):```suggestion
# prepare index mapping from sorted values to unsorted values.
```
Could you run spelling check tool if possible?
wschin(2019-08-01 00:05:23):You can do
```python
y = y[argsorted_indices]
indices = indices[argsorted_indices]
counts=counts[argsorted_indices]

```
wschin(2019-08-01 00:06:38):Could you split this test into smaller pieces? One test function would be better to include only one node.
wschin(2019-08-01 00:08:07):Can this reference implementation handle N-D tensor? Looks like numpy.take can deal with N-D cases.
prasanthpul(2019-06-28 20:08:51):Can you explain why this should not be a function? 
jiafatom(2019-06-28 20:31:10):> Can you explain why this should not be a function?

The algorithm logic for CropAndResize is (1) so complicated  (2) quite different from existing RoiAlign, so it is too challenging to use existing ops to construct it.
jiafatom(2019-07-01 20:53:01):cc @wschin @linkerzhang @ebarsoum
BowenBao(2019-07-02 18:00:55):Can you explain what is the difference between existing RoIAlign, and the reason why choosing not to add this as a mode for RoIAlign?
jiafatom(2019-07-02 18:08:12):> Can you explain what is the difference between existing RoIAlign, and the reason why choosing not to add this as a mode for RoIAlign?

For CropAndResize and RoiAlign, the algorithm to calculate the output is totally different. (1) They choose different locations in original input. (2) CropAndResize uses bilinear or nearest interpolation, but RoiAlign uses weighted sampling. Therefore, the output result is different. 

The reason not uses mode: (1) the implementation is totally different. So this is basically two algorithm. (2) the interface is similar, but we still see some attributes can only be uses one or the other: extrapolation_value only used in CropAndResize, sampling_ratio, spatial_scale only uses in RoiAlign.
BowenBao(2019-07-02 19:50:55):@jiafatom Thanks for the explanation. 

> > Can you explain what is the difference between existing RoIAlign, and the reason why choosing not to add this as a mode for RoIAlign?
> 
> For CropAndResize and RoiAlign, the algorithm to calculate the output is totally different. (1) They choose different locations in original input. (2) CropAndResize uses bilinear or nearest interpolation, but RoiAlign uses weighted sampling. Therefore, the output result is different.

For (1) What are the location differences? The crop_and_resize uses [y1,x1,y2,x2], and is normalized by image size v.s. RoiAlign [x1,y1,x2,y2], integer + optional spatial_scale. This part can be handled with transpose and mul/div. Are there other differences that I'm missing?

(2) Is there a way to represent the resizing in crop_and_resize/RoiAlign using existing onnx ops? Like Resize/AveragePool?

> The reason not uses mode: (1) the implementation is totally different. So this is basically two algorithm. (2) the interface is similar, but we still see some attributes can only be uses one or the other: extrapolation_value only used in CropAndResize, sampling_ratio, spatial_scale only uses in RoiAlign.

It is confusing to add another op with a totally different name that does almost the same thing if ignoring the implementation details. Can we re-design both to be functions? 
```
# pre-process locations
for (size_t i = 0; i < num_of_rois; ++i) {
    cat(output, resize(crop(images[batch_indices[i]], rois[i]), ...), axis=0)
}
# post-process
```
jiafatom(2019-07-02 20:34:45):> @jiafatom Thanks for the explanation.
> 
> 
> Can you explain what is the difference between existing RoIAlign, and the reason why choosing not to add this as a mode for RoIAlign?
> 
> For CropAndResize and RoiAlign, the algorithm to calculate the output is totally different. (1) They choose different locations in original input. (2) CropAndResize uses bilinear or nearest interpolation, but RoiAlign uses weighted sampling. Therefore, the output result is different.
> 
> For (1) What are the location differences? The crop_and_resize uses [y1,x1,y2,x2], and is normalized by image size v.s. RoiAlign [x1,y1,x2,y2], integer + optional spatial_scale. This part can be handled with transpose and mul/div. Are there other differences that I'm missing?
> (2) Is there a way to represent the resizing in crop_and_resize/RoiAlign using existing onnx ops? Like Resize/AveragePool?
> 
> The reason not uses mode: (1) the implementation is totally different. So this is basically two algorithm. (2) the interface is similar, but we still see some attributes can only be uses one or the other: extrapolation_value only used in CropAndResize, sampling_ratio, spatial_scale only uses in RoiAlign.
> 
> It is confusing to add another op with a totally different name that does almost the same thing if ignoring the implementation details. Can we re-design both to be functions?
> # pre-process locations
> for (size_t i = 0; i < num_of_rois; ++i) {
>     cat(output, resize(crop(images[batch_indices[i]], rois[i]), ...), axis=0)
> }
> # post-process

For location, it means the location of the input tensor corresponded to the output tensor, not the roi location mentioned here. Actually it is that the algorithm is totally different, so the code is actually different and cannot be reused.
BowenBao(2019-07-03 01:05:59):> For location, it means the location of the input tensor corresponded to the output tensor, not the roi location mentioned here. Actually it is that the algorithm is totally different, so the code is actually different and cannot be reused.

After offline discussion, the main difference above is from the resize step. 

If the resize step in crop_and_resize is equivalent to onnx::resize, then we should have enough operators to represent crop_and_resize. We don't need to add operator crop, onnx::slice should be enough. The algorithm might be too complicated for the current function design, as it requires roughly more than 10 operators. 

I'd suggest renaming the proposed op CropAndResize to something closer to RoIAlign. 

jiafatom(2019-07-08 21:42:41):@wschin @linkerzhang @ebarsoum can we proceed this PR? any comments?
ebarsoum(2019-07-11 05:58:00):@jiafatom  what the difference between the resize in resize_and_crop and onnx:resize?  Also, I agree with bowen the name should change (i.e. RoiCropAndResize).
jiafatom(2019-07-11 18:22:05):> @jiafatom  what the difference between the resize in resize_and_crop and onnx:resize?  Also, I agree with bowen the name should change (i.e. RoiCropAndResize).

onnx:resize treats the input tensor as a whole (no crop there).  crop_and_resize "crops" the input tensor based on input rois first, and then do resize. In fact, crop_and_resize does not need generate the cropped image, but calculated the image coordinate instead.
CLAassistant(2019-07-24 00:56:54):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=2142) <br/>All committers have signed the CLA.
postrational(2019-08-22 12:20:33):Since the update to Resize op #2057 was merged, would it now be possible to express this logic as a `Function`?

Regardless of how complex the `Function` may be, if it is possible to express this in terms of simpler ops, we should always provide this as a form of reference implementation.

If it's not possible, what specifically is blocking it?
postrational(2019-09-11 16:21:49):Since #2057 was merged, it should now be possible to implement this as an ONNX `Function`.

Please refactor the PR to do this and we can add it to version 1.7 of ONNX.
jiafatom(2019-09-11 17:23:40):> Since #2057 was merged, it should now be possible to implement this as an ONNX Function.
> Please refactor the PR to do this and we can add it to version 1.7 of ONNX.

This is on ONNX backlog and will be picked up in rotation.
jiafatom(2020-11-06 19:35:31):Had offline discussion on this. There is debating discussion on whether to implement it as a function. We have a custom op RoiAlign to make mask-rcnn model work. So we hold this implementation.
ebarsoum(2019-07-11 05:59:06):Add formula or pseudocode in the description.
ebarsoum(2019-07-11 05:59:27):How it is used?
ebarsoum(2019-07-11 06:01:32):Why one test case? What about with different extrapolation_value?
ebarsoum(2019-07-11 06:01:47):Can you add a reference implementation?
ebarsoum(2019-07-11 06:02:27):Add link to the equivalent TensorFlow version.
jiafatom(2019-07-11 19:11:19):added.
jiafatom(2019-07-11 19:13:16):For each element in the output tensor, crop_and_resize finds its corresponding coordinates in the original input tensor (image). If this coordinate is out of bound (for example, < 0), the extrapolation value is applied. For each element Y[m, c, ch, cw] in the output tensor, the image coordinate in the original X, (ch_x, cw_x), is calculated at first, based on ch, cw, R[m, :], and B[m]. Then Y[m, c, ch, cw] = mode(X[B[m], c, ch_x, cw_x], extrapolation_value).
jiafatom(2019-07-11 19:13:26):added
jiafatom(2019-07-11 19:21:27):CropAndResize is now a contrib op in onnxruntime, see the C++ implementation here:
https://github.com/microsoft/onnxruntime/blob/master/onnxruntime/contrib_ops/cpu/crop_and_resize.cc
Do you mean adding a python implementation? The code is a bit long and this node cannot be easily composed by existing onnx ops. Take existing onnx RoiAlign op for example, there is no reference implementation either because code is long. 

This is a unit test, so the test example is already good to guard the result. In addition, I provide the link for tensorflow implementation. Then it should be enough.
jiafatom(2019-07-11 21:50:13):Will add 
jiafatom(2019-07-11 21:50:23):Will add
ebarsoum(2019-07-18 20:33:50):Can you describe `extrapolation_value` in more details?
ebarsoum(2019-07-18 20:34:14):Add equation or pseudocode?
jiafatom(2019-07-18 20:59:34):I have the python implementation [here](https://github.com/onnx/onnx/pull/2142/files/4f82d4f9f37e8980e29320fc20cc1f2a7026e412#diff-bd6ef8521750dfb968d157511646c345), which is very lengthy. Here I already mentioned `Y[m, c, ch, cw] = mode(X[B[m], c, ch_x, cw_x], extrapolation_value)`, it is clear now.
jiafatom(2019-07-18 21:02:00):This description here is same as tensorflow there. I can add "when the corresponding input coordinate is not available" (because of out of tensor bound) there.
ebarsoum(2019-07-23 17:47:37):I know that the description match TF one, but it is lacking. Can you add more details?
jiafatom(2019-07-23 17:52:06):This [PR](https://github.com/onnx/onnx/pull/2057) will add crop to Resize op, so the extrapolation_value will be used there. The plan is that RoiCropAndResize will be a function of Resize. Does it make sense? I will hold this PR until Resize is finalized.
CLAassistant(2019-07-22 22:10:09):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=2143) <br/>All committers have signed the CLA.
lara-hdr(2019-07-23 16:31:21):@gramalingam @linkerzhang could you please take a look at this PR?
lara-hdr(2019-07-30 18:13:30):@linkerzhang could you take a look?
I'll rebase and resolve the conflict once reviewed to avoid rebasing continuously :)
spandantiwari(2019-07-31 23:01:31):@postrational - could you please help review this PR. We are looking to merge this soon as we are blocked by it. We are waiting to get two approvals. Thanks.
spandantiwari(2019-08-03 21:26:44):Thanks @gramalingam for all the discussion about Gather/Scatter ops and for reviewing and merging this. 

Thanks @wschin for your review.
gramalingam(2019-07-24 22:06:01):Minor rewording suggestion: "This operator is deprecated. Please use ScatterElements, which provides the same functionality."
gramalingam(2019-07-24 22:42:22):Here is a suggested rewording of the documentation:

GatherElements takes two inputs `data` and `indices` of the same rank r >= 1 and an optional attribute axis that identifies an axis of `data` (by default, the outer-most axis, that is axis 0). It is an indexing operation that produces its output by indexing into the input `data` tensor at index positions determined by elements of the `indices` tensor. Its output shape is the same as the shape of `indices` and consists of one value (gathered from the `data`) for each element in `indices`.

For instance, in the 3-D case (r = 3), the output produced is determined by the following equations:
```
  out[i][j][k] = input[index[i][j][k]][j][k]  # if dim == 0
  out[i][j][k] = input[i][index[i][j][k]][k]  # if dim == 1
  out[i][j][k] = input[i][j][index[i][j][k]]  # if dim == 2
```
This operator is the inverse of ScatterElements.

Example 1:
```
  data = [
      [1, 2],
      [3, 4],
  ]
  indices = [
      [0, 0],
      [1, 0],
  ]
  axis = 1
  output = [
      [
        [1, 1],
        [4, 3],
      ],
  ]
```

Example 2:
```
  data = [
      [1, 2, 3],
      [4, 5, 6],
      [7, 8, 9],
  ]
  indices = [
      [1, 2, 0],
      [2, 0, 0],
  ]
  axis = 0
  output = [
      [
        [4, 8, 3],
        [7, 2, 3],
      ],
  ]
  ```

gramalingam(2019-07-24 22:44:24):Please use ``` for blocks above. Dot dot dot does not seem to have the same effect.
gramalingam(2019-07-24 22:46:05):Also suggest adding "This operation is similar to Torch's gather operation."
gramalingam(2019-07-24 23:04:06):Suggested rewording:

ScatterElements takes three inputs `data`, `updates`, and `indices` of the same rank r >= 1 and an optional attribute axis that identifies an axis of `data` (by default, the outer-most axis, that is axis 0). The output of the operation is produced by creating a copy of the input `data`, and then updating its value to values specified by `updates` at specific index positions specified by `indices`. Its output shape is the same as the shape of `data`.

For each entry in `updates`, the target index in `data` is obtained by combining the corresponding entry in `indices` with the index of the entry itself: the index-value for dimension = axis is obtained from the value of the corresponding entry in `indices` and the index-value for dimension != axis is obtained from the index of the entry itself.

For instance, in a 2-D tensor case, the update corresponding to the [i][j] entry is performed as below:
```
   output[indices[i][j]][j] = updates[i][j] if axis = 0, 
   output[i][indices[i][j]] = updates[i][j] if axis = 1,
```
This operator is the inverse of GatherElements. It is similar to Torch's Scatter operation.

Example 1:
```
  data = [
      [0.0, 0.0, 0.0],
      [0.0, 0.0, 0.0],
      [0.0, 0.0, 0.0],
  ]
  indices = [
      [1, 0, 2],
      [0, 2, 1],
  ]
  updates = [
      [1.0, 1.1, 1.2],
      [2.0, 2.1, 2.2],
  ]
  output = [
      [2.0, 1.1, 0.0]
      [1.0, 0.0, 2.2]
      [0.0, 2.1, 1.2]
  ]
```

Example 2:
```
  data = [[1.0, 2.0, 3.0, 4.0, 5.0]]
  indices = [[1, 3]]
  updates = [[1.1, 2.1]]
  axis = 1
  output = [[1.0, 1.1, 3.0, 2.1, 5.0]]
  ```

gramalingam(2019-07-25 16:36:13):Minor nit: Adding block quote ``` before the "data = [" line and closing it at the end of the example will help format the example. Same comment for all examples.
gramalingam(2019-07-25 16:42:54):I think it would help to share the updated documentation used for ScatterElements for Scatter also. It will help make it clear they have the same definition.
lara-hdr(2019-07-25 16:46:34):I initially left it as is to be consistent with the other examples in this file.
But just updated all the gather/scatter ops to have the quotes.
gramalingam(2019-07-25 16:51:21):I mean the first (replace the old documentation with the new documentation), with the deprecation line alone being extra.
wschin(2019-08-02 16:35:22):Please add a comment to describe the value of `y`, for example,
```suggestion
        y = scatter_elements(data, indices, updates, axis)
        // print(y) produces
        // [...
```
This comment is also for another test above.
gramalingam(2019-07-01 21:11:58):Please note that the documentation is generated. The op-definition in the defs file will need to be updated. See step 5 of https://github.com/onnx/onnx/blob/master/docs/AddNewOp.md … The op definition is in https://github.com/onnx/onnx/blob/master/onnx/defs/controlflow/defs.cc 
CLAassistant(2019-07-24 00:56:41):[![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/onnx/onnx?pullRequest=2144) <br/>Thank you for your submission! We really appreciate it. Like many open source projects, we ask that you all sign our [Contributor License Agreement](https://cla-assistant.io/onnx/onnx?pullRequest=2144) before we can accept your contribution.<br/>**1** out of **2** committers have signed the CLA.<br/><br/>:white_check_mark: linkerzhang<br/>:x: xykong58<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/onnx/onnx?pullRequest=2144) it.</sub>
gramalingam(2019-09-13 19:59:24):Scott's suggestions look good to me (distinguishing "implicit code" and "explicit code" etc.). Is this PR still active? Can @xykong58 address these comments? If not, I can create a new PR to update the documentation.
xykong58(2019-09-18 01:50:43):@gramalingam
Enough discussions on the issue, sounds like it is better to create a new PR for updating the document.  

gramalingam(2022-05-31 23:08:41):Closing this PR since PR #2337 covered this.
wschin(2019-07-01 17:51:54):With `b_out`, `b` inside the loop body can probably be removed. I personally like changing `b_out` below to `b` or adding `b_out=b` right after this loop.
gramalingam(2019-07-01 21:09:40):I think we can use "b" uniformly inside the loop, and add b_out=b after the loop. (See suggested change to the ONNX example discussed above.)
wschin(2019-07-02 05:51:26):Look like we need to add `keepgoing_out = keepgoing` right after the loop too. In addition, we need modify the description of
```
graph body-net (
        %i[INT32, scalar]
…
```
accordingly.
wschin(2019-07-02 05:59:08):It looks not smooth. The value of `b_in` may change from iteration to iteration, so adding an `Assignment` to update `b` is closer to the sementic. @gramalingam any comment to the style change? I feel it will slightly change the original design.
If we use `b` in each iteration, it means `b_out` is not accessible until the last iteration is finished and the last `b_in` is assigned to `b_out`. Nevertheless, in the original design, `b_out` looks like an `alias` of `b`, so they are available at the same time.
xykong58(2019-07-04 21:38:32):I removed the changes to operator.md, and made the changes to the comments in defs.cc as suggested.
One issue  is that the "Loop" documentation does not specify the semantics for
zero-trip, i.e., if the loop body is not executred, what will be the outputs. I modified the equivalent C code to make the outputs hold  the values of the initial, and Scan-outputs will still be uninitialized.

linkerzhang(2019-07-12 17:42:22):@gramalingam 
gramalingam(2019-07-12 23:19:37):My personal preference would be to put the above two lines (assignments to b_out and keepgoing_out) _after_ the loop. There needs to be only one "loop-carried" version of each variable, and those seem to be "b" and "keepgoing". Technically, both have the same effect, so this is just a minor nitpick.
skottmckay(2019-07-17 23:04:28):In the zero-trip case, I think it's correct to have the outputs hold the initial values for the loop carried dependencies  as I don't think they should be dropped due to having zero iterations. 

I'd describe the 'scan_outputs' as being empty rather than uninitialized. Outside of the scope of the C example, but the shape returned for a scan_output in the zero-trip case should have at least one dimension with a value of 0 in the first dimension as that represents the number of iterations, and downstream nodes should be able to assume that dimension exists.
skottmckay(2019-07-18 01:04:49):>	 [](start = 0, length = 1)

nit: tab
skottmckay(2019-07-18 01:43:12):However the C example seems flawed in that there's no good way to represent what is happening in the graph example. 

a and b are outer scope values, and there's a new scope for the loop implementation where b_in is provided as a parameter and b_out is a return value. there's no equivalent to that setup with a 'for' loop and that's where it gets confusing. 

Would the following be better?

    void loop_func()
    {
      /* User-defined code (enclosing scope) */
      int a = 3, b = 6;
      bool keepgoing = true; // Analogous to input cond
      /* End user-defined code */
      /* Implicitly-defined code */
      const int max_trip_count = 10; // Analogous to input M
      int user_defined_vals[]; // Imagine this is resizable
      /* End implicitly-defined code */

      /* initialize values that will be passed around during each iteration of the loop.
         also handles zero loop case */
      bool keepgoing_out = keepgoing;  // condition
      int b_out = b; // loop carried dependency
      
      for (int i=0; i < max_trip_count && keepgoing_out; ++i) {
        /* implicitly-defined code */  
        int b_in = b_out;  
        int keepgoing_in = keepgoing_out;

        /* User-defined code (loop body) */
        int my_local = a + b; // Reading values in the enclosing scope is fine
        b_out = a - b_in; // writes fine as b_out is the output value of the loop-carried dependency 
        keepgoing_out = my_local > b_out; 
        int user_defined_val = b_in + b_in;
        /* End user-defined code */

        /*implicitly-defined code */
        user_defined_vals[i] = user_defined_val;
      }

      // my_local = 123; // Can't do this. my_local was defined in the the body
      
      // These below values are live-out from the loop and therefore accessible
      // b_out; keepgoing_out; user_defined_vals; 
      // and user_defined_vals are concated output
    }

skottmckay(2019-07-18 01:45:52):I think this should be called user_defined_val. It's in the Loop implementation not the loop body that these get aggregated (one item per loop iteration) and returned as the user_defined_vals output.
CLAassistant(2019-07-02 07:29:53):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=2146) <br/>All committers have signed the CLA.
yan12125(2019-07-02 11:12:39):Tests timed out on xcode9.3 image with `PYTHON_VERSION=python3 ONNX_ML=0`. I think it's due to instability of Travis CI workers and not relevant to this change.
linkerzhang(2019-07-25 23:48:08):PR #2199 made similar fix and merged. Thank you! @yan12125 
yan12125(2019-07-26 11:59:22):> PR #2199 made similar fix and merged.

Thanks for the info! That's a good news :)
CLAassistant(2019-07-22 21:50:07):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=2148) <br/>All committers have signed the CLA.
prasanthpul(2019-07-22 22:39:52):Please sign the updated CLA
houseroad(2019-07-02 21:53:09):props for the doc
wschin(2019-07-04 06:57:14):```suggestion
                shape(input_2) = ('b', 4)
```
wschin(2019-07-04 06:59:31):nit: do you think using dictionary for `input_dims` and `output_dims` would be better? It could reduce the likelihood of making mistakes.
wschin(2019-07-04 07:02:54):Would you check if the existing shape and the provided shape are compatible? Changing `[-1, 3]` to `[2, 3]` is reasonable but I am not sure if you want to allow `[1, 2, 3]` to `[4, 5, 6]`.
gramalingam(2019-07-22 23:17:50):Hi: this can cause potential problems if the generated symbolic name happens to be the same as the name of a symbolic dimension used elsewhere. I suggest either using "dim_proto.ClearField("dim_param"); dim_proto.ClearField("dim_value")" to clear out both fields or raising an exception here.
gramalingam(2019-07-22 23:20:21):I agree. A dictionary would be better.
BowenBao(2019-09-11 00:33:37):good catch, added.
BowenBao(2019-09-11 00:34:02):indeed, updated to dictionary
BowenBao(2019-09-11 00:34:36):Thanks, I have updated to raising an exception here.
wschin(2019-09-11 15:29:35):pdb? Is it needed?
```suggestion
```
BowenBao(2019-09-11 16:54:22):guess not ... 
CLAassistant(2019-07-08 07:05:30):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=2155) <br/>All committers have signed the CLA.
linkerzhang(2019-07-16 17:35:01):@LeicongLi  would you mind to sign the CLA?
llc-intel(2019-07-17 04:18:37):@linkerzhang Thanks! CLA has been signed.

CLAassistant(2019-07-11 21:07:17):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=2163) <br/>All committers have signed the CLA.
gramalingam(2019-07-11 21:28:45):Hi, can you please update the op documentation to clarify what the shuffle order means? Ideally, it would be great to add an equation of the form below, filling in what goes into "..."
```
    output[i,j,k,m] = input[...]
```
gramalingam(2019-07-15 23:14:52):Thanks for adding the description. I think that the operator documentation needs to be regenerated and committed. Please see: https://github.com/onnx/onnx/blob/master/docs/CONTRIBUTING.md#generated-operator-documentation 

neginraoof(2019-07-18 21:04:43):@ebarsoum could you please review this PR?
gramalingam(2019-07-23 19:17:21):@houseroad : any comments/feedback on this op extension? Thanks.
neginraoof(2019-07-26 16:15:28):@gramalingam @houseroad can we merge this?
gramalingam(2019-07-16 23:15:37):Hi, is this addition showing up in the operators.md? I can't quite see it. I think that for the documentation, we could describe the logic as below:
In the default DCR mode, the output y is computed from the input x as below:
```python
   b, c, h, w = x.shape 
   tmp = np.reshape(x, [b, blocksize, blocksize, c // (blocksize**2), h, w])
   tmp = np.transpose(tmp, [0, 3, 4, 1, 5, 2])
   y = np.reshape(tmp, [b, c // (blocksize**2), h * blocksize, w * blocksize])
```

In the CRD mode, the output y is computed from the input x as below:
```python
   b, c, h, w = x.shape 
   tmp = np.reshape(x, [b, c // (blocksize ** 2), blocksize, blocksize, h, w])
   tmp = np.transpose(tmp, [0, 1, 4, 2, 5, 3])
   y = np.reshape(tmp, [b, c // (blocksize ** 2), h * blocksize, w * blocksize])
```
gramalingam(2019-07-16 23:16:41):If the example is useful, that can be added separately, following the above as "For example, for an input value of …, the output valus is …"
neginraoof(2019-07-17 20:34:14):Sounds good. I modified the description. You should be able to see it on operators.md now
CLAassistant(2019-07-12 03:41:34):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=2164) <br/>All committers have signed the CLA.
linkerzhang(2019-07-16 17:32:48):thank you!
gramalingam(2019-07-15 19:44:54):Close and reopen to trigger CI
gramalingam(2019-07-16 20:15:02):I hadn't noticed that @hariharans29 had already created a PR for shape-inference for the same op at https://github.com/onnx/onnx/pull/2133 : I will talk with him to resolve these PRs.
gramalingam(2019-07-17 20:28:45):I had an offline discussion with Hari. The purpose of the current PR is to introduce unification utilities to help simplify our shape-inference code. It uses RoiAlign to illustrate the usage. Hari offered to reconcile these changes with his own PR. @hariharans29 : please take a look at this PR, thanks.
hariharans29(2019-07-17 20:45:45):Looks good. The simple helpers make it easy to review the overall logic :). I will try and adopt these in my PRs and contribute more helpers if need be. Thanks!
CLAassistant(2019-07-23 07:49:07):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=2168) <br/>All committers have signed the CLA.
wschin(2020-01-24 01:20:23):The operator `Gradient` is now a part of #2314 so this PR can be closed.
wschin(2019-07-18 20:47:04):```suggestion
example, the gradient of Y with respect to H can be done in
```
gramalingam(2019-07-18 21:04:19):change to "the partial derivatives of a specified tensor with respect to some other tensors"? I think of "dy/dx" as the derivative of y with respect to x.
gramalingam(2019-07-18 21:08:32):I suggest replacing "dO/dW" by a valid identifier, like "dO_dW" or "first_order_gradient". 
gramalingam(2019-07-18 21:09:07):Suggest using a valid identifier (as above).
gramalingam(2019-07-18 21:17:50):How? We don't have a way to give a name to the gradient-graph and reuse it. Do you mean that we can have multiple gradient nodes with the same attribute values?
gramalingam(2019-07-18 21:24:03):It might be worth adding an explanatory note somewhere, something like "The gradient operator is slightly different from the standard operators. It is conceptually a higher-order operator that takes a computation-graph as an attribute, but we abuse the attribute mechanism to describe the computation-sub-graph implicitly by listing the inputs and output of the computation-sub-graph in the current graph.'
gramalingam(2019-07-18 21:24:39):Are there typos here? I am a bit confused, because I am not sure what is Y_1, H_1, and the picture below shows W_1 and Z_1. Are you trying to say that it is not necessary for the inputs to be identical to the list xs specified in the attributes?
gramalingam(2019-07-18 21:27:45):If the above is the case, it would be helpful to add that "The implementation can be optimized in the common case where the inputs are identical to the attributes." … because if the inputs are not identical to the attributes, the implementation has to do extra work to create a duplicate of the forward-graph for the actual-inputs.
gramalingam(2019-07-18 21:29:37):change "the attribute" to "the tensors named in the attribute"
gramalingam(2019-07-18 21:37:02):Is it correct to say the following? If xs=[X_1, ..., X_m], then no X_i can depend on X_j (i <> j): that is, there cannot be a dependence-path in the graph from X_j to X_i? Furthermore, do we need to say the list in xs must be complete? If we have Y = X_1 + X_2, is it valid or invalid to specify xs=["X_1"]?
gramalingam(2019-07-18 21:37:53):Why is "y" shown as a list here? It is a single string according to the definition below.
wschin(2019-07-20 22:37:31):No problem.
wschin(2019-07-20 22:42:10):Humm, but `d(dO/dW)dX` would be changed to something like `d_dO_dW_dX` and become less readable.
wschin(2019-07-20 22:43:35):You're right it's misleading. I will remove it.
wschin(2019-07-20 23:05:22):Do you think we need to allow
```
 if the inputs are not identical to the attributes, the implementation has to do extra work to create a duplicate of the forward-graph for the actual-inputs
```
? Looks like it's not necessary.
wschin(2019-07-20 23:06:46):Ok.
wschin(2019-07-20 23:09:48):Yes. The inputs named in `xs` must be independent variables; that is, they cannot be upstream variable of each other.

The list must be complete. Otherwise, we cannot finish a forward path due to missing input (in your example, we cannot compute `Y` without `X_2`).
wschin(2019-07-20 23:11:04):Fixed. Thanks a lot for poiniting this out.
gramalingam(2019-07-23 16:53:13):If we don't support it, that's okay with me (since I can't immediately think of use-cases where we would want to use it). But, then, the documentation should make this explicit (and implementations should check that the input-list and attribute-list match).
gramalingam(2019-07-23 17:17:15):I see your point. But my concern is that some users will copy this blindly and use "dO/dW" as a tensor name. This can lead to potential problems downstream. Technically, the ONNX documentation says that tensor names should be valid C identifiers. If the ONNX checker were to enforce this, that would be somewhat helpful. But it does not seem to do that. (I remember Niklas was trying to add such a check ... but it doesn't seem to exist today … not sure why. Possibly because a number of models were violating this then.) We should probably sort this out one way or the other in the ONNX standard. Until then, I think it would be better to try to stick to the identifier restriction. 

We could use simply use names like D1 and D2 to denote the first and second derivative in the example (and in the text we could say D1 denotes dO/dW, for example).
newling(2019-09-06 12:55:14):This feels to me like an implementation detail, not a definition of what the Gradient Operator does. 
wschin(2020-01-24 01:24:15):Removed. Thanks!
bddppq(2019-07-16 17:24:57):no need to uninstall, your next line will force the version.
gramalingam(2019-07-17 00:26:32):Looks good to me. A pragmatic question: when will we start imposing these requirements? Will it apply to updates of existing ops (e.g., fixing shape-inference-logic error in existing ops) … this might make fixes somewhat expensive. I think it might be better to fix existing ops gradually, as time permits, and not require this for bug-fixes to existing ops.
ebarsoum(2019-07-17 01:48:11):@gramalingam  that is a good question, we will enforce that on current and new PR.  Let's open an issue to track them and ask community for help.
wschin(2019-07-16 23:35:25):Is it `prefer inputs over attributes`? I am fine with both.
ebarsoum(2019-07-17 01:46:59):It is based on the SIG feedback, attributes are easy to optimize.
prasanthpul(2019-07-17 03:49:23):suggest remove this para and instead incorporate it into the first para as commented above
prasanthpul(2019-07-17 03:56:17):Since the document, once published, become our process rather than a proposal for improving the process, suggest replace last sentence with "The ONNX specification includes a core set of operators that enable many models. It is a non-goal to add all possible operators, however more operators are added as needed to cover evolving needs"
prasanthpul(2019-07-17 03:56:59):remove this section as it's covered by the changes to the first para
prasanthpul(2019-07-17 03:59:34):suggest add a section for "4 steps to add an operator" to give an overview of the process before diving into details:
1) Decide whether to add an operator or function
2) Submit PR to propose an operator/function
3) Review of PR by Operators SIG
4) Merging of PR and inclusion in next ONNX release

wschin(2019-07-17 07:16:20):Sounds good.
ebarsoum(2019-07-17 22:48:21):You mean merge both para?
ebarsoum(2019-07-17 22:50:13):`proposing` isn't for the doc, it is for the operator. What the criteria for someone to propose a new operator.
prasanthpul(2019-07-17 22:50:39):just delete this paragraph.
prasanthpul(2019-07-17 23:02:47):I suggest moving these 2 sentences to the first paragraph for better flow
prasanthpul(2019-07-17 23:03:27):thanks for adding these. of course, each of these should have a section below with more details :)
ebarsoum(2019-07-17 23:06:23):I thought the detail section cover the detail?
ebarsoum(2019-07-17 23:07:03):Do you want to redo the entire flow?
prasanthpul(2019-07-18 01:13:54):Not really. Let me submit a quick PR to your fork
hobei(2019-07-18 08:27:33):Based on my experience of implementing BatchNorm using this approach, the current framework lacks to full support. i.e. converting attributes to tensors etc. I think we need to make sure that we add the needed features to make this option suitable, we may even need additional operations to allow function operations to manipulate the tensors correctly
ebarsoum(2019-07-18 17:35:13):@hobei can you propose what is missing?
gramalingam(2019-07-18 17:42:35):@hobei : one could use the "Constant" op to convert an attribute (of the function) into a tensor inside the function body. But I agree that there are other limitations (e.g., doing computation on attributes). One option we were discussing to increase the expressiveness of the function mechanism is to enable registering C++ code that generates the function-body (given access to all attribute values, etc.). Would that be useful? As Emad asked, it would be helpful to identify what is needed.
wschin(2019-07-18 00:12:49):Could you add shape inference and output tests for new types?
linkerzhang(2019-07-18 17:04:30):I would like to add it when we have Alibaba's auto test-gen tool moved into ONNX org later, otherwise, it's duplicating work.
hobei(2019-07-18 08:23:48):A test case (example) with a negative axis value would also be helpful and could be verified with the backend tests.
skottmckay(2019-07-18 10:16:41):> A test case (example) with a negative axis value would also be helpful and could be verified with the backend tests.

Added a test to the shape inferencing tests. Let me know if that's sufficient.
gramalingam(2019-07-23 19:21:16):Wouldn't it be better to bump the op version (opset) number? That would make sense if we treat this as an extension of the op's spec. If we treat this as a bug-fix, that's not needed, but it seems to me that treating it as an extension is somewhat better.
skottmckay(2019-07-23 22:23:07):> Wouldn't it be better to bump the op version (opset) number? That would make sense if we treat this as an extension of the op's spec. If we treat this as a bug-fix, that's not needed, but it seems to me that treating it as an extension is somewhat better.

Currently the ONNX Runtime implementation of Split handles a negative axis so fwiw this is just bringing the shape inferencing into line with that (not that an implementation gets to force changes of the spec though). There are also a number of other ops in the ONNX Runtime implementation where a negative axis is handled even though the spec for the op doesn't explicitly state whether that is allowed (Softmax/LogSoftmax/TopK/Scan/Reduce*), so it looks like this is another gray area of the ONNX spec which has been around a while. #842. 

There are only two places I could see where using a negative value for an axis was explicitly disallowed (Squeeze/Unsqueeze), so it seems like they are in general supported. Maybe the ONNX spec should say that is the case, unless explicitly disallowed, in order to clarify things. Not sure if that change requires an opset bump though given it's just being more flexible (assuming no ops change to explicitly disallow of course). 


gramalingam(2019-07-23 22:40:30):Okay, thanks. Given the comments elsewhere that negative axis should be uniformly supported everywhere, it seems reasonable to treat this as a bug-fix.
CLAassistant(2019-07-23 22:41:04):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=2177) <br/>All committers have signed the CLA.
linkerzhang(2019-07-29 17:31:53):Thank you very much for the change! Please also add more clarification for "axis" attribute in the op definition, so that ONNX users do not need to find the negative axis handling logic from shape inference codes.
jspisak(2019-07-22 00:30:15):Thanks for the comments @prasanthpul. I'll create a dir in the steering committee area as well as add this to the contributor.md file. 
prasanthpul(2019-07-21 06:15:37):I suggest we keep this out of the main ONNX repo and submit to the steering committee repo instead. People developing on/with ONNX don't need to download random company's CCLA submissions. 
prasanthpul(2019-07-21 06:16:27):By submitting into steering committee repo, the steering committee members will automatically be notified. And we dont have to update this list each year after elections
prasanthpul(2019-07-21 06:20:40):Any reason not to add these instructions to CONTRIBUTING.md or to README.md? Otherwise it's not easy for people to find. 
CLAassistant(2019-07-22 00:36:29):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=2184) <br/>All committers have signed the CLA.
jspisak(2019-09-11 12:53:23):@onnxbot test this please
prasanthpul(2020-01-24 21:08:52):close for now. update per LF AI guidelines
prasanthpul(2019-07-22 17:27:48):Let me setup a CODEOWNERS file in steering-committee repo so this step will not be needed. Any PR committing a file into the CCLA directory will tag the steering committee.

This will eliminate an extra manual step and also reduce the number of places we have to keep steering committee membership updated
prasanthpul(2019-07-22 17:28:23):Why does legal need to review each one? if it follows the template, then it should be just adding the github IDs to the whitelist.
houseroad(2019-07-22 22:08:30):Btw, where is the CCLA form I can download?
CLAassistant(2019-07-23 00:39:17):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=2187) <br/>All committers have signed the CLA.
askhade(2019-07-23 02:11:28):FunctionVerification.VerifyFunctionOps test is failing because of a bug in the test itself. I will add a fix for this soon. In the meanwhile please proceed to review the code.
askhade(2019-07-23 18:00:12):> FunctionVerification.VerifyFunctionOps test is failing because of a bug in the test itself. I will add a fix for this soon. In the meanwhile please proceed to review the code.

I will add the fix for this test as a separate PR... I dont want to mix these 2 changes in the same PR. To work around this issue I changed the Input and Output names in the schema from "X" and "Y" to "x" and "y" this resolves the test issue. 
askhade(2019-08-10 00:37:17):@houseroad , @ebarsoum please let me know if you have any questions or concerns
Thanks!
gramalingam(2019-07-23 04:41:10):Is that supposed to be Q_Min or Q_Max?
gramalingam(2019-07-23 04:47:03):I assume that this is not being used currently and that T2 is restricted to one type (until Functions are extended to simplify coding this up)? It may be worth mentioning this in the documentation.
gramalingam(2019-07-23 04:53:30):Minor nit: The "Fused" part is more of an implementation-detail or concern. It would be better to give it a name that describes what the function is intended to do. But I can see naming it is a bit challenging since the name "QuantizeLinear" is already used up. Would "DynamicLinearQuantization" or some name like that work?
askhade(2019-07-23 17:06:54):It is supposed to be Q_Min... What we are doing here is making sure 0 is included in the input range to guarantee that zero can be uniquely represented in the quantized range.
Example: if input range is [-3, -1] then this would adjust max to 0 and the new range will be [-3, 0]
Similarly if the input range is [1, 3] then line:137 will adjust min to 0 so the new range will become [0, 3]
askhade(2019-07-23 17:07:25):Yes, I will add it in the doc
askhade(2019-07-23 17:08:06):Agree... I was struggling with the right name for this so chose to go with Fused... I will change it to DyamicQuantizeLinear 
gramalingam(2019-07-23 22:29:08):Sorry, just noticed that it is used (in the Cast op below), but it is restricted to be uint8? It would be better to say "Currently, this is required to be uint8 (value 2)."
askhade(2019-07-23 23:42:39):My bad ... I also missed it...
gramalingam(2019-07-25 00:55:58):Ideally, we should say "scalar, that is a 0D tensor" in the preceding two lines. Is there any reason we need to say "or 1D tensor"? Let us try to uniformly use tensors with rank 0 for scalars.
askhade(2019-07-25 18:27:09):I added 1D tensor of size 1 to relax the requirement for y_scale and y_zero_point to be "Scalars" in onnx terms... the requirement is really that they be of size 1 .i.e 1 scale and zero point per layer. 
linkerzhang(2019-07-30 22:35:55):this attribute is actually specifying the value of "T2", so, does that mean only one is needed, otherwise, is there a way to verify to ensure the two have no conflict with each other?
gramalingam(2019-07-30 22:56:10):The role of this attribute is the same as that in the cast op: it specifies what type to cast to. We need both, since both type-inference and the runtime will choose the final type based on the attribute value. (Of course, it is not needed right now since only one type is supported, but it will be needed if and when multiple output-types are supported.) But you are right about checking/verifying: we could add a shape-and-type-inference function to check the right values are specified … is there some other way to add a simple check?
gramalingam(2019-07-30 22:56:34):Or, we could omit this for now, since we are not using it currently.
askhade(2019-07-30 22:59:08):No we cannot omit this ... since we need a way to translate T2 to an attribute which Cast can use in the functionBody... plus can we always rely the type info to be present? 

I am in favor of adding a shape inference for this function instead
gramalingam(2019-07-30 23:54:24):No, we won't translate T2 to the attribute. (Usually, the computation order is the other way: the attribute will be used to infer the value of T2.) However, in this case, you can set the attribute value of Cast to be the constant "2", since that is the only case being supported right now, right?
gramalingam(2019-07-30 23:58:31):To add more context: I think that once we have the ability to specify the FunctionBody using a lambda, that function will be the right place to do the check: since we may actually generate different bodies based on the value of this attribute. So, it seems like we could postpone this issue to that extension in whatever is the most convenient way.
askhade(2019-07-31 17:29:39):Eventually when we enable this for multiple data types we will need this attribute... Right now if you both think this may cause confusion then I am ok to remove this and make cast attr as a const.

However I expect when a runtime implements kernel for this function it should verify that T2 and attr values dont conflict. This check does not necessarily have to be in ONNX.
linkerzhang(2019-08-01 01:19:49):My suggestion please: if there's an existing way to do the check, it's ok to keep the attribute, otherwise, we should remove it right now and think of adding it as well as a way to do verification.
askhade(2019-08-03 00:11:33):I will remove it right now... let's add it when it is necessary. 
ebarsoum(2019-08-13 18:51:08):Can you provide the equation or pseudocode of the fused function?
ebarsoum(2019-08-13 18:57:04):Can you add a reference implementation?
CLAassistant(2019-07-23 01:22:05):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=2188) <br/>All committers have signed the CLA.
linkerzhang(2019-07-23 18:08:00):@souptc Thank you! Please sign the CLA.
souptc(2019-07-24 00:54:19):I have signed the CLA through the link on the CI, but still shows Pending. anything else i need to do?
linkerzhang(2019-07-27 00:09:27):@souptc it's showing "chentaMS" does not sign the CLA.
souptc(2019-07-29 01:32:20):Hmm... it become tricky, to login as "chentaMS", it need the verification code by text message, but I didn't enable the global service on my us phone, and i am at China now... so I can't log in...

@linkerzhang , could anyone else take this change? it just one line change.
souptc(2019-07-29 02:18:06):Finally get this done with my wife's help...
CLAassistant(2019-07-23 21:19:09):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=2193) <br/>All committers have signed the CLA.
HectorSVC(2019-07-23 21:19:46):https://github.com/onnx/onnx/issues/2192
linkerzhang(2019-07-24 00:17:37):looks like specifying python 3.6 in travis ymal file does not work as expected. It's still using python 3.7
prasanthpul(2019-07-25 20:44:05):@anderspapitto can you comment? looks like you worked on the mac travis most recently
linkerzhang(2019-07-25 21:10:46):https://github.com/onnx/onnx/pull/2199 is fixing the issue :). Let's see.
linkerzhang(2019-07-25 22:16:51):issue fixed by PR #2199 
askhade(2019-07-25 19:25:48):this works for Linux but is ignored on macOS or Windows
Check this: https://docs.travis-ci.com/user/languages/python/#running-python-tests-on-multiple-operating-systems

askhade(2019-07-25 19:53:43):I think this is where you need to make the change :
https://github.com/onnx/onnx/blob/ba76e4508eecf08797c2c5b7b9ca196fabbeab0f/.travis/before_install.sh#L31

harryskim(2019-07-24 22:50:11):Is there any guidance from the design team for a specific requirement on the definition of "high quality" (e.g. file size, pixel dimensions, etc)? Otherwise LGTM.
prasanthpul(2019-07-24 22:55:33):@jspisak do you have a recommendation?
jspisak(2019-07-25 03:16:48):@prasanthpul - we typically request 300dpi (best if physical printing is required or a vector file which can be saved in any resolution. I would add this language to the requirements.
prasanthpul(2019-07-25 06:03:22):updated
CLAassistant(2019-07-25 20:51:46):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=2199) <br/>All committers have signed the CLA.
wschin(2019-08-05 17:18:46):Overall this PR looks good to me. I have one comment for changing an axis bound in your test.
wschin(2019-08-05 17:07:14):```suggestion
for i in range(x.ndim):
```
maybe?
```
print(np.expand_dims(x, axis=len(x) + 1).shape)
```
produces
```
(3,4,5,1)
```
which is an undefined behavior in ONNX Unsqueeze. Note that
```
print(np.expand_dims(x, axis=len(x)).shape)
```
also produces
```
(3,4,5,1)
```
.
wschin(2019-08-05 17:11:48):I am not sure if this is the valid test. The original tensor is 3-D, so the min axis you can expand is 0 and the max one is 3. Did I miss something? I found what I missed. The axes are output's axis indexes.
JamesAllingham(2019-08-05 17:22:59):I'll make this `x.ndim + 1` if that is okay? Based on your other comment it seems like you are happy that this is defined behaviour. Edit: nevermind, that is my mistake, I'll make your change :)
askhade(2019-08-05 20:17:40):Looks like this file was manually updated... Please use gen_doc.py to create Operators.md and Changelog.md and stat_coverage.py to update TestCoverage.md
(make sure you set environment variable ONNX_ML=0 before building onnx and then run these scripts) 
fdwr(2019-10-23 20:45:26):@askhade : I agree with the message (particularly for CPU), but I'd also point out that on some execution providers (particularly for GPU), it's already too late to throw any kind of error from deep inside the shader execution where the indices are read. So a sane fallback in that case is to either clamp the index or use index 0.
gramalingam(2019-07-27 23:20:37):Thanks for the fix. Just want to note, as a general comment, that (a) The runtime MUST catch all such errors, (b) Shape-inference catches such errors on a best-effort basis. In a number of cases, the required information to catch the error is not available statically, as shape-inference time. I should probably be posting this comment in the discussion in the other issues since there seems to be a confusion about the role of shape-inference and runtime in catching errors.
gramalingam(2019-07-29 05:18:49):The loop termination condition is missing the "<" comparison.
wschin(2019-08-05 16:02:37):What do you mean by `already` supported? If the negative axis was not explicitly mentioned, I'd feel it was not officially supported. If my feeling is valid, I'd suggest to bump their operator versions and consider `negative` axis as a new feature.
skottmckay(2019-08-06 06:40:09):> What do you mean by `already` supported? If the negative axis was not explicitly mentioned, I'd feel it was not officially supported. If my feeling is valid, I'd suggest to bump their operator versions and consider `negative` axis as a new feature.

The shape inferencing code already supported a negative axis, so this PR is largely (with the exception of Concat where support is being added) making the documentation reflect the implemented behavior. 

See https://github.com/onnx/onnx/pull/2177 for some additional discussion. 

FWIW, recently Gather and Split have been updated to add support for negative axis values without operator version changes. Personally this sort of change feels more like a bug fix for consistency.  
gramalingam(2019-08-06 16:41:19):@wschin : this is a gray area (depending on whether we see this as fixing an incompleteness-bug in the specification/documentation or extending the specification), etc.. As discussed in the thread Scott referenced above, it appears there was a decision to uniformly support negative axes quite a while back. Overall, it seems keeping the same version is simpler for almost all cases. For concat, bumping the op version may be desirable if there is an implementation out there that doesn't support negative axes. I guess if someone has a concern in this regard, we could bump it for concat.
wschin(2019-08-07 15:53:23):Are we sure that all other runtimes follow the same rule?
skottmckay(2019-08-09 00:21:19):> Are we sure that all other runtimes follow the same rule?

Which runtimes are you concerned about? 

CNTK seems to have negative axis support. https://docs.microsoft.com/en-us/python/api/cntk/cntk.axis.axis?view=cntk-py-2.7
I believe pytorch does as the model that started the whole negative axis discussion was converted from pytorch: https://github.com/onnx/onnx/issues/814
TensorFlow supports it: e.g. https://www.tensorflow.org/api_docs/python/tf/math/argmax

wschin(2019-08-13 15:44:43):I mean Intel's and NVIDIA's previous run-time engines. There could be also some smaller partners such as [ONNC](https://github.com/ONNC/onnc).
postrational(2019-08-22 11:52:07):We should merge this change to clear up the negative-values question, which was unclear in the ONNX spec. Only question is, whether to bump operator versions or not.

There are definitely runtimes (like Intel's nGraph), which don't support this yet. If we bump versions, then theoretically we would only allow negative values starting from the next operator version. As many runtimes already support this feature implicitly for current versions, this may be more confusing than helpful. Perhaps we don't need to bump it.

I would suggest adding a set of simple **unit-tests** to this PR, which check the negative `axis` values on all ops which should support them.

skottmckay(2019-08-22 16:47:26):Given there's an ONNX release coming up, we should choose between the previously accepted changes for Gather and Split where no op version increment was done, or increment the op version for all the ops where negative axis support is being explicitly added. I don't mind which, but we should decide soon. @linkerzhang what are your thoughts?
gramalingam(2019-08-22 17:14:56):I personally lean towards bumping the op version. The drawback is the "noise" and confusion this may generate (and possibly a negative perception about the stability of op specs in ONNX). If people think that these disadvantages outweigh the advantages, I am okay with not bumping it. (There was a proposal in todays operator SIG meeting to bring this up in tomorrow's ONNX workshop discussion.)
skottmckay(2019-08-22 19:18:26):Let's see if it gets discussed and resolved at the workshop tomorrow. I can add unit tests and bump op versions as needed. 
wschin(2019-09-04 17:39:25):Per my personal feeling, we will increase the version number. Making something from undefined to defined sounds a breaking change.
wschin(2019-09-10 20:23:48):This PR is merged into #2260 and #2281, so please let me close it for now. Feel free to reopen if I miss something. Thank you very much!
BowenBao(2019-08-12 21:08:03):cc @spandantiwari @ebarsoum @linkerzhang @wschin @gramalingam @postrational 
gramalingam(2019-08-15 17:32:44):A question: I believe indices should not have a repeated value, is that correct? That is, we cannot have two or more updates for the same index-location. If so, let us add that to the documentation. (I guess this is common with scatter too, I presume.)
BowenBao(2019-08-15 20:38:08):@gramalingam Thanks for your detailed suggestions! I have updated the doc with clarification. You are right duplicate entries of indices is not supported. 
spandantiwari(2019-08-16 17:18:05):@gramalingam - thanks for the review!
spandantiwari(2019-08-16 17:20:42):@bddppq - for his review.
BowenBao(2019-08-19 22:16:26):@prasanthpul could you include this in ONNX 1.6 milestone as well? Thanks.
spandantiwari(2019-08-13 01:01:58):May be better to not have the reference to TF.
gramalingam(2019-08-15 17:45:40):Suggest extending this to
```
`indices` is an integer tensor. Let k denote indices.shape[-1], the last dimension in the shape of `indices`.
 `indices' is treated as a (q-1)-dimensional tensor of k-tuples, where each k-tuple is a partial-index into `data`.
Hence, k  can a value at most the rank of `data`. When k equals rank(data), each update entry specifies an
update to a single element of the tensor. When k is less than rank(data) each update entry specifies an
update to a slice of the tensor.
```
gramalingam(2019-08-15 18:03:45):Suggest extending this to
```
`updates` is treated as a (q-1)-dimensional tensor of replacement-slice-values. Thus, the
first (q-1) dimensions of updates.shape must match the first (q-1) dimensions of indices.shape.
The remaining dimensions of `updates` correspond to the dimensions of the
replacement-slice-values. Each replacement-slice-value is a (r-k) dimensional tensor,
corresponding to the trailing (r-k) dimensions of `data`.  Thus, the shape of `updates`
must equal indices.shape[0:q-1] ++ data.shape[k:r-1], where ++ denotes the concatenation
of shapes.
gramalingam(2019-08-15 18:29:33):Can you add an iteration loop around the update equation (the second line)? I think we could use numpy's ndindex to express the iteration. Something like below
```python
   update_indices = indices.shape[:-1]
   for idx in np.ndindex(update_indices):
      output[indices[idx]] = updates[idx]
```
You may have to adjust a few things above to make it run, but as pseudo-code, hopefully it will be clear.
gramalingam(2019-08-15 21:17:58):Suggest adding: "The order of iteration in the above loop is not specified. In particular, indices should not have duplicate entries: that is, if idx1 != idx2, then indices[idx1] != indices[idx2]. This ensures that the output value does not depend on the iteration order."
BowenBao(2019-08-15 22:28:23):Thanks! added as well to the sample implementation in backend test scatternd.py. 
fdwr(2019-08-23 04:24:25):     `indices' <- closing quote should be backtick
BowenBao(2019-08-23 18:11:44):Thanks!
ajaysg-zz(2019-08-10 05:15:09):https://github.com/onnx/onnx/issues/1671#issuecomment-444314992

please follow this and add in symbolic_opset9.py. Then onnx is able to export
spandantiwari(2019-08-15 18:16:43):cc: @postrational for review. 
spandantiwari(2019-08-16 17:20:09):cc: @bddppq 
BowenBao(2019-08-19 22:16:47):@prasanthpul could you include this in ONNX 1.6 milestone as well? Thanks.
spandantiwari(2019-08-13 00:45:42):nit: consider saying "floating-point tensors" for clarity.
spandantiwari(2019-08-13 00:47:25):Are we specifying behavior when rank >3? If not, would it make sense to check for that here?
spandantiwari(2019-08-13 00:48:01):Do we want to support complex types (like TF) at this point?
BowenBao(2019-08-13 01:33:39):The shape inferencing covers for all rank >= 2 cases, and errors out when rank < 2. I do forgot to add checks for sizes of the inner-most 2 dimensions. Thanks. 
BowenBao(2019-08-13 01:33:48):Currently we are not seeing model use cases for complex types. We can extend dtype support in future opsets when necessary.
wschin(2019-08-13 15:38:50):Please print out the values of `mat_w` and `mat_h`.
wschin(2019-08-13 15:40:23):Very nice design which can correctly produce scalar shapes.
wschin(2019-08-13 15:41:13):Please add the output shape (i.e., scalar) when the input is 2-D.
BowenBao(2019-08-13 17:20:26):Added, thanks!
postrational(2019-08-23 15:16:17):`det_impl` should be replaced with `np.linalg.det` in these examples to explain what it does.
BowenBao(2019-08-23 18:11:34):done, thanks!
hariharans29(2019-08-13 20:10:39):CC: @gramalingam @ebarsoum 
gramalingam(2019-08-15 18:43:00):I am not sure about this. "undefined" is not a valid type (in most contexts). If there is some context where we need to use the "undefined" type, it would be better to clarify the situations where this is permitted and used: the use-cases/scenarios where we need this.
hariharans29(2019-08-15 20:41:00):@gramalingam - Well undefined is still valid within the spec of ONNX isn't it ?
skottmckay(2019-08-16 01:04:55):Is there something that says TypesWrapper should only have values that are always valid in a model? The change is to add a value that is defined in the proto which matches the comment:
  // DataType strings. These should match the DataTypes defined in onnx.proto	

tensor<undefined> seems valid as a placeholder that's treated as 'must be replaced by type inferencing prior to running'. Only real use-case I'm aware of is for a subgraph input, as that needs some type info like a rank, but not necessarily the element type as that can be inferred from the parent graph. 
hariharans29(2019-08-16 23:11:50):> Is there something that says TypesWrapper should only have values that are always valid in a model? The change is to add a value that is defined in the proto which matches the comment:
> // DataType strings. These should match the DataTypes defined in onnx.proto
> 
> tensor seems valid as a placeholder that's treated as 'must be replaced by type inferencing prior to running'. Only real use-case I'm aware of is for a subgraph input, as that needs some type info like a rank, but not necessarily the element type as that can be inferred from the parent graph.

I agree. The intention of this map only seems to be holding the data type strings for the data types in onnx.proto. If that is indeed the actual intention, this map's contents shouldn't define which types are valid in a model and which are not.
gramalingam(2019-08-16 23:23:21):Doesn't this generalization now allow users to define a op that takes as input a "tensor(undefined)"? If we are introducing this generalization, it seems worth clarifying where it is legal and where it is not. We can start conservatively, allowing it in special cases where it seems necessary and useful.
hariharans29(2019-08-17 00:22:40):> Doesn't this generalization now allow users to define a op that takes as input a "tensor(undefined)"? If we are introducing this generalization, it seems worth clarifying where it is legal and where it is not. We can start conservatively, allowing it in special cases where it seems necessary and useful.

Thanks Rama. That is a good suggestion. I have a question though - I went over the IR.md file and couldn't find a mention of "undefined" type being prohibited anywhere for graph inputs. Is is=t implicitly stated somewhere ? (Sorry if I missed it)

EDIT: Ah I think I found it:

"The following data types are supported by ONNX for inputs and outputs of graphs and nodes as well as the the initializers of a graph."

The table that follows this statement doesn't contain "undefined"

And it is also mentioned as below in the onnx.proto file - 

  message Tensor {
    // This field MUST NOT have the value of UNDEFINED
    // This field MUST have a valid TensorProto.DataType value
    // This field MUST be present for this version of the IR.
    optional int32 elem_type = 1;
    optional TensorShapeProto shape = 2;
  }
linkerzhang(2019-08-16 04:50:01):Merging ONNX and ONNX-ML IR was agreed in infra SIG meeting on 7/1/2019 as https://github.com/onnx/sigs/blob/master/infra/meetings/001-20190701.md. Especially for types, the difference between DNN and traditional ML can/will be shown by different op domains. there's no need having two different IR formats for the differentiation.
linkerzhang(2019-08-17 06:02:23):@bddppq  IR version is bumped at most once for one ONNX release (following opset version bump policy). @gramalingam has already bumped the IR version to 6 when adding sparse tensor related support, so that, this PR does not need to bump IR version any more, make sense?
linkerzhang(2019-08-30 09:33:13):@prasanthpul given this PR is not merging ONNX-ML with ONNX, I'm not updating the docs to remove ONNX-ML concept.
spandantiwari(2019-08-19 20:50:33):I am wondering if we still need this comment here? If this can be optional indeed, we can use Sequence to export heterogeneous tensors of different datatypes also by keeping `elem_type` unspecified. 
wschin(2019-08-20 21:02:41):Can you remove this line?
wschin(2019-08-20 21:03:09):This line looks redundant.
wschin(2019-08-20 21:07:21):Supporting heterogeneous tensors can probably be done by adding another type `Tuple`. Sequence stills sounds like a homogeneous type like `IEnumerable<ElementType>` in C#. This `Sequence` is for containing non-tensor elements with the same type like the predicted probabilities of all classes, `[{"ClassA", 0.5, "ClassB", 0.25, "ClassC", 0.25}, {"ClassA", 1, "ClassB", 0, "ClassC", 0}]`.
wschin(2019-08-20 21:15:40):```suggestion
    //        support classical ML operators. DNN operators SHOULD restrict their input
```
wschin(2019-08-20 21:16:37):```suggestion

    // The type of sparse tensor. This type is different than tensor.
```
wschin(2019-08-20 21:17:00):```suggestion
    //        support classical ML operators. DNN operators SHOULD restrict their input
```
gramalingam(2019-08-20 23:27:47):I believe Spandan is talking about an unbounded list/sequence, but where the elements don't have the same type. I think there are some details concerning such a type worth sorting out first. E.g., do we just want a list of tensors of different types, or do we want truly heterogeneous lists (a list where some elements are tensors, other elements are lists of tensors, other elements are maps, etc.)? I guess it should be the later to be truly general-purpose. More importantly, we would need support for some form of dynamic-cast from untyped values to make use of such a type. I think it would be helpful to have a more comprehensive proposal (e.g., take a simple example with heterogeneous tensors and see what's required to support it).
gramalingam(2019-08-20 23:28:28):In short, I think we can do that generalization separately, so that this PR can be kept simple.
spandantiwari(2019-08-21 18:59:46):Yes, I think hetergeneous lists is a use case that we need to target. But I agree that it can designed separately to keep this PR simple.
linkerzhang(2019-08-22 01:08:37):no, this can't be removed.
linkerzhang(2019-08-22 01:09:18):no, it's not. see line 520 before.
prasanthpul(2019-08-26 23:06:19):extra characters at end
prasanthpul(2019-08-26 23:24:38):extra characters at end
prasanthpul(2019-08-26 23:24:42):extra characters at end
prasanthpul(2019-08-26 23:24:58):Removed on purpose?
prasanthpul(2019-08-26 23:27:45):There is a discussion to consider renaming this from Sequence to List or something like that
linkerzhang(2019-08-30 09:16:28):these files were auto-generated.... :) let me double check.
wschin(2019-09-03 22:10:56):If we need heterogeneous types in one single variable, I'd personally consider a new type `Tuple` instead of `Sequence`. `Sequence` is more like a collection type where elements are not stored in consecutive memory blocks.
CLAassistant(2019-08-20 08:19:40):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=2246) <br/>All committers have signed the CLA.
wutiantong(2019-08-21 02:12:20):This PR fix #2198
wschin(2019-08-21 04:52:25):Thanks a lot for this fix. Could you please add a test to avoid the problem?

[Update] An initializer is not necessarily an input. Why do you add that initializer as an input?
wutiantong(2019-08-21 06:33:31):> Thanks a lot for this fix. Could you please add a test to avoid the problem?
> 
> [Update] An initializer is not necessarily an input. Why do you add that initializer as an input?

It seems to be unavoidable for current ir.h : 
`struct Node` need to invoke `addInput()` with a `Value *` for each initializer,
and `Value *` must be held by some node's `outputs_` ( by invoking `addOutput()` ),
however, graph's `input_` node is the only choice.

I suppose, when exporting graph, trace back inputs made with this way and drop them.
Or maybe we can extend `struct Node` with a new member to hold initializers' `Value *`, obviously, some more adjustments will be involved.
spandantiwari(2019-08-21 23:38:33):
> It seems to be unavoidable for current ir.h :
> `struct Node` need to invoke `addInput()` with a `Value *` for each initializer,
> and `Value *` must be held by some node's `outputs_` ( by invoking `addOutput()` ),
> however, graph's `input_` node is the only choice.
> 

Yes, this is a larger issue. That is, the graph IR here is mostly following ONNX IR v3, where it assumes each initializer MUST be an input. That is no longer true in IR v4 and therefore the graph IR might not be able to handle such situations. 
The solution is to fix this graph IR grounds up. It may take a larger fix as you are alluding to.


Zehaos(2020-04-23 07:48:37):Any update?
gramalingam(2019-08-22 05:01:26):Re. updating ops like concat/split: I feel it may be better to introduce new ops (e.g., ConcatSequence) allowing both the older form and newer form. Once implementations stabilize (e.g, if it becomes clear that there is no cost/overhead between the two versions), if necessary, we can deprecate the older form. 
spandantiwari(2019-08-23 18:29:24):> Re. updating ops like concat/split: I feel it may be better to introduce new ops (e.g., ConcatSequence) allowing both the older form and newer form. Once implementations stabilize (e.g, if it becomes clear that there is no cost/overhead between the two versions), if necessary, we can deprecate the older form.

I agree with this.
spandantiwari(2019-08-23 18:36:10):This is relevant to this PR and maybe even https://github.com/onnx/onnx/pull/2244 which brings in Sequence and Map types in ONNX. Do we want to rethink the name `Sequence` for the datatype and these new ops, given that it is also commonly used for data "sequences" for RNN models? This may cause confusion, if only initially, between the two because the `Sequence` type used here is not really what will be used for sequences for RNN models. 
BowenBao(2019-08-23 20:09:11):> This is relevant to this PR and maybe even #2244 which brings in Sequence and Map types in ONNX. Do we want to rethink the name `Sequence` for the datatype and these new ops, given that it is also commonly used for data "sequences" for RNN models? This may cause confusion, if only initially, between the two because the `Sequence` type used here is not really what will be used for sequences for RNN models.

I agree. There is also an existing operator named `ReverseSequence`, which is unrelated to the `Sequence` type.
BowenBao(2019-08-24 00:11:23):> Let me leave one more comment

Thanks for your comments! These are very helpful suggestions. 
BowenBao(2019-09-03 22:36:38):@prasanthpul - can we include this in 1.6?

@linkerzhang @houseroad @postrational - for review
BowenBao(2019-09-07 00:46:27):cc @wschin for review as well.
gramalingam(2019-09-12 19:24:35):Ideally, we should make all the sequence ops (except the Concat and Split versions) generic: that is, work for sequences of all kinds, not just sequences of tensors. Unfortunately, our type constraint specification language does not let us specify this currently. The type-constraint specification string allows us to say only things like "sequence(tensor(int))" and we need to generalize this to allow us to say "sequence(tensor(T))" where T denotes a type-variable (either constrained by other constraints or not). It would be great if we could do this, either as part of this PR or as a separate PR, depending on effort required.
BowenBao(2019-09-13 21:05:59):Thanks for your review @gramalingam !
gramalingam(2019-08-22 00:44:37):I don't think propagateElemTypeFromInputToOutput will work … that assumes that the output type is a Tensor, which is not the case here. We need another utility function for sequence here.
gramalingam(2019-08-22 00:46:18):Are indices zero-based? Is the position required to be <= current length?
gramalingam(2019-08-22 00:47:36):As above. We need a different utility function for sequences, which are different from tensors.
BowenBao(2019-08-22 22:11:41):Updated definition for position. 
shinh(2019-08-23 23:02:32):IIUC, this corresponds to `np.concatenate`. I wonder if it makes sense to support `np.stack` as well. `np.stack` could be implemented by `Loop`-ing for the input sequence, `Unsqueeze` in each tensor in the sequence, and `ConcatFromSequence`, but I think this is a bit tedious. I think one solution is adding an attribute like `new_axis=false`. If this is true, this operator behaves like `np.stack`
shinh(2019-08-23 23:04:26):Similar to my comment on concat, I think it'd be verbose to realize `list(a_tensor)` with this op. IIUC, we need to loop over the output sequence and apply Squeeze.
shinh(2019-08-23 23:31:04):I think we'll want to take `start`, `end`, and `step` like `Slice` op?
BowenBao(2019-08-24 00:02:16):Thanks! That makes sense, adding `new_axis` as attribute default to `0` to support both `concat` and `stack`. 
BowenBao(2019-08-24 00:07:47):Added attribute `keepdims` default to `1`. Also making input `split` optional, so that if not provided the split size is 1. `keepdims` will be ignored if `split` is provided, since the only case we need `keepdims` is when split size is 1.
BowenBao(2019-08-24 00:10:38):That will probably be a separate operator(or function). This operator returns `Tensor`, while the `Slice` equivalent returns `Sequence`. 
shinh(2019-08-24 02:50:39):Ah yeah, that makes sense!
gramalingam(2019-08-27 23:04:37):It would be better to generalize this so that it takes an input of type "Sequence<T>" and propagates the type T to the output, no matter whether T is a Tensor type or some other type (like another Sequence type, for example). The version below seems restricted to the case where "T" is a Tensor type. I think the more general version is even simpler, since we just must copy the type T.
gramalingam(2019-08-27 23:07:32):On second thoughts: of course, may be it is a bit more complicated since we may want to copy the type without the shape. We could do that using a recursive utility function to omit any shape information from the copied type. But it still seems better to make it general purpose.
gramalingam(2019-08-27 23:08:42):Similar to previous comment. It would be better to generalize this to "copy input type to output type without shape" so that it works for all recursive types (including "Sequence<Sequence<Tensor<float>>>"
gramalingam(2019-08-27 23:13:38):We should have an "else" that fails if the inferred type and existing type are not of the same kind.
gramalingam(2019-08-27 23:15:39):I think we should just do "checkShapesAndTypes(inferredElemType, existingElemType)" at this point instead of the code below … that will work correctly when we have types such as "Sequence<Sequence<Tensor>>"
gramalingam(2019-08-27 23:20:00):Change to "mergeShapesAndTypes(inferredType.sequence_type().elem_type(), existingType->mutable_sequence_type()->mutable_elem_type())" and delete the previous function ... so that it will work for "Sequence<Sequence<...>>"
BowenBao(2019-09-03 21:14:25):makes sense, updated.
BowenBao(2019-09-03 21:15:15):checks added
gramalingam(2019-09-03 23:12:03):I think this check has to be moved up before copying the shape from input 0.
spandantiwari(2019-09-09 21:19:03):Nit: Theoretically it is possible to have a `Sequence` of other types, such as `Map` or `Sequence`. It may be helpful to add a simple clarification that this op is intended for `Sequence` of `Tensor` only (it is implied from the phrasing, but I suggest being explicit). 
spandantiwari(2019-09-09 21:20:38):nit: space needed after 'scalar', i.e. 'scalar ('
spandantiwari(2019-09-09 21:22:02):nit: phrasing. 'Keep the split dimension or not. Default 1, which means we keep the split dimension. If input 'split' is specified, this attribute is ignored.'
BowenBao(2019-09-09 22:33:11):thanks, fixed.
BowenBao(2019-09-09 23:28:43):Yes we need to be explicit on types. The input type constraint shows `sequence<tensor<elem_type>>`, that should be enough .. 
shinh(2019-09-10 00:14:46):I think the `elem_type` of `pos` and `pos_at` should be int64, not float (1):
```
>>> [(i.name, i.type.tensor_type.elem_type) for i in onnx.load('backend/test/data/simple/test_sequence_model1/model.onnx').graph.input]
[('X', 1), ('Y', 1), ('Z', 1), ('pos', 1), ('pos_at', 1)]
```
shinh(2019-09-10 00:58:25):Here `x` has float64 dtype but `make_graph` sets float32.
shinh(2019-09-10 01:25:01):How about making this an optional input for consistency with `SequenceInsert`? I think it should behave like `SequenceErase(seq, -1)`.
BowenBao(2019-09-10 23:50:34):Good catch, this is indeed a bug. Thanks @shinh 
BowenBao(2019-09-10 23:51:04):makes sense, updated.
gramalingam(2019-09-12 18:48:25):What happens if the sum of entries in split is not equal to length of input tensor?
gramalingam(2019-09-12 18:53:37):The explanation of what happens if input split is not provided is missing from the documentation. Suggest adding it.
gramalingam(2019-09-12 18:56:02):Change last line to "If input split is specified and has a value different from a scalar-value of 1, this attribute is ignored."
gramalingam(2019-09-12 19:48:52):Sorry: first, we need to decide what to do in the above situation (if input split happens to be a scalar-value 1 and keepdims attribute is specified as false), because this is a potentially ambiguous situation. Once we decide, we should clarify this clearly in the documentation. Obviously, it impacts the shape-inference code also.
gramalingam(2019-09-12 20:16:44):It would be better to rename this function since it has nothing to do with sequences. I suggest using a name like "UnionShapeInfo". (We actually merge or combine shapes two different ways in different contexts: in some other places we take the "intersection" of shapes and here is a "union" of shapes. So, using the word "union" would also be helpful.)
gramalingam(2019-09-12 20:17:52):Basically, this represents "target-shape = Union (target-shape, source_shape)".
gramalingam(2019-09-12 20:18:43):Adding a comment-line as above would be helpful to document the function.
gramalingam(2019-09-12 20:19:30):And it may also be worth moving it to shape-inference.h file
BowenBao(2019-09-12 23:59:01):Thanks that's a good catch. That is an invalid case and errors out. Shape inference code actually covers this scenario, I'm updating the doc to add this as well.
BowenBao(2019-09-13 00:03:53):Actually it is defined as the opposite. Regardless of what value input 'split' carries, as long as it is provided, 'keepdims' is ignored. There is no ambiguity from the definition itself. 
BowenBao(2019-09-13 00:04:44):makes sense. Changing name and adding comments.
gramalingam(2019-09-13 16:59:22):Thanks for renaming it. I still think that this function has nothing specific to "Sequence". It deals only with Tensor type and its shape. So, I think that the "ToSequence" part of the name can be dropped.
gramalingam(2019-09-13 17:09:53):If we have zero inputs, we will have a problem, since we cannot infer the output type (which is why "SequenceEmpty" needs a "dtype" attribute). The default interpretation for a variadic input is 1-or-more inputs. So, this case should not happen for a valid model. May be a comment that this cannot happen or fail_type_inference may be helpful.
wschin(2019-09-13 21:46:28):Can I stack [3, 2]-tensor and [5, 4]-tensor? Do you have any constraints in sequence elements' shapes?
wschin(2019-09-13 21:47:23):[nit] I don't think we need this many tensors, especially complex64 and complex 128.
wschin(2019-09-13 21:48:22):```suggestion
<dd>Position of the tensor in the sequence. Negative value means counting positions from the back. Accepted range in `[-n, n - 1]`, where `n` is the number of tensors in 'input_sequence'. It is an error if any of the index values are out of bounds. It must be a scalar(tensor of empty shape).</dd>
```
wschin(2019-09-13 21:56:18):Very nice description.
wschin(2019-09-13 21:58:03):```suggestion
        # 3. SequenceAt(1):                 -> z
```
wschin(2019-09-13 22:01:16):Future task: we need for single operator. There are serveral tests at grahp level but we need single-node test for fine-level tests. The type of sequence can be Python list, `[x, y, z]`, where `x`, `y`, `z` are numpy tensors.
wschin(2019-09-13 22:03:07):```suggestion
        elem_shape_denotation=None,  # type: Optional[List[Text]]
```
wschin(2019-09-13 22:08:26):It should be a TypeProto but I don't know if the current framework allows it.
wschin(2019-09-13 22:09:55):```suggestion
          const size_t numInputs = ctx.getNumInputs();
```
wschin(2019-09-13 22:15:19):```suggestion
            "Tensors.",
```
gramalingam(2019-09-13 22:18:50):Agree. We don't have any existing examples for doing this. We could use a string-based language (as in the specification of input/output types of operators) for specifying this.
BowenBao(2019-09-13 22:26:37):No, I believe the constraints are the same with np.concatenate. This is taken from the Concat spec. I'll update it then.
BowenBao(2019-09-13 22:51:20):The current proto doesn't support it. It's good to add it once we are extending the functionalities further to support nested sequences. 
linkerzhang(2019-08-22 23:06:08):thank you!

Please check this doc https://github.com/onnx/onnx/blob/master/docs/AddNewOp.md and specifically follow this step "Update the documentation and generate the test data. 
Running the script to update the doc and generate the test data." to update docs and generate test data accordingly and submit them (doc update and test data) in this PR.
wschin(2019-09-04 17:38:24):@neginraoof, could you please rerun the document/test generation script?
daquexian(2019-08-25 12:08:42):I'm on a new computer now, and I cannot re-generate the docs due to https://github.com/onnx/onnx/issues/2049 (The script tried to downloads opset 11 models which don't exist, and I'm tired to find the workaround once again). I think it deserves more attention than it receives now. At least it is easy to upload the missing models :|
daquexian(2019-08-26 01:53:06):I have re-generated the docs
hariharans29(2019-08-28 06:00:48):Should I make a similar change for Gather_Nd. Alternatively, could you please include Gather_Nd when the op gets in (hopefully tomorrow) in this PR? That way all the negative indices spec updates can go in together ? :)
neginraoof(2019-08-28 15:01:13):@hariharans29 I can add GatherND in this PR
hariharans29(2019-08-29 18:48:52):> @hariharans29 I can add GatherND in this PR

Thanks @neginraoof. I actually had to make some formatting changes in the doc, so I went ahead and made the Gather_ND spec support negative indices (based on your change here). As part of your change, I think you can refine the sentence in GathertND if you want. Thanks!
neginraoof(2019-08-30 16:11:17):@hariharans29 Thanks, I just rebased. The docs are consistent now.
neginraoof(2019-09-04 20:09:24):@wschin Sure. I updated the PR as requested.
neginraoof(2019-09-06 04:03:38):cc @spandantiwari @ebarsoum @gramalingam @lara-hdr
neginraoof(2019-09-09 20:33:24):cc @gramalingam @ebarsoum  please review
BowenBao(2019-08-26 21:35:54):```suggestion
    def export_gather_neg_indices():  # type: () -> None
```
BowenBao(2019-08-26 21:36:08):same as above
BowenBao(2019-08-26 21:37:23):```suggestion
    def export_gather_elements_neg_indices():  # type: () -> None
```
BowenBao(2019-08-26 21:37:34):same above
BowenBao(2019-08-26 21:43:00):```suggestion
            "Tensor of int32/int64 indices, of any rank q. All index values are expected to be within bounds [-s, s-1] "
```
BowenBao(2019-08-26 21:53:43):Where is the update for `GatherElements` and `ScatterElements`?
neginraoof(2019-08-26 22:26:59):Missed to include the file. Just pushed it.
BowenBao(2019-08-26 22:42:02):```suggestion
            "Tensor of int32/int64 indices, with the same rank r as the input. All index values are expected to be "
```
wschin(2019-08-31 00:51:52):Could you please add some equations to explain output? For example, when `axis=0`, we have
```latex
k = indices[i_{0}, …, i_{q-1}]
output[i_{0}, …, i_{q-1}, j_{0}, …, j_{r-2}] = input[k , j_{0}, …, j_{r-2} ]
```
and
```
output[i_{0}, …, i_{q-1}, j_{0}, …, j_{r-2}] = input[j_{0}, k, j{1}, …, j_{r-2} ]
```
when `axis=1`.
wschin(2019-08-31 00:56:32):```suggestion
<dd>(Optional) Axis along which one-hot representation in added. This attribute indicates the added axis in the output tensor. Default: axis=-1. axis=-1 means that the additional dimension will be inserted as the innermost/last dimension in the output tensor.</dd>
```
wschin(2019-08-31 00:58:17):Please add equations such as
```
output[i, j, k, input[i, j, k]] = 1 for all i, j, k and 0 otherwise.
```
if `axis = -1`, and
```
output[input[i, j, k], i, j, k] = 1 for all i, j, k and 0 otherwise.
```
when `axis=0`.
neginraoof(2019-09-03 22:41:33):Done
neginraoof(2019-09-03 22:41:54):Done
neginraoof(2019-09-03 22:42:00):Done
wschin(2019-09-04 17:20:21):```suggestion
output[i_{0}, …, i_{q-1}, j_{0}, …, j_{r-2}] = input[j_{0}, k, j_{1}, …, j_{r-2} ]
```
wschin(2019-09-04 17:27:45):Please add something like
```
        # print(y) produces
        # [[1.0, 1.1, 2.1, 4.0, 5.0]]
```
wschin(2019-09-04 17:28:07):Please add something like
```
        # print(y) produces
        # [[1.0, 1.1, 2.1, 4.0, 5.0]]
```
wschin(2019-09-04 17:30:34):```suggestion
      the range [-depth, depth-1] (boundaries are inclusive) will result in one-hot representation with all 'off_value' values in the
```
spandantiwari(2019-09-06 23:22:32):nit: space after sentence end, i.e. ' in the output tensor. Default'. 
spandantiwari(2019-09-06 23:26:41):This sentence is not phrased well. We should correct it.
houseroad(2019-09-11 17:53:03):Fix the conflict?
wschin(2019-09-11 22:27:35):The broadcasting file is referenced in many operators' signatures, for example, https://github.com/onnx/onnx/blob/master/docs/Operators.md#add. I am not sure if we also need to update a C string somewhere in def.cc.
prasanthpul(2019-09-11 22:31:43):@wschin yes, it is referenced as a relative link by the doc gen srcipt. If we do the step 2 change (update generation of operators.md) then it will be a non-issue. Alternatively need to update it now to reference the subdirectory and then update it again later to remove the subdir from the path.

My preference is to wait on merging this for now and try to get the phase 2 done too. I've updated title to WIP to reflect this shouldn't be merged yet
askhade(2021-10-08 17:08:36):@prasanthpul : Is this PR still relevant? Is there any plan for phase 2? What is the motivation for this work?
CLAassistant(2019-08-28 20:57:52):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=2266) <br/>All committers have signed the CLA.
gramalingam(2019-08-28 21:06:51):Shape-dimension with neither dim-param not dim-value set are allowed. So, we should not fail in this case.
gramalingam(2019-08-28 21:08:23):Should we check if the parameter string is a valid value? The standard says it should be a valid C identifier, but people have been using other strings (like "?" etc.). So, there is a potential compatibility concern for some existing models.
snnn(2019-08-28 21:11:15):Why it is allowed? Should it have something?
gramalingam(2019-08-28 21:16:13):If it has neither, it is interpreted as an unknown dimension. A dim-param is useful if we want to indicate that several different dimensions have the same value (or to give a meaningful name). If an exporter creates multiple dim-params of unequal values, it has to create unique names for them. Shape inference also uses this feature to avoid creating unique names.
snnn(2019-08-28 21:20:21):> If an exporter creates multiple dim-params of unequal values, it has to create unique names for them

I think it should be fairly easy for converters to do so .  I'm wondering if the rule is clear in the spec. If the spec explicitly says it's optional, I'll revoke this PR. 

gramalingam(2019-08-28 22:16:59):Please see: #681 (comment) which was the starting point for the decision to allow both to be not-set. This has been clarified in the documentation/spec also: please see: https://github.com/onnx/onnx/blob/master/docs/IR.md#static-tensor-shapes .
gramalingam(2019-08-28 22:21:41):But the check for negative dim-value may still be useful.
hariharans29(2019-09-05 22:53:18):@ebarsoum @gramalingam  - @stevenlix said he wanted this logic. So could you kindly take a look when you have a couple of minutes please ?
hariharans29(2019-09-12 22:33:17):@gramalingam  - thanks for reviewing. I resolved conflict with master. Please let me know if you have any concerns merging. 
CLAassistant(2019-08-30 01:48:01):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=2271) <br/>All committers have signed the CLA.
CLAassistant(2019-08-30 18:51:27):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=2276) <br/>All committers have signed the CLA.
CLAassistant(2019-08-31 17:27:29):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=2278) <br/>All committers have signed the CLA.
wschin(2019-09-04 17:36:13):What's the relation between https://github.com/onnx/onnx/pull/2207 and this PR?
neginraoof(2019-09-04 19:19:17):@wschin I just saw this PR. Looks like it does not increase the version number, and it' missing tests for negative axes. This PR also includes most recent ops. I can merge that PR and make sure those fixes are included here.
skottmckay(2019-09-06 04:03:25):> @wschin I just saw this PR. Looks like it does not increase the version number, and it' missing tests for negative axes. This PR also includes most recent ops. I can merge that PR and make sure those fixes are included here.

Please do. Can you also rev the Split opset for consistency? Negative axis support was added there recently without that happening (https://github.com/onnx/onnx/pull/2177).


neginraoof(2019-09-06 19:11:43):cc @BowenBao @spandantiwari to review 
spandantiwari(2019-09-06 23:36:58):An FYI comment for all - It was discussed during the SIG/Operators WG in ONNX workshop that it is safer option to bump up the opset version of these ops now that the spec is clear that negative axes are allowed.
wschin(2019-09-10 15:56:17):> r = rank(input). [](start = 118, length = 17)

Very nice comment.
wschin(2019-09-10 15:57:38):> A negative value means counting dimensions from the back. Accepted range is [-rank, rank-1]. [](start = 27, length = 93)

If the old description allows negative axis, the old signature should still allow it. My feeling is that we should not change something from defined to undefined in existing operator.

[Update] Ah, this is the latest Split and you didn't change any sementics. #Resolved
wschin(2019-09-10 16:02:48):>input [](start = 93, length = 5)

input-->data
Please use the tensor names below.
wschin(2019-09-10 16:03:10):>input [](start = 93, length = 5)

input-->data
wschin(2019-09-10 16:05:28):>[-rank, rank-1] [](start = 105, length = 15)

where rank is the rank of inputs.
wschin(2019-09-10 16:08:48):>input [](start = 128, length = 5)

data-->input.
Please also double-check other places. Thank you.
wschin(2019-09-10 16:13:55):>OneHot-11"></a>**OneHot-11**</a> [](start = 13, length = 32)

We don't need to bump the version of OneHot because the default "axis" value in OneHot-9 is already "-1". It means negative axis is explicitly allowed.

[Update] My bad. I see that the `indices` wasn't negative before.
wschin(2019-09-10 16:15:53):>input [](start = 160, length = 5)

input->data
wschin(2019-09-10 16:16:33):>input [](start = 160, length = 5)

data
wschin(2019-09-10 16:18:34):>input [](start = 160, length = 5)

input->data.
Please double-check all reduction's signatures. Thanks.
wschin(2019-09-10 16:20:35):>input [](start = 129, length = 5)

data
wschin(2019-09-10 16:26:59):It's really nice that you discarded the change to Split-2 and add this new one to allow negative axis!
wschin(2019-09-10 16:27:23):>rank [](start = 106, length = 4)

[-r, r-1] where r = rank(input).
wschin(2019-09-10 16:27:37):>input [](start = 158, length = 5)

data
wschin(2019-09-10 16:28:28):>input [](start = 162, length = 5)

data
wschin(2019-09-10 16:33:11):>argmin_use_numpy [](start = 9, length = 16)

Your PR is already great. Just wondering if you can append something like
```
# The content of result is
# [1, 2, 3, 4]
```
to be more specific to the reader.
shinh(2019-09-17 03:11:47):I think this check does not follow the definition of `Softmax`? The document of `Softmax` says "Input does not need to explicitly be a 2D vector; rather, it will be coerced into one".
JamesAllingham(2019-09-06 16:49:08):Should I also be adding a converter to upgrade recurrent ops from version 7 to 11? (Like those [here](https://github.com/onnx/onnx/tree/master/onnx/version_converter/adapters)?) I assume so, but I haven't seen any docs referring to writing converters, so any advice would be much appreciated.
wschin(2019-09-10 20:18:38):> Should I also be adding a converter to upgrade recurrent ops from version 7 to 11? (Like those here?) I assume so, but I haven't seen any docs referring to writing converters, so any advice would be much appreciated.

Maybe https://github.com/onnx/onnx/blob/57372f39631fddc720287a4703040c0551d4ee48/docs/VersionConverter.md? I will take a closer to your changes soon. They are not small, so we need to be careful.
JamesAllingham(2019-09-11 07:50:55):> 
> 
> > Should I also be adding a converter to upgrade recurrent ops from version 7 to 11? (Like those here?) I assume so, but I haven't seen any docs referring to writing converters, so any advice would be much appreciated.
> 
> Maybe https://github.com/onnx/onnx/blob/57372f39631fddc720287a4703040c0551d4ee48/docs/VersionConverter.md? I will take a closer to your changes soon. They are not small, so we need to be careful.

Thanks for pointing to that doc, I missed it somehow. I'll take a look at writing a converter. I think that it would be great if the doc outlined the steps required for implementing a converter & linked to a good example of a PR adding a coverter, so I'll also try and add that.

I do understand that these are not insignificant changes, so I am more than happy for you to take your time reviewing this. I'm also more than happy to discuss any issues that come up and make changes as necessary. 
JamesAllingham(2019-10-22 15:13:50):@matteosal
gramalingam(2019-10-22 17:09:23):A couple of comments:
(a) The breaking change for num_directions is a potential concern. Why not make this also controlled by an attribute so that, by default, the old behavior is preserved?
(b) The scan operation (https://github.com/onnx/onnx/blob/master/docs/Operators.md#scan ) allows users to specify any axis value for the sequence axis (instead of just axis 0 or axis 1). Would that be helpful in some contexts, or is that an unnecessary generalization/complication?
postrational(2020-09-22 16:38:34):Superseded by: https://github.com/onnx/onnx/pull/2922
CLAassistant(2019-09-05 21:04:09):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=2288) <br/>All committers have signed the CLA.
gramalingam(2019-09-05 21:46:46):It would help other users if we could document our guidelines for ensuring proto2 and proto3 compatibility (as well as compatibility across different languages in case it makes a difference). My understanding (for this PR) is the following, is this correct? DO NOT use the has_foo() method for any field "foo" of primitive or enum types; it is treated as being present with a default value (if absent in the serialized representation). However, for all the non-primitive types (which are always declared optional in ONNX) as well as oneof fields, it is safe to use the "has_foo" method. Is this correct?
yuslepukhin(2019-09-05 22:56:10):> 
> 
> It would help other users if we could document our guidelines for ensuring proto2 and proto3 compatibility (as well as compatibility across different languages in case it makes a difference). My understanding (for this PR) is the following, is this correct? DO NOT use the has_foo() method for any field "foo" of primitive or enum types; it is treated as being present with a default value (if absent in the serialized representation). However, for all the non-primitive types (which are always declared optional in ONNX) as well as oneof fields, it is safe to use the "has_foo" method. Is this correct?

To be a little more specific. The reason we are able not to use `has_*()` on the primitives is because when we assign a value to them we also set the enum. So if enum is not set then it is `UNDEFINED` (0)  by default, otherwise simply read the primitive type because we know the value is correct. This works when primitives are intentionally paired with enums and 0 means `UNDEFINED`. There are comments in the proto files to that effect.

We also do not want to use `has_*()` on `oneof` fields because in that case proto compiler automatically generates a corresponding enum. That enum value can tell us which field is set if any.

Otherwise, we can use `has_*()` providing we are built with `proto2` library. `proto3` compiler does not generate `has_*()` methods, which would become an issue if we migrated to `proto3`.


yuslepukhin(2019-09-06 17:10:42):@houseroad Can you, please, be a bit more specific as to what you mean to change existing CI to proto3? Current code will not compile with proto3 interfaces, mostly because of the extensive use of `has_*()` methods. So that would call for refactoring. However, changing to proto3 would require even more discussions. I am trying alleviate a specific pain point where compatibility is required.
wschin(2019-09-10 20:29:52):Do you mean that you make some `undefined` behaviors to `defined`? If yes, please bump their version numbers.
JamesAllingham(2019-09-11 07:55:28):> 
> 
> Do you mean that you make some `undefined` behaviors to `defined`? If yes, please bump their version numbers.

Yes, I suppose I did... these defaults were mentioned in [conv.py](https://github.com/onnx/onnx/blob/57372f39631fddc720287a4703040c0551d4ee48/onnx/backend/test/case/node/conv.py) but I suppose that isn't a proper way to define things. I'll bump the version numbers.
wschin(2019-09-17 05:17:06):Could you merge the conflicts? I think we can just keep both of your and master's changes and rerun document/test update.
wschin(2019-09-10 20:29:29):What's the default value of `group`?
JamesAllingham(2019-09-11 07:52:01):Good catch! The default value is 1. I'll add that.
JamesAllingham(2019-09-12 09:24:59):Actually, that is already in the spec, on line 907. It isn't mentioned in the `description` but is described by the `default_value`, so it does appear in `Operators.md`.
wschin(2019-09-12 23:23:28):Is it feasible to add a test to prevent this problem?
gramalingam(2019-09-12 23:26:19):Some description of what went wrong would also be helpful (if we know) along with the testcase.
BowenBao(2019-09-13 00:19:55):> Is it feasible to add a test to prevent this problem?

That's a good point. For shape inference tests we rarely test with specified opset version. That implies by default whenever an operator gets updated, their previous versions are no longer tested.
wschin(2019-09-10 20:25:52):The version of this `Resize` is 10. Is it compatible with 7's shape inference code?
BowenBao(2019-09-10 20:56:27):Yes, it was added in https://github.com/onnx/onnx/pull/2085/. Ops between opset7 and opset10 were calling it. I feel naming it with lowest supported opset version makes more sense then highest. If you think it's not too verbose i could also change it to `resizeShapeInference_opset7_to_10` ... 
wschin(2019-09-10 21:17:32):It'd be better to call it `*7_to_10` because you already saw a stupid question from me. :)
hariharans29(2019-09-10 21:23:17):nit: const auto* maybe?
hariharans29(2019-09-10 21:47:34):move this line before the 'if' condition ? We can do TypeInference even if we can't proceed with ShapeInference

EDIT: I understand that this is not part of the change, but can you please make a similar change to Upsample-7 as well ? 
BowenBao(2019-09-11 20:19:35):sure, that makes sense.
Note this PR is just reverting back the shape inference function for these ops < opset 11. I would prefer keeping the Upsample-7 the way it was to reduce the risk of another regression.
hariharans29(2019-09-11 20:52:00):sure Bowen :)
gramalingam(2019-09-12 21:07:12):The utility function unifyDim (see https://github.com/onnx/onnx/blob/95252c2adec185e305e34486c6756ece9aa8f57f/onnx/defs/shape_inference.h#L734 ) does this. 
gramalingam(2019-09-12 21:13:28):These checks are fine, but I believe that they are not needed. The way inference works, these functions return an output type and shape, and the caller will check for compatibility with existing type/shape and combine them appropriately.
BowenBao(2019-09-12 23:26:52):I'm only reverting back a change that causes regression in resize shape inference. Thus I want to  avoid modifying the original code as much as possible ... 
CLAassistant(2019-09-10 20:38:49):[![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/onnx/onnx?pullRequest=2295) <br/>Thank you for your submission! We really appreciate it. Like many open source projects, we ask that you sign our [Contributor License Agreement](https://cla-assistant.io/onnx/onnx?pullRequest=2295) before we can accept your contribution.<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/onnx/onnx?pullRequest=2295) it.</sub>
prasanthpul(2019-09-10 22:05:52):Was this PR intentional? What is the purpose?
wschin(2019-09-10 21:52:14):Could you please add a test into [this file](https://github.com/onnx/onnx/blob/7988d8360b11e6003560076e9b1d4aa426db3244/onnx/test/shape_inference_test.py#L2097)?
jignparm(2019-09-10 22:35:01):> Could you please add a test into [this file](https://github.com/onnx/onnx/blob/7988d8360b11e6003560076e9b1d4aa426db3244/onnx/test/shape_inference_test.py#L2097)?

Done.
wschin(2019-09-11 04:11:00):@ebarsoum, how does this PR look to you?
CLAassistant(2019-09-11 06:41:50):[![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/onnx/onnx?pullRequest=2299) <br/>Thank you for your submission! We really appreciate it. Like many open source projects, we ask that you sign our [Contributor License Agreement](https://cla-assistant.io/onnx/onnx?pullRequest=2299) before we can accept your contribution.<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/onnx/onnx?pullRequest=2299) it.</sub>
ufon8(2019-09-11 06:47:59):_( ͡°ᴥ ͡° ʋ)_
wschin(2019-09-12 00:24:00):```suggestion
        raise TypeError("The element type in the input tensor is not defined.")
```
autoih(2019-09-12 00:47:20):Thanks @wschin for the revision. I've updated it.  
CLAassistant(2019-09-12 00:19:17):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=2306) <br/>All committers have signed the CLA.
ebarsoum(2019-09-15 02:43:23):Can you sign the CLA?
wschin(2019-11-19 17:16:02):How did the old logic pass all existing RNN tests..
wschin(2019-11-19 17:17:35):[nit] It'd be nice if you can do something like 
```suggestion
    fail_shape_inference("Input tensor must have at least %d dimensions", number_of_max_conv_rnn_input_dims);
```
My syntax is definitely wrong. Just want to have a concrete example for things in my mind.
wschin(2019-11-19 17:23:15):`dir` is not a good variable name. Maybe just `direction`? The same comment holds for the following places.
wschin(2019-11-19 17:24:49):```suggestion
             ('w', TensorProto.FLOAT, (number_of_directions, hiddensize, inpsize)),
```
Please double-check all similar places. Thanks.
wschin(2019-11-19 17:26:01):```suggestion
             ('w', TensorProto.FLOAT, (num_directions, 4 * hiddensize, inpsize)),
```
wschin(2019-11-19 17:26:41):```suggestion
             ('w', TensorProto.FLOAT, (num_directions, 3 * hiddensize, inpsize)),
```
wschin(2019-11-19 17:30:02):```suggestion
        self._assert_inferred(graph, [make_tensor_value_info('y', TensorProto.STRING, (None,))])  # type: ignore
```
maybe?
wschin(2019-11-19 17:30:16):```suggestion
        self._assert_inferred(graph, [make_tensor_value_info('y', TensorProto.FLOAT, (7,))])
```
wschin(2019-11-19 17:31:11):```suggestion
        self._assert_inferred(graph, [make_tensor_value_info('y', TensorProto.FLOAT, (15, 7))])
```
If previously we use tuples for representing shapes, we can still use them for consistency.
wschin(2019-11-19 17:32:08):Could you add new tests instead of overloading existing ones when possible? Thanks.
wschin(2019-11-19 19:50:20):Could you check all tensors shapes when they exist in its shape inference code?
houseroad(2019-09-13 18:26:30):@gramalingam do you have more context on in which condition, subgraph should not provide type information?
gramalingam(2019-09-13 18:29:08):@houseroad : see Issue https://github.com/onnx/onnx/issues/2286 and the earlier PR https://github.com/onnx/onnx/pull/2009 
CLAassistant(2019-09-17 06:29:37):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=2314) <br/>All committers have signed the CLA.
prasanthpul(2019-09-17 18:46:27):Are the new ops only useful in training scenarios? If so, they should be put in a separate namespace. It doesn't have to be "ai.onnx.training", it can be "ai.onnx.lossfunction" or something like that
wschin(2019-09-17 21:39:10):> Are the new ops only useful in training scenarios? If so, they should be put in a separate namespace. It doesn't have to be "ai.onnx.training", it can be "ai.onnx.lossfunction" or something like that

Sounds good. I will put them into `ai.onnx.training`.
spandantiwari(2019-10-01 23:36:22):How should a backend decide which parameters in the graph are trainable and which are not? Do we consider all the initializers to be 'trainable' or only those that are captured in `update_binding`. This question comes up when backends are trying to optimize the graphs using constant folding, and trying to decide which of the initializers are true constants (non-trainable) that can be folded.
wschin(2019-12-03 16:44:44):> How should a backend decide which parameters in the graph are trainable and which are not? Do we consider all the initializers to be 'trainable' or only those that are captured in `update_binding`. This question comes up when backends are trying to optimize the graphs using constant folding, and trying to decide which of the initializers are true constants (non-trainable) that can be folded.

There is no direct concept of trainable. As long as you use `update_binding` to update a initializer, that initializer is considered trainable because its value would be altered at the end of each training iteration. In contrast, as long a initializer's name is not the value of any key-value pairs in `update_binding`, that initializer is considered as a constant.
chinhuang007(2019-12-10 23:01:55):Okay, I believe that means trainable parameters are inferred based on initializer and update_binding. Sounds reasonable.
wschin(2019-12-10 23:03:57):> Okay, I believe that means trainable parameters are inferred based on initializer and update_binding. Sounds reasonable.

Yes!
tbennun(2020-01-13 22:49:20):@wschin Reviewed the PR. Overall it looks good, especially the definition of the versatile `Gradient` operator. If you don't mind answering, I have a few questions:

1. Why is `TensorInfoProto` repeated? Is there an example of where this could be used?
2. Is the difference between `onnx-ml.proto*` and `onnx.proto*` intentional? Specifically, the definition of `TensorInfoProto` and repeated vs. optional (going back to question 1).
3. Could you explain the use of `FunctionProto` as the training algorithm? I somewhat agree with @TMVector that it would be functionally equivalent to define a `GraphProto` for the update step, but I may be missing something.
4. If I would like to apply weight decay to my training step, is it possible to do with the current representation of `algorithm`? Right now the documentation specifies "loss node, gradient node, optimizer node, and calls to the inference function", but it would be nice to use the names of the weight tensors there directly.

One minor note: the documentation of `Gradient` is missing arrows from `Y` to the operator.
rajeevnalawadi(2020-01-16 19:05:34):Since we are covering mixed precision training (FP32 / FP16) in the operators, should we also add support for facilitating quantization aware training scenarios requiring support for FP32 / Int8 data precision type in training operators ?
wschin(2020-01-21 07:40:55):> @wschin Reviewed the PR. Overall it looks good, especially the definition of the versatile `Gradient` operator. If you don't mind answering, I have a few questions:
> 
> 1. Why is `TensorInfoProto` repeated? Is there an example of where this could be used?

It's useful when there are multiple assignments in a single training iteration. For example, one GAN iteration contains something like
```
w = w + g(w, z)
z = z + g(w, z)
```
In this case, `w = w + ...` would be mapped to one `TrainingInfoProto` and `z = z + ...` would be another one.

> 2. Is the difference between `onnx-ml.proto*` and `onnx.proto*` intentional? Specifically, the definition of `TensorInfoProto` and repeated vs. optional (going back to question 1).

No. I have regenerated them. Thanks.

> 3. Could you explain the use of `FunctionProto` as the training algorithm? I somewhat agree with @TMVector that it would be functionally equivalent to define a `GraphProto` for the update step, but I may be missing something.

The original idea is to have something like C++ template algorithm (`FunctionProto`'s inputs and outputs are not typed), but with the existence of `input`, `output`, and `initializer`, the training thing looks much closer to a `GraphProto`. I just change `FunctionProto` to `GraphProto`.

> 4. If I would like to apply weight decay to my training step, is it possible to do with the current representation of `algorithm`? Right now the documentation specifies "loss node, gradient node, optimizer node, and calls to the inference function", but it would be nice to use the names of the weight tensors there directly.

Yes, the user can do `update_direction = weight_decay * w + gradient_of_w` and then compute `w_new = w + update_direction`. Basically, we hope users can compose their training algorithm as a computation graph similar to the inference ones.

> 
> One minor note: the documentation of `Gradient` is missing arrows from `Y` to the operator.

`Y` is a function of `W and X`. Because `W` and `X` are already given, `Gradient` doesn't need `Y` to compute `dY/dW` and `dY/dX`.

@tbennun, does it make sense to you?
wschin(2020-01-21 07:45:40):> Since we are covering mixed precision training (FP32 / FP16) in the operators, should we also add support for facilitating quantization aware training scenarios requiring support for FP32 / Int8 data precision type in training operators ?

If the model maker wants to use, say FP16, they can specify inputs and outputs as FP16 in `TrainingInfoProto.algorithm.input` and `TrainingInfoProto.algorithm.output`. Similar to inference graph, training algorithm is also a computation graph users can freely compose with operators working on different precision.

@rajeevnalawadi, I am not familiar with Int8 training. Please let me know if I am doing wrong.
wschin(2020-01-22 21:31:27):@TMVector, could you please sign the license/cla check? I took some of your suggested changes directly. Thanks.
wschin(2020-01-23 19:11:32):> @wschin PR looks great IMO, I only had a few minor comments. Since the files are auto-generated the comments are replicated throughout the generated files, except for higher level comments, which I only added to the input file.

@tbennun, your comments are addressed in [Commit 393cac5](https://github.com/wschin/onnx/commit/393cac53055314b955e0fc83c5808608b1ce7b33). Please take a look. Thanks.
wschin(2020-01-23 21:05:25):There are a few force-pushes to get license/cla passed. I have an invalid email in two of my old commits, so we need to redo the entire history... To see historical commits, please go to [this branch](https://github.com/wschin/onnx/commits/training-backup).
wschin(2020-01-24 17:09:50):> @wschin Looks good. Would be nice if you could credit the initializer contribution somehow though :)

@tbennun

Already added your name in a commit message but I will make it more explicit. Thanks again! :)

[Update] Just change this PR's top message. How does it look? I will also improve some commit messages and do another force-push.

[Update] Commit message 77333af is edited to explicit show our names on Github.
spandantiwari(2020-01-24 23:17:25):> > How should a backend decide which parameters in the graph are trainable and which are not? Do we consider all the initializers to be 'trainable' or only those that are captured in `update_binding`. This question comes up when backends are trying to optimize the graphs using constant folding, and trying to decide which of the initializers are true constants (non-trainable) that can be folded.
> 
> There is no direct concept of trainable. As long as you use `update_binding` to update a initializer, that initializer is considered trainable because its value would be altered at the end of each training iteration. In contrast, as long a initializer's name is not the value of any key-value pairs in `update_binding`, that initializer is considered as a constant.

OK. To summarize, the necessary and sufficient condition for an initializer to be considered trainable is that it is included in `update_binding`. If it is not then it is considered a constant and it is fair game to use it for optimizations such as constant folding. There's no other condition required to mark a initializer as trainable/non-trainable (e.g. including the initializer in list of graph inputs to mark is trainable etc). 

Is this correct? If yes, we should add this to the description somewhere to be clear.
wschin(2020-01-24 23:28:58):> > > How should a backend decide which parameters in the graph are trainable and which are not? Do we consider all the initializers to be 'trainable' or only those that are captured in `update_binding`. This question comes up when backends are trying to optimize the graphs using constant folding, and trying to decide which of the initializers are true constants (non-trainable) that can be folded.
> > 
> > 
> > There is no direct concept of trainable. As long as you use `update_binding` to update a initializer, that initializer is considered trainable because its value would be altered at the end of each training iteration. In contrast, as long a initializer's name is not the value of any key-value pairs in `update_binding`, that initializer is considered as a constant.
> 
> OK. To summarize, the necessary and sufficient condition for an initializer to be considered trainable is that it is included in `update_binding`. If it is not then it is considered a constant and it is fair game to use it for optimizations such as constant folding. There's no other condition required to mark a initializer as trainable/non-trainable (e.g. including the initializer in list of graph inputs to mark is trainable etc).
> 
> Is this correct? If yes, we should add this to the description somewhere to be clear.

Yes, it's correct. I will extend the spec to explicit reveal that point. Thank you!

[Update] Extended a small paragraph right above `update_binding`:
```
// Whether an initializer is trainable is determined by the values of
// key-value pairs in "update_binding". If the name of an initializer
// cannot be found as a value in the "update_binding"s of all
// TrainingInfoProto's, that initializer is not trainable and therefore
// considered as a constant.

repeated StringStringEntryProto update_binding = 3;  
```
fdokic(2020-02-04 08:28:50):Minor note: I noticed `optional StringStringEntryProto initialization_binding = 3;`  is currently optional but I think it should be repeated just as `update_binding`, in order to  support more than one binding for initialization? Thanks!
wschin(2020-02-04 17:32:11):> Minor note: I noticed `optional StringStringEntryProto initialization_binding = 3;` is currently optional but I think it should be repeated just as `update_binding`, in order to support more than one binding for initialization? Thanks!

You're absolutely right.
wschin(2020-02-14 08:33:52):> As we agreed in today SIG meeting, let's do the following:
> 
> 1. Rename global_initializer to mutable_initializer.
> 2. Change the type to string, which contains the name of the tensors in the initializer that are mutable.

The last commit includes all requested changes. Please take a look. Thank you.
wschin(2019-11-09 01:00:34):```suggestion
//     should be updated by which tensor. Its type is a list of key-value pairs.
```
TMVector(2020-01-09 20:52:37):I think the "each initializer graph" sentence is a hangover from a previous iteration, although capuring "zero inputs" is still valuable.
TMVector(2020-01-09 21:28:27):```suggestion
            "T3",
```
TMVector(2020-01-09 21:29:20):```suggestion
            "Constrain initial learning rate to float scalars.")
```
TMVector(2020-01-09 21:29:55):```suggestion
            "Constrain iteration count to 64-bit integer scalars.")
```
TMVector(2020-01-09 21:32:07):```suggestion
            "Constrain input and output types to float tensors.")
```
TMVector(2020-01-09 21:36:11):```suggestion
            "Constrain learning rate to float scalars.")
```
TMVector(2020-01-09 21:36:25):```suggestion
            "Constrain iteration count to 64-bit integer scalars.")
```
TMVector(2020-01-09 21:36:35):```suggestion
            "Constrain input and output types to float tensors.")
```
TMVector(2020-01-09 21:37:48):This could check the number of inputs and fail shape inference like `Adam` does
TMVector(2020-01-09 21:40:16):```suggestion
            "Constrain learning rate to float scalars.")
```
TMVector(2020-01-09 21:40:30):```suggestion
            "Constrain iteration count to 64-bit integer scalars.")
```
TMVector(2020-01-09 21:40:40):```suggestion
            "Constrain input and output types to float tensors.")
```
TMVector(2020-01-09 22:01:40):The comments talk about computing `dO/dW`, but the graph and attributes look to compute `dY/dW`. `L` isn't included as an input to either `Gradient` node, and `y=Y` instead of `y=O` on the first `Gradient` node.
TMVector(2020-01-09 22:42:27):I still found this confusing at first. How about this? (new in italics, order also changed) 🙂 

> *The tensors named in attributes `xs`, `zs`, and `y` define the graph used, but the inputs to the `Gradient` node define the values at which the gradient is computed. We can feed different tensors to the identified graph.* For example, one can compute the gradient of Y with respect to H *at a specific value of H, H_1,* by ~substituting Y_1 into Y and H_1 as into H~ providing that value as an input to the `Gradient` node..
>
> When the inputs of Gradient are the tensors named in "xs", *and "zs"* the computation
can be optimized. More specifically, a forward pass can be reused if the
gradient is computed via reverse-mode auto-differentiation.
TMVector(2020-01-09 22:43:21):The comment above talks about dY/dH, but this graph conveys dY/dW and dY/dZ and misses X.
TMVector(2020-01-09 22:44:55):```suggestion
            "Constrain outputs to floating-point tensors.")
```
TMVector(2020-01-09 22:49:43):If you really want all tensor types, then you can use `OpSchema::all_tensor_types()`
TMVector(2020-01-09 22:50:28):```suggestion
            "Allow inputs to be any kind of tensor."));
```
TMVector(2020-01-09 23:24:33):```suggestion
The loss for one sample, l_n, can calculated as follows
```
TMVector(2020-01-09 23:25:21):```suggestion
            "Usualy, it's a one-hot representation of ground-truth class.",
```
TMVector(2020-01-09 23:37:47):IMO the loss operators should not have the `reduction` attribute. This can be implemented by adding the appropriate node (e.g. `ReduceSum`) on the output. The [documentation for adding new operators](https://github.com/onnx/onnx/blob/master/docs/AddNewOp.md#step-1-proposing-a-new-operatorfunction-) states that new operators should be functions or primitives where possible, implying trivially composite operators should be avoided.
TMVector(2020-01-10 00:46:42):This seems like a separate feature that is big enough to warrant a separate PR and discussion.
TMVector(2020-01-10 00:48:31):Could this be a `GraphProto` instead?
wschin(2020-01-21 05:11:52):Thanks for catching this. I changed it to multiple outputs. Similar to training algorithm, the assignment of initial values are defined by `update_binding` now. In other words, this spec treats model initialization as a kind of `training`.
wschin(2020-01-21 06:36:37):I will address those comments in a separated PR. To speedup the merge of training spec, I feel it'd be better to merge operators (loss and optimizer) and spec separately. 
wschin(2020-01-21 06:40:26):Makes sense. Since most losses are composable, we need to propose them one-by-one like what we did in the beginning. I will continue the optimizer and loss work in difference places. This PR will only contain spec-wise changes.
wschin(2020-01-21 06:47:32):This field can be used to store information, for example, inference graph, shared by both training and inference. The training graph is usually inference graph + optimizer + gradient + loss. If we remove this field, user may need to create the inference graph twice (once in `ModelProto.graph` and once `TrainingInfoProto.algorithm`).

Alternatively, to avoid duplicated inference graph definitions, we can say that ModelProto.graph is callable inside TrainingInfoProto.algorithm.
wschin(2020-01-21 06:48:08):Just changed it to `GraphProto` and merge `input`, `output`, and `initializer` into that 'GraphraphProto`. I feel GraphProto is better too now. Thanks.
wschin(2020-01-21 18:11:22):You're totally right. "L" should be an input to both `Gradient`'s and the first `Gradient` should have `y="O"`. Thanks!
wschin(2020-01-21 18:20:03):Looks great. I will merge your idea into this spec.
wschin(2020-01-21 18:28:29):Sorry about that. I fixed the graph.
wschin(2020-01-21 19:05:55):Step and epoch can be updated in algorithm too. For example, an algorithm graph can contain `t_next = t + 1` and in `update_binding`, we have a pair `(t_next, t)`. For implementing `epoch`-index in `algorithm`, one can set the size of `epoch` as algorithm's input and the current `epoch` index can be computed by `t_epoch = t / epoch_size`, where `epoch_size` is a graph input.
wschin(2020-01-21 21:15:37):Ok.
wschin(2020-01-21 21:54:55):```suggestion
  //   algorithm.
```
wschin(2020-01-21 21:55:33):```suggestion
  // - The new message can store gradient-based training
```
wschin(2020-01-22 23:09:39):Per our WG discussion, we will rollback things around `FunctionProto` and make inference graph callable from `TrainingInfoProto`.
tbennun(2020-01-23 08:43:37):```suggestion
  It is possible to represent high-order differentiation using Gradient operators.
```
tbennun(2020-01-23 08:45:22):```suggestion
For example, given the following linear model:
```
tbennun(2020-01-23 08:47:34):```suggestion
<dd>Input tensor names of the differentiated sub-graph. It contains only the necessary differentiated inputs of a (sub-)graph. Variables (usually called intermediate variables) that can be generated by inputs cannot be included in this attribute.</dd>
```
tbennun(2020-01-23 08:49:21):```suggestion
<dd>Input tensor names of the differentiated sub-graph. It contains only the necessary non-differentiated inputs of a (sub-)graph. Variables (usually called intermediate variables) that can be generated by inputs cannot be included in this attribute.</dd>
```
tbennun(2020-01-23 08:50:27):```suggestion
<dd>Allow outputs to be any kind of tensor.</dd>
```
tbennun(2020-01-23 08:52:10):```suggestion
  For example, given the following linear model:
```
tbennun(2020-01-23 08:53:14):```suggestion
<dd>Input tensor names of the differentiated sub-graph. It contains only the necessary differentiated inputs of a (sub-)graph. Variables (usually called intermediate variables) that can be generated by inputs cannot be included in this attribute.</dd>
```
tbennun(2020-01-23 08:53:42):```suggestion
<dd>Input tensor names of the differentiated sub-graph. It contains only the necessary non-differentiated  inputs of a (sub-)graph. Variables (usually called intermediate variables) that can be generated by inputs cannot be included in this attribute.</dd>
```
tbennun(2020-01-23 08:55:53):```suggestion
<dd>Allow outputs to be any kind of tensor.</dd>
```
tbennun(2020-01-23 08:59:41):```suggestion
For example, given the following linear model:
```
tbennun(2020-01-23 09:00:50):```suggestion
            "contains only the necessary differentiated "
```
tbennun(2020-01-23 09:01:08):```suggestion
            "intermediate variables) that can be generated by inputs "
```
tbennun(2020-01-23 09:01:32):```suggestion
            "contains only the necessary non-differentiated "
```
tbennun(2020-01-23 09:01:44):```suggestion
            "inputs of a (sub-)graph. Variables (usually called "
```
tbennun(2020-01-23 09:01:56):```suggestion
            "intermediate variables) that can be generated by inputs "
```
tbennun(2020-01-23 09:02:30):```suggestion
            "Allow outputs to be any kind of tensor.")
```
tbennun(2020-01-23 09:03:14):@wschin Perhaps "any kind of differentiable tensor type"?
tbennun(2020-01-23 09:16:32):```suggestion
// them once constitutes a full training algorithm step. The initializer update
```
tbennun(2020-01-23 09:16:47):```suggestion
// them once constitutes a full training algorithm step. The initializer update
```
tbennun(2020-01-23 09:17:05):```suggestion
// them once constitutes a full training algorithm step. The initializer update
```
tbennun(2020-01-23 09:17:17):```suggestion
// them once constitutes a full training algorithm step. The initializer update
```
tbennun(2020-01-23 09:17:32):```suggestion
// them once constitutes a full training algorithm step. The initializer update
```
tbennun(2020-01-23 09:18:26):```suggestion
// The execution of TrainingInfoProto has two modes. The user can choose to execute
```
tbennun(2020-01-23 09:18:40):```suggestion
// The execution of TrainingInfoProto has two modes. The user can choose to execute
```
tbennun(2020-01-23 09:18:46):```suggestion
// The execution of TrainingInfoProto has two modes. The user can choose to execute
```
tbennun(2020-01-23 09:18:51):```suggestion
// The execution of TrainingInfoProto has two modes. The user can choose to execute
```
tbennun(2020-01-23 09:18:58):```suggestion
// The execution of TrainingInfoProto has two modes. The user can choose to execute
```
tbennun(2020-01-23 09:21:37):```suggestion
  // This field represents a training algorithm step. Given required inputs, it computes
```
tbennun(2020-01-23 09:25:13):```suggestion
  // This field represents a training algorithm step. Given required inputs, it computes
```
tbennun(2020-01-23 09:25:19):```suggestion
  // This field represents a training algorithm step. Given required inputs, it computes
```
tbennun(2020-01-23 09:25:27):```suggestion
  // This field represents a training algorithm step. Given required inputs, it computes
```
tbennun(2020-01-23 09:25:40):```suggestion
  // This field represents a training algorithm step. Given required inputs, it computes
```
tbennun(2020-01-23 09:28:53):This sentence is no longer relevant and should be removed (same applies for the rest of the proto files).
tbennun(2020-01-23 09:30:32):```suggestion
  // into the training graph, the
```
tbennun(2020-01-23 09:30:59):```suggestion
  // into the training graph, the
```
tbennun(2020-01-23 09:31:11):```suggestion
  // into the training graph, the
```
tbennun(2020-01-23 09:31:14):```suggestion
  // into the training graph, the
```
tbennun(2020-01-23 09:31:19):```suggestion
  // into the training graph, the
```
tbennun(2020-01-23 09:31:32):```suggestion
  // user needs to save "y = x - r * g" into a TrainingInfoProto.algorithm. To
```
tbennun(2020-01-23 09:31:41):```suggestion
  // user needs to save "y = x - r * g" into a TrainingInfoProto.algorithm. To
```
tbennun(2020-01-23 09:31:48):```suggestion
  // user needs to save "y = x - r * g" into a TrainingInfoProto.algorithm. To
```
tbennun(2020-01-23 09:31:55):```suggestion
  // user needs to save "y = x - r * g" into a TrainingInfoProto.algorithm. To
```
tbennun(2020-01-23 09:32:03):```suggestion
  // user needs to save "y = x - r * g" into a TrainingInfoProto.algorithm. To
```
tbennun(2020-01-23 09:33:02):```suggestion
  // In general, the user can use "update_binding" to assign any output of
```
tbennun(2020-01-23 09:33:11):```suggestion
  // In general, the user can use "update_binding" to assign any output of
```
tbennun(2020-01-23 09:33:15):```suggestion
  // In general, the user can use "update_binding" to assign any output of
```
tbennun(2020-01-23 09:33:20):```suggestion
  // In general, the user can use "update_binding" to assign any output of
```
tbennun(2020-01-23 09:33:25):```suggestion
  // In general, the user can use "update_binding" to assign any output of
```
wschin(2020-01-23 17:12:48):Really appreciate your suggestions! Thank you!
wschin(2020-01-23 18:22:06):I will change it to `Allow inputs to be any kind of floating-point tensor`. Maybe with some advanced math techniques, other types are differentiable too. 
wschin(2020-01-23 19:02:48):This paragraph is changed to
```
  // The field algorithm.node can invoke ModelProto.graph as a node.
  // The initializers defined in ModelProto.graph.initializer are visible in
  // the "algorithm"
```
chinhuang007(2020-01-24 18:06:49):Would be nice to describe what typically goes into initialization. Is it just like algorithm without inputs? I really like the comment below for algorithm, "In general, this graph contains loss node, gradient node, optimizer node, increment of iteration count, and calls to the inference graph."
wschin(2020-01-24 21:52:53):Yes, `initialization` is an algorithm without input.

I will add
```
+  // Usually,
+  // trainable tensors in neural networks are randomly initialized.
+  // To achieve that, for each tensor, the user can put a random operator such
+  // as RandomNormal or RandomUniform in TrainingInfoProto.initialization.node
+  // and assign its random output to the specific tensor using "update_binding".
```
prasanthpul(2020-01-24 23:29:09):ONNX no longer supports experimental ops. We deprecated all existing experimental ops 2 releases ago. Ops SIG should clarify whether this is true only for ai.onnx domain or all domains
gramalingam(2020-01-25 00:02:44):They are not all in "xs". Do you mean all "xs" followed by all "zs"?
wschin(2020-01-25 00:17:14):I fixed it. Thanks.
wschin(2020-01-25 00:17:21):Fixed. Thanks a lot.
sveta-levitan(2020-01-25 01:19:49):Please replace "to" with "with respect to"
wschin(2020-01-25 01:28:54):Do you mind if I use `w.r.t.`? I also used in in some other places. Or it's better not to abbreviate?
sveta-levitan(2020-01-25 03:25:15):w.r.t. is fine, thank you!
wschin(2020-01-25 19:09:15):Ok. Thanks!
gramalingam(2020-01-26 19:39:03):What domain/namespace does this go into? Ideally, this should probably be a special namespace/domain.
gramalingam(2020-01-26 19:46:40):I don't understand this. (a) Does this mean that this field is optional and may be missing? (b) If it is missing, what is the initial value assigned to initializers?
gramalingam(2020-01-26 20:30:51):A couple of related points: (a) Should we have a separate update_binding for initialization and a separate one for algorithm? Otherwise, we need to be clear whether we expect the two graphs to use same output-names for one given initializer, or allow both (init_x, x) and (update_x, x) in the bindings. (b) Should we swap the order of names in the update_bindings? That has two advantages: it is more consistent with the ordering in an assignment statement. It also clarifies that only one update-value should be specified for an initializer (since the first entry is the "key").
gramalingam(2020-01-26 20:40:09):What does this mean, given that the initialization is also inside a TrainingInfoProto? I think I understand repeating a list of "update algorithm" in one iteration, but the initialization is a problem.
wschin(2020-01-27 01:34:23):There will be two execution modes.

1. Training: sequentially execute all `TrainingInfoProto`'s `algorithm` graphs.
2. Initialization: sequentially execute all `TrainingInfoProto`'s `initialization` graphs.

wschin(2020-01-27 01:37:24):(a) this field can be missing. (b) because this default graph doesn't produce any output, the user cannot use `update_binding` assign initial values to any initializers. Thus, the default initialization behavior is not to change any initializer.
wschin(2020-01-27 01:46:56):(a) My understanding is that we allow both (init_x, x) and (update_x, x) in `update_binding`. It means `update_binding` is an union of assignments from initialization and training.

(b) I personally feel `(key, value)=(W_new, W)` is closer to what will happen in the runtime. After finishing the computation of `algorithm` and produce an output `W_new`, we need a mapping from `W_new` to the updated initializer. If we store `(key, value)=(W, W_new`), we will need to convert '(W, W_new)` to `(W_new, W)` when creating output-to-initializer mapping.
gramalingam(2020-01-27 01:56:33):Re. (a): I think splitting the two simplifies the semantics. Re. (b) also: I think we should think in terms of defining the semantics formally (using appropriate programming/mathematical notation). My opinion is that difference in this sense is not much, but the order (lhs, rhs) is common for an assignment and similarly (variable, value) is common for a binding in functional languages.

Mathematically, the underlying state is a "map" M that maps tensor-variables to their values. The execution of a single update-binding (W, W_new) updates M to M[W => W_new]. 
wschin(2020-01-27 02:15:41):Ok. 4801755 moves Gradient to `ai.onnx.training`.
wschin(2020-01-27 02:29:42):Re Re (a): Ok. I will create `initialization_binding` and `algorithm_binding` and remove `update_binding`.

Re Re (b): Ok. I will swap keys and values.
gramalingam(2020-01-27 18:29:28):Can we clarify the semantics by adding the following?
```
In particular, this defines two functionalities: an initialization-step and a training-iteration-step.
The semantics of the initialization-step is that all initializers (in ModelProto.graph and in all
TrainingInfoProto.algorithm) are first initialized as specified by the initializers in the graph,
and then by the initialization_bindings in every instance in training_info.
The semantics of the training-iteration-step is that all the initializers are updated as specified
by the update_bindings in every instance in training_info.
```
In particular, I have a question: is my understanding of the initialization above correct?
In the ambiguous case where multiple initialization values are specified for W (e.g., if
the graph specifies an initializer value for initializer W,
and one or more initialization_bindings also specify a value for W.)
wschin(2020-01-27 18:46:53):I think the semantic of initialization applies to training as well. We need to compute new weights from those initialized ones.

For ambiguity, only one of `algorithm` and `initialization` can be executed at the same time. If the user choose to train the model, they needs to execute all `algorithm`s and ignore all `initialization`s. If the user choose to reset the model, all `algorithm`s will be ignored and only `initialization`s will be sequentially executed.

wschin(2020-01-27 19:20:22):I added your clarification to the protobuf files. Thanks.
postrational(2020-01-29 14:50:54):```suggestion
// Several inference components are usable in TrainingInfoProto; the list
```
postrational(2020-01-29 14:51:13):```suggestion
//   1. ModelProto.graph is a callable inside TrainingInfoProto.algorithm.
```
postrational(2020-01-29 14:53:27):Should we document `Graph` as a new operator?
TMVector(2020-01-29 16:22:24):```suggestion
        node.domain() == "ai.onnx" || node.domain() == AI_ONNX_TRAINING_DOMAIN) {
```
gramalingam(2020-01-29 16:38:51):Why not just call a graph using its name as the "op_type"?
TMVector(2020-01-29 16:41:16):It would be great if we could add checks to the checker that these names are all valid values in scope
TMVector(2020-01-29 16:43:21):How about this?
`The gradient of the tensor specified in attribute \"y\" with respect to each of the tensors specified in attribute \"xs\"`
wschin(2020-01-29 17:02:27):@gramalingam, I would like to make the Graph callable even if it's name is `LSTM`. Is it ok?
@postrational, I am not sure if `operator` is the best way to describe it. This behavior sounds more like a new syntax. We can do that more formally in another place but `TrainingInfoProto` will be its only user for now.
wschin(2020-01-29 17:53:12):Trainable weights must be an input in `ModelProto.graph.input`? @gramalingam, @spandantiwari, any comments?  I feel moving `trainable` semantic entirely into `update_binding` (and `initialization_binding`) is a cleaner way than having `ModelProto.graph` and `TrainingInfoProto` to jointly determine if weights are trainable.
wschin(2020-01-30 16:59:19):```suggestion
  optional StringStringEntryProto initialization_binding = 3;
```
TMVector(2020-01-30 22:36:07):I agree with you @wschin :+1: 
TMVector(2020-01-30 22:50:08):By using the name as the graph identifier, you will only be able to have one instance, since node names should be unique. I think it would be better to use an attribute, e.g. `graph_name`.
TMVector(2020-01-30 22:53:20):[IR.md](https://github.com/onnx/onnx/blob/master/docs/IR.md#nodes) says
> The graph MUST use single static assignment for all node outputs, which means that all node output names MUST be unique within a graph. In the case of a nested subgraph, a node output name MUST be distinct from the names from the outer scopes that are visible in the nested subgraph.

So I think you could be more explicit -- I think it would be in the spirit of the spec thus far to say value names must be unique in `ModelProto.graph`, `TrainingInfoProto.initialization.nodes`, and `TrainingInfoProto.algorithm.nodes`.
wschin(2020-01-30 23:08:38):I will switch to `NodeProto.op_type == ModelProto.graph.name` as suggested by @gramalingam.  This strategy implies callable graphs can't have names conflicting with existing ONNX operators.
gramalingam(2020-01-30 23:28:47):I think they are two separate aspects. (a) Should we pass the trainable-weights as explicit-inputs to ModelProto.graph or should we treat trainable-weights as "shared variables" with initializers of the same name in ModelProto.graph? (b) How does a runtime figure out the set of trainable inputs? You are right with respect to (b): it is sufficient to look at only update-bindings. However, this does not depend on the answer we choose for (a). I think for (a) treating the trainable-weights as explicit-inputs is more consistent with the existing spec (it will surprise fewer people/implementations who may be assuming this), and it seems cleaner. 
wschin(2020-01-31 02:03:12):Looks good. I will do
```
            "Outputs",
            "The gradient of the tensor specified by the attribute \"y\" "
            "with respect to each of tensors specified in the "
            "attribute \"xs\". The i-th output is the gradient of \"y\" with "
            "respect to the i-th tensor specified in the attribute \"xs\".",
            "T2",
```
linkerzhang(2020-01-31 02:12:48):I agree on using the graph name as the “op_type", which is much more extendable and also following the same naming convention as "Function".
linkerzhang(2020-01-31 02:14:02):Thanks for the fixing!

Question: do we need this explicit name binding/mapping? In my opinion. using the same name string should be good enough, instead of adding one more mapping table. Thoughts?
wschin(2020-01-31 03:39:49):It may break SSA in a graph. If inside the training algorithm graph, we store the following two equations to update `W`:
```
W, X->Sub->W // first equation which updates W
W, Y->Add->Z // second one which depends on W
```
where `W`, `X`, `Y`, and `Z` are tensors. Should `W, Y -> Add -> Z` use the output `W` of `Sub`?
wschin(2020-01-31 05:54:13):What does it mean? When `ModelProto.graph` is called in `TrainingInfoProto.algorithm`, its inputs and outputs would be mapped by position like how a FunctionProto is called. Thus, `ModelProto.graph` and `TrainingInfoProto.algorithm` can have `X, Y ---> Add ---> Z` at the same time as long as the value names in `TrainingInfoProto.algorithm` doesn't conflict with the `initializer` names in `ModelProto.graph.initializer`.

Let's consider a  case in my mind.

In `ModelProto.graph`, we have

- inputs: A, B
- initializer: C
- output: D
- nodes: 
```
A, B ---> Add ---> Z
Z, C ---> Mul ---> D
```

In `ModelProto.training_info[0].algorithm`, we have

- inputs: A, B
- initializer: C_1 (C from `ModelProto.graph.initializer` is visible to `ModelProto.training_info[0].algorithm`, so we can't use C as an initializer name)
- output: D, D_1
- nodes: 
```
A, B ---> Add ---> Z
Z, C (C is from `ModelProto.graph.initializer`) ---> Mul ---> D
Z, C_1 ---> InferenceGraph ---> D_1
```

Here in that training graph, we can't use `C` as any initializer or output name because all values in `ModelProto.graph.initializer` are visible to that training graph. For other variable names `A`, `B`, `C`, and `Z`, we can use them freely, because `ModelProto.graph.inputs`, `ModelProto.graph.outputs`, and all other intermediate variables produced by `ModelProto.graph.node` are not accessible from the training graph.
wschin(2020-01-31 05:58:14):I'd like to add it in a separated PR. There are several checks we can add after this PR. We would be better to have everyone to agree on the core idea and then start extending it and fixing missing parts.
TMVector(2020-01-31 13:32:17):That conflates names from the `Graph` namespace and the `Operator` namespace. cref https://github.com/onnx/onnx/blob/master/docs/IR.md#names-within-a-graph

I think the `Graph` operator is a much clearer mechanism:
```
{ op_type = "Graph",
  domain = "", # Default ai.onnx domain
  name = "node_name_used_for_diagnostics_only",
  attribute = {
    "graph_name" = <ModelProto.graph.name>,
  },
}
```
wschin(2020-01-31 18:42:02):@TMVector, It seems we need to use `op_type=ModelProto.graph.name`. Because the names of `NodeProto`s must be unique in a single graph, if we do `op_type=Graph + op_name=ModelProto.graph.name`, we can only call the inference once in a training graph.

Let's consider calling `InferenceGraph` twice as an example. If we use `op_type=Graph + op_name=ModelProto.graph.name`, we will have
```
X --> Node(name="MyInferenceGraphName", op_type="Graph") -->Y
X_1 --> Node(name="MyInferenceGraphName", op_type="Graph") -->Y_1
```
which breaks the uniqueness of operator names. Maybe it's not a problem but we need another discussion to see if everyone agrees on it. I personally feel the uniqueness of operator names is a nice property for debugging.

In contrast, if we do `op_type=ModelProto.graph.name`, we can have
```
X --> Node(name="CallMyGraph", op_type="MyInferenceGraphName") -->Y
X_1 --> Node(name="CallMyGraphAgain", op_type="MyInferenceGraphName") -->Y_1
```
Of course, it means the callable graphs can't use names such as LSTM, Conv, etc.
wschin(2020-01-31 22:18:54):I am not sure if I fully understand the question (a). I think for (a), the current training spec is doing `treat trainable-weights as "shared variables" with initializers of the same name in ModelProto.graph`, because `ModelProto.graph.initializer` is visible to `TrainingInfoProto.algorithm`.

I feel there are some kinds of variables:
1. trainable, inference-time constant: in `ModelProto.graph.initializer` and `update_binding`
2. trainable, not inference-time constant: in `ModelProto.graph.input`, `ModelProto.graph.initializer`, and `update_binding`
3. not trainable, inference-time constant: in `ModelProto.graph.initializer`
4. not trainable, not inference-time constant:  in `ModelProto.graph.input`, `ModelProto.graph.initializer`

If we always put `trainable` weights in `ModelProto.graph.input`, the categories (1) and (2) will be merged into one category. Is it ok?

[Update]
If we create a model-level field, `ModelProto.global_initializer`, to store variables shared by both of inference and training-algorithm graphs. We can cleanly define these variable kinds.

1. trainable, inference-time constant: in `ModelProto.global_initializer` and `update_binding`
2. trainable, not inference-time constant: in `ModelProto.graph.input`, `ModelProto.global_initializer`, and `update_binding`
3. not trainable, inference-time constant: in `ModelProto.graph.initializer`
4. not trainable, not inference-time constant:  in `ModelProto.graph.input`, `ModelProto.graph.initializer`
gramalingam(2020-01-31 23:36:14):I personally feel it is okay to merge categories (1) and (2) into one. But it depends on the overall scenarios we want to support: do we imagine that after training, we would export an inference-model optimized for inference, or do we always want to leave the model in the trainable-format forever even when used primarily for inference? In the first scenario, the distinction is not relevant. In the second scenario, the distinction might be slightly relevant. 
wschin(2020-02-01 01:10:03):I feel ONNX can have a tool to convert and optimize trainable models to inference-only models. For inference-focusing models, I feel there should be no training information for smaller model size. With the optimizer states in Adam, Adagrad, and so on, the model size can be 3x larger than the inference-only version.
linkerzhang(2020-02-01 04:55:39):fair.
TMVector(2020-02-01 12:10:54):I completely agree that training models should be able to be optimized for deployment, but if you combine (1) and (2) then you can't _mechanically_ infer which trainable inputs/initializers should be inference-time constants.
TMVector(2020-02-01 12:34:50):We are in complete agreement that we shouldn't use `NodeProto.name` to store the `GraphProto.name`. I will try to exand on my original comment.
> By using the name as the graph identifier, you will only be able to have one instance, since node names should be unique. I think it would be better to use an attribute, e.g. graph_name.

### Store graph name in NodeProto.op_type
Using `NodeProto.op_type` would look like this:
```python
onnx.helper.make_node(
    op_type = model.graph.name,
    domain = "",
    name = "any_valid_c_identifier_for_diagnostic_purposes_only",
    inputs = ["a", "b", "c"],
    outputs = ["x", "y", "z"],
)
```
That conflates names from the `Graph` namespace (`GraphProto.name`) and the `Operator` namespace (`NodeProto.op_type`). cref https://github.com/onnx/onnx/blob/master/docs/IR.md#names-within-a-graph

Users seeing the op_type in error messages, performance logs, etc. or just trying to learn onnx will probably be confused that they can't find the operator in the Operators.md list.

Additionally, as you have pointed out, graphs cannot use names from the default operator set, and new releases of the ai.onnx operator set could add names which break existing models and graphs.

### Store the graph name in an attribute
I am suggesting an alternative: I think a `Graph` operator is a much clearer mechanism:
```python
onnx.helper.make_node(
    op_type = "Graph",
    domain = "",
    name = "any_valid_c_identifier_for_diagnostic_purposes_only",
    inputs = ["a", "b", "c"],
    outputs = ["x", "y", "z"],
    # Attributes:
    graph_name = model.graph.name,
)
```

The `Graph` operator can be listed in the Operators.md document, with clearly described semantics and limitations (e.g. currently only usable in training info to refer to the ModelProto.graph inference graph). It would be a special operator requiring runtime support similarly to how the `Gradient` operator requires runtime support.
TMVector(2020-02-01 13:15:06):Fair enough. I misread that part of the spec that I quoted to mean you could uniquely identify a value in the entire graph, but I don't think that's actually the case, so I don't think my suggestion really makes sense anyway.
wschin(2020-02-01 13:52:30):@linkerzhang, @gramalingam , adding a Graph operator sounds good to me. We just need to carefully define which names can be referenced by `graph_name`. Maybe in its first version, the only allowed name is `ModelProto.graph.name` but this syntax is extensible to call a Graph in a graph pool stored somewhere.
gramalingam(2020-02-03 04:46:56):I agree with @TMVector 's point about (not conflating) the two namespaces. Using a "Graph" operator is possible solution. However, we do also have the alternative mechanism ("functions") for representing a "callable subgraph". Do we want these two mechanisms? (I know at some point the training proposal using "functions" and then it was eliminated at some stage, and I don't fully know the history/reasoning behind that decision. I don't mean to resurrect a settled issue, but was wondering.)
wschin(2020-02-03 07:50:12):There are two reasons.
1. `Function` is an alternative of `Operator`, so it contains attributes which are not used when representing a graph.
2. Previous `TrainingInfoProto` contains initializers and `Function`. We can use `Graph` to cover both. The semantic of invoking a `Graph` is well-defined; in contrast, we need to further define how the inputs, outputs, and initializers are referenced by `Function`.
gramalingam(2020-02-03 17:02:50):I think there is an answer to @TMVector 's concern. Well, two answers: (a) We can use the inputs of the trainable-graph (the update algorithm) as the inputs of the optimized inference-model generated after training, (b) The optimization-tool can give users control over the generated model to control whether trained-parameters should be made inputs of the generated model.
gramalingam(2020-02-03 17:16:10):Here is my mental model of what's in the model-file:
```
   function inference(input X, input W) {
      initial value W = ..., C = ...;
   }
   function training-step(input X, input Y) {
      static TW; // trainable weight
      ...
      Z = inference(X, TW)
      ...
      TW = update(...)
   }
```
If we want a trainable weight that should not be a constant in an inference-model that is exported, we can make this an input of the update algorithm (option 1) or just make it user-controllable for the optimizer-and-export step.

I am not saying this is ideal. It is a compromise either way. I think this will be less disruptive. The problem is that the earlier design of the inference-model had some partial-support built-in in anticipation of training, and now that we have full training-support we can simplify things, but it will have an impact on existing infrastructure.
wschin(2020-02-03 17:21:21):(a) might not always work. Let's consider a real click-prediction problem. The user saved a trainable linear model (trainable parameters: linear coefficient vector `w` and a bias value `b`) and invoked a runtime to train it. 
Let's then assume
- `w` and `b` are in `ModelProto.graph.input` and `ModelProto.graph.initializer`.
- ~~`w` and `b` are in `TrainingInfoProto.algorithm.input` and `ModelProto.graph.initializer`.~~ (We can't do this due to name conflict. `ModelProto.graph.initializer` are visible in `TrainingInfoProto.algorithm`.)
- `w` and `b` can be found as keys in `TrainingInfoProto.update_binding`

In the inference-only model, would (a) make `b` an input? Note that `b` must be an input and `w` should not.

For (b), the user who wants to use a trained model might not be the author of the original trainable model. It's very difficult (and almost impossible when you have large neural networks) for the end user to figure out the mathematical meanings of those inputs and initializers. Thus, the end user won't be able to use (b) to fully optimize the model.
wschin(2020-02-03 17:38:37):@gramalingam, currently we don't have any trainable graph exported. I feel switching to a simpler semantic might lead to a easier life.

@spandantiwari, how troublesome it is to Pytorch exporter if we only use `update_binding` to determine trainable tensors?
spandantiwari(2020-02-03 20:03:16):PyTorch exporter does not have any training semantics at all today. But this question is quite relevant going forward.

The two choices:
1) Specify trainable parameters only through `update_binding` mapping.
2) In addition to 1), the trainable parameters must also be listed as forward graph input.

I find that just having 1) is necessary and sufficient. I see two advantages:
a) The spec is clear and minimal. 
b) It decouples the semantics of training vs inference from existing spec (and consequently existing exporters) to just the newly added training spec. So, if someone wants to encapsulate a graph exported for inference only in `TrainingInfoProto.algorithm`, they don't have to go back and re-export the graph to satisfy 2), i.e. re-export graph with trainable parameters listed as graph input. They can just take the existing graph as is and encode the differentiation between trainable and non-trainable parameters in `update_binding`. 

For keeping 2), I can see that some backends already may have optimizations that are based on whether an initializer is part of the graph input or not. For that, we can be flexible and choose to go with 2) even though it is not necessary. 

Finally, even if we adopt only 1) in the spec, it is quite possible that some exporters choose to export training graphs that have trainable parameters listed as graph input, and that should be fine. In fact, this doesn't even have to be restricted to training export only, but can be done for inference graphs also. And the backends can still keep their logic for which parameters to optimize. 

TMVector(2020-02-06 00:12:31):The spec says this for nodes:
> Input and outputs are positionally associated with operator inputs and outputs. Attributes are associated with operator attributes by name.

I think it would make sense to follow the same convention for `GraphCall` of positionally associating inputs and outputs. Optional inputs can be left out by providing `""`, and the same works for outputs.
E.g.
```python
onnx.helper.make_node(
    op_type= "GraphCall",
    inputs = ["X_1", "", "Z_1"],
    outputs = ["Y_1"], # Could use ["", "Y_1"] if there were two outputs
    # Attributes:
    graph_name = "MyInferenceGraph",
)
```
wschin(2020-02-06 16:45:48):Ok. Let's follow this align-by-position convention. Thanks for pointing this out.
TMVector(2020-02-06 17:27:55):```suggestion
as visualized below.
```
TMVector(2020-02-06 17:28:52):```suggestion
            "Inputs fed to the invoked graph. "
```
TMVector(2020-02-06 17:31:05):I'm not quite sure how you can disambiguate, but this could be read as `graph_name = "ModelProto.graph.name"`, rather than the value of `ModelProto.graph.name`

Maybe
> The only allowed value is the name of the inference graph, which is stored in \"ModelProto.graph.name\" in the ONNX model format.
gramalingam(2020-02-06 18:11:39):"fet" => "fed"
gramalingam(2020-02-06 18:18:34):I think "Notice that this syntax does NOT ..." needs to be replaced. I am not sure I understand what exactly it means. I think it is more important to specify the semantics: what happens (at runtime) when the "W" is omitted. I guess this line means that the corresponding initializer's current-value will be passed in implicitly (and it is NOT the case that the initializer value specified in the called-graph will be used). This semantics is, of course, different from the current semantics of a user-call to an inference-graph. This is the reason I preferred to _explicitly_ pass W. But if we allow it to be implicit, we should explain that clearly here.
gramalingam(2020-02-06 18:20:22):"inputs" => "inputs and outputs"
gramalingam(2020-02-06 18:25:41):Just a minor nit: I think the documentation of the operator is not the right place for capturing such constraints. Ideally, the set of "callable graphs" should be defined/constrained in TrainingInfoProto; may be one day we might even generalize it to allow more graphs to be called.
wschin(2020-02-06 18:45:20):`Notice that this syntax ...` means `W` is passed to the invoked graph by reference. The runtime doesn't make a copy of the current `W`. This also means --- for all invoked graphs without `W` explicitly provided, the same `W` (as a global variable) is shared (not copied) in every graph.

I agree `explicitly passing W` into the graph makes thing clearer. It implies that all trainable tensors are also inputs of inference graph, which sounds more easier than defining this complicated global-variable concept.

@TMVector, @chinhuang007, @spandantiwari, @linkerzhang, any comments?  
wschin(2020-02-06 21:42:06):@TMVector, your words are directly copied to the spec. Thank you.

@gramalingam, At the end of `TrainingInforProto`'s documentation, I explicitly indicates the uses of GraphCall can only happen in TrainingInfoProto.algorithm. Also, I add one sentence to ModelProto.graph --- Operators in `ai.onnx.training` domain cannot be included in this field.  
TMVector(2020-02-07 01:07:52):### Pseudo code
My understanding of how training would work (as the spec is currently written) is below (in pseudo python code). I'm trying to emphasise the details that we are discussing, but this should be equivalent to any implementation in terms of results.
```python
def bind(bindings, src, dst):
    for (k,v) in bindings.items():
        dst[k].value = src[v].value
def run(graph, constants, inputs, callable={}):
    """Compute the graph with the given inputs and constants.
    The 'graph' can contain GraphCall to graphs in 'callable'.
    Does NOT mutate graph/constants/inputs."""
    return ...

# Load the model. The model is conceptually trained to some point (0 or more iterations).
model = onnx.load(session.model_path)
# Unlike for inference, we don't allow constant folding of trainable constants
optimize(model, constant_folding_blacklist=trainable_initializers(model))

# Initialize pretty much the same way as for inference.
# I'm explicitly deleting the initializers here to show we are referring only to the
# constant buffers in all graph evaluations henceforth.
# Similarly, these constant buffers could be attached to the graph, I'm making
# them explicit to hopefully make it clearer when they do and don't change.
graph_constants = create_buffers_from(model.graph.initializer)
del model.graph.initializer
algo_constants = [create_buffers_from(info) for info in model.training_info]
for info in model.training_info:
    del info.algorithm.initializer

# Training intialization process (if starting training from scratch);
# overwrites constants.
if session.train_from_scratch:
    for (i, info) in enumerate(model.training_info):
        # Evaluate the initialization graph, which might include generating random
        # tensors. No inputs, and no visibility of initializers outside of itself.
        outputs = run(
            graph = info.initialization,
            constants = create_buffers_from(info.initialization.initializer),
            inputs = [],
            callable = {},
        )
        # Copy the outputs of the initialization graph to the constants referenced
        # via initialization_binding.
        bind(
            bindings = info.initialization_binding,
            src = outputs,
            dst = graph_constants + algo_constants[i],
        )

# Training process
# The spec doesn't specify how data is consumed, but I'll assume we use one batch per
# training info per iteration. Not sure how iterations/training_infos interact.
data = session.get_data_generator()
for iteration in range(session.training_iterations):
    for (i, info) in enumerate(model.training_info):
        # Run the algorithm graph, with access to the inference graph constants
        # and the ability to call the inference graph with a GraphCall node.
        outputs = run(
            graph = info.algorithm,
            constants = graph_constants + algo_constants[i],
            inputs = {**next(data)},
            callable = {
                model.graph.name: (lambda inputs:
                    run(model.graph, graph_constants, inputs, callable={})
                )
            },
        )
        # Copy the outputs of the algorithm graph to the constants referenced
        # via update_binding.
        bind(
            bindings = info.update_bindings,
            src = outputs,
            dst = graph_constants + algo_constants[i],
        )

# Save the model now it has been trained for more iterations
model.graph.initializer = create_initializers_from(graph_constants)
for (i, info) in enumerate(model.training_info):
    info.algorithm.initializer = create_initializers_from(algo_constants[i])
onnx.save(model, session.out_path)
```
Note the constants are updated atomically after each evaluation of a graph -- each evaluation always sees the latest set of constants, but they don't change until after the graph has been evaluated.

As I understand the spec, the result of training should be equivalent to doing this for each iteration:
1. Load model
2. Run algorithm (equivalent to freezing inference graph as an operator and then running algorithm with GraphCall nodes replaced with that operator)
3. Update initializer values from algorithm output based on update_binding
4. Save model

As soon as you've done one iteration the values of the previous iteration are inaccessible (unless you explicitly kept them, or they aren't trainable).

Does this seem right @wschin?

### Regarding optional inputs
It seems the behaviour for optional inputs _within an iteration_ should look identical to the behaviour at inference time. If the input is provided then that tensor name is bound to the input value, if the input is not provided, the tensor name is bound to the constant.

I agree that sentence is confusing:
> Notice that this syntax sugar does NOT eliminate the edge connecting "W" and the GraphCall.

There isn't really an edge between the `W` tensor and the `GraphCall` node when that input isn't provided -- but there is a default constant `W` which is _in_ the graph called by `GraphCall`, and that is what is used for the tensor name `W` within the graph.

### Thoughts
Some thoughts having done the exercise of writing it out:
* Should it be specified how data is consumed? Is the flow is `info[0](data[0]); info[1](data[1]); info[0](data[2]); ...`?
* I'm not sure I understand/remember why having multiple training_infos is helpful, unless I misunderstood how they should be used.
* As I read it, each `algorithm` can only update the inference graph and itself, so each `algorithm` would need to have its own iteration count etc. Is that what you were thinking?
wschin(2020-02-07 07:40:29):> As I understand the spec, the result of training should be equivalent to doing this for each iteration:
> 
> 1. Load model
> 2. Run algorithm (equivalent to freezing inference graph as an operator and then running algorithm with GraphCall nodes replaced with that operator)
> 3. Update initializer values from algorithm output based on update_binding
> 4. Save model
> 
> As soon as you've done one iteration the values of the previous iteration are inaccessible (unless you explicitly kept them, or they aren't trainable).
> 
> Does this seem right @wschin?

Yes. The only thing we need to clarify here is `what tensor to use when the user doesn't specify an optional input when calling the inference graph`. In your code, all initializers are used as `global variables`. If I invoke the inference graph twice, the same initializer may be connected to nodes in the two graphs. We can describe such two graph calls in Python:
```python
W_init = Tensor([1, 2, 3])
def inference_graph(X, W=None): # W is optional
  if W == None:
    W = W_init # W is a reference to W_init
  Y = X * W
  return Y
X_1 = Tensor([-1, 0, 1])
X_2 = Tensor([2, 4, 6])
Y_1 = inference_graph(X_1)
Y_2 = inference_graph(X_2)
Z = Y_1 + Y_2
```
If we call back-propagation on `Z` agains `W_init`, the trainable tensor `W_init` may receive its gradient `dZ/dW_init`. The user might feel strange because there is no explicit connection between `Z` and `W_init`. Thus, to avoid implicit edges in the training graph, @gramalingam prefers to do this
```python
W_init = Tensor([1, 2, 3])
def inference_graph(X, W): # W is not optional
  Y = X * W
  return Y
X_1 = Tensor([-1, 0, 1])
X_2 = Tensor([2, 4, 6])
Y_1 = inference_graph(X_1, W_init)
Y_2 = inference_graph(X_2, W_init)
Z = Y_1 + Y_2
```
Another way to avoid implicit edges is 
```python
W_init = Tensor([1, 2, 3])
def inference_graph(X, W=None): # W is optional.
  if W == None:
    W = deepcopy(W_init) # W is NOT a reference to W_init
  Y = X * W
  return Y
X_1 = Tensor([-1, 0, 1])
X_2 = Tensor([2, 4, 6])
Y_1 = inference_graph(X_1)
Y_2 = inference_graph(X_2)
Z = Y_1 + Y_2
```

> 
> ### Regarding optional inputs
> It seems the behaviour for optional inputs _within an iteration_ should look identical to the behaviour at inference time. If the input is provided then that tensor name is bound to the input value, if the input is not provided, the tensor name is bound to the constant.

The constant can be passed into optional input slot by a reference or a copy as described in my pseudo code above (by-reference case is the first snippet and the 3rd snippet is for by-copy case).

> 
> I agree that sentence is confusing:
> 
> > Notice that this syntax sugar does NOT eliminate the edge connecting "W" and the GraphCall.
> 
> There isn't really an edge between the `W` tensor and the `GraphCall` node when that input isn't provided -- but there is a default constant `W` which is _in_ the graph called by `GraphCall`, and that is what is used for the tensor name `W` within the graph.

Sorry for repeating my comment. Again, we don't only need to determine which value to use, we also need to determine if the value is pass-by-value or pass-by-reference.

> 
> ### Thoughts
> Some thoughts having done the exercise of writing it out:
> 
> * Should it be specified how data is consumed? Is the flow is `info[0](data[0]); info[1](data[1]); info[0](data[2]); ...`?

No. Input schema of `TrainingInfoProto.algorithm` already encodes such information. If the model author want to train the model iteration-by-iteration, the input shape could be [1, C, H, W]. If the model author thinks a batch should be used, the input shape could be [N, C, H, W]. Similarly, if the author wants to see the entire training data set, we might end up with input shape [10000000, C, H, W].

> * I'm not sure I understand/remember why having multiple training_infos is helpful, unless I misunderstood how they should be used.

It's for GAN. In GAN iterations, two parameters are alternatively updated so there are two assignments in one iteration.
```python
W = W_init
D = D_init
for i in range(10000):
  W = W - f(W, D)
  D = D - f(W, D)
```
To encode two assignments, we need two `TrainingInfoProto`.

> * As I read it, each `algorithm` can only update the inference graph and itself, so each `algorithm` would need to have its own iteration count etc. Is that what you were thinking?

Yes. It allows different stage to determine if its training is successful. In mix-precision training, the user don't need to increase iteration count if gradient is `NaN`.
TMVector(2020-02-07 11:04:44):### RE: optional inputs
> we don't only need to determine which value to use, we also need to determine if the value is pass-by-value or pass-by-reference.

It sounds like we agree that one execution of a training_info algorithm looks like:
1. Evaluate the algorithm graph
2. Update the constants _according to update_binding_

Inference (step 1) has no capability for mutation; only the update step (step 2) does, and the update step is precisely defined by the `update_binding` map. The update binding is not affected by whether you use an optional input or not, or what those inputs are -- there is no implicit backprop and update, only explicit binding of which constants to update, and explicit calculation of what the new values should be.

Here's a somewhat contrived example:
```
graph MyInferenceGraph (%x, optional %w) {
  default %w = [1]
  return MatMul(%x, %w)
}
graph algorithm (%data, %label) {
  // We have access to initializers from MyInferenceGraph, namely the default %w

  // Forward pass with no override of %w
  %score = GraphCall(%data, "", graph_name="MyInferenceGraph")

  // Forward pass where we for some reason scale %w by 1.1
  %s = [1.1]
  %larger_w = Mul(%w, %s)
  %score1 = GraphCall(%data, %larger_w, graph_name="MyInferenceGraph")

  // Ensemble the two forward passes
  %ensemble_score = Mean(%score, %score1)

  // Compute delta we want to subtract from %w
  %d_label = Sub(%label, %ensemble_score)
   // dw/dlabel at w=%w (not 1.1*%w)
  %d_w = Gradient(%data, %w, xs=["%data", "%w"], y="%d_label")
  
  // Explicitly calculate new value for %w
  %new_w = Sub(%w, %d_w)
  return %new_w
}
```
and your update_binding would look like:
```python
{ "w": "new_w" }
```

### RE: Thoughts
Thanks for you answers to my questions. Coming back on the data one: so training is only defined for one call; it doesn't loop over a dataset? I.e. the spec defines behaviour of
```python
def train(model, x):
    "Run through training_info once"
    for info in model.training_info:
        outputs = run(info.algorithm, x) # Same input to all infos
        model = update(model, outputs)
    return model
```
and the user can loop over it, or give a generator (e.g. batch generator) to the runtime which will loop over it.
wschin(2020-02-07 17:13:28):> ### RE: optional inputs
> > we don't only need to determine which value to use, we also need to determine if the value is pass-by-value or pass-by-reference.
> 
> It sounds like we agree that one execution of a training_info algorithm looks like:
> 
> 1. Evaluate the algorithm graph
> 2. Update the constants _according to update_binding_
> 
> Inference (step 1) has no capability for mutation; only the update step (step 2) does, and the update step is precisely defined by the `update_binding` map. The update binding is not affected by whether you use an optional input or not, or what those inputs are

Yes!

> -- there is no implicit backprop and update, only explicit binding of which constants to update, and explicit calculation of what the new values should be.

What method is used in your mind for passing optional inputs into the inference graph? Pass-by-value? Pass-by-reference? Pass-by-reference may lead to implicit back propagation edge.

Pass-by-value example:
```python
W_init = Tensor([1, 2, 3]) # Global constant.
def inference_graph(X, W=None): # W is optional.
  if W == None:
    W = deepcopy(W_init) # W is NOT a reference to W_init; we pass W in by value.
  Y = X * W
  return Y
X_1 = Tensor([-1, 0, 1])
X_2 = Tensor([2, 4, 6])
Y_1 = inference_graph(X_1)
Y_2 = inference_graph(X_2)
Z = Y_1 + Y_2
```

Pass-by-reference example:

```python
W_init = Tensor([1, 2, 3]) # Global constant.
def inference_graph(X, W=None): # W is optional
  if W == None:
    W = W_init # W is a reference to W_init; we pass W in by reference.
  Y = X * W
  return Y
X_1 = Tensor([-1, 0, 1])
X_2 = Tensor([2, 4, 6])
Y_1 = inference_graph(X_1)
Y_2 = inference_graph(X_2)
Z = Y_1 + Y_2
```

> 
> Here's a somewhat contrived example:
> 
> ```
> graph MyInferenceGraph (%x, optional %w) {
>   default %w = [1]
>   return MatMul(%x, %w)
> }
> graph algorithm (%data, %label) {
>   // We have access to initializers from MyInferenceGraph, namely the default %w
> 
>   // Forward pass with no override of %w
>   %score = GraphCall(%data, "", graph_name="MyInferenceGraph")
> 
>   // Forward pass where we for some reason scale %w by 1.1
>   %s = [1.1]
>   %larger_w = Mul(%w, %s)
>   %score1 = GraphCall(%data, %larger_w, graph_name="MyInferenceGraph")
> 
>   // Ensemble the two forward passes
>   %ensemble_score = Mean(%score, %score1)
> 
>   // Compute delta we want to subtract from %w
>   %d_label = Sub(%label, %ensemble_score)
>    // dw/dlabel at w=%w (not 1.1*%w)
>   %d_w = Gradient(%data, %w, xs=["%data", "%w"], y="%d_label")
>   
>   // Explicitly calculate new value for %w
>   %new_w = Sub(%w, %d_w)
>   return %new_w
> }
> ```

If we choose pass-by-value to pass optional inputs into the first `GraphCall`, the first `GraphCall` has no effect on `%d_w`. In contrast, if we choose pass-by-reference, the following two calls are equivalent

```
%score = GraphCall(%data, "", graph_name="MyInferenceGraph")
```

```
%score = GraphCall(%data, %w, graph_name="MyInferenceGraph")
```

Thus, the first `GraphCall` may affect the value of `%d_w`.

It seems to me the first one is more convenient to model authors. The second one is more compiler friendly (because no implicit edges is in computation graph). I think we should do `pass-by-reference` now (yesterday on Gitter, I prefer `pass-by-value`).

> 
> and your update_binding would look like:
> 
> ```python
> { "w": "new_w" }
> ```
> 
> ### RE: Thoughts
> Thanks for you answers to my questions. Coming back on the data one: so training is only defined for one call; it doesn't loop over a dataset? I.e. the spec defines behaviour of
> 
> ```python
> def train(model, x):
>     "Run through training_info once"
>     for info in model.training_info:
>         outputs = run(info.algorithm, x) # Same input to all infos
>         model = update(model, outputs)
>     return model
> ```
> 
> and the user can loop over it, or give a generator (e.g. batch generator) to the runtime which will loop over it.

If the author wants to loop through a data set, the training graph should contain a loop which loops to the given input. The training spec only says `Hey, do this and then your model can be improved` but the end-user really have no idea about the mathematical meaning of the training graph. A training graph can encode 1/1000/any training iterations.
linkerzhang(2020-02-08 00:47:24):this is not what you want, I believe, please run gen_proto.py --ml --lite.
wschin(2020-02-08 01:16:09):Done. Thank you!
wschin(2020-02-08 18:01:45):@TMVector, in the spec of `GraphCall` we now clearly say the global initializer `W` would be used whenever `W` is needed but missing. It's `pass-by-reference` and matches your python code.

```
The variable "W" is an optional input in the called graph. If the user omits it,
the input names of GraphCall becomes ["X_1", "", "Z_1"] and the initializer
of "W" may be passed into the called graph by reference. Pass-by-reference means
that

- There is a global variable "W" (the initializer).
- That global "W" is used whenever "W" is required but missing.

From the view of computation graph, the Conv operators invoked by GraphCall's
without specifying "W" as an input may be connected the global "W" variable.
This rule applies to all optional inputs in the called graph.
```
gramalingam(2020-02-09 01:35:58):Short summary of my opinion: specify the W parameter explicitly in the GraphCall (which is the second/middle solution among the three that Wei-Sheng presented). Is there any good reason why we should not do this?

Re. @wschin 's option-1 vs. option-3 : I am not sure I understand the difference here. In fact, I would prefer to simplify both options and write it as
```python
   def inference_graph(X, W=None): # W is optional
      return X * ( W_init if W == None else W)
```
Mathematically, from a semantics-specification, perspective they are the same. The interesting question (which Wei-Sheng refers to) is whether they are equivalent from the perspective of automatic-differentiation. I think they should be. An extra "identity assignment" should not affect or confuse the differentiation-algorithm.

I guess Wei-sheng may be referring to the details of the differentiation-algorithm and the backprop graph generated. Obviously, it could be somewhat more cumbersome depending on the exact option chosen. I feel specifying W explicitly would be simpler than either of the reference/value variant discussed.
wschin(2020-02-09 06:34:16):Let me list the three styles again for convenience. I added some comments to Option 3 to explain its difference comparing with Option 1.

Option 1: pass-by-reference (TVMector's python code).

```python
W_init = Tensor([1, 2, 3])
def inference_graph(X, W=None): # W is optional
  if W == None:
    W = W_init # W is a reference to W_init
  Y = X * W
  return Y
X_1 = Tensor([-1, 0, 1])
X_2 = Tensor([2, 4, 6])
Y_1 = inference_graph(X_1)
Y_2 = inference_graph(X_2)
Z = Y_1 + Y_2
```

Option 2: @gramalingam's suggestion. It's the cleanest representation but trainable initializers must be inference inputs.

```python
W_init = Tensor([1, 2, 3])
def inference_graph(X, W): # W is not optional
  Y = X * W
  return Y
X_1 = Tensor([-1, 0, 1])
X_2 = Tensor([2, 4, 6])
Y_1 = inference_graph(X_1, W_init)
Y_2 = inference_graph(X_2, W_init)
Z = Y_1 + Y_2
```

Option 3: Pass-by-value. Similar to Option 1 but we create an independent copy of `W_init` each time `W`'s value is missing but required. Using Option 3 may generate backward graph different than Option 1.

```python
W_init = Tensor([1, 2, 3])
def inference_graph(X, W=None): # W is optional.
  if W == None:
    # W is NOT a reference to W_init.
    # Calling a copy ctor means that another initializer `W_init_copied` is created.
    # The original `W_init` is not used at all in this scope.
    # There is no `Identity` between `W_init` and `W_init_copied`.
    W = tensor_copy_constructor(W_init) 
  Y = X * W
  return Y
X_1 = Tensor([-1, 0, 1])
X_2 = Tensor([2, 4, 6])
Y_1 = inference_graph(X_1)
Y_2 = inference_graph(X_2)
Z = Y_1 + Y_2
```
I completely agree with @gramalingam that Option 2 is the best to an IR. I prefer Option 1 a little bit more because it doesn't force users to add trainable tensors to inference graph's input list. I might be wrong because I assume that data-scientist users want to directly interact with the IR and I want to make their life easier. Another reason to support Option 2 is that even if we choose Option 2 for the 1.7 release, we can easily switch to Option 1/3 in 1.7.1.
TMVector(2020-02-09 23:49:45):I'm so sorry, I completely missed that this was about gradient flow 🤦‍♂.

Yeah okay, I see the merit of passing all trainable tensors as inputs now, sorry @gramalingam.
* It would make the behaviour around the `Gradient` operator a lot more obvious
* You need to do this anyway to implement multiple steps of gradient descent within a single `TrainingInfoProto.algorithm` like @wschin mentioned (without just reimplementing the forward graph in the training graph)

And @wschin makes a great point that this could change in the future.

I do still think it's really important that the inference graph should be ready for deployment without manual effort though. I'm imagining a use-case where users (possibly without ML expertise) are downloading pre-trained models which they can fine-tune with their own data, but this would also be valuable in other scenarios, just to make deployment easy.
Throwing around some ideas of how that could be achieved:
* `GraphCall` could have the ability to override constants that aren't in the input list. This has the benefit of keeping training and inference semantics quite separate. This could be implemented by listing "new inputs" in an attribute, which would make those named initializers into inputs for this particular `GraphCall`.
  E.g. if the inference graph looked like
  ```
  graph MyInferenceGraph(x) {
    %y = Mul(%x,  %w)
    return %y
  }
  ```
  the `GraphCall` in the training graph might look like
  ```python
  onnx.helper.make_node(
    op_type = "GraphCall",
    inputs = ["x0", "w0"],
    outputs = ["y0"],
    # Attrs
    graph_name = "MyInferenceGraph",
    new_inputs = ["w"],
  )
  ```
* Assume that if an input with a default is also in any `update_binding`, then it is inference-time constant. This disallows trainable, inference-time inputs with defaults.

Side note:
If the behaviour of options 1 and 3 are both things we want, what do you think about introducing a `StopGradient` operator, like Pytorch [`torch.Tensor.detach()`](https://pytorch.org/docs/stable/autograd.html#torch.Tensor.detach)?
wschin(2020-02-11 00:56:43):Thanks @TMVector and @gramalingam for those brilliant ideas.

After reviewing all our discussions, I am shocked for how things could be complicated if we don't use Option 2. What do you guys think if we go with Option 2 and make future steps based on user feedback? In my opinion, it's better to take a more **conservative** step when making such a fundamental architecture decision.

[Update] Here are [some changes](https://github.com/onnx/onnx/pull/2314/commits/7d382bf8edec06f994fc19a3c0d08edc027baf1c) needed to use Option 2. Those changes are equivalent to Option 2 (trainable tensors) + Option 3 (training-time constants in the inference graph). Notice that Option 3 is not compatible to Option 1, so we can't switch to Option 1 easily afterwards.
TMVector(2020-02-11 16:31:21):Okay, so my understanding is:
* All initializers used within the graph of a `GraphCall`  (e.g. `w`) are treated as though they were detached from gradient flow (i.e. `w' = StopGradient(w)`). This includes initializers that are used when optional inputs aren't provided.
* You can only take the gradient with respect to inputs of `GraphCall`s, but you can pass an initializer as an input to `GraphCall` explicitly. Note only uses of input in the body of the inference graph will pass-through gradient flow, all the uses of the initializer stop the gradient.

Some examples:
```
graph InferenceGraph_1(optional %w = [7]) {
  const c% = [11]
  %y = %w * %c
  return %y
}
graph training() {
  %y1 = GraphCall("", graph_name="InferenceGraph_1")
  // Invalid! %w is not used to calculate %y.
  // We can't calculate any gradients because there are no inputs!
  // The function being differentiated is 'y = StopGradient(w) * StopGradient(c)'.
  // %dy_dw = Gradient(%w, %x0, xs=["%w"], zs=["%x0"], y="%y1")

  // %dy_dw is [11] (StopGradient(c)).
  // The function being differentiated is 'y = w * StopGradient(c)'.
  %y2 = GraphCall(%w, graph_name="InferenceGraph_1")
  %dy_dw = Gradient(%w, xs=["%w"], y="%y2")

  // %dy_dc is [11] (StopGradient(c)), not [22] (2*c).
  // The function being differentiated is 'y = c * StopGradient(c)'.
  %y3 = GraphCall(%c, graph_name="InferenceGraph_1")
  %dy_dc = Gradient(%c, xs=["%c"], y="%y3")

  // %dy_dc is [2*11*11] (2*c*StopGradient(c)'), not [3*11*11] (3*c^2).
  // The function being differentiated is 'y = (c*c) * StopGradient(c)'.
  %c2 = Mul(%c, %c)
  %y3 = GraphCall(%c2, graph_name="InferenceGraph_1")
  %dy_dc = Gradient(%c, xs=["%c"], y="%y3")

  // %dy_dc is [3*11*11] (2*c^2), not [3*11*11 + 7] (3*c^2 + w).
  // The function being differentiated is 'y = c^3 + StopGradient(w) * StopGradient(c)'.
  %c3 = Pow(%c, 3)
  %y4 = GraphCall("", graph_name="InferenceGraph_1")
  %y5 = Add(%c3, %y4)
  %dy_dc = Gradient(%c, xs=["%c"], zs=["%w"], y="%y5")
}
```

Does that sound right?
wschin(2020-02-11 22:12:58):> * All initializers used within the graph of a `GraphCall`  (e.g. `w`) are treated as though they were detached from gradient flow (i.e. `w' = StopGradient(w)`). This includes initializers that are used when optional inputs aren't provided.

Yes before today's WG meeting. Now we stay closer to your original Option 1. Two model level attributes are created for storing global (shared by training and inference) variables:

- ModelProto.global_initializer
- ModelProto.global_sparse_initializer

> * You can only take the gradient with respect to inputs of `GraphCall`s, but you can pass an initializer as an input to `GraphCall` explicitly. Note only uses of input in the body of the inference graph will pass-through gradient flow, all the uses of the initializer stop the gradient.
> 

We can't put any restriction on `Gradient` computation --- it's well-defined by math. It means `You can only take the gradient with respect to inputs of GraphCalls` is not quite valid.
TMVector(2020-02-11 23:45:05):```suggestion
  // cannot be found as a key in the "update_binding"s of any
```
TMVector(2020-02-11 23:48:14):```suggestion
  // The following denote mutable global variables visible in ModelProto.graph as well as
```
TMVector(2020-02-11 23:50:34):Wouldn't it be more consistent to say that graph initializers cannot have the same name as any global initializer? The spec currently requires node outputs in subgraphs to not hide any tensors from parent graphs.
TMVector(2020-02-12 14:29:30):So when you use the gradient operator in the training graph, on a graph containing a `GraphCall`:
* Global initializers preserve gradient flow even when used by the inference graph but not explicitly passed to the `GraphCall`
* Inference graph initializers aren't visible to the training graph so you can't take the gradient w.r.t. them
* If the inference graph has an input with the same name as a global initializer, then that input is optional

Is that right?

I'm guessing the main purpose in adding the global intializers is to make it clearer that these are implicitly used by `GraphCall` without the `GraphCall` having to explicitly pass values for each one?
postrational(2020-02-12 14:48:05):```suggestion
  // as well as all graphs in TrainingInfoProto. These variables are initialized
```
wschin(2020-02-12 16:43:19):```suggestion
  // - Make inference graph callable from TrainingInfoProto via GraphCall operator.
```
wschin(2020-02-12 16:45:02):```suggestion
  // - Add model-level initializer lists. They stores global variables which are visible to the inference graph and all TrainingInfoProto's. Those global variables can be modified by the execution of TrainingInfoProto.
```
wschin(2020-02-12 16:45:12):```suggestion
```
wschin(2020-02-12 16:52:19):Graph initializer means `declaring a variable so local variable hides outer-scope's variable`. Operator output means `declaring a variable if that variable doesn't exist; if that variable exists, it will be an assignment which we want to avoid`.
wschin(2020-02-12 17:00:30):> So when you use the gradient operator in the training graph, on a graph containing a `GraphCall`:
> 
> * Global initializers preserve gradient flow even when used by the inference graph but not explicitly passed to the `GraphCall`

Yes. Global variable is referenced if we don't find a local variable with the required name (here the name is "W").

> * Inference graph initializers aren't visible to the training graph so you can't take the gradient w.r.t. them

Yes. They are inference-time and training-time constants.

> * If the inference graph has an input with the same name as a global initializer, then that input is optional

Yes.

> 
> Is that right?
> 
> I'm guessing the main purpose in adding the global intializers is to make it clearer that these are implicitly used by `GraphCall` without the `GraphCall` having to explicitly pass values for each one?

Yes! It also forces the initialization/training algorithm not to touch inference-only-constants.
wschin(2020-02-12 17:13:28):```suggestion
  repeated TensorProto global_mutable_initializer = 30;
```
maybe?
wschin(2020-02-12 17:43:21):Merge these two lists into a string list? Each element is a name of `ModelProto.graph.initializer`.
gramalingam(2020-02-12 18:48:43):(a) I thought the above version also allowed initializers from the training-algorithm?
(b) I forget whether today's meeting said it is okay to explicitly specify this, or just infer it from update_bindings?
(c) We didn't discuss one point in meeting: whether this list allows any (ModelProto.graph) initializer or only initializers that appear in input?
wschin(2020-02-12 18:58:27):> (a) I thought the above version also allowed initializers from the training-algorithm?

If the mutable initializer is only needed in a training graph, its name won't appear in the global variable name list.

> (b) I forget whether today's meeting said it is okay to explicitly specify this, or just infer it from update_bindings?

Ok with both but it seems people like `update_bindings` more.

> (c) We didn't discuss one point in meeting: whether this list allows any (ModelProto.graph) initializer or only initializers that appear in input?

No because
1. We only need `initializers` (trainable and mutable tensors) for training. I don't feel an input is a mutable thing.
2. If an input is promoted to global, multiple graph calls will have to use the same global input, which sounds strange.

ebarsoum(2020-02-13 01:36:47):Let's make it string. All tensors are in global_initializer, the name of the mutable subset are in global_mutable_initializer.
wschin(2020-02-13 16:37:57):Ok. I change this field to `global_mutable_initializer_name` and it can only contains names of `ModelProto.graph.initializer`. Do you want `TrainingInfoProto.algorithm.initializer` and `TrainingInfoProto.initialization.initializer` to be mutable and globally-visible?
CLAassistant(2019-09-16 08:52:29):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=2317) <br/>All committers have signed the CLA.
TMVector(2019-09-16 08:57:08):Hmm, I'm not sure I'm happy to sign that CLA at this time...
gramalingam(2019-09-16 17:41:46):Thanks for catching this. Created a duplicate PR for this change.
prasanthpul(2019-09-16 17:44:26):@TMVector let me know if you have any specific question about the CLA. It's a standard Apache CLA.
TMVector(2019-09-17 14:17:47):Thanks @prasanthpul, checked with work and it's fine to sign :grin:
snnn(2019-09-17 18:39:07):I think it should say: 
when IR_VERSION < 4, .... , else .... .

Because a model with IR_VERSION ==3 is still legit. 


gramalingam(2019-09-17 19:03:34):@snnn : the way it has been worded, it is valid for all IR_VERSIONs.
linkerzhang(2020-01-15 00:05:33):I'm viewing this clarification is still correct and good, as part of the spec (docs).

Shape inference (functionality wise) is not a MUST. I'm viewing that as a helper tool. It should/can be either fixed in this repo or users fix them in their own repo (env).
snnn(2019-09-17 20:29:13):However, this feature is experimental, it may break shape inference.  And we're not going to fix it.
TMVector(2019-09-18 08:46:22):I cannot find anything saying external raw data is experimental in code/docs/PR comments, so if that's true then documenting its experimental status should be high priority.  
AFAICT, external data is supported at least by the ONNX Python API (onnx/onnx#678) and ONNX Runtime (microsoft/onnxruntime#520 e.g. test file `opset9/tf_resnet_v2_50/model.onnx`).

I wouldn't expect it to break type inference, since the type/shape is stored in the [`data_type`/`dims` fields](https://github.com/onnx/onnx/blob/master/onnx/onnx.in.proto#L351-L356) on TensorProto, not the `raw_data` field.

Apologies if I misunderstood something.
wschin(2019-09-18 16:28:48):It can breaks shape inference, for example, when `Reshape`'s shape is stored in external data.
TMVector(2019-09-18 16:33:16):Okay, that makes sense.

Maybe a dumb question, but why are tensors in external files considered inaccessible for shape inference?
snnn(2019-09-18 17:25:04):It could, but it need a big effort. 
wschin(2019-09-17 22:21:48):It's be nice if you can add a test.
gramalingam(2019-09-17 23:31:26):```
   It's be nice if you can add a test.
```
Agree. I am not quite sure when this manifests itself. I understand that "is" is a pointer-equality comparison (same object comparison), but here we are comparing primitives. In simple test-cases, "is" seems to work like "==" for primitive type … but may be it fails when object values are serialized from C++ and received in python? However, after this change, gen_doc.py works fine for me.
CLAassistant(2019-09-18 00:27:48):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=2328) <br/>All committers have signed the CLA.
shinh(2019-09-19 01:18:55):Thanks for your review! I think I handled all of your comments. Please take a look again?
hariharans29(2019-09-19 20:45:32):> Thanks for your review! I think I handled all of your comments. Please take a look again?

Just some minor comments to be addressed. Looks good otherwise. :)
shinh(2019-09-20 02:04:36):Thanks for your review! Let me check a couple of things before I actually change them.
shinh(2019-09-24 09:52:02):The test was randomly failing so I pushed it several times to trigger the rebuild.
shinh(2019-09-27 02:25:30):@hariharans29 Ping?
shinh(2019-10-04 03:28:23):@wschin: Friendly reminder :)
hariharans29(2019-10-04 04:29:07):@gramalingam @ebarsoum @houseroad -  pinging more people for reviews 

@shinh - Don't worry. This PR has been tagged for release 1.7. 1.6 just got done last week. So it will be picked up when the PRs start getting merged for the next release (maybe sooner).
shinh(2019-10-04 07:14:10):@hariharans29: Got it, thanks! I'm worried another person makes a similar patch and there'll be duplicated effort. It happened once in https://github.com/onnx/onnx/pull/1855 (and you were the author of https://github.com/onnx/onnx/pull/2041 :).
hariharans29(2019-10-04 07:32:30):@shinh - I see. Sorry, I should have checked if there was already a PR for Expand shape inference. Someone on my team needed it urgently, and I just wrote it without checking for any existing PRs. 
hariharans29(2019-09-18 23:47:24):we can still do rank inference as the ranks of the outputs will be the same as the rank of the input before returning.
hariharans29(2019-09-18 23:48:45):should this be an error ? 
hariharans29(2019-09-18 23:55:51):Per spec, it says if 'split' is not provided, split it into equal sized parts, so should the case where splitDimValue % ctx.getNumOutputs() != 0 be an error condition ?
hariharans29(2019-09-19 00:02:04):Otherwise, we might have to arbitrarily deal with it like the logic in 444-448. The logic is not prescribed per spec. So I think this should be an error condition as well. To me there are 3 possible error conditions in this op-

1) When 'split' is present and its count does not match number of outputs
2) When 'split' is present and its sum does not match input shape's corresponding value in the 'axis' dimension
3) When 'split' is not present and  input shape's corresponding value in the 'axis' dimension cannot be split evenly to the number of outputs

shinh(2019-09-19 01:17:38):Done
shinh(2019-09-19 01:17:47):Done
shinh(2019-09-19 01:17:54):Done
hariharans29(2019-09-19 20:41:01):Nit: I don't think we explicitly add tests for failure scenarios. Seems like these 2 may not be needed.
hariharans29(2019-09-19 20:42:49):can you please add 2 more tests - 

1) One with split attribute is missing and the default 'equal' split occurs for each output
2) One with (valid) negative axis

hariharans29(2019-09-19 20:44:50):nit 1: It seems a little ugly to store the same value 'ChunkSize' in the vector for each output. it would be great if the single value 'chunkSize' can just be handled below when you actually assign the dim value.

nit 2: 'chunkSize' doesn't seem c++ style. You can consider using 'chunk_size'
shinh(2019-09-20 01:55:14):Isn't the existing test case, `test_split_negative_axis` enough to cover these two cases? Do you want me to split it into two tests?
shinh(2019-09-20 02:03:40):As for 2, fixed.

As for 1, let me double-check if you like the change, since I feel current code is better. If I follow your suggestion, I think we'll have two copies of the for loop which assigns `dim_value` like

```
if (getRepeatedAttribute(ctx, "split", split)) {
   ...
   for (size_t i = 0; i < ctx.getNumOutputs(); i++) {
       *ctx.getOutputType(i)->mutable_tensor_type()->mutable_shape() =
            shape;
        ctx.getOutputType(i)
            ->mutable_tensor_type()
             ->mutable_shape()
             ->mutable_dim(axis)
             ->set_dim_value(split[i]);
    }
} else {
  ...
  int chunk_size = split_dim_value / num_outputs;
   for (size_t i = 0; i < ctx.getNumOutputs(); i++) {
       *ctx.getOutputType(i)->mutable_tensor_type()->mutable_shape() =
            shape;
        ctx.getOutputType(i)
            ->mutable_tensor_type()
             ->mutable_shape()
             ->mutable_dim(axis)
             ->set_dim_value(chunk_size);
    }
}
```
Is this what you want?
shinh(2019-09-20 02:03:55):Removed
hariharans29(2019-09-27 07:13:48):You are right. :) I did not notice the other test.
hariharans29(2019-09-27 07:14:18):That's fine. We can just keep it the way it is for now.
autoih(2019-09-18 20:38:43):Looks, can't use an instance to run the assertion. 
wschin(2019-09-18 16:06:56):+some people from SIG operator @ebarsoum, @spandantiwari, @gramalingam for visibility. I personally consider this change reasonable for some reasons below.

1. We have `MatMul` and `Add` but still introduced `Gemm` for this opportunity of further computation optimization. Because 2-D matrix multiplication is even more fundamental than Gemm, I feel it's reasonable to allow it in Gemm (a symbol stands for ultimate optimization).
2. Most frameworks can easily implement it given their existing `Gemm` code.
3. From user's perspective, it's sometimes annoying to use `Gemm` because we need to create zero initializer only for matching the spec.

To be fair, I also should note that this change is not going to increase the expressiveness of ONNX.
spandantiwari(2019-09-18 18:21:41):@wschin @JamesAllingham Just curious if this is being tracked for 1.6 release. If not, then we may have to update the opset version number in the PR from 11 to 12. 
wschin(2019-09-18 19:49:13):> @wschin @JamesAllingham Just curious if this is being tracked for 1.6 release. If not, then we may have to update the opset version number in the PR from 11 to 12.

It's a nice to have thing in 1.6. If we can't make it on time, we can bump the opset version if you want.
wschin(2019-09-18 19:52:35):@JamesAllingham, we need a shape inference test as well.
JamesAllingham(2019-09-18 20:47:32):> 
> 
> @JamesAllingham, we need a shape inference test as well.

I've added a test but I wasn't 100% sure what you wanted here so let me know if you wanted something else.
wschin(2019-09-18 20:56:45):> @JamesAllingham, we need a shape inference test as well.
> 
> I've added a test but I wasn't 100% sure what you wanted here so let me know if you wanted something else.

Ah, my bad. I missed the last file.
JamesAllingham(2019-09-18 21:01:40):> 
> 
> > @JamesAllingham, we need a shape inference test as well.
> > I've added a test but I wasn't 100% sure what you wanted here so let me know if you wanted something else.
> 
> Ah, my bad. I missed the last file.

No, I only just added it, you didn't miss anything! 
wschin(2019-09-18 19:42:08):```suggestion
  This operator supports **unidirectional broadcasting** (tensor C should be unidirectional broadcastable to tensor A * B); for more details please check [the doc](Broadcasting.md). This operator has **optional** inputs/outputs. See [the doc](IR.md) for more details about the representation of optional arguments. An empty string may be used in the place of an actual argument's name to indicate a missing argument. Trailing optional arguments (those not followed by an argument that is present) may also be simply omitted.
```
wschin(2019-09-18 19:43:32):```suggestion
<dd>Optional input tensor C. If not specified, the computation is done as if C is a scalar 0. The shape of C should be unidirectional broadcastable to (M, N).</dd>
```
wschin(2019-09-18 19:44:15):Very nice. It matches the spec perfectly.
wschin(2019-09-18 19:47:07):ver9 --> ver11.
JamesAllingham(2019-09-18 20:45:19):Fixed :)
JamesAllingham(2019-09-18 20:45:43):Good change, I've implemented it.
JamesAllingham(2019-09-18 20:46:20):I ended up adding a new line rather than a space, let me know if you feel strongly about the space and I'll change it.
wschin(2019-09-18 20:55:52):New line sounds good. Thanks.
spandantiwari(2019-09-18 21:09:57):nit: Minor point about code style - consider adding braces even for single line scopes to be consistent with coding style in this file.
JamesAllingham(2019-09-18 21:23:06):Will do 👍 
CLAassistant(2020-09-10 01:32:14):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=2332) <br/>All committers have signed the CLA.
askhade(2020-12-10 23:12:29):@TMVector : The change looks good to me can you please sign the DCO and add a test case for this. Thanks!
gramalingam(2020-12-11 00:57:55):Thanks @TMVector for this PR. I am trying to understand the intention of the original code. I wonder if this requires some more cleanup. Considering ```convert_model_to_external_data``` : it is unclear how it is supposed to save multiple tensors in the same file when it is not using different offsets for different tensors. Further, I guess it is supposed to be used in conjunction with ```save_model```; it seems to me it would be better to expose one method that combines both together. I realize that this issue is not due to the current PR. Adding @yufenglee @postrational for comments. It would be nice to update the python API documentation also to clarify the intended usage.
askhade(2020-12-14 05:48:21):save_external_data creates a new file for a tensor if it already does not exist (see line 149). It uses the name of the tensor as filename. There should be a check here to validate filename and if it is not valid then it should generate a random name for this file. For example tensor names can include characters like ':' , ',' , ';' which are invalid in a filename. 
jcwchen(2020-12-18 01:21:51):Close it because another similar PR has been merged. Please reopen it if you have other thoughts. Thank you!
gramalingam(2020-12-11 00:59:30):Would help to clarify in the documentation that only raw_data tensors will be transformed, not typed tensors.
gramalingam(2020-12-11 01:03:08):Unclear if this change is required here. It seems to be a minor utility; may be better to not even expose this externally. But if it is used externally, the correctness depends on the usage, I suppose. But I am okay with adding this as extra check for now.
jcwchen(2020-12-13 07:20:35):Other than tensor without 'raw_data', do all other kinds of tensors need to be converted to external data? (The original method looks like brute-force to me and large model will be separated into a lot of files in this case) If yes, ORT would encounter some issues from some tensors with external data. https://github.com/onnx/onnx/issues/3157 Tensor checking and unpacking with external data needs to be fixed.

Once the decision is made, it would be great if ONNX can clarify the usage of external data here: https://github.com/onnx/onnx/blob/master/docs/IR.md#external-tensor-data. Thank you for the contribution!



hariharans29(2019-09-18 23:19:01):This is incorrect as the input need not be 2D. This was a comment in the PR #2281  

Effectively once we know that the output shape is the same as the input shape, there is no point in any other validation. This is based on Rama's comment in #2132 - 

"I feel that validation and generating output shape are distinct sub-problems. We do the best we can for both. Even if we cannot do validation here, we can still generate information..."

So I just reverted the shape inference function altogether
skottmckay(2019-09-18 23:23:21):If we're doing the best we can we can still validate the axis so I think that should remain.
gramalingam(2019-09-18 23:33:13):Just to clarify the context and the point of my comment: there were cases where the inference function was "returning" while there was still useful steps it could perform (e.g., setting the output rank). My point was that we should DO it. This case is a bit different, since it is a "fail_shape_inference". If the rank is not required to be 2, it seems erroneous to enforce it, so dropping it seems the right thing. For the axis check: as Scott says, there seems nothing wrong with retaining it.
wschin(2019-09-18 23:37:10):What does `{name}` mean?
gramalingam(2019-09-18 23:40:18):It is replaced by the value of the parameter name, as in two lines below.
hariharans29(2019-09-18 23:42:01):This line is from above. Please check.
hariharans29(2019-09-18 23:43:07):It should be okay to leave as it is. This doc generator is shared between Softmax, Hardmax, LogSoftmax and the name is replaced in the generated doc.
hariharans29(2019-09-19 00:03:31):Retained the validation logic. Thanks for clarifying. :)
wschin(2019-09-19 03:42:21):```suggestion
        // The value of b_out is used as the value of b_in in the next iteration. Note that it doesn't affect the value of the b_in passed into this operator.
        %b_out = Sub(%a, %b_in)
```
We don't have assignment operator. To match the C code below, we need some comments to explain it.
wschin(2019-09-19 03:45:14):```suggestion
          // The variable b_in is passed into the block by value, so the original value of b_in won't be changed.
          b_in = b_out;
```
gramalingam(2019-09-20 03:08:15):I added a few more comments to address the two points you mention.
wschin(2019-09-20 04:33:46):There is no `b` anymore. This statement doesn't explicitly say that `the last value of b_out in one iteration will be the value of b_in in the next iteration`.
wschin(2019-09-20 04:39:02):[nit] Given the `return …` here, it looks like the Loop will terminate at the end of the first iteration.
gramalingam(2019-09-23 18:17:35):I think it is more than 1 iteration (in the whole example, you mean?), though it may terminate in 2 or 3 iterations (haven't checked fully). I just reused the existing loop logic, thought it is an artificial one. 
gramalingam(2019-09-23 18:19:05):That's what the pseudo-code is for, to show how b_out is copied into b_in that assignment statement. Explanatory comments may be more useful outside, in the description. I will see if I can add any more to clarify things there.
wschin(2019-10-07 20:19:45):[nit]
```suggestion
        // These below values are bound to the outputs of the loop and therefore accessible
```
prasanthpul(2020-02-01 02:22:40):Looks like the URL fixes have already been made via some other PR
fdwr(2019-09-20 22:50:16):"Additional"

"coordinate indices"
adtsai(2019-09-20 22:51:44):Typo - "additional"
adtsai(2019-09-20 22:55:24):Q: is "selection of the computed values" equivalent to saying that it affects only the shape of the output?
wschin(2019-09-20 22:56:29):Yes.
wschin(2019-09-20 22:59:36):Thanks!
wschin(2019-09-20 22:59:44):Will fix it.
fdwr(2019-09-20 23:00:15):How about:
"Additional elements added to the computed output tensor shape, extending the side with higher coordinates. Each padding value in `output_padding` must be less than or equal to the corresponding stride value. By default, this attribute is a zero vector of size equal to the output tensor rank. Note that this attribute doesn't directly affect the computed output values. It only controls the shape of the computed values. This is also called `adjs` or `adjustment` in some frameworks. If `output_shape` is explicitly provided, then `output_padding` does not contribute additional size to `output_shape` but is used to determine the `pads`."
wschin(2019-09-20 23:39:53):I will merge your ideas into the description.
wschin(2019-09-23 18:45:44):```suggestion
    auto* if_output = ctx.getOutputType(i);
```
wschin(2019-09-23 18:46:07):```suggestion
    *if_output = *then_output;
```
hariharans29(2019-09-23 18:46:39):nit: some minor formatting issue
kevinch-nv(2019-09-23 18:47:34):Typo: `same same data type`. Also the wording of the last sentence is awkward. What is a `conflict shape`?
kevinch-nv(2019-09-23 18:47:44):Same typo as above
hariharans29(2019-09-23 18:47:56):Is it better to just be explicit ? 

"The `then_branch` and `else_branch` may produce tensors with different shapes"
kevinch-nv(2019-09-23 18:48:04):Same as above
kevinch-nv(2019-09-23 18:51:55):Wording is awkward here, perhaps `avoid calling self._make_graph here for subgraph generation as extra reshape nodes will be created`
wschin(2019-09-23 19:16:47):Sure.
wschin(2019-09-23 19:18:11):I will add a explicit example.
gramalingam(2019-09-23 19:36:29):"same same" to "same"
gramalingam(2019-09-23 19:40:14):It is unclear what this ("conflict shapes would be unknown") means to an user interested in creating a model. Suggest changing this to "Corresponding tensors produced by the then-branch and else-branch are not required to have the same or compatible (static) shapes.". We could talk about how shape-inference will handle it, but I don't think that is part of the operator semantics. 
gramalingam(2019-09-23 19:47:18):There are a few issues with this logic. What we need to do is set the resulting shape as the "union" of the shapes produced by the THEN and ELSE branch. I recommend using this function: https://github.com/onnx/onnx/blob/c5af774aa950f6a5cf1a7b5f37d014fb2fbd82bc/onnx/defs/sequence/defs.cc#L21 

yuslepukhin(2019-09-23 21:05:25):Should we output separate message if  any of these are not set (NOT_SET) value?
yuslepukhin(2019-09-23 21:06:21):Suggest to avoide has_type() function. Simply type() would return a default instance which would have value_case() NOT_SET
yuslepukhin(2019-09-23 21:07:17):Use switch statement instead of has_*() functions which are absent in proto3. You want this to run in ML.NET.
gramalingam(2019-09-23 21:23:32):I believe it is not a problem with "one of" fields … the has_* functions are generated even in proto3 for "one of", if I am not mistaken
gramalingam(2019-09-23 21:31:11):See https://developers.google.com/protocol-buffers/docs/reference/cpp-generated#oneof-numeric-fields and https://developers.google.com/protocol-buffers/docs/reference/cpp-generated#oneof-embedded-message-fields … both proto2 and proto3 support "has_*" for one of fields. It is only for primitive-type fields that are not "one of" we have the proto2/proto3 issue (IIUC).
gramalingam(2019-09-23 21:41:58):Suggestion: "If corresponding outputs from the then-branch and the else-branch have static shapes S1 and S2, then the shape of the corresponding output variable of the if-node (if present) must be compatible with both S1 and S2 as it represents the union of both possible shapes." If we want to add a more detailed explanation, suggest adding to the operator description or IR.md (for the notion of "compatible").
hariharans29(2019-09-23 22:24:57):I think @wschin just copied this over to old.cc. This is the previous shape inference logic. This is now legacy. I think it is safer not to "enhance" this further unless there is a bug to be patched. We could be introducing new regressions in existing models using If-1.
wschin(2019-09-23 23:46:50):Yes!
wschin(2019-09-24 00:13:53):Added. Thank you!
gramalingam(2019-09-24 00:39:59):Just the "UnionShapeInfo(…)" should be sufficient, I think. No need for if-condition or the else-branch.
gramalingam(2019-09-24 00:42:21):Actually, it is valid for the first output to (a) Have no shape set, or (b) A shape of rank 1 set, with neither dim-value nor dim-param set, or (c) A shape of rank 1 with a unique dim-param. 
gramalingam(2019-09-24 00:45:31):How about changing it to "In contrast, the first output cannot have the shape [2] since [2] and [3] are not compatible."
wschin(2019-09-24 02:13:14):You're right.
wschin(2019-09-24 02:25:01):Sure.
wschin(2019-09-24 02:25:42):Ok. I have replaced part of my description with your idea.
wschin(2019-09-24 02:29:36):I feel the current description is more precise. From this description, adding Reshape itself is not a problem, but adding input required by Reshape causes the problem.
wschin(2019-09-24 02:29:50):Sure.
wschin(2019-09-24 02:32:16):I feel it's a bigger problem in the whole codebase. If you agree, I'd like to open another issue for addressing it globally.
wschin(2019-09-24 03:51:47):Using UnionShapeInfo produces wrong shapes in my test. Can I roll back to my original function?
gramalingam(2019-09-24 04:57:41):"(a) no type set" => "(a) no shape set"
wschin(2019-09-24 05:42:17):Nice catch! Thank you.
hariharans29(2019-09-24 01:42:55):CC: @wschin @gramalingam 
wschin(2019-09-24 18:11:03):Do we have tests for unsorted cases?
hariharans29(2019-09-24 18:16:06):> Do we have tests for unsorted cases?

I will add shape inference test for now. 
hariharans29(2019-09-24 18:35:02):@spandantiwari @wschin - added backend test for unsorted axes too
hariharans29(2019-09-24 01:47:20):won't work with existing logic.
spandantiwari(2019-09-24 17:31:14):+1 for mentioning about duplicate entries. 
spandantiwari(2019-09-24 17:55:32):As discussed, we should consider adding a note clarifying that the order of axes specified in the `axes` attribute does not matter, and that they should be sorted before being applied.
hariharans29(2019-09-24 18:15:34):I added this line - 

The order of values in `axes` does not matter and can come in any order.

I think this is good for a high level spec. Details like the operator will sort them after correcting negative values are superfluous, I feel. What do you think ?
hariharans29(2019-09-24 20:19:45):axes values in jumbled order
hariharans29(2019-09-24 17:33:29):Shouldn't this have TypeAndShapeInference function as before ? Please see #2269 for how the function looked before the change. This PR effectively removes type inferencing which was done previously and may lead to new regressions.
wschin(2019-09-24 17:44:21):You are right. Thanks.
prasanthpul(2019-09-24 23:05:33):Is the table in https://github.com/onnx/onnx/blob/master/docs/Versioning.md updated?
kevinch-nv(2019-09-24 23:14:13):@prasanthpul @houseroad I'll make one final commit after this one bumping the Version number in both those places, and that commit will be the one that is tagged as v1.6.0. I'll then cherry pick the version number update back into master once everything is done. 
CLAassistant(2019-09-25 01:14:52):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=2353) <br/>All committers have signed the CLA.
kevinch-nv(2019-09-25 19:16:36):@wschin looks like CI failed for one of the builds (https://travis-ci.org/onnx/onnx/jobs/589584006), perhaps a machine hang? Can this be wavied?
prasanthpul(2019-09-25 19:20:42):I've restarted that particular job.
wschin(2019-09-25 03:38:52):```suggestion
1.6.0|6|11|2
```
In ML domain, there is a LabelEncoder in opset 2.

If you find doing another CI build takes too long, I am ok with a post-release fix. This is just a documentation problem.
gramalingam(2019-10-06 18:09:44):Some of the shape-inference test-cases seem to end with a segmentation-fault in the CI.
gramalingam(2019-10-02 22:33:49):Can we separate out the basic-verification done by Verify previously from the new checks you added into separate functions? I believe that the idea was that in TypeAndShapeInferenceFunction one could assume that the basic-verification has happened successfully … so it doesn't need to check if a required-attribute is missing, etc. The new checks you added looks like it has to happen after the TypeAndShapeInferenceFunction is called. So, we may need to split it.
RandySheriffH(2019-10-02 23:19:01):Good idea - like this?
gramalingam(2019-10-03 17:13:02):I think we should fail_check if the type doesn't exist. (Same for outputs.)
gramalingam(2019-10-03 17:23:35):Yes, looks good, thanks! 

Can we also extend this checker:https://github.com/onnx/onnx/blob/c963586d0f8dd5740777b2fd06f04ec60816de9f/onnx/checker.py#L87 

Extend it to do the basic checking (as currently) and then call shape-inference for the type-inference and checking? May be it can be optionally invoked (controlled by an optional parameter) so that we preserve the existing behavior if invoked as currently? 
RandySheriffH(2019-10-03 22:59:53):Now using inference context as suggested.
The context handles the nullptr situation by setting corresponding type as nullptr.
RandySheriffH(2019-10-03 23:18:08):CR updated according with new checking routine, let's see.
gramalingam(2019-10-04 17:40:29):I think it would be better to do:
```C++
if (param_type->value_case() == VALUE_NOT_SET) {
   if (all_types.size() == 1 || type_constraints.find(type_str) != type_constraints.end()) {
      *param_type = Utils::DataTypeUtils::ToTypeProto(all_types.size() == 1 ? *all_types.begin() : type_constraints[type_str]);
   }
   continue;
}
```
gramalingam(2019-10-04 17:55:36):It helps handle other type-cases (ONNXML has more types than ONNX: ONNX has only tensor-type and sequence-type).
RandySheriffH(2019-10-04 18:07:23):Good suggestion, thanks!
RandySheriffH(2019-10-04 18:07:28):Done.
gramalingam(2019-10-04 20:58:49):Why not replace the above lines with "*param_type = input_type_proto"? I think that is simpler and more general too.
gramalingam(2019-10-04 20:59:49):I don't think we need to handle the "nullptr == param_type" case. The branch-statements assume it is non-null anyway.
RandySheriffH(2019-10-04 21:27:31):No - will remove soon.
RandySheriffH(2019-10-04 21:28:26):Good idea, will do.
gramalingam(2019-10-06 18:07:57):DataType and the related utils are used to use a pointer (to a string) guaranteed to be unique for a type so that pointer-comparison is sufficient to check for type-equality. Is there a need to use string instead of DataType here?
RandySheriffH(2019-10-07 16:30:49):No - it is just for debugging failed cases, will go back with pointers shortly.
RandySheriffH(2019-10-07 22:29:27):done.
gramalingam(2019-10-08 20:06:01):The logic looks correct. But I think it can be simplified a bit since if we take the THEN branch above (VALUE_NOT_SET), we set the param_type (to a correct type) and so we don't really need to do the checks below. So, I think I would just make the checking logic below the ELSE part. Does that make sense? The THEN part does inference, and the ELSE part checks correctness of pre-specified type.
RandySheriffH(2019-10-08 23:34:59):Yes that's sensible approach - the reason I am not making if an "ELSE" is because if sometime in the future we place some new logic there inferring the type (apart from using those we already have like allowed types and types constraints), we might well wish the inferred type get scrutinized. 
CLAassistant(2019-09-30 08:34:33):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=2375) <br/>All committers have signed the CLA.
wschin(2019-09-30 16:58:05):@kevinch-nv, please take a look. This PR is almost the same to your last PR to `rel-1.6.0` branch.

@chinhuang007, could you also update the version number of `ai.onnx.ml` domain? You can find it in https://github.com/onnx/onnx/commit/553df22c67bee5f0fe6599cff60f1afc6748c635.
prasanthpul(2019-09-30 18:50:15):I believe we generally do this by cherry picking that PR in its entirety from 1.6 branch into master, rather than making the changes separately.
chinhuang007(2019-09-30 22:35:32):@wschin updated the doc
@prasanthpul feel free to close this PR if cherry pick is the typical process
wschin(2019-10-01 15:33:24):I agree with @prasanthpul. Could you cherry-pick it from `rel-1.6.0`? You can do
1. git checkout master
2. git cherry-pick 553df22
3. git commit -m "Cherry-pick 1.6.0 version number"
4. git push -f chinhuang007:update_version_number
kevinch-nv(2019-10-03 04:40:00):Opened #2385 to handle the cherry-picking of the version back to master. Closing this PR.
autoih(2019-09-30 21:00:00):Hi @prasanthpul, do you have any suggestions about fixing the CI error? 
prasanthpul(2019-09-30 21:13:41):i've restarted that particular job. the mac build often intermittently fails for some reason
autoih(2019-09-30 21:20:21):Thanks @prasanthpul !
wschin(2019-10-01 19:47:23):```suggestion
    # type: (np.ndarray, Optional[Union[int, List[int]]], int, int) -> List[np.ndarray]
```
Can we use numpy type such as `np.int64` as the `int` here?
wschin(2019-10-01 19:49:42):Why do you ignore the type?
wschin(2019-10-01 19:50:25):Can we reuse those implementations to create examples in `Operators.md`?
BowenBao(2019-11-19 00:46:08):Everywhere else also uses int. I'd prefer using int here as well. In addition as below, it seems some numpy methods don't work well with mypy.
BowenBao(2019-11-19 00:46:24):some numpy methods don't work well with mypy
BowenBao(2019-11-19 00:47:51):Yes. But to create examples in Operators.md, we need to update the proto to enable Sequence Tensor Proto (in order to save test data in sequence format). That should be a separate PR of its own.
skottmckay(2019-10-02 23:47:06):> Update doc and pass CI please

Done
CLAassistant(2019-10-02 23:00:56):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=2384) <br/>All committers have signed the CLA.
fdwr(2019-10-02 23:07:16):@linkerzhang Here is the documentation aspect of the issue.
prasanthpul(2019-10-03 21:20:25):can you fix the conflict?
fdwr(2020-01-16 19:24:12):Fixed PR here, with correct CLA-associated commits: https://github.com/onnx/onnx/pull/2555
prasanthpul(2019-10-03 21:20:09):I think these are all experimental operators, right? You should clarify that these are experimental ops that have been deprecated. The concept of experimental ops is also deprecated now.

Non-experimental operators will remain in documentation even after deprecation. Example is Scatter op which was deprecated in opset 11
prasanthpul(2019-10-03 21:22:55):Consider adding this info to https://github.com/onnx/onnx/blob/5ca0a09e6846fe70c9dda8b5034f4a1454c6d130/docs/ManagingExperimentalOps.md and linking to it instead. Experimental ops do not have any versioning and now the concept itself is deprecated, so it's odd to list them in the mainline documentation.
ebarsoum(2019-10-09 00:28:23):Why not use size_t and cast based on that?
ebarsoum(2019-10-09 00:43:03):Is there a case where actual inputs > formal input?
gramalingam(2019-10-09 20:00:28):No. I can add a check and throw an error in this case, if that's what you mean.
ebarsoum(2019-10-09 20:03:26):What I mean why not just use actual inputs?
gramalingam(2019-10-09 20:17:30):Actually: sorry, I was wrong. This is possible with variadic parameters. I will modify the logic to do the right thing.
skottmckay(2019-10-18 19:22:31):Should this be
('indices', TensorProto.INT64, None)],
and
make_tensor_value_info('y', TensorProto.FLOAT, None)?
skottmckay(2019-10-18 19:23:04):nit: propagate
hariharans29(2019-10-18 20:38:02):You are right. Fixed, thanks.
hariharans29(2019-10-18 20:38:07):Fixed, thanks
CLAassistant(2019-10-22 07:23:35):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=2408) <br/>All committers have signed the CLA.
houseroad(2019-10-22 17:05:57):Btw, there is no good way to represent sequence and map type weights in initializer list. We have created an issue: https://github.com/onnx/onnx/issues/2410
yinghai(2019-10-23 04:58:12):What's the status about this? 
houseroad(2019-10-22 16:59:08):Wanna try sequence: https://github.com/onnx/onnx/blob/master/onnx/onnx.proto#L515
yinghai(2019-10-22 17:44:59):How to use this? 
skottmckay(2019-10-31 06:25:27):Would be better to move this down so it's after the axis is validated. 
shinh(2019-11-13 08:09:34):Sorry for the latency, done!
skottmckay(2019-10-31 05:56:56):Why not explicitly handle all currently defined values? 

  enum ValueCase {
    kTensorType = 1,
    kSequenceType = 4,
    kMapType = 5,
    kSparseTensorType = 8,
    kOpaqueType = 7,
    VALUE_NOT_SET = 0,
  }; #Closed
askhade(2019-10-30 01:12:56):@daquexian : I noticed the formula for coordinate transformation which is specified in the spec https://github.com/onnx/onnx/blob/3ea3b0e04cc0cb0f2fd250cf40b7e284d8bb945c/onnx/defs/tensor/defs.cc#L1678 is different than the one in the interpolate_1d method used in resize.py 
https://github.com/onnx/onnx/blob/3ea3b0e04cc0cb0f2fd250cf40b7e284d8bb945c/onnx/backend/test/case/node/resize.py#L133 
https://github.com/onnx/onnx/blob/3ea3b0e04cc0cb0f2fd250cf40b7e284d8bb945c/onnx/backend/test/case/node/resize.py#L134
Can you clarify why it is different?


Either ways we need to change one so that they both match... If the change is needed in resize.py then I can add it to this PR... if it is required in defs.cc then we will need an opset bump


daquexian(2019-10-30 01:29:34):Nice catch. I think the changes is needed in defs.cc. The implementation in resize.py has been verified to be compatible with PyTorch [here](https://gist.github.com/daquexian/4bab98a7e706ad95616567433c1234b8)
askhade(2019-10-30 03:53:51):> Nice catch. I think the changes is needed in defs.cc. The implementation in resize.py has been verified to be compatible with PyTorch [here](https://gist.github.com/daquexian/4bab98a7e706ad95616567433c1234b8)

This means we need to update the opset version for this change... I will include it as part of another PR.

There are more changes required in defs.cc for example : 

Here scales is meant to be optional but it is not declared as optional... "OpSchema::Optional" needs to be explicitly specified for any optional inputs. 
https://github.com/onnx/onnx/blob/3ea3b0e04cc0cb0f2fd250cf40b7e284d8bb945c/onnx/defs/tensor/defs.cc#L1742


I am planning to make these changes as part of the next PR and bump up the opset version
aviaisr(2019-11-18 09:50:21):Can you please check this issue? It shows a similar error to issue 2423. 
I'm working with the latest master 1.6.0

https://github.com/onnx/onnx/issues/2459#issue-524251228
jcwchen(2022-06-09 21:25:57):This should be covered by this: https://github.com/onnx/onnx/pull/2491. Close this one now. Thanks for the contribution.
askhade(2019-10-31 20:37:28):fixes #2427  and #2415 
CLAassistant(2019-11-01 02:17:04):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=2430) <br/>All committers have signed the CLA.
kit1980(2019-11-20 23:07:58):Any interest in fixing these?
linkerzhang(2020-01-14 23:55:27):@kit1980 do you want to resolve the comments of removing the unnecessary codes in the PR?
gramalingam(2019-11-21 00:18:39):occurrence
gramalingam(2019-11-21 00:18:50):occurrence
gramalingam(2019-11-21 00:21:53):Can you add a brief note why the changes in this file are needed? I vaguely remember seeing a thread (you may have written) long back about some motivating problem. It would be good to document it. Thanks!
kit1980(2019-11-21 00:24:28):Sorry, I didn't intend this to be a part of this PR.
gramalingam(2019-11-07 18:50:14):LGTM. It would be good to have some toy unit test case for the added code. But, off the top of my head, I am not sure whether we can do anything simple without adding an op or two that uses "datetime". Can we create some dummy op that uses datetime, but is not part of the standard onnx opset?
yuslepukhin(2019-11-07 21:15:36):> 
> 
> LGTM. It would be good to have some toy unit test case for the added code. But, off the top of my head, I am not sure whether we can do anything simple without adding an op or two that uses "datetime". Can we create some dummy op that uses datetime, but is not part of the standard onnx opset?

It looks like ops that are not defined in ONNX are ignored: https://github.com/onnx/onnx/blob/master/onnx/backend/test/case/node/__init__.py#L76
gramalingam(2019-11-11 20:58:29):@houseroad : any feedback on this?
yuslepukhin(2019-11-13 18:17:48):@houseroad Can you, please, check if the changes are addressed?
gramalingam(2019-11-15 18:15:55):@houseroad : do the updates look fine to you? Thanks!
houseroad(2019-11-11 21:07:02):why remove UNIT64 case?
houseroad(2019-11-11 21:07:27):Why remove this?
houseroad(2019-11-11 21:07:44):endline charactor?
houseroad(2019-11-11 21:08:23):please don't mix too much formatting in one PR.
yuslepukhin(2019-11-12 21:33:18):Oversight

---
In reply to: [344909641](https://github.com/onnx/onnx/pull/2437#discussion_r344909641) [](ancestors = 344909641)
yuslepukhin(2019-11-12 21:34:02):This was copied as one of the build products. I will re-do change manually.

---
In reply to: [344909908](https://github.com/onnx/onnx/pull/2437#discussion_r344909908) [](ancestors = 344909908)
yuslepukhin(2019-11-12 21:34:46):Will do the change outside VS which often applies formatting automatically

---
In reply to: [344910141](https://github.com/onnx/onnx/pull/2437#discussion_r344910141) [](ancestors = 344910141)
CLAassistant(2019-11-07 13:04:43):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=2439) <br/>All committers have signed the CLA.
gramalingam(2019-11-07 23:54:22):Can you also generate the other proto files that are generated from onnx.in.proto and check them in?
haohaibo(2019-11-08 01:52:54):Re-generated onnx.proto and onnx.proto3
gramalingam(2019-11-08 02:42:18):There are also the onnx.ml versions of the protobuf: if you use genproto.py, the option "-ml" will generate the onnx-ml version protos.
haohaibo(2019-11-08 02:58:25):Thanks for your kindly reminder. I used gen_proto.py option "--ml --lite" to re-generate `onnx-ml.proto` and `onnx-ml.proto3` files
haohaibo(2019-11-15 02:26:05):Hi @gramalingam This PR has been pending for about 7 days.
CLAassistant(2019-11-07 21:35:23):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=2441) <br/>All committers have signed the CLA.
daquexian(2019-11-15 13:39:59):@houseroad Is this PR ok to merge? Thanks!
daquexian(2019-11-19 02:29:50):@houseroad Any comments? Thanks!
daquexian(2019-11-23 02:06:50):@houseroad @linkerzhang Is this PR ok to merge? Thanks!
daquexian(2020-01-09 01:06:18):@houseroad @linkerzhang I have fixed the flake8 error. The CI error is not related to this PR
daquexian(2020-01-28 08:49:41):@linkerzhang @houseroad Is it ok to merge this PR? Thanks!
daquexian(2020-02-13 13:00:50):The appveyor CI error is not related to this PR
daquexian(2020-02-16 13:44:09):@linkerzhang @houseroad Is it ok to merge this PR? Thanks!
houseroad(2019-11-12 18:30:26):Could you further explain why using `i+1+j` as the index here?
daquexian(2019-11-13 02:53:11):> Could you further explain why using `i+1+j` as the index here?

It inserts n inputs of `cur_input_node` at index i+1~i+1+(n-1), and remove `cur_input_node` at index i. As a result, `cur_input_node` is replaced by its inputs **inplace**, instead of the wrong way: always append its inputs at the end.
linkerzhang(2019-11-26 06:03:35):Can you please put this clarification as comments in the source code?

btw, do you mind adding some test cases to avoid regression?
daquexian(2019-11-28 08:46:42):@linkerzhang Thanks! I have added the comments and updated the test
snnn(2019-12-06 22:54:10):Should it be: Building ONNX with Protobuf as shared lib is only available for MSVC compilers and that too when USE_MSVC_STATIC_RUNTIME environment variable is set to **0**.?
snnn(2019-11-13 07:26:49):No. It won't work.
See: line 467 in your change
snnn(2019-11-13 07:28:01):"/MT" causes the application to use the multithread, static version of the run-time library. Defines _MT and causes the compiler to place the library name LIBCMT.lib into the .obj file so that the linker will use LIBCMT.lib to resolve external symbols.

In this case, it must use "/MD"
skottmckay(2019-11-13 08:19:33):'cd protobuf' between clone and checkout? #Closed
skottmckay(2019-11-13 08:21:12):I would suggest adding this path to the user environment variables so it's more permanent. Otherwise it's easily forgotten and the next time they try to rebuild they can get errors and not remember that a custom protobuf directory needed to be added to the path. 
yuslepukhin(2019-11-14 00:30:57):We did not want to change the default but the env var provides an option for /MD[d]

---
In reply to: [345602405](https://github.com/onnx/onnx/pull/2452#discussion_r345602405) [](ancestors = 345602405)
snnn(2019-11-14 00:36:22):See #1995 

It interferes in creating VS Solutions that support multiple configurations.

snnn(2019-11-14 00:37:30):See #1563
snnn(2019-11-14 00:38:29):https://stackoverflow.com/questions/24460486/cmake-build-type-is-not-being-used-in-cmakelists-txt
snnn(2019-11-14 00:41:58):Would be better if you can check if ONNX_USE_MSVC_STATIC_RUNTIME is OFF.
askhade(2019-11-14 01:14:53):earlier -DONNX_USE_MSVC_STATIC_RUNTIME was always set to ON state... now we have an option to set this to OFF .... I think default is /MD so we can get rid of else... but I need to test this before pushing this change
snnn(2019-11-14 01:21:27):Why the version number is missed?
yuslepukhin(2019-11-14 19:09:13):>USE_MSVC_STATIC_RUNTIME  [](start = 0, length = 24)

This is would be false by default here?
yuslepukhin(2019-11-14 19:11:03):Yes, just need to come up with a way without if/else clause

---
In reply to: [346084942](https://github.com/onnx/onnx/pull/2452#discussion_r346084942) [](ancestors = 346084942)
askhade(2019-11-14 19:11:03):USE_MSVC_STATIC_RUTIME will be tru by default which means we will use /MT option (current option) by default

---
In reply to: [346494736](https://github.com/onnx/onnx/pull/2452#discussion_r346494736) [](ancestors = 346494736)
askhade(2019-11-14 20:42:06):yes added the check... For current versions of protobuf from conda-forge which are built as a shared lib "ONNX_USE_MSVC_STATIC_RUNTIME" needs to be turned OFF
snnn(2019-11-14 21:17:46):Why don't use the latest libprotobuf?
askhade(2019-11-14 21:52:38):Because latest one is built as a shared lib... Current change to appveyor simply forces it use the package version which it was using earlier and not take the latest update...

The ideal change to this CI pipeline should be to use our recommended way .i.e. build protobuf locally instead of pulling from conda-forge... but this change is a bigger one and will need all important stakeholders to agree which means it will take longer... Since right now all the PRs are blocked I want to push this change and then open discussion for the above mentioned change
snnn(2019-11-14 21:58:38):But the 3.5.x version has a serious bug on Windows: https://github.com/protocolbuffers/protobuf/pull/4777 
https://github.com/protocolbuffers/protobuf/issues/4773
askhade(2019-11-14 22:04:38):Well appveyor CI was already using 3.5.2 version... as I mentioned the aim of this PR for appveyor CI is to just force it its old behavior. Its good that you pointed this out... I am starting another thread to get a consensus on the right fix for this scenario. From my perspective the right fix is to simply build protobuf locally so that we have a better control... relying on conda-forge for protobuf has proven to be very painful so far,
snnn(2019-11-14 22:12:26):>  simply build protobuf locally

But in conda, you shouldn't do that. Because people can install protobuf from conda, and they can't safely build onnx if onnx has a local protobuf lib, in such case you don't know which protobuf lib/dll/header files will be used. 
gramalingam(2019-11-14 23:09:06):Don't you need to specify the version for protobuf? I use protobuf=3.5.1 in a conda install.
gramalingam(2019-11-14 23:10:15):Are there any requirements or recommendations for python version?
gramalingam(2019-11-14 23:14:09):Is the preceding explanation (under "Source", which overlaps the anaconda instructions below) for Linux? If so, please add a sub-section title "Linux" or as appropriate.
gramalingam(2019-11-14 23:18:03):A clarification about the environment variable ONNX_ML would be helpful here (in the "Windows" section).
gramalingam(2019-11-14 23:24:58):Any instructions for a clean uninstall & rebuild? I know I had to do it some times, but I was using conda environments (not sure about outside).
snnn(2019-11-14 23:33:44):Indeed cmake doesn't support uninstall, neither Windows nor Linux. 
snnn(2019-11-14 23:34:49):You may install the files into a separated folder and delete the whole folder afterwards. 
askhade(2019-11-14 23:39:15):Do we have an official supported version for protobuf which is common for all platforms or perhaps one for each platform ?
Windows CI uses 3.5.2 : Changming pointed out an existing issue with this version  protocolbuffers/protobuf#4777 protocolbuffers/protobuf#4773
Linux and MacOS use 3.10.0

I will test to see if 3.10.0 works for windows too... but am curious do we use these versions right now just because they work or is there any reasoning behind it?

askhade(2019-11-15 01:22:38):In CIs - for Linux we test using python 2.7 and 3.6 , on Mac we test using 3.7 and 3.7 whereas on in windows ci we test with python 3.5, 3.6, 3.7

Right now I do not want to add any recommendations without understanding what is our official stance. There is no other documentation which lists the officially supported versions...  
gramalingam(2019-11-15 18:22:42):I believe there are some cleanup instructions for conda (removing the onnx folder from site-packages) which helps sometimes.
gramalingam(2019-11-15 18:33:13):For example, one mysterious problem I have run into a few times is the following: I add a new op, run setup. I then run the document-generator, but it seems to access the old schema registry, not the updated one with the new op. I never figured out what was going on. 
askhade(2019-11-15 18:58:31):Yes there are a bunch of issues in this area too... We have a script update_docs... do you see the same problem when you run that script instead of independently running gen_doc? If yes then we should just fix that script to do the right thing... The existing doc for adding a new operator mentions this script.
lara-hdr(2019-11-20 21:24:30):@gramalingam can we merge this?
The new CI failure is unrelated.
gramalingam(2019-11-18 18:00:45):Wether => Whether
spandantiwari(2019-11-20 18:19:56):We should also add explicitly what is the behavior when `select_last_index=False` (the default) in this blurb as well.
spandantiwari(2019-11-20 18:20:49):nit: Consider replacing "resulted tensor" with "resulting tensor" or just "output tensor" everywhere in these two ops. I think that is grammatically more correct.
askhade(2019-11-18 21:05:15):@linkerzhang , @fdwr : Add the clarification to the spec as discussed
CLAassistant(2019-11-22 10:57:28):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=2470) <br/>All committers have signed the CLA.
CLAassistant(2019-11-22 14:46:01):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=2471) <br/>All committers have signed the CLA.
linkerzhang(2019-11-26 05:57:59):do you mind to add some test cases so that this can't be regressed?
XavierAtShanghai(2019-11-27 08:57:46):In the next commit, by doing infer_shapes we can make sure the graph and intput/output tensors are consistent before and after the optimization.  

Without the fix:
```
PASSED optimizer_test.py::TestOptimizer::test_fuse_transpose_default
PASSED optimizer_test.py::TestOptimizer::test_fuse_transpose_default_no_fuse
PASSED optimizer_test.py::TestOptimizer::test_fuse_transpose_into_gemm
FAILED optimizer_test.py::TestOptimizer::test_fuse_transpose - RuntimeError: Inferred shape and existing shape differ in dimension 0: (4) vs (2)
```

With the fix:
```
PASSED optimizer_test.py::TestOptimizer::test_fuse_transpose
PASSED optimizer_test.py::TestOptimizer::test_fuse_transpose_default
PASSED optimizer_test.py::TestOptimizer::test_fuse_transpose_default_no_fuse
PASSED optimizer_test.py::TestOptimizer::test_fuse_transpose_into_gemm
```
CLAassistant(2019-11-24 10:53:02):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=2473) <br/>All committers have signed the CLA.
daquexian(2019-11-29 09:57:06):The CI errors are not related to this PR
snnn(2020-01-03 21:31:52):https://github.com/onnx/models/issues/141 
CC: @vinitra .  We should restore the model pipelines and refresh these models.


linkerzhang(2020-01-05 20:11:03):@daquexian do you mind to update this PR with resolving the comments and conflicts please? thanks!
daquexian(2020-01-08 11:05:54):@linkerzhang Sure, I'll update it soon
daquexian(2020-01-08 13:20:59):I'll update the docs tomorrow
daquexian(2020-01-28 11:04:57):@linkerzhang The CI error is not related to this PR. Is it ok to merge it?
daquexian(2020-11-08 14:27:41):Closing this PR as I think @TMVector is right :D
linkerzhang(2019-12-03 06:41:31):this definition should be updated, given non-one axis value is supported (with no change to the input).
TMVector(2020-02-17 22:21:15):This will infer an incorrect rank!
claassistantio(2019-11-30 04:06:27):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=2482) <br/>All committers have signed the CLA.
xkszltl(2019-12-01 03:24:00):CI failure unrelated in python 3.5 builds: `IPython 7.10+ supports Python 3.6 and above.`
snnn(2019-12-07 03:23:52):LGTM
linkerzhang(2019-12-13 01:55:27):can you also check the appveyor CI failure please? thank you!
xkszltl(2019-12-13 02:56:56):@linkerzhang As commented earlier in this thread, it has nothing to do with this PR as the CI pipeline itself is broken.

> CI failure unrelated in python 3.5 builds: IPython 7.10+ supports Python 3.6 and above.
gramalingam(2019-12-03 16:58:37):change "us" to "as"
gramalingam(2019-12-03 17:01:19):I guess the new replacement description will be in addition to the old definition, right?
ebarsoum(2019-12-03 18:08:56):Yes.
linkerzhang(2019-12-09 02:40:57):This is about issue #2490 
claassistantio(2019-12-09 14:21:11):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=2491) <br/>All committers have signed the CLA.
linkerzhang(2019-12-10 02:20:09):or make_model API should be used in this tutorial :).
AlexMuresan(2019-12-10 09:58:00):@linkerzhang will look into it. Should I make a new pull request when I complete the changes or should I just ammend this commit?
linkerzhang(2019-12-12 20:21:35):a new PR is good. thank you!
linkerzhang(2019-12-09 02:40:35):Thank you very much for improving the tutorials!

Do you mind to add comment in this doc to clarify the operator set versioning please? say, this is based on Pad-2 or maybe Pad-5.
linkerzhang(2019-12-09 02:41:49):otherwise, the same confusion will happen when Pad updated in future again.
AlexMuresan(2019-12-09 14:23:34):Ok. I've done the necessary changes and created a new pull request that seems to have added to this one. Hope everything is ok.

Thank you for the input!
houseroad(2019-12-29 06:34:18):Here ONNX_USE_PROTOBUF_SHARED_LIBS is always false, so we never enter this branch.

Shall we check the cmake version instead?
snnn(2020-01-06 22:53:41):So, ONNX_USE_PROTOBUF_SHARED_LIBS  is always false, I didn't see any place it get set. 

The Protobuf_USE_STATIC_LIBS  is introduced in cmake 3.9. 
From cmake 3.9 to 3.15, the variable "Protobuf_USE_STATIC_LIBS" only affects Linux builds.  If it is OFF, it won't matter. If it's ON, then libprotobuf.so will be ignored during search because the option forces using protobuf as a static lib. As you pointed, it may not be a good idea.  Let me think it again.





shinh(2019-12-13 02:30:56):@RandySheriffH and @gramalingam: Could you review this PR? I think we cannot use 
 InferShapes(Graph) now and this issue was introduced in https://github.com/onnx/onnx/pull/2367
lgtm-com[bot](2019-12-16 08:04:43):This pull request **introduces 1 alert** when merging a0c4c361f714dd448e7850e54eec254e5c1e3f79 into 477a9b87715d614f8b7540a69c144b177275baa2 - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-ed69ceb30d73eb465ffd29a3175672801896716b)

**new alerts:**

* 1 for Unsigned comparison to zero
neginraoof(2019-12-17 00:35:26):@BowenBao @lara-hdr please review
cc @ebarsoum @linkerzhang @spandantiwari 
neginraoof(2020-01-06 18:30:12):@gramalingam Can we close this?
neginraoof(2020-01-08 17:32:53):cc @gramalingam for review.
Thanks.
gramalingam(2020-01-16 23:18:51):@houseroad : any feedback?
neginraoof(2020-01-21 19:16:22):cc @postrational @houseroad 
Would you please help review this PR for adding Einsum op
spandantiwari(2020-01-24 19:15:35):Thanks @houseroad, @gramalingam, and @lara-hdr for your review.
lara-hdr(2019-12-17 20:57:44):can we have this as an attribute instead or is there a reason to have it as an input?
lara-hdr(2019-12-17 21:09:37):")." -> typo?
lara-hdr(2019-12-17 21:11:32):rename "Einsum_ver11_doc" to "Einsum_ver12_doc"
lara-hdr(2019-12-17 21:47:10):mhmm. .  actually tensor(string) are not supported in PyTorch, how would we support this as an input in the PyTorch exporter?
gramalingam(2019-12-17 21:47:39):Yes, I think we should make it an attribute.
gramalingam(2019-12-17 21:53:21):I think it would be helpful to add a precise, complete, and unambiguous description of this language. From the description I have seen in other frameworks, there are some choices and ambiguities in what Einstein summation notation means and allows.
gramalingam(2019-12-17 22:01:05):It would be good to have a simple grammar for the language too. For example, are white-spaces allowed in the string? It is a good idea to allow spaces in the string and skip them. When I look at the rank-inference logic, the use term.size() makes me wonder if this assumes that there are no white-spaces in the string.
neginraoof(2019-12-17 22:05:12):Yes, that makes sense
neginraoof(2019-12-18 21:38:29):Thanks @gramalingam, I've updated the doc with more details about equation string.
lara-hdr(2019-12-19 18:21:04):nit: too many spaces
neginraoof(2019-12-19 19:07:16):Thanks!
gramalingam(2019-12-19 19:41:38):Whitespace generally includes other characters like tab and newline. I think it may be better to either use "isspace" function or change the documentation if only blank characters are permitted. I see that the numpy implementation is used as the reference implementation ... we could go with whatever numpy supports and document it.
gramalingam(2019-12-19 19:45:17):A check to handle the case where number of operands here exceeds numInputs would be helpful.
gramalingam(2019-12-19 19:47:52):Can you clarify what happens for the ellipsis? E.g., all occurrences of ellipsis on the left-hand-side must denote the same number of dimensions. Is there broadcasting or not?
gramalingam(2019-12-19 19:54:39):When an index variable is repeated on the left-hand-side, are the corresponding dimensions required to be equal?
gramalingam(2019-12-19 20:03:36):Adding an explanation as below (please correct it as appropriate) right at the beginning may be helpful:

An einsum of the form `term1, term2, term3 -> output-term` produces an output tensor using the following equation:
```
   output[output-term] = implicit-sum( input1[term1] * input2[term] * input3[term3])
```
where the implicit-sum performs a summation over all the indices occurring in in the input terms (term1, term2, and term3) that do not occur in the output-term.
gramalingam(2019-12-21 03:00:33):Isn't the number of indices i, j, k and their order determined by output-term? So, it may be better to write "[output-term]" instead of "[i][j][k]", using the same notation as in "[term1]" etc. Alternatively, we should explain the relationship between "[i][j][k]" and output-term: this explanation is shared by "term1]" etc.. 
neginraoof(2019-12-21 20:28:33):It think now it looks fine. Except that the matrix multiplication in the equation is not very clear. In some cases, depending on the shape of the operands, this could be expand + multiply.
neginraoof(2019-12-21 20:29:50):This is in line 1714
neginraoof(2019-12-21 20:30:28):Addressed this.
spandantiwari(2020-01-06 19:20:13):We should remove `tensor(string)` from supported types and keep only numeric types. 
neginraoof(2020-01-08 00:07:59):Thanks a lot for the note. I've updated the docs.
TMVector(2020-01-11 23:26:08):```suggestion
            "Constrain input and output types to all numerical tensor types.")
```
gramalingam(2020-01-14 23:09:42):The problem is what happens in line 1701 with "ctx.getInputType(num_operands)->tensor_type()"? This may crash before we get to line 1714 if num_operands > numInputs, right?
gramalingam(2020-01-14 23:13:27):May be a test-case with num_operands > num_inputs would also help.
gramalingam(2020-01-14 23:21:39):Thanks. I still think that it is better to say "output[output-term]" instead of "output" in the equation.
neginraoof(2020-01-15 16:55:32):Sure. Does it make sense to have a negative shape inference test?
gramalingam(2020-01-15 17:15:46):I think so. Test cases to check that the behavior is acceptable when preconditions are not satisfied are useful. E.g., something like in: https://github.com/onnx/onnx/blob/dfa4384c3f8fe9d804032f992e6f818fc54152b0/onnx/test/shape_inference_test.py#L109 
askhade(2019-12-17 22:35:04):@linkerzhang @ebarsoum : Please take a look
@fdwr , @tracysh, @nickfeeney :  FYI
linkerzhang(2019-12-18 18:39:19):this line is not needed. In this case, MaxPool is just doing its regular job of finding out the max number, it does not really care the semantics behind each inputs.
askhade(2019-12-18 19:39:35):@linkerzhang @ebarsoum : Please take a look
@fdwr , @tracysh, @nickfeeney : FYI
gramalingam(2019-12-19 19:21:36):It looks like this might also be related to the training-proposal to extend the IR to support training: please see: https://github.com/onnx/onnx/pull/2314 ... training needs more support, since it also needs to describe how to update the weights after processing a batch.
tbennun(2019-12-20 18:08:47):@linkerzhang Thanks! Posted more details on the gitter.

@gramalingam Yes, this PR is related to #2314 but currently orthogonal to it, as the initializers there are still defined as Tensors.
linkerzhang(2020-01-02 18:43:44):@wschin 
linkerzhang(2020-01-02 18:50:50):Comments based on chatting over gitter room - https://gitter.im/onnx/Infra:
1. Both IR.md and *.proto files should be commented (clarifying the 1:1 name mapping is still there between the only-one output of these graphs and initializers (weights).
2. Comments from training work group folks if any @wschin @SherlockNoMad @chinhuang007 

One more question, is it necessary to have one graph (with one output) mapping to one weight? how about just using one graph to cover all weights?
tbennun(2020-01-02 21:04:50):There is no functional difference between having one graph to initialize all tensors or having a graph per tensor, but there are two main reasons why we thought it was best to use a graph per tensor:
* In both cases, in order to create different weights for each tensor, you would have to replicate the graph nodes.
* Using the name field of a GraphProto is more straightforward to implement (both saving and loading) rather than iterating over nodes in the singular initializer graph and updating the output names. This is also more equivalent to how tensor and sparse tensor initializers are defined now, so the implementations would remain similar.
linkerzhang(2020-01-07 18:37:49):With more thoughts of supporting training (plus the training proposal #2314 ),

I'd suggest,
1. move this initialization field as part of training proposal (TrainingInfoProto).
2. if my understanding is correct, this initialization for training should be run only once (for training reproducibility), in this way, I'd still suggest to have only one GraphProto to initialize all trainable weights.
3. The initialization graph's outputs will have exactly the same name as the "initializers" in inference graph. Say, in "initializers" there's a tensor named "a", if this "a" is trainable, there should be an output tensor named "a" in initialization graph.

Thoughts?
tbennun(2020-01-07 19:37:59):@linkerzhang I agree with everything. We also had a discussion in the training WG that supports this decision. As @wschin said, adding graph initializers to `TrainingInfoProto` has the additional benefit of being able to restart training on a pre-trained ONNX file, and the behavior of saving/loading models is more well defined.

I will now close this PR and create one targeted at the branch in #2314, so that it will appear as part of that PR. I will incorporate notes (2) and (3) from the message above. 

Thank you all for the important feedback!
linkerzhang(2019-12-19 18:32:04):don't really understand the scenario of having a "Graph" as initializers of a model. Do you mind to elaborate that in our gitter room. https://gitter.im/onnx/Infra
TMVector(2020-01-03 09:56:11):How about this?
```suggestion
graph_initializer|Graph[]|A list of initializers that are represented by graphs, which describe how to compute the initializer tensor. Each initializer graph has zero inputs and one output, which acts as the initializer tensor. The initializer graph name acts as the initializer name.
```

TMVector(2020-01-03 10:12:25):How about this?
> Initializers (see above) described by graphs. Each GraphProto must have a name (which acts as the initializer name), zero inputs, and one output (which acts as the initializer tensor).
TMVector(2020-01-03 10:12:35):I think the type/shape constraint is implied by the spec already, but if you think it is worth being explicit, then IMO it should be on the `initializer` field since it applied to all initializers.
tbennun(2020-01-03 11:27:58):I agree, this is why I didn't mention the type/shape constraint in the first iteration. The zero inputs requirement is also important. Thanks!
wschin(2020-01-03 17:49:46):I don't quite sure the reasons behind graph_initializers. If an initializer is input-dependent, it should be computed from the input in the main computation graph. If an initializer is input-independent, it sounds a constant and the current initializer support is sufficient. Are there cases we must compute initializers from separated graphs? Or this new field is mainly for convenience?

ONNX training spec will support some ways to **compute** the initializers and possibly have a similar semantic here. It's also strange if we have a main graph, training graph, and initialization graph --- should we execute initialization graph each time we execute the training graph? If not, when should initialization graph be computed? These two questions make me feel initialization is a part of training graph, rather than a part of inference-oriented graph.
tbennun(2020-01-03 18:31:43):I see. I created the PR after we saw that the training spec does not include an initialization graph. Training is the main purpose for this change.

IMO there may still be other reasons to keep the initialization separate from training. For instance, if for inference some weights are represented by large tensors that can be simply computed, it would be better to initialize tensors once. Currently, if they are added as constant initializers, the file size would be large. If they are added as nodes to the graph, they will be recomputed every time, which might not be efficient. 

It would probably be fine if this change will only be applied in `TrainingInfoProto`. What do you think?
wschin(2020-01-03 20:55:03):If we want to use graph to compute large constant tensor, we can specify that logic in the main inference graph (that is, we insert **nodes** into inference graph). Is it necessary to define another field for that logic?

For combining this proposal and training, we need to first list out the scenarios we want to support. For example, if we only want to express `initialization + training` in the training graph, we can merge `initialization` and `training` into one single graph and extra fields are not necessary.
linkerzhang(2020-01-07 01:59:18):@wschin I am understanding this PR is trying to add a field to show the initialization logic of weights in training scenario. @tbennun
 correct me if I'm wrong. This is why I think this should align with training proposal.
tbennun(2020-01-07 07:10:09):@linkerzhang yes, that is the goal.
wschin(2020-01-07 17:07:33):If we put some a layer's weight into `graph_initializer`, it means we don't necessarily have a placeholder for the tensor, which can cause some problems when training algorithm updates its value. For example, if I define `W` as a graph initializer, where should I save its value produced by the training algorithm?

The signature of `graph_initializer` looks fine. We just need to refine its semantic. It should be an optional step to update `existing` initializers rather than defining new initializers.
wschin(2020-01-07 17:10:17):```suggestion
  // initialized. Each GraphProto must have a name (the same to one
```
As mentioned above, graph initializers can't be used to declare initializers. It can only specify how existing initializers should be computed.
wschin(2020-01-07 17:11:34):Should we put the entire initialization stage into a single graph? I feel it's cleaner than having a list of graphs.

[Update]
Now, I think we should make it a list. In training proposal, we have TrainingInfoProto as a repeated field. I think for each, TrainingInfoProto, we should have one initialization graph.
ebarsoum(2020-02-07 01:30:24):@xadupre can you provide more details on: "Operator or function? the operator can be composed with other ONNX operators but the graph depends on the parameter metric."
xadupre(2020-02-07 12:47:14):CDist computes all pairs $\forall i,j, dist(X_i, Y_j)$, broadcast takes care of looping on j, operator Scan handles the loop on i: it execute N times the same distance broadcasted on N examples. Each distance is different and ends up being converted into ONNX with a different graph. There are >= 20 different distances for function cdist in scipy, it means >= 20 ONNX graphs to implement to implement CDist as a function. If it's an operator, there is no need to specify these graphs in ONNX. So implementing CDist as a function with all possible distances would take quite some time.
snnn(2020-05-03 23:16:32):This operator is often used in conjunction with[GaussianProcessRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.gaussian_process.GaussianProcessRegressor.html). Users are free to choose any one of the distance function as a part of the kernel function. So it's hard to say which distance function is less important than the others. I suggest we should add all of them(as @xadupre listed)  at one time, instead of frequently changing the spec. 

askhade(2019-12-19 22:16:01):Why are all these test configurations being removed? The CI is failing right now and the ideal fix should be to fix the CI ad not remove the failing configs? 
xadupre(2019-12-19 22:29:59):Sorry for that, I just wanted to have faster iterations, I forgot to uncomment them before creating the PR.
xadupre(2019-12-19 23:10:49):Python 3.5 is failing with this message: *IPython 7.10+ supports Python 3.6 and above*.
xadupre(2019-12-20 15:11:54):I fixed the appveyor build (force the use of IPython 7.9 for python 3.5). This fix would unblock many PR too.
snnn(2020-01-03 21:26:34):Scipy supports much more metrics. Do we need any of these in the future? If yes, we'd better to add them here now.
xadupre(2020-01-04 09:08:17):I kept the list to what is commonly used. Here is the whole list:
```
'seuclidean' 'cosine' 'correlation' 'hamming' 'jaccard'
'chebyshev' 'canberra' 'braycurtis' 
'yule' 'dice' 'kulsinski' 'rogerstanimoto' 'russellrao' 'sokalmichener' 'sokalsneath'
'mahalanobis' 'wminkowski' 'haversine'
```
See [cdist](https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.cdist.html), and [DistanceMetric](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.DistanceMetric.html?highlight=distancemetric#sklearn.neighbors.DistanceMetric).

So you think I should do all of them?
EmmaNingMS(2020-01-07 21:39:12):@linkerzhang @ebarsoum could you help review this op and give some guidance here? 
snnn(2020-01-07 22:09:27):I suggest we put all of them here, to avoid bumping up the op version in the future. 
xadupre(2020-01-08 10:32:56):Before I add them, what do you think of the rest of the PR? Is this the only required change?
claassistantio(2019-12-20 00:29:37):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=2519) <br/>All committers have signed the CLA.
CaseyCarter(2019-12-20 01:05:59):FYI, it looks like AppVeyor CI is broken due to python version mismatch.
linkerzhang(2019-12-27 19:59:24):it's better to add this within a ifdef? (a macro of new msvc version?)
CaseyCarter(2019-12-30 19:53:48):I think not. This header *always* needs the contents of `<ostream>`, adding the `#include` here when it's unnecessary due to transitive inclusion (e.g. on older MSVC versions) has nearly no cost due to include guards and `#pragma once`.
skottmckay(2020-01-05 21:39:35):LGTM
hariharans29(2020-01-02 12:00:04):`clear_dim_param()` checks if `dim_param` is present before attempting to clear it.
hariharans29(2020-01-02 12:01:33):The output with the previous logic would have been (1, 'a', 1) which is incorrect as the output shape need not have the particular dimension to have the same value corresponding to the input dimension's `dim_param`
bddppq(2020-01-03 18:13:40):waiting for https://github.com/pytorch/pytorch/pull/31827
snnn(2020-01-02 19:19:44):Why do you choose a nightly build instead of a release build?
Why do you fix the ort version to this specific number?
snnn(2020-01-02 19:20:53):So, we are dropping support for python 2.x?
bddppq(2020-01-02 20:55:40):For this pull request, we are only changing the CI job that verifies the integration between ONNX and PyTorch and onnxruntime backends to use Python3. This makes senses because both backends have dropped py2 support already.
There are still py2 other CI jobs (running on Travis) that checks ONNX is py2 compatible. Given the developers of the Python language started dropping support for py2 from 01/02/2020, I think there should be discussions in the corresponding SIG to decide on the timeline of dropping py2 support universally in ONNX.
bddppq(2020-01-02 20:57:19):This version was copied from the PyTorch CI. Maybe @spandantiwari @houseroad could answer why this version specifically.
houseroad(2020-01-02 21:10:30):Since we keep adding new translation logic into the pytorch master branch, we would like to test it end to end. Some ops are only supported in nightly dev build. So we prefer dev build in the CI.

It will make things simpler if we make these two versions consistent. 
snnn(2020-01-03 18:42:20):But, does it mean only pytorch team can maintain it and change it?
houseroad(2020-01-03 18:45:30):@spandantiwari and his team help us maintain the version.
linkerzhang(2020-01-05 20:08:28):please fix ci failure - "
+flake8
2326
./onnx/backend/test/case/node/clip.py:68:24: E222 multiple spaces after operator"
TMVector(2020-01-06 19:40:44):Okay, I actually ran some diffs on the text form of the protobufs, and it turns out the differences are all from ir/opset version updates. I will take it that means it's okay to only update tests actually changed by this PR.

What is the convention with regard to backend tests -- just overwrite the tests for old ir/opset versions?

Is it okay to change the test to loop over dtypes like I have? (I'm thinking of the docs mainly).

@linkerzhang feedback on testing and above questions would be really helpful :)
TMVector(2020-01-06 19:41:13):Looks like circleci failing is due to ORT not producing Python 2.7 builds anymore
TMVector(2020-01-06 22:24:15):And the failures now look to be nbval/nbformat related
askhade(2020-01-07 21:45:21):Looks like you are adding a new test for every combination of test case + data type. For example 
test_clip_default_inbounds_float32
test_clip_default_inbounds_int32
test_clip_default_max_float32
test_clip_default_max_int32

This is adding too many files which in my opinion are unnecessary... I would suggest you to pick a few out of these.... I do not believe we need a test case for each and every data type
TMVector(2020-01-08 12:03:19):There is now only one additional test (clip_default_int8) which adds 3 test files.

I would love to see a full backend test suite which tested support for data types, but I guess that's outside the scope of this PR anyway.
linkerzhang(2020-02-16 07:43:10):@TMVector can you fix the conflicts so that the PR can be merged?
linkerzhang(2020-02-25 09:41:40):merging #2608 introduces new conflicts .....

@TMVector  can you update it again?
bddppq(2020-01-09 07:13:57):it should be 1.1.0.dev1228
houseroad(2020-01-09 20:05:28):oops
claassistantio(2020-01-10 03:21:24):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=2540) <br/>All committers have signed the CLA.
CLAassistant(2020-03-23 07:17:09):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=2541) <br/>All committers have signed the CLA.
neginraoof(2020-01-13 22:17:35):cc @postrational 
Could you please help review this PR?
neginraoof(2020-01-17 18:59:27):cc @BowenBao @spandantiwari @houseroad for review
BowenBao(2020-01-17 19:11:09):Should we update SplitToSequence as well?
gramalingam(2020-01-18 00:47:38):Is this just a clarification of the specification of the existing op or it is extending the specification of the existing op? I think it is the first case: if so, I suggest changing the title of the PR (e.g., to "Clarify that split supports zero length splits"). 
gramalingam(2020-01-18 00:49:15):Just to clarify: any changes to the op spec require bumping the op's version number.
neginraoof(2020-01-18 02:08:30):@gramalingam Thanks! I think this is a clarification for split spec, that's why I didn't bump the version. Please let me know if you think otherwise.
neginraoof(2020-01-21 19:19:20):Thanks @gramalingam. Let me know if we can merge this.
gramalingam(2020-01-21 20:31:20):LGTM. But do you know why some of the CI is failing?
neginraoof(2020-01-23 22:07:46):cc @gramalingam I think this PR is ready for merge. Thanks.
gramalingam(2020-01-21 18:57:59):Why both "Values should be >= 0." and "All values must be positive." ? May be better to drop the second sentence, to avoid confusion.
shinh(2020-01-15 05:21:48):@linkerzhang This is a follow-up PR for #2328 and the same fix for shape inference of Split-11 was done for Split-2.
jcwchen(2020-12-28 18:38:16):Hi @shinh,
Thank you so much for proposing this! I think it can help fix the shape inference bug for Split... Could you resolve the merge conflict? Thanks.
shinh(2020-12-29 03:57:05):Unlike previous approach which factored out a function to share code among Split-11 and Split-2, this time, I just copied the shape inference of Split-11 to Split-2. This way seems to be more consistent with other operator definitions.

At first, I tried to remove the support for negative axis from the one of Split-2 since Split-2 did not mention negative axis. However, it made some tests fail so I decided to handle negative axis even in Split-2.

This is quite an old PR. I don't have the model fixed by this PR anymore. If this patch does not look good and you still want to fix Split-2, I'd suggest you create another PR. I'll close this PR then.

jcwchen(2021-01-05 18:41:42):@linkerzhang Could you help to merge this? Thank you.
gramalingam(2021-01-05 21:10:53):@shinh : thanks for updating the fix. Regarding
```
At first, I tried to remove the support for negative axis from the one of Split-2 since Split-2
did not mention negative axis. However, it made some tests fail so I decided to handle negative
axis even in Split-2.
```
that is troubling. I guess there is some mismatch between the spec (which doesn't mention negative
axes) and the test-cases (which use negative axes)? Were there many test-cases that failed? I wonder if we should fix the test-cases. If it is complicated, it is worth at least adding a comment to the shape-inference code to mention this reason, to help future maintenance.

jcwchen(2021-01-05 21:37:53):@gramalingam Makes sense. https://github.com/onnx/onnx/tree/master/onnx/backend/test/data/pytorch-converted/test_GLU This model consists the split node with negative which might break the test previously.
gramalingam(2021-01-06 05:06:18):LGTM. I am afraid it needs your signoff. We can merge it once we have the signoff.
shinh(2021-01-06 07:33:21):Done!
linkerzhang(2020-01-31 02:47:44):no need checking allow_negative_axis here.
shinh(2020-02-27 09:39:29):Done, sorry for the large latency!
gramalingam(2021-01-05 21:01:17):A minor point: if the "split" attribute is specified, then we can infer the output dimensions, even if the input-shape does not have a dim-value for the split-axis, isn't that right? 
jcwchen(2021-01-06 00:23:44):Could you add some comments here for explanation? Thank you. Something like:
```suggestion
          // Previously Split-2 does not mention how to deal with negative axis
          // However, there is an existing test onnx/backend/test/data/pytorch-converted/test_GLU
          // using Split-2 with negative axis and it is hard to be regenerated. 
          // To compromise, handle negative axis for Split-2 here. 
          if (axis < 0) {
```
shinh(2021-01-06 01:22:13):I agree. Do you want to do this in this PR? I'm afraid it'd take time for me to update this PR if we do this in this PR.
gramalingam(2021-01-06 05:01:49):I think it's fine to do that as a separate PR.
linkerzhang(2020-02-16 03:53:25):@shinh can you resolve the conflicts? thanks!
shinh(2020-02-27 15:50:11):Resolved. CLA seems to be yellow long time. I've already signed the CLA (e.g., https://github.com/onnx/onnx/pull/2549)
linkerzhang(2020-03-03 03:15:50):@shinh there're conflicts again :(. I can help to monitor and merge it when conflicts resolved and ci passes next time.
shinh(2020-03-07 13:06:58):Just clicking "Update branch" button worked this time, thanks!
linkerzhang(2020-03-10 05:18:33):The "Update branch" button may be only visible for you :), There's no such button shown on my view. You may need to click it again.
shinh(2020-03-10 09:50:41):Ah, I see. I saw other people merged my PR to HEAD several times (e.g., https://github.com/onnx/onnx/pull/2620) and I thought maintainers have the access to the button. It seems my guess was wrong and they manually merged my PRs.

Anyway, I'll try merging this PR as often as I can. Thanks!
shinh(2020-10-02 06:52:09):Now this was fixed by https://github.com/onnx/onnx/pull/2778
lara-hdr(2020-01-24 21:40:26):NLLLoss in PyTorch has an additional attribute ignore_index, which when specified ignores the specified target value (and the loss is then averaged over the non-ignore targets only).
https://pytorch.org/docs/stable/nn.html?highlight=nllloss#torch.nn.NLLLoss

I am thinking of a scenario where we have a NLLLoss with mean reduction, with an ignore_index. 

In that case, with the current introduced ONNX::NLLLoss, we have to:
1 - Create a ONNX::NLLLoss with no reduction.
2 - Identify the indices in 'target' that are equal to the ignore_index.
3 - Set the values of the identified indices to 0 in the output of ONNX::NLLLoss.
4 - Create a ONNX::ReduceSum on the last output.
5 - Divide the reduced sum by (N - Number_of_ignored_indices), to compute the mean over the non-ignored targets.

Given all these steps, would it be preferable to add ignore_index as a parameter?
LMK if you think of a better way to support this.
wschin(2020-01-28 19:56:51):Does it cover both of Tensorflow and Pytorch scenarios?
lgtm-com[bot](2020-01-28 23:08:07):This pull request **introduces 1 alert** when merging 26ee7343cebd53631e6ffbf4fe3df89e5a89af8f into 121114107c38ec49bab4955c13cb82b7675bfc47 - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-6fd21e698e019e8404e771ba34721f894f981304)

**new alerts:**

* 1 for Non\-virtual destructor in base class
liqunfu(2020-01-29 18:30:12):@postrational @houseroad , please review this PR. thanks!
wschin(2020-01-31 23:10:10):Are there some shape inference tests?
liqunfu(2020-02-07 01:36:06):@TMVector , thank you so much for your review and suggestions. It helped a lot. It did also help to avoid introducing a bug by this PR. Will you please take time to check if this PR is at its current state satisfactory? If so, would you sign of it? Thanks.
gramalingam(2020-01-24 00:23:34):Hi, I think it will be better if we use a single "std::function<std::vector<NodeProto>(InferenceContext&)>" instead of a vector of pairs of (condition, body). In particular, this will be useful when we want to use the value of an attribute to create a specific ONNX node. Would it be possible to generalize this way?
gramalingam(2020-01-24 00:25:05):Sorry, I think some confusion happened in the HTML formatting with special characters. I mean a function that takes an input of type "InferenceContext&" and returns the body (a vector<NodeProto>).
gramalingam(2020-01-24 00:28:16):I am not sure if this filtering is a good idea, because the input signature of the function should match parameters passed at call-site. This filtering could break things. It seems okay if a function body has an unused input.
wschin(2020-01-28 00:08:08):Maybe
```suggestion
    NegativeLogLikelihood,
```
?
TMVector(2020-01-29 18:26:47):This doesn't account for (recursively) nested subgraphs (such as `Loop.body`). However I agree with @gramalingam that inputs shouldn't be filtered anyway.
TMVector(2020-01-31 00:07:20):The generated docs don't say this is operator has a function, because [gen_doc.py](https://github.com/onnx/onnx/blob/master/onnx/defs/gen_doc.py#L204-L207) only checks for `has_function`. It also isn't listed in the table of contents as one of the operators with a function registered.
wschin(2020-01-31 22:40:58):```suggestion
  A NegativeLogLikelihood operator computes (weighted) negative log likelihood.
```
wschin(2020-01-31 22:42:05):Please add: if `k=0`, the input shape is reduced to `(N, C)`.
wschin(2020-01-31 22:43:20):What is the log-probabilities for a sample? Is it `input[i, :, k, l, m, ...]`?
wschin(2020-01-31 22:43:51):```suggestion
  The operator's "target" input tensor has the shape of (N, d1, d2, ..., dk). It contains classifications (one of C classes) for N x d1 x d2 x ... x dk samples. 
```
Let's use operator when we can.
wschin(2020-01-31 22:44:40):cla
```suggestion
  The op's "target" input tensor has the shape of (N, d1, d2, ..., dk). It encodes class labels (one of C classes) for N x d1 x d2 x ... x dk samples. 
```
wschin(2020-01-31 22:45:41):```suggestion
  The loss for a sample at n, d_1, d_2,...d_k being classified as class c = target[n][d_1][d_2]...[d_k] is computed as:
```
`a sample at n, d_1, d_2,...d_k` is not a formal description. Do you mean `input[n, :, d_1, d_2, ..., d_k]`? Maybe you can say the loss value incurred by `input[n, :, d_1, d_2, ..., d_k]` is computed by
```
loss[n][d_1][d_2]...[d_k] = -input[n][c][d_1][d_2]...[d_k],
```
where
```
c = target[n][d_1][d_2]...[d_k].
```
wschin(2020-01-31 22:52:55):```suggestion
  If "reduction" attribute is set to "none", the operator's output will be the above loss with shape (N, d1, d2, ..., dk).
```
wschin(2020-01-31 22:59:10):```suggestion
def compute_nll(input, target, weight=None, reduction='mean'):  # type: ignore
```
wschin(2020-01-31 23:00:35):Does it produce the same outputs as Pytorch's NLLLoss? Do you have a tiny script you can add to this PR's message?
wschin(2020-01-31 23:13:47):```suggestion
<dd>Type of reduction to apply to loss: none, sum, mean (default). 'none': the output is the loss for each sample in the batch. 'sum': the output will be summed into a scalar. 'mean': similar to the sum mode, but further divide the sum by the batch_size.</dd>
```
Also, what is `batch_size`? I can't find its definition in the doc.
liqunfu(2020-02-04 22:24:06):added shape tests
liqunfu(2020-02-04 22:53:43):```
# this is the test script to verify equivalency between pytorch and nllloss.py test data. 
def compute_nll_loss(input, target, weight=None, reduction='mean'):  # type: ignore
    ''' Compute nll_loss '''
    input_shape = input.shape

    # GatherElement(-input, target)
    if len(input_shape) == 2:
        N, C = input_shape
        neg_gather_element_input = np.zeros((N, ), dtype=np.float32)
        for i in range(N):
            neg_gather_element_input[i] = -input[i][target[i]]
    else:
        N, C, dim1, dim2 = input_shape
        neg_gather_element_input = np.zeros((N, dim1, dim2), dtype=np.float32)
        for i in range(N):
            for d1 in range(dim1):
                for d2 in range(dim2):
                    neg_gather_element_input[i][d1][d2] = -input[i][target[i][d1][d2]][d1][d2]

    loss = neg_gather_element_input
    if weight is not None:
        # Gather(input=weight, index=target)
        gather_weight = np.take(weight, target)

        loss = gather_weight * loss
        if reduction == 'mean':
            return loss.sum() / gather_weight.sum()

    if reduction == 'none':
        return loss
    elif reduction == 'mean':
        return np.mean(loss, keepdims=False)
    elif reduction == 'sum':
        return np.sum(loss, keepdims=False)

import numpy as np
from itertools import product
import torch
import torch.nn as nn
from numpy.testing import assert_allclose, assert_array_equal

# options 
reduction_options=['none', 'mean', 'sum'] 
N, C, height, width = 2, 3, 4, 5

sample_shape_option = [(), (height, width)]
weight_options=[None, np.random.randn(C).astype(np.float)]

for config in list(product(sample_shape_option, reduction_options, weight_options)):
    sample_shape, reduction, weight = config

    if weight is None:
        weight_pt = None
    else:
        weight_pt = torch.from_numpy(weight).float()

    loss = nn.NLLLoss(weight=weight_pt, reduction=reduction)
    log_softmax_ = nn.LogSoftmax(dim=1)

    data_ = torch.randn(N, C, *sample_shape)
    input_ = log_softmax_(data_)
    target = torch.empty((N, *sample_shape), dtype=torch.long).random_(0, C)

    input_.shape, target.shape

    output_pt = loss(input_, target)
    output_pt.shape, output_pt

    my_loss = compute_nll_loss(input_, target, weight=weight, reduction=reduction)
    assert_allclose(my_loss, output_pt.numpy(), rtol=1e-05)
    my_loss - output_pt.numpy()
    print ("pass: ", sample_shape, reduction, weight, my_loss - output_pt.numpy())
'
```
SherlockNoMad(2020-02-04 23:18:50):>AddQueriedFunctionBody [](start = 9, length = 22)

general speaking, I feel that this approach is not extensible... 

In this op, there is 6 combinations....
What about other ops if the options are combinatorial.... 

Is this the designed way to handle conditional function body?
wschin(2020-02-05 00:14:26):@gramalingam, @linkerzhang might have some opinions.
wschin(2020-02-05 00:42:22):```suggestion
<dd>Type of reduction to apply to loss: none, sum, mean (default). 'none': the output is the loss for each sample.'sum': the output will be summed. 'mean': the sum of the output will be divided by the sum of applied weights.</dd>
```
wschin(2020-02-05 00:46:03):```suggestion
  The "input" tensor contains log-probabilities for input[n, :, d_1, d_2,..., d_k] being in a class of [0, C).
```
wschin(2020-02-05 00:46:32):```suggestion
  The loss value for input[n, :, d_1, d_2,..., d_k] being classified as class c = target[n][d_1][d_2]...[d_k] is computed as:
```
Copy-and-paste? :)
wschin(2020-02-05 00:48:32):```suggestion
  The operator's "target" input tensor has the shape of (N, d1, d2, ..., dk). It encodes class labels (one of C classes) for N x d1 x d2 x ... x dk samples.
```
wschin(2020-02-05 00:50:33):Yeah, compute_negative_log_likelihood_loss is better.
gramalingam(2020-02-05 04:26:18):My suggestion (which is up earlier) is to register a single function that takes the Context as input and returns the function-body.
liqunfu(2020-02-05 17:36:43):fixed
liqunfu(2020-02-05 17:44:55):If the options are combinatorial, then there will be combinatorial number of graph bodies. I do not see how it can be reduced in the current framework. Maybe it will be solved with MLIR dialect?
gramalingam(2020-02-05 19:00:00):@liqunfu : if we define a single function that takes the Context as input and returns the function-body, it will look like below:
```
   NodeSet nodes;
   nodes.add(common_prefix_nodes);
   if (condition1(context)) nodes.add(variant1_A); else nodes.add(variant1_B);
   nodes.add(common_middle_nodes);
   if (condition2(context)) nodes.add(variant2_A); else nodes.add(variant2_B);
   nodes.add(common_suffix);
```
If this is converted to the existing approach, we will have to map every execution-path in the above code to a different (condition,body) pair, leading to the combinatorial explosion that @SherlockNoMad is talking about.
TMVector(2020-02-06 01:16:23):ONNX compilers and runtimes are required to correctly handle operators with [optional inputs and outputs](https://github.com/onnx/onnx/blob/master/docs/IR.md#optional-inputs-and-outputs). Although functions are somewhat underdocumented, I think it is more consistent that runtimes should be required (and able) to correctly handle function bodies which don't use those optional inputs/outputs. IMO this would be easier to implement too.

Also, this filtering seems like it could be a big source of bugs in the future, and it already has at least two that I can see: it will remove inputs that aren't optional if they aren't used by the function body, and it will remove inputs that are used within a subgraph but not by a node in the top-level function graph.

---

For those reasons, I would **strongly** recommend this be changed to not filter inputs.
linkerzhang(2020-02-06 08:10:44):are you going to have different implementation in ORT?
linkerzhang(2020-02-06 08:20:42):do you allow that an optional input exists but the other optional input before it does not exist?
linkerzhang(2020-02-06 08:21:01):is this needed?
liqunfu(2020-02-06 16:25:01):@gramalingam this sounds very good. Let me give it a try.
liqunfu(2020-02-07 00:50:35):discussed with @gramalingam offline. The op signature need to be kept otherwise the provider kernel will fail. I will update this according to reviewers' comment.
liqunfu(2020-02-07 00:51:29):totally understand and agree now. Updated the PR to remove FilterFunctionBodyByName.
liqunfu(2020-02-07 00:54:02):Yes. with @gramalingam and @TMVector 's comment implemented, a functionProto will keep all inputs (required and optional). Thus optional inputs may be provided in any combination.
liqunfu(2020-02-07 00:59:08):Not in this nllloss case. But I would assume cases where a function graph is built depending on existence of its optional outputs. 
liqunfu(2020-02-07 01:00:10):yes.
liqunfu(2020-02-07 01:29:10):updated the PR according to @gramalingam 's suggestion.
gramalingam(2020-02-07 21:21:33):I think @linkerzhang may be suggesting that we enrich this interface by explicitly adding interface method like:
```
   virtual bool hasInput(size_t index);
```
I think that is useful because intermediate optional inputs may be absent. Similarly, I think we will need
```
  virtual const TypeProto* getInputType(size_t index) const = 0;
```
to allow type-dependent specialization.

(However, I am okay if we add the above as a separate PR also, to get this PR in first.)
TMVector(2020-02-07 23:25:35):It looks like you are broadcasting to use the [`GatherElements`](https://github.com/onnx/onnx/blob/master/docs/Operators.md#GatherElements) operator and then slicing back to the unbroadcasted shape, but I think `Unsqueeze`ing should be enough -- the requirement is that the `data` and `indices` inputs are the same rank, not that they are the same shape. I haven't tried this, but the second GatherElements example demonstrates something similar.

(Or you could use the [`Expand`](https://github.com/onnx/onnx/blob/master/docs/Operators.md#Expand) operator to do the broadcast)
TMVector(2020-02-07 23:45:42):You could also compare named dimensions (`dim_param`) here and for `weight`
TMVector(2020-02-07 23:49:39):```suggestion
                    const auto input_dim_1 = input_shape.dim(1);
```
TMVector(2020-02-07 23:50:44):Indentation is off in this `if` body
TMVector(2020-02-07 23:51:39):```suggestion
                // otherwise output is a scalar.
```
TMVector(2020-02-08 00:02:41):Optional inputs can also be omitted by providing an empty string. I'm not sure if that case would currently be correctly handled or not?
```python
> node = onnx.helper.make_node(op_type="Nll", inputs=["a", "b", ""], outputs=["z"])
> len(node.input)
3
```
liqunfu(2020-02-08 01:06:25):it is tricky with dim_param. A dim_param mismatch is not necessarily a failure.
liqunfu(2020-02-08 01:17:18):your first option works. Thanks!
liqunfu(2020-02-08 01:19:41):replaced numInput/numOutput with hasInput(i)/hasOutput(i).  
linkerzhang(2020-02-08 02:45:18):YES, Rama is correct!

If allowing the case that I mentioned, then current interface is not enough.
jignparm(2020-01-16 02:43:19):N.B. PR  #2552 and  PR #2553 deal address the same issue in 2 different ways. Only **one** of these PRs should be merged.

gramalingam(2020-02-05 04:49:25):LGTM. I prefer this to the other option (2553), since this maintains backward compatibility. Any opinions @linkerzhang @houseroad ?
neginraoof(2020-02-21 18:25:46):Can we keep the same behavior using -1 dim value, and don't add "allow zero"?
jignparm(2020-02-21 22:25:31):> Can we keep the same behavior using -1 dim value, and don't add "allow zero"?

Currently '0' means 'copy from input'. Are you suggesting that '-1' should be updated so that it means 'copy from input'? This seems like an odd change -- most frameworks use '-1' to mean 'infer from remaining dimensions'. 

 
There is also PR #2553 that removes the special meaning of zero (so special values can start from -1 downwards). Perhaps this is what you meant instead?


BowenBao(2020-11-05 23:10:15):ping: any updates on this? I think after several releases of ONNX, the version needs to be opset 14.
gramalingam(2021-01-26 22:09:14):Closing this as this was recreated as https://github.com/onnx/onnx/pull/3113 .
gramalingam(2020-02-05 04:42:00):Change "output dimension" to "value in the 'shape' input"
gramalingam(2020-02-05 04:42:24):change "desired" to "corresponding"
gramalingam(2020-02-05 04:42:53):change "output dimension" to "value in the 'shape' input"
jignparm(2020-02-15 01:29:04):Done!
jignparm(2020-02-15 01:29:12):Done!
jignparm(2020-02-15 01:33:55):Done!
jignparm(2020-01-16 02:43:32):N.B. PR  #2552 and  PR #2553 deal address the same issue in 2 different ways. Only **one** of these PRs should be merged.
diyessi(2020-09-22 16:59:31):PaddlePaddle seems to be the source of the 0 behavior. Using a value that cannot appear in a shape, such as -2, would still allow the copy behavior, but not be confusable with a valid length in a shape. The PaddlePaddle shape is specified with an attribute, so it will have a constant value and it should be easy to convert a 0 to a -2 in an ONNX conversion.
gramalingam(2021-01-26 22:10:45):Closing this. Please see https://github.com/onnx/onnx/pull/3113 for the follow-up.
fdwr(2020-01-16 21:16:16):FYI @prasanthpul.
gramalingam(2020-01-22 22:00:43):@linkerzhang : are circleci and appveyor CI expected to fail? Is it okay to merge in PRs if these CIs fail?
wschin(2020-01-22 23:26:30):#2565 is a temporal solution.
gramalingam(2020-01-23 03:35:57):Hope this works! Thanks for the fix.
neginraoof(2020-01-23 17:28:11):@wschin Can we merge this please?
vinitra-zz(2020-01-22 23:26:19):numpy can be removed from this command? It might be double installed.
vinitra-zz(2020-01-22 23:27:55):pip==19
wschin(2020-01-22 23:30:45):Removed at the protobuf installation line. I feel protobuf itself is difficult enough to occupy one line.
neginraoof(2020-01-23 22:14:37):@wschin 
An example model that needs this is a torchvision model, Mask RCNN.
In case of no detection, the model creates an empty tensor of size (0, B, ...)
neginraoof(2020-01-31 21:05:23):cc @postrational @houseroad. Could you please help review this PR? Thanks.
wschin(2020-01-23 21:08:24):Must be `>0`? I believe the `=` case corresponds to a scalar and we already have `empty tensor` for that.
neginraoof(2020-01-23 22:12:25):Yes, but we can a multidimensional tensor with one dim = 0.
Like:
numpy.full((2, 0), 4)
numpy.random.randn(2, 0)
or
torch.randn([0, 2, 3])
gramalingam(2020-01-28 21:12:52):Just to clarify: the output is expected to be an empty tensor, so the constant-value used is immaterial. Is that right?
neginraoof(2020-01-28 21:17:28):Yes, that's correct.
lara-hdr(2020-01-23 19:35:13):@SherlockNoMad @wschin for review
lara-hdr(2020-01-29 23:39:31):Thank you @wschin!

@SherlockNoMad lmk if there are any additional comments, so we can merge this as soon as possible :)
wschin(2020-01-30 00:12:23):@lara-hdr, we need another approval from operator SIG to merge it. 
lara-hdr(2020-01-30 17:59:44):@gramalingam for review
lara-hdr(2020-01-31 20:28:24):cc @houseroad @postrational @linkerzhang 
lara-hdr(2020-02-05 23:57:56):@ebarsoum CI is green. Thanks!
wschin(2020-01-27 18:21:56):If we make it an attribute, we can do static shape inference for both of training and inference. If we make it an input, I am not sure how ONNX shape inference would work.
wschin(2020-01-27 18:26:12):Those outputs (mean, var, saved_mean, saved_var) cannot be consumed by any other operators. Otherwise, SSA assumption will be broken. Could we make it explicitly here by saying that `this output cannot be an input of any other operator`. If time is allowed, we should modify the checker to reflect this as well.
wschin(2020-01-27 18:28:19):If we really make `mode` an input, we need to check if it's a Boolean scalar in the shape inference below.
wschin(2020-01-27 18:32:17):>ratio [](start = 19, length = 5)

Can we check if this is an scalar inside the shape inference below?
lara-hdr(2020-01-27 19:48:13):If we make it an attribute, then training_mode would be fixed once the model is exported.
So if we export the model in training_mode, we would not be able to run the model in inference mode since batch_norm will have a fixed attribute value of training_mode.
I am thinking that this should not be an issue, is that correct? In that case, I will change it to be an attribute instead.
lara-hdr(2020-01-27 19:48:39):makes sense; I will add it explicitly in the doc.
SherlockNoMad(2020-01-27 20:00:07):There is a scaling involved when ratio is non-zero 

const float scale = 1.f / (1.f - ratio);

Yi = scale * Xi * mask_i
SherlockNoMad(2020-01-27 20:01:50):>tensor(float [](start = 18, length = 12)

ratio can be all float type 
including fp16
SherlockNoMad(2020-01-27 20:14:49):>training_mode [](start = 8, length = 13)

great! This flag has to be an input, instead of an attribute, since we need to modify it during the runtime. 

Please add this to the PR description. 


wschin(2020-01-27 21:13:49):My original concern is  its dynamic output schema which depends on an input flag. Can we make them optional (like LSTM's outputs)? It can probably make ONNX's static shape inference working again.
wschin(2020-01-27 21:22:17):Let's just make `mean` and `var` inaccessible. The other two will be accessed by training operators and they don't break SSA.
lara-hdr(2020-01-27 21:25:40):This needs to be an an input following the comment: https://github.com/onnx/onnx/pull/2568#discussion_r371459340 .
You mean make the output mean/var optional? It is already the case.
wschin(2020-01-28 16:57:10):But you mentioned that if `training_mode=True`, those outputs won't be populated. This doesn't match the behavior of LSTM --- an optional output will be always populated as long as the user specifies that in the output list.
wschin(2020-01-28 16:59:09):Could you please add `equations` to formally describe these two cases' behaviors?
wschin(2020-01-28 17:01:02):@lara-hdr, could you include Sherlock's equation in the spec if we haven't done that yet?
lara-hdr(2020-01-29 00:28:41):got it. I updated the doc for them to be optional
wschin(2020-01-29 00:30:43):From the description here, it looks like `saved_mean = ReduceMean(X, axis=channel_index)`. Is it correct?
wschin(2020-01-29 00:43:55):What is `_i`? Could you please give it a formal definition? The input name below is `data`. Could you please change `input_i` to `data_i` for consistency? Thanks.
wschin(2020-01-29 00:46:32):Nice!
wschin(2020-01-29 00:51:31):```suggestion
<dd>Saved mean used during training to speed up gradient computation.</dd>
```
Because there is no notion of `training` and `non-training` operators, we probably need to drop the new description. It's fine for existing operators to consume `saved_*` as long as it doesn't break SSA.
wschin(2020-01-29 00:56:14):The `running_mean` in the input list is named to `mean`. Does it mean this equation is actually
```suggestion
  mean = mean * momentum + saved_mean * (1 - momentum)
```
?
lara-hdr(2020-01-29 18:26:57):actually saved_mean=ReduceMean(X, axis=all_except_channel_index)
the computation of axis is in the reference implementation : https://github.com/onnx/onnx/blob/b24b4fff5a4f83cd34347c46af1842fa74bc7e76/onnx/backend/test/case/node/batchnorm.py#L26

for example : if the input size is (1, 2, 1, 3), the nb channel is at index 1, and nb_channel = 2. 
So saved_mean is reduced over indices (0,2,3), and saved_mean should be of size (2).
wschin(2020-01-29 21:19:37):Thanks a lot for the nice explanation. May we add it to the spec? As you have seen, I am a poor user who misinterpret this sentence. For a mathematical operator, I feel it's always better to write done things in a mathematical way. :)
wschin(2020-01-29 21:25:14):```suggestion
  The statistics are updated as follows:
```
Would the update happen in inference mode? If the computation rules depend on `training_mode`, we need to have the equations listed for both cases (`training_mode`=True/False).
wschin(2020-01-29 21:26:48):```suggestion
  to flatten the input shape to (N x C*D1*D2 ..*Dn) before a BatchNormalization operator.
```
wschin(2020-01-29 21:28:05):`running_mean` --> `mean`? Is this attribute used in inference mode?
wschin(2020-01-29 21:35:09):`float -> floating-point` also `floating -> floating-point`.
wschin(2020-01-29 21:37:50):```suggestion
  scale = 1. / (1. - ratio) if training_mode=true else 1.
```
lara-hdr(2020-01-29 21:39:15):Dropout does not have a training_mode attribute. 
Do you think we should introduce it?
lara-hdr(2020-01-29 21:51:49):thanks! I updated the doc
wschin(2020-01-29 22:18:51):```suggestion
  Note that this Dropout scales the masked input data by the following equation, so to convert the trained model into inference mode, the user can simply replace this Dropout with an Identity operator.
```
wschin(2020-01-29 22:19:09):```suggestion
```
wschin(2020-01-29 22:19:23):```suggestion
  scale = 1. / (1. - ratio).
```
wschin(2020-01-29 22:19:42):Sorry for making this back and forth.
wschin(2020-01-29 22:22:33):```suggestion
                fail_shape_inference("Ratio of Dropout must be a scalar.");
```
lara-hdr(2020-01-29 22:51:47):you are suggesting to remove the whole sentence "Note that ... computed as:" ?
lara-hdr(2020-01-29 22:52:38):nvm just noticed the other comment.
wschin(2020-01-29 22:53:55):No. I add another comment for deleting my stupid idea.
gramalingam(2020-01-30 21:49:34):I didn't understand the last two comments. For output values that will not be used in training_mode=false, the spec should not force the computation of those values.
gramalingam(2020-01-30 22:17:29):I think we just need to (locally) rename the outputs of the operator to be different from the inputs of the operator for the sake of clarity: e.g., use, output_mean and output_var instead of mean and var. I don't think there needs to be any constraints like "this output cannot be an input of any other operator".
gramalingam(2020-01-30 22:18:31):The names used here are "formal parameter names", and have no connection to the "actual parameter names" used in an actual node containing a call to this op.
gramalingam(2020-01-30 22:28:05):More specifically: when training_mode=false, which output-values need to be computed?
gramalingam(2020-01-30 22:35:07):Before we use the i-th inputShape, we should check if an i-th inputShape is available (using either https://github.com/onnx/onnx/blob/121114107c38ec49bab4955c13cb82b7675bfc47/onnx/defs/shape_inference.h#L269 or https://github.com/onnx/onnx/blob/121114107c38ec49bab4955c13cb82b7675bfc47/onnx/defs/shape_inference.h#L274 ). It is possible to have input N, but have no shape for it (if shape is unknown).
gramalingam(2020-01-30 22:38:01):Needs a check that the inputShape is available first.
lara-hdr(2020-01-30 22:45:20):'mean', 'var', 'saved_mean', and 'saved_var' are only needed for training.
But they are optional parameters (so they have static shapes and we can do the shape inference), so even if training_mode=False and the user specify them as output, they will be outputs of the op
lara-hdr(2020-01-31 00:14:10):I updated the shape inference to include the check, thanks!
SherlockNoMad(2020-01-28 19:31:24):Looks good to me. Wei-Sheng will help you further review this in details. 
wschin(2020-01-30 00:18:24):This PR introduces a new operator. Please make sure this PR follows the [rules](https://github.com/onnx/onnx/blob/master/docs/AddNewOp.md) agreed by Operator SIG
KsenijaS(2020-02-03 19:16:19):This PR is dependent upon Liqun's PR (https://github.com/onnx/onnx/pull/2551) that introduced property HasQueriedFunction and that's why some tests are failing.
KsenijaS(2020-02-04 23:43:37):Please review @houseroad @postrational 
spandantiwari(2020-02-06 18:23:17):Support for weighted mean has been removed. Does that mean there's no immediate use case for it? 
KsenijaS(2020-02-06 21:19:11):> Support for weighted mean has been removed. Does that mean there's no immediate use case for it?

No it hasn't, tensorflow is using weighted mean.
SherlockNoMad(2020-01-24 23:36:19):>mean [](start = 21, length = 4)

A few more examples for "sum and none"

and one example for with weight 
SherlockNoMad(2020-01-24 23:36:58):>[1.2, 2.5] [](start = 21, length = 10)

Some example for 3D input
lara-hdr(2020-01-24 23:38:03):We could add the shape and type inference here for the output.
We can propagate the type and shape when the reduction in none, otherwise it's a tensor of shape 1
lara-hdr(2020-01-24 23:42:57):If you can add tests for the shape inference here as well https://github.com/onnx/onnx/blob/master/onnx/test/shape_inference_test.py (maybe one with reduction and one without).
wschin(2020-01-27 22:13:28):```suggestion
    MeanSquaredDistance,
```
If it's called `Error`, I'd feel it should be in `ai.onnx.training` domain being introduced in #2314.
wschin(2020-01-28 17:04:11):Does it mean `score` and `label` must be N-element vectors?
wschin(2020-01-28 17:05:43):Before introducing this equation (output of loss function), we need to describe the shapes of `score` and `label` (inputs of loss function).
lara-hdr(2020-01-28 23:43:21):The MSE op does not have any keepdims/axes attribute, so it should not be part of the tests.
lara-hdr(2020-01-29 00:17:18):since some of the logic repeats in the tests you could add a function mse(input, target, reduction='mean', w=None) to call in each of the test. Something like :

```
def mse(input, target, reduction='mean', w=None):
  out = np.square(input - target)
  if w is not None:
    out = out * w
  if reduction == 'mean':
    out = out.mean()
  elif reduction == 'sum':
    out = out.sum()
 return out
```



lara-hdr(2020-01-29 00:24:01):yes, @KsenijaS could you update the documentation with the input shape?
lara-hdr(2020-01-29 00:31:23):The output shape is the same only when reduction==none.
so if reduction is none we want to propagateElemTypeFromInputToOutput(ctx, 0, 0),
otherwise the output is a scalar (output shape with 0 dimension)
lara-hdr(2020-01-29 00:35:51):make sure to add the files generated for mse with ./update_docs.sh (only the ones you added).
(it should generate some .onnx and .pb files for these tests)
wschin(2020-01-30 18:53:18):Why is `none` mapped to `axes=[0]`?
lara-hdr(2020-02-03 21:42:31):nit: a couple of lines are not aligned in this file
lara-hdr(2020-02-03 21:43:37):missing X_Pow here?
lara-hdr(2020-02-03 21:50:51):can you check that inputShape is available as explained in https://github.com/onnx/onnx/pull/2568#discussion_r373230828
spandantiwari(2020-02-04 21:51:01):I agree. Renaming it to `MeanSquaredDistance` and keeping it in the default domain would be my recommendation.
gramalingam(2020-02-05 00:13:25):Is weights allowed for 'mean'? If it is allowed, does it mean compute a weighted-mean (which would require dividing by the sum-of-weights, and not the batch-size)?
lara-hdr(2020-02-06 18:31:07):It would be good to have a reference implementation function.
As explained in the comment in the deleted file : https://github.com/onnx/onnx/pull/2570#discussion_r372130505

KsenijaS(2020-02-06 19:36:08):weights are used only in tensorflow and are allowed for mean, and it is divided by batch-size
SherlockNoMad(2020-02-11 22:41:55):>ReduceMean [](start = 37, length = 10)

default behavior is reducing a tensor to a scalar

[M, N, K] -> [1, 1, 1]

Is this the expected bahavior? 
SherlockNoMad(2020-02-11 22:42:41):>fn(GetOpSchema<ONNX_OPERATOR_SET_SCHEMA_CLASS_NAME(Onnx, 12, MeanSquaredDistance)>()); [](start = 4, length = 86)

duplicated?
SherlockNoMad(2020-02-11 22:42:56):>	 [](start = 0, length = 1)

tab to space
KsenijaS(2020-02-11 23:20:00):default behavior is when reduction is mean and output is scalar

gramalingam(2020-02-12 00:17:32):(a) I guess these can be multi-dimensional? If so, suggest changing "vector" to "tensor".
(b) It seems to me that either we should require score and label to have same shape or allow them to be broadcastable. Which is it?
gramalingam(2020-02-12 00:18:34):change to "broadcastable to the shape of L" (which may differ from the shape of scores if we allow broadcast between scores and label)
gramalingam(2020-02-12 00:20:37):else we should propagate a scalar-shape, which is the empty list of dimensions, I think. May be something like "updateOutputShape(ctx, 0, TensorShapeProto());"?
gramalingam(2020-02-12 00:21:53):I mean only in the case where reduction is "none"
gramalingam(2020-02-12 00:23:18):In the then-branch, we may need to do broadcast, if broadcast is allowed.
wschin(2020-02-12 18:37:14):Line 14301-14305 can be shortened to

This operator first computes a element-wise squared loss tensor `L` via
```
L = Pow(Sub(score, label), 2)
```
wschin(2020-02-12 18:37:50):```suggestion
  Finally, L is optionally reduced:
```
wschin(2020-02-12 18:38:17):```suggestion
<dd>Type of reduction to apply to loss: none, sum, mean(default). 'none': the output is the loss for each sample in the batch.'sum': the output will be summed into a scalar. 'mean': the the output with `reduction=sum` will be further divided by the the first dimension of `scores`.</dd>
```
wschin(2020-02-12 18:41:24):Any numerical comparison between this reference implementation and the pytorch (or tensorflow) corresponding one?
wschin(2020-02-12 18:42:16):Please also add 1-D and 4-D cases. Thank you.
KsenijaS(2020-02-13 02:15:43):I think that makes it less readable
KsenijaS(2020-02-13 23:39:40):```
import torch
import torch.nn as nn
import numpy as np
from numpy.testing import assert_allclose

def mean_squared_distance(input, target, reduction='mean', w=None):
    out = np.square(input - target)
    if w is not None:
        out = np.multiply(out, w)
    if reduction == 'mean':
        out = np.mean(out)
    elif reduction == 'sum':
        out = np.sum(out)
    return out

loss = nn.MSELoss()
input = torch.randn(3, 5)
target = torch.randn(3, 5)
output = loss(input, target)

my_loss = mean_squared_distance(input.numpy(), target.numpy())
assert_allclose(my_loss, output.numpy(), rtol=1e-05)
```
gramalingam(2020-02-14 01:33:20):change to
```C++
   if (reduction.compare("none") == 0) {
      if (hasInputShape(ctx, 0)) {
         propagateShapeFromInputToOutput(ctx, 0, 0);
      }
   } else {
      updateOutputShape(ctx, 0, TensorShapeProto());
   }
```
claassistantio(2020-01-24 23:21:20):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=2571) <br/>All committers have signed the CLA.
wschin(2020-01-30 00:14:35):We have a [guideline](https://github.com/onnx/onnx/blob/master/docs/AddNewOp.md) for adding new operator. Could you please take a look and make sure all requirements are met? Many thanks.
KsenijaS(2020-02-04 17:34:34):this PR is dependent upon Liqun's PR (https://github.com/onnx/onnx/pull/2551) where AddQueriedFunctionBody was introduced. And that's why some of the tests are failing.
KsenijaS(2020-02-04 23:43:11):Please review @houseroad @postrational 
SherlockNoMad(2020-02-11 22:32:29):>   **Operators with function registered:**

Does this op belong here ?

---
Refers to: docs/Operators.md:163 in 9ca0b54. [](commit_id = 9ca0b5496cb06594eef68fb94738eb18e034c011, deletion_comment = False)
SherlockNoMad(2020-02-19 17:24:27):I just realized that the second output is missing in this spec. 
The second optional output should be log_prob = logsoftmax(input), this will output will be used for speeding up the backward computation. 

See https://aiinfra.visualstudio.com/Lotus/_git/onnxruntime/pullrequest/5592 for details 
linkerzhang(2020-02-21 01:02:51):@ebarsoum @postrational any more comments on this PR?
lara-hdr(2020-01-31 23:04:56):nit: to many spaces
lara-hdr(2020-01-31 23:06:25):could you add the shape inference
lara-hdr(2020-02-04 18:24:54):can you verify that 'scores' and 'label' have the same shape in the shape inference?
lara-hdr(2020-02-04 18:25:55):nit: align for readability (same comment for lines 101 -110-119-129)
SherlockNoMad(2020-02-04 23:12:18):>		 [](start = 0, length = 2)

tab to space 

in all source files
SherlockNoMad(2020-02-04 23:14:09):>ReduceMean [](start = 30, length = 10)

do we need to set axes here ? 
Does default value works ?

SherlockNoMad(2020-02-04 23:16:46):>AddQueriedFunctionBody [](start = 2, length = 22)

general speaking, I feel that this approach is not extensible... 

In this op, there is 6 combinations....
What about other ops if the options are combinatorial.... 
lara-hdr(2020-02-04 23:41:13):no need to specify axes here.

If the reduction is specified, ReduceMean will be computed over the whole ouput (axes=None, and output will be scalar).
For more specific cases requiring computing the mean over specific axes, there is the possibility of using a subgraph with ONNX::SoftmaxCrossEntropy with 'none' reduction followed ONNX::ReduceMean with the desired axes.

How does that sound?
wschin(2020-02-04 23:55:49):```suggestion
                                     reduction=reduction)
```
wschin(2020-02-04 23:57:18):May we do
```
l = SoftmaxCrossEntropy(x, labels)
```
? We can add comments in `SoftmaxCrossEntropy` to indicate which lines are for `Softmax`.
wschin(2020-02-05 00:01:30):Where is `weights` in this node? Also, can we make `weights` an attribute?
wschin(2020-02-05 00:04:30):These three line clearly indicate that this operator can be composed... I don't insist this should be proposed as a function though. Can you merge the three lines into a single function and called it in every test? It's not very clear if we have different numpy reference implementations for different `reduce` and `input` configurations....
wschin(2020-02-05 00:06:28):`crossentropy.py`? Isn't it `SoftmaxCrossEntropyLoss`?
wschin(2020-02-05 00:09:24):Could we make this a full reference implementation
```
def softmax_cross_entropy(x, y, w=None):
  # Softmax
  ...
```
? It'd be even better if you have a Pytorch script showing that this reference implementation matches whatever Pytorch is doing. Please also confirm that Tensorflow can also use this operator.
wschin(2020-02-05 00:10:35):I feel it should be an attribute.
wschin(2020-02-05 00:11:52):```suggestion
### <a name="SoftmaxCrossEntropyLoss"></a><a name="softmaxcrossentropy">**SoftmaxCrossEntropy**</a>
```
SoftmaxCrossEntropy ---> SoftmaxCrossEntropyLoss

The first function also outputs normalization coefficient but the second term doesn't.
wschin(2020-02-05 00:13:05):Please add a case with the present of `weight`, if we decide not to move `weight` to the attribute list.
gramalingam(2020-02-05 00:19:45):Change "scores" to "shape(scores)"?
gramalingam(2020-02-05 00:20:09):change "labels" to "shape(labels)"?
gramalingam(2020-02-05 00:25:07):What about the K-dimensional loss? I don't quite understand how the above equations apply in that case.
gramalingam(2020-02-05 00:26:57):It isn't the same dimension as "scores", right? May be better to say [batch_size] or etc.
gramalingam(2020-02-05 00:30:12):Is the same computation applied to each of of the k-dimensional points separately?
KsenijaS(2020-02-06 21:43:52):in order to be consistent with MSELoss weight should be an input.
KsenijaS(2020-02-07 22:26:21):scores and label don't have the same shape.
KsenijaS(2020-02-07 22:31:03):weights will stay an input to be consistent with other loss functions
KsenijaS(2020-02-07 22:55:44):It's the same as in NllLoss, SoftmaxCrossEntropy = Log(Softmax(scores)) + NllLoss(scores, labels).
L is the same shape as labels and if there is reduction, L is flattened and then sum or mean is performed and output is scalar.
SherlockNoMad(2020-02-11 22:28:21):>SoftmaxCrossEntropyLoss [](start = 14, length = 23)

 nit. 
how about just call it SoftmaxCrossEntropy
SherlockNoMad(2020-02-11 22:29:30):ah... I see WeiSheng's comment ... 

---
In reply to: [377937804](https://github.com/onnx/onnx/pull/2573#discussion_r377937804) [](ancestors = 377937804)
SherlockNoMad(2020-02-11 22:30:41):>	 [](start = 0, length = 1)

tab to space 
Please also set it in the IDE to only use space
SherlockNoMad(2020-02-11 22:31:21):If it's already universally called *Loss, I am ok with the current naming

---
In reply to: [377938280](https://github.com/onnx/onnx/pull/2573#discussion_r377938280) [](ancestors = 377938280,377937804)
SherlockNoMad(2020-02-11 22:35:41):>NegativeLogLikelihoodLoss [](start = 33, length = 25)

How do we propagate attribute "reduction "to the function body node? 
gramalingam(2020-02-12 00:33:25):is the LHS supposed to be l[i][d1][d2]...[dk] ?
gramalingam(2020-02-12 00:37:21):"groud" => "ground"
gramalingam(2020-02-12 00:39:18):if reduction is none, the output is a scalar, right? we can set the output shape for that case also.
KsenijaS(2020-02-12 00:45:44):If reduction is none, output is the same shape as labels. If reduction is not none then output is scalar. Do you want me to add that case?
KsenijaS(2020-02-12 00:46:22):yes, l_i is of shape l[i][d1][d2]...[dk]
gramalingam(2020-02-12 06:26:09):Yes, sorry that's what I meant. Please do add it.
gramalingam(2020-02-12 06:27:07):I think it is better to change the LHS to l[i][d1]...[dk]. Otherwise, it is unclear where the d1 to dk in the RHS come from.
wschin(2020-02-12 07:35:02):```suggestion
      l_i = -y[i][c][d1][d2]..[dk] * weights[c], if 'weights' is provided.
```
wschin(2020-02-12 08:09:41):I check the document of ONNX Softmax. If the input is N-D tensor with shape (N, C, D1, ..., Dk), it will be flattened into a 2-D tensor with shape (N, C * D1 * ... * Dk). I don't quite sure this matches the Softmax needed in Pytorch SoftmaxCrossEntropy.
KsenijaS(2020-02-12 17:07:31):If there is reduction that means output is a scalar, so it doesn't matter. Only when reduction is none the shape matters, I could try to use Reshape op for that
KsenijaS(2020-02-12 17:09:10):Also I think we should use LogSoftmax instead of Log and Softmax
KsenijaS(2020-02-12 17:13:23): In Pytorch output of Softmax is of the same dimension and shape as input.
wschin(2020-02-12 18:07:57):To compose Pytorch Softmax, we probably only need `Div(X, ReduceSum(Exp(X), axis=1, keepdims=True))`. Please double-check.
wschin(2020-02-12 18:16:52):```suggestion
This operator first computes a loss tensor whose shape is identical to the labels input.
If the input is 2-D with shape (N, C), the loss tensor may be a N-element vector L = (l_1, l_2, ..., l_N).
If the input is N-D tensor with shape (N, C, d1, d2, ..., dk), the loss tensor L may have (N, d1, d2, ..., dk) as its shape and L[i,][j_1][j_2]...[j_k] denotes a scalar element in L.
After L is available, this operator can optionally do a reduction operator to combine loss elements along a specific axis.
```
Let's review the words and avoid direct use of existing document.
wschin(2020-02-12 18:21:19):```suggestion
  shape(scores): (N, C) where C is the number of classes, or (N, C, D1, D2,..., Dk),
```
because we have `y[i][c][d1][d2]..[dk]` below.
wschin(2020-02-12 18:23:01):```suggestion
  Finally, L is optionally reduced:
```
wschin(2020-02-12 18:27:03):I think `weight` also applies to the `sum` and `none` mode. The description here seems saying `weight` only applies to `mean`. In addition, the equation to compute weighted loss is unclear. Is it L_weighted = Mul(L, weight)?
wschin(2020-02-12 18:27:49):```suggestion
<dd>The predicted outputs with shape [batch_size, class_size], or [batch_size, class_size, D1, D2 , ..., Dk], where K is the number of dimensions.</dd>
```
because we already use `d1`, ..., `dk` as indexes above. Please change other places as well. Thanks.
wschin(2020-02-12 18:31:31):We need some tests with N-D input.
KsenijaS(2020-02-13 18:20:11):I think it is Div(Exp(X - Max(X)), ReduceSum(Exp(X-Max(X)), axis=1, keepdims=True))
KsenijaS(2020-02-13 19:05:12):yes, but when there is a weighted mean, then mean is not averaged by the number of elements in the output but by the sum of weights

KsenijaS(2020-02-13 22:33:38):above there is a description that applies to all reduction types and in case of reduction=mean it has to be averaged by sum of weights as well
KsenijaS(2020-02-13 23:35:02):Test numerical values:
```
import torch
import torch.nn as nn
import numpy as np
from numpy.testing import assert_allclose

def softmaxcrossentropy(x, target, weight=None, reduction='mean'):  # type: (np.ndarray) -> np.ndarray
    max_x = np.max(x, axis=1, keepdims=True)
    exp_x = np.exp(x - max_x)
    p = exp_x / np.sum(exp_x, axis=1, keepdims=True)
    inp = np.log(p)
    input_shape = inp.shape
    if len(input_shape) == 2:
        N, C = input_shape
        neg_gather_element_input = np.zeros((N, ), dtype=np.float32)
        for i in range(N):
            neg_gather_element_input[i] = -inp[i][target[i]]
    if len(input_shape) == 3:
        N, C, D = input_shape
        neg_gather_element_input = np.zeros((N, D), dtype=np.float32)
        for i in range(N):
            for d in range(D):
                neg_gather_element_input[i][d] = -inp[i][target[i][d]][d]

    loss = neg_gather_element_input
    if weight is not None:
        gather_weight = np.take(weight, target)
        loss = gather_weight * loss
        if reduction == 'mean':
            return loss.sum() / gather_weight.sum()

    if reduction == 'mean':
        loss = np.mean(loss)
    if reduction == 'sum':
        loss = np.sum(loss)

    return loss

loss = nn.CrossEntropyLoss()
input = torch.randn(3, 5)
target = torch.empty(3, dtype=torch.long).random_(5)
output = loss(input, target)

my_loss = softmaxcrossentropy(input.numpy(), target.numpy())
assert_allclose(my_loss, output.numpy(), rtol=1e-05)
```
wschin(2020-02-14 00:04:05):Sorry. So there are two behaviors? What are equations of them?
KsenijaS(2020-02-14 00:21:45):For mean, yes. If weights is provided it's divided by sum of weights otherwise by the number of elements in output. I didn't know we need equations for that, it seems pretty easy.
gramalingam(2020-02-14 01:02:01):May be useful to change "output is averaged by sum of weights" to "output is weighted-mean".
wschin(2020-02-01 04:40:19):Do we have tests for the new logic?
wschin(2020-02-01 04:41:25):Would it be suitable to check if output shape is valid here? Does the current master branch break this test?
wschin(2020-02-01 04:43:18):This is a great test for `Identity`. May we have tests for `Dropout`, `Pad`, and `Transpose` as well? Thanks.
daquexian(2020-02-01 11:47:30):Could you elaborate, please? What should be tested? :)
daquexian(2020-02-01 11:50:45):The check of output shape is contained in [this method](https://github.com/onnx/onnx/pull/2574/files#diff-0653f936835022eecf61d5127741c081R134), which checks the optimized model. The current master branch will fail on this method as the output shape is missing. 
daquexian(2020-02-01 11:53:36):Sure :) Thanks.
daquexian(2020-02-13 13:05:13):Added
jeremycochoy(2020-01-31 11:28:11):Thanks @TMVector and @linkerzhang for your feedback.

 * Regarding the Tensor/Scalar issue raised by TMVector, I can say that the following code pass the shape inference test. But I don't know if it is enough to say if everything is fine if the second argument is a scalar and not a tensor (I don't know if this 1.f isn't converted to a tensor implicitly).
```
          {// nodes: {outputs, op, inputs, attributes}                                                           
            FunctionBodyHelper::NodeDef{{"alpha"}, "Constant", {}, {{"value", 1.f}}},                                        
            {{"X_alpha"},
             "Div",
             {"X", "alpha"}
            },
            {{"Y"}, "Elu", {"X_alpha"}}})));
```

 * Regarding the second problem (using the actual attribute):

I made some attempt to create a constant node that recover the `alpha` attribute from the `Celu` operator using AtributeProto. Although the code compile, the shape inference test is a huge failure. In order to understand what is happening, I simplified the body of the function.

If I run the shape inference test with the following body, I get a nice "Y" of empty shape.
```
FunctionBodyHelper::Const<float>("Y", 1.0f)
```
```
E             name: "Y"
E             type {
E               tensor_type {
E                 elem_type: 1
E                 shape {
E                 }
E               }
E             }
```

but if I try to run the shape inference test with the attribute (see code below), then no "Y" is inferred at all.
```
FunctionBodyHelper::NodeDef{{"Y"}, "Constant", {}, {MakeRefAttribute("value", AttributeProto::FLOAT, "alpha")}}
```
```
E       AssertionError: ({'X', 'Y'}, {'X'})
E       assert {'X', 'Y'} == {'X'}
E         Extra items in the left set:
E         'Y'
E         Use -v to get the full diff
```

On my side I am stuck. Looking at `Const` and `ToVector` implementation didn't gave me any new idea to test. Do you have any idea of what is happening? Is it related to this Tensor/Scalar problem? 🙃
wschin(2020-02-01 07:44:43):> Thanks @TMVector and @linkerzhang for your feedback.
> 
> * Regarding the Tensor/Scalar issue raised by TMVector, I can say that the following code pass the shape inference test. But I don't know if it is enough to say if everything is fine if the second argument is a scalar and not a tensor (I don't know if this 1.f isn't converted to a tensor implicitly).
> 
> ```
>           {// nodes: {outputs, op, inputs, attributes}                                                           
>             FunctionBodyHelper::NodeDef{{"alpha"}, "Constant", {}, {{"value", 1.f}}},                                        
>             {{"X_alpha"},
>              "Div",
>              {"X", "alpha"}
>             },
>             {{"Y"}, "Elu", {"X_alpha"}}})));
> ```
> 
> * Regarding the second problem (using the actual attribute):
> 
> I made some attempt to create a constant node that recover the `alpha` attribute from the `Celu` operator using AtributeProto. Although the code compile, the shape inference test is a huge failure. In order to understand what is happening, I simplified the body of the function.
> 
> If I run the shape inference test with the following body, I get a nice "Y" of empty shape.
> 
> ```
> FunctionBodyHelper::Const<float>("Y", 1.0f)
> ```
> 
> ```
> E             name: "Y"
> E             type {
> E               tensor_type {
> E                 elem_type: 1
> E                 shape {
> E                 }
> E               }
> E             }
> ```
> 
> but if I try to run the shape inference test with the attribute (see code below), then no "Y" is inferred at all.
> 
> ```
> FunctionBodyHelper::NodeDef{{"Y"}, "Constant", {}, {MakeRefAttribute("value", AttributeProto::FLOAT, "alpha")}}
> ```
> 
> ```
> E       AssertionError: ({'X', 'Y'}, {'X'})
> E       assert {'X', 'Y'} == {'X'}
> E         Extra items in the left set:
> E         'Y'
> E         Use -v to get the full diff
> ```
> 
> On my side I am stuck. Looking at `Const` and `ToVector` implementation didn't gave me any new idea to test. Do you have any idea of what is happening? Is it related to this Tensor/Scalar problem? 🙃

As described [here](https://github.com/onnx/onnx/blob/master/docs/Operators.md#attributes-9), the `value` attribute should be an `tensor`, not a `float`.
jeremycochoy(2020-02-01 12:41:03):> > Thanks @TMVector and @linkerzhang for your feedback.
> > 
> > * Regarding the Tensor/Scalar issue raised by TMVector, I can say that the following code pass the shape inference test. But I don't know if it is enough to say if everything is fine if the second argument is a scalar and not a tensor (I don't know if this 1.f isn't converted to a tensor implicitly).
> > 
> > ```
> >           {// nodes: {outputs, op, inputs, attributes}                                                           
> >             FunctionBodyHelper::NodeDef{{"alpha"}, "Constant", {}, {{"value", 1.f}}},                                        
> >             {{"X_alpha"},
> >              "Div",
> >              {"X", "alpha"}
> >             },
> >             {{"Y"}, "Elu", {"X_alpha"}}})));
> > ```
> > 
> > 
> > 
> > * Regarding the second problem (using the actual attribute):
> > 
> > I made some attempt to create a constant node that recover the `alpha` attribute from the `Celu` operator using AtributeProto. Although the code compile, the shape inference test is a huge failure. In order to understand what is happening, I simplified the body of the function.
> > If I run the shape inference test with the following body, I get a nice "Y" of empty shape.
> > ```
> > FunctionBodyHelper::Const<float>("Y", 1.0f)
> > ```
> > 
> > 
> > ```
> > E             name: "Y"
> > E             type {
> > E               tensor_type {
> > E                 elem_type: 1
> > E                 shape {
> > E                 }
> > E               }
> > E             }
> > ```
> > 
> > 
> > but if I try to run the shape inference test with the attribute (see code below), then no "Y" is inferred at all.
> > ```
> > FunctionBodyHelper::NodeDef{{"Y"}, "Constant", {}, {MakeRefAttribute("value", AttributeProto::FLOAT, "alpha")}}
> > ```
> > 
> > 
> > ```
> > E       AssertionError: ({'X', 'Y'}, {'X'})
> > E       assert {'X', 'Y'} == {'X'}
> > E         Extra items in the left set:
> > E         'Y'
> > E         Use -v to get the full diff
> > ```
> > 
> > 
> > On my side I am stuck. Looking at `Const` and `ToVector` implementation didn't gave me any new idea to test. Do you have any idea of what is happening? Is it related to this Tensor/Scalar problem? 🙃
> 
> As described [here](https://github.com/onnx/onnx/blob/master/docs/Operators.md#attributes-9), the `value` attribute should be an `tensor`, not a `float`.

Unfortunately, after hours digging documentation and code, I can't figure a way to convert a scalar (from `MakeRefAttribute("alpha", Attribute\
Proto::FLOAT)`) to a tensor constant.
I left a comment pointing the the problematic line in the body of the function.

PS: If this is not possible, then maybe there is still a way to cheat with the `Gemm` instruction (it is the only instruction I found which take a scalar attribute and do his product with a tensor). But I would need some help to create the 1x1 tensor input matrices.
wschin(2020-02-01 18:09:21):> > > Thanks @TMVector and @linkerzhang for your feedback.
> > > 
> > > * Regarding the Tensor/Scalar issue raised by TMVector, I can say that the following code pass the shape inference test. But I don't know if it is enough to say if everything is fine if the second argument is a scalar and not a tensor (I don't know if this 1.f isn't converted to a tensor implicitly).
> > > 
> > > ```
> > >           {// nodes: {outputs, op, inputs, attributes}                                                           
> > >             FunctionBodyHelper::NodeDef{{"alpha"}, "Constant", {}, {{"value", 1.f}}},                                        
> > >             {{"X_alpha"},
> > >              "Div",
> > >              {"X", "alpha"}
> > >             },
> > >             {{"Y"}, "Elu", {"X_alpha"}}})));
> > > ```
> > > 
> > > 
> > > 
> > > * Regarding the second problem (using the actual attribute):
> > > 
> > > I made some attempt to create a constant node that recover the `alpha` attribute from the `Celu` operator using AtributeProto. Although the code compile, the shape inference test is a huge failure. In order to understand what is happening, I simplified the body of the function.
> > > If I run the shape inference test with the following body, I get a nice "Y" of empty shape.
> > > ```
> > > FunctionBodyHelper::Const<float>("Y", 1.0f)
> > > ```
> > > 
> > > 
> > > ```
> > > E             name: "Y"
> > > E             type {
> > > E               tensor_type {
> > > E                 elem_type: 1
> > > E                 shape {
> > > E                 }
> > > E               }
> > > E             }
> > > ```
> > > 
> > > 
> > > but if I try to run the shape inference test with the attribute (see code below), then no "Y" is inferred at all.
> > > ```
> > > FunctionBodyHelper::NodeDef{{"Y"}, "Constant", {}, {MakeRefAttribute("value", AttributeProto::FLOAT, "alpha")}}
> > > ```
> > > 
> > > 
> > > ```
> > > E       AssertionError: ({'X', 'Y'}, {'X'})
> > > E       assert {'X', 'Y'} == {'X'}
> > > E         Extra items in the left set:
> > > E         'Y'
> > > E         Use -v to get the full diff
> > > ```
> > > 
> > > 
> > > On my side I am stuck. Looking at `Const` and `ToVector` implementation didn't gave me any new idea to test. Do you have any idea of what is happening? Is it related to this Tensor/Scalar problem? 🙃
> > 
> > 
> > As described [here](https://github.com/onnx/onnx/blob/master/docs/Operators.md#attributes-9), the `value` attribute should be an `tensor`, not a `float`.
> 
> Unfortunately, after hours digging documentation and code, I can't figure a way to convert a scalar (from `MakeRefAttribute("alpha", Attribute\ Proto::FLOAT)`) to a tensor constant.
> I left a comment pointing the the problematic line in the body of the function.
> 
> PS: If this is not possible, then maybe there is still a way to cheat with the `Gemm` instruction (it is the only instruction I found which take a scalar attribute and do his product with a tensor). But I would need some help to create the 1x1 tensor input matrices.

I will try something on my side. In the meanwhile, what do you think if we make `alpha` an input?
linkerzhang(2020-02-02 05:27:35):I think "I can't figure a way to convert a scalar (from MakeRefAttribute("alpha", Attribute\ Proto::FLOAT)) to a tensor constant" needs to be fixed. Logically, the body graph is referring an attribute outside (which should be an AttributeProto) and the "Constant" OP will use the attribute and output a Tensor.
jeremycochoy(2020-02-02 08:31:37):@wschin Personally, I really don't mind moving alpha as an input. But it may be very confusing for both developper of user if CELU and ELU have completely different interface. If this approach get merged, it also means supporting it for a long time. 😅

@linkerzhang Would be awesome. I think anyone who implement a new function op that do not directly forward its arguments will end up having the exact same problem, and a clean way to move the scalars into the graph would solve this. Do you have any idea on how this could be archived?

linkerzhang(2020-02-03 02:31:58):@jeremycochoy The AttributeProto itself was designed to support this kind of reference already, though the utility function is missing, I guess.

PR #2583 should resolve it.

MakeRefAttribute("value", "alpha", AttributeProto::FLOAT) for the Constant node in the function body.

TMVector(2020-02-03 09:55:58):@linkerzhang I think that will work if the `alpha` attribute is a tensor, but ideally it would be a naked float. Maybe `Constant` should be changed to promote non-tensor values to scalar tensors?
jeremycochoy(2020-02-03 12:13:23):> @jeremycochoy The AttributeProto itself was designed to support this kind of reference already, though the utility function is missing, I guess.
> 
> PR #2583 should resolve it.
> 
> MakeRefAttribute("value", "alpha", AttributeProto::FLOAT) for the Constant node in the function body.

@linkerzhang 
Isn't essentially the same thing of https://github.com/onnx/onnx/pull/2575/files#diff-8073bde925403bcdfa7d23c68d914d97 present in the current PR? (although your ordering of arguments feel more natural to me)
wschin(2020-02-03 17:07:30):> @linkerzhang I think that will work if the `alpha` attribute is a tensor, but ideally it would be a naked float. Maybe `Constant` should be changed to promote non-tensor values to scalar tensors?

We might need to support `floats` in addition to `float`. The fundamental cause here is that `Attribute` and `Graph` use different numerical type systems. `Attribute` has `float`, `floats`, and `tensor`. `Graph` only has `tensor`. I think changing `Constant` will be a nice and small change to bridge these two systems -- because `Attribute` is always a constant in graphs.

@TMVector, @jeremycochoy, any comments? 
jeremycochoy(2020-02-03 20:04:50):> > @linkerzhang I think that will work if the `alpha` attribute is a tensor, but ideally it would be a naked float. Maybe `Constant` should be changed to promote non-tensor values to scalar tensors?
> 
> We might need to support `floats` in addition to `float`. The fundamental cause here is that `Attribute` and `Graph` use different numerical type systems. `Attribute` has `float`, `floats`, and `tensor`. `Graph` only has `tensor`. I think changing `Constant` will be a nice and small change to bridge these two systems -- because `Attribute` is always a constant in graphs.
> 
> @TMVector, @jeremycochoy, any comments?

To me it sounds the best solution, and it definitively makes sense to convert both float and floats to their corresponding tensors.
linkerzhang(2020-02-04 01:44:34):@jeremycochoy yep, it's the same as the one in current PR (I missed the part).

One more solution is removing AttributeProto and having TensorProto be used to store Attribute, to unify the two type systems (one tensor type system and one attribute type system).

AttributeProto and its type system were designed at the very beginning, to introduce a simpler way of having "scalar" attribute data. However, it introduces many troubles when specifying operator spec. For example, it's really hard to specify when attribute needs to share the same type as input or output (now most cases are using "Tensor" attribute type).

This PR reminds me again that the benefit of AttributeProto and its type system is not that much, while many troubles introduced.

I'd suggest to remove them and have only one type system in ONNX.

@gramalingam @wschin @jeremycochoy @TMVector What do you think please?
wschin(2020-02-04 17:28:06):> @jeremycochoy yep, it's the same as the one in current PR (I missed the part).
> 
> One more solution is removing AttributeProto and having TensorProto be used to store Attribute, to unify the two type systems (one tensor type system and one attribute type system).
> 
> AttributeProto and its type system were designed at the very beginning, to introduce a simpler way of having "scalar" attribute data. However, it introduces many troubles when specifying operator spec. For example, it's really hard to specify when attribute needs to share the same type as input or output (now most cases are using "Tensor" attribute type).
> 
> This PR reminds me again that the benefit of AttributeProto and its type system is not that much, while many troubles introduced.
> 
> I'd suggest to remove them and have only one type system in ONNX.
> 
> @gramalingam @wschin @jeremycochoy @TMVector What do you think please?

If we did this in the beginning, that would be perfect. However, I am not sure how the removal of `scalar` attributes could impact existing models.

If back-compatible is vital (which I think yes), it seems the only feasible solution is to modify `Constant`.
linkerzhang(2020-02-05 01:02:00):Changing "Constant" op is not a bad idea, however, it also means that multiple attribute types should be allowed when defining an op, which is not supported right now.

The other way is to add utility functions to make AttributeProto and TensorProto transferable from each other, given that they're indeed transferrable in this case. PR #2584 may help. Thoughts?

PR #2584  will ask ONNX partners (for example, onnx runtime) to update the implementation to handle the new statement (attribute type can be float, floats, int, ints, string, strings when expected attribute type is tensor), so ONNX IR version may also be bumped with the statement.

@wschin @gramalingam @TMVector @jeremycochoy
jeremycochoy(2020-02-05 15:01:21):I do not understand clearly the implication of a such choice, both regarding backward compatibility, impact on signatures of new and existing operators, and the work it represent of backend implementors.

So I will let you take this hard decision. 😅
gramalingam(2020-02-05 19:51:26):@linkerzhang : re. changing the Constant op: the solution adopted in some ops is to define multiple attributes (foo_int of type int and foo_float of type float) with the requirement that exactly one of these attributes should be defined. We could do this. This would be more compatibility-preserving. If we need this in multiple places, a utility like PR #2584 would be helpful ... but, as mentioned, it is more disruptive in terms of compatibility.
linkerzhang(2020-02-06 01:51:13):@gramalingam I checked the repo and there're only "Constant“ and "ConstantOfShape" use AttributeType::TENSOR, so that your suggestion of adding more attributes to them may be OK.

But again, please guys help to think it twice on PR #2584 whether it's better :).

To unblock this PR, @jeremycochoy do you mind to send a PR per @gramalingam suggested above (adding "value_int", "value_ints", "value_float", "value_floats", "value_string", "value_strings" attributes to Constant op). Thank you!
jeremycochoy(2020-02-06 07:48:08):I will have a look today :)

Edit: Working on it. May takes me some times as I am discovering the code base at the same time. 🙂 
linkerzhang(2020-02-10 23:47:50):CI needs to be fixed and passed. :)
jeremycochoy(2020-02-11 09:07:41):> CI needs to be fixed and passed. :)

It seams to run on all the CIs and the documentation is up to date. I also rebased on the master.
linkerzhang(2020-02-12 00:46:18):@spandantiwari @houseroad there's pytorch ci failure. You may take a look at it.
jeremycochoy(2020-02-12 09:24:16):> @spandantiwari @houseroad there's pytorch ci failure. You may take a look at it.

From what I understand the CI is having trouble downloading some models (HTTP 403) :( .
```

self = <urllib.request.HTTPDefaultErrorHandler object at 0x7f3137d84978>
req = <urllib.request.Request object at 0x7f3108b8e518>
fp = <http.client.HTTPResponse object at 0x7f3108b8e358>, code = 403
msg = 'Forbidden', hdrs = <http.client.HTTPMessage object at 0x7f3108b8e0b8>

    def http_error_default(self, req, fp, code, msg, hdrs):
>       raise HTTPError(req.full_url, code, msg, hdrs, fp)
E       urllib.error.HTTPError: HTTP Error 403: Forbidden

/usr/lib/python3.6/urllib/request.py:650: HTTPError
```
fdwr(2020-05-26 20:24:27):@jeremycochoy : This is an excellent description for a new operator (explaining why it's being added, where it came from, the actual equation used, and even an alternate Python implementation), and I'll point to it as a good example in the future. 👍
PallHaraldsson(2020-06-14 13:28:08):Thanks, I had no idea about the useful CELU. Is there a reason to use `min`? I mean `max` only (for ReLU) is natural, implying one test, and while having both implies two, and I wouldn't trust a compiler to know only one test and branch needed, and if you don't exclude it, then you always have to calculate the slow `exp` (I'm sure it could be faster, i.e. both implementations could be optimized more), and that it can be 583 times slower:

```
julia> x = 1.1; α = 1.0

julia> ONNX_CELU(x, α)=max(0,x/α)+min(0,(exp(x/α)-1))

julia> @btime ONNX_CELU($x, $α);  # only add $ for @btime (not @time) that requires: using BenchmarkTools
  13.999 ns (0 allocations: 0 bytes)

julia> ONNX_CELU(x, α)=if x >= zero(x) x/α else exp(x/α)-1 end

julia> @btime ONNX_CELU($x, $α);
  0.024 ns (0 allocations: 0 bytes)

julia> x = -1.1

julia> @btime ONNX_CELU($x, $α);
  11.627 ns (0 allocations: 0 bytes)
```
Yes, only 20% faster on the other side, and maybe with values all over the place (ca. 50-50% split?) it's not as useful an optimization as I would think?
jeremycochoy(2020-06-15 08:08:10):Hi @PallHaraldsson 

To be honest, I am efraid that if you want to have something really optimized you'd need the backend to provide a specific implementation for celu that replace this graph function (this is 100% possible and any backend can chose th implementation that fits it's specific targeted hardware).
PallHaraldsson(2020-06-16 20:56:51):Yes, and FYI, I found an even better activation function (in part since it's also continuously differentiable, why better than ReLU):

Mish: A Self Regularized Non-Monotonic Neural Activation Function
https://arxiv.org/pdf/1908.08681v1.pdf

>In Tensorflow[11], the function definition of Mish can be written as x * tf.math.tanh(tf.softplus(x)) while in Torch[12] it is x * torch.tanh(F.softplus(x)). For improved results over ReLU, it is advised to use a slightly lower learning rate for Mish.

It's also compared to ELU and some variant (while not CELU).

Also interesting, and supposed advantages contrary to those supposed above (such as bounded at low):

PLU: The Piecewise Linear Unit Activation Function
https://arxiv.org/abs/1809.09534

I implemented it like this for fewer assembly instructions (and only one branch):
```
julia> function PLU(x)
         stripped=abs(x)
         s=sign(x)
         if stripped <= 1.0
           return x
         else
           return 0.1*(x-s)+s
         end
       end

julia> @code_native PLU(1.0)  # to see assembly. I always get 0.024 ns by timing with @btime
```

And if you're interested, a very simple idea here (using two "opposite", but similar, I wander if you could do similar for two dissimilar, e.g. those above?):

https://arxiv.org/pdf/1709.04054.pdf

>We propose a simple extension to the ReLU-family of activation functions that allows them to shift the mean activation across a layer towards zero. Combined with proper weight initialization, this alleviates the need for normalization layers. We explore the training of deep vanilla recurrent neural networks (RNNs) with up to 144 layers, and show that bipolar activation functions help learning in this setting.  On the Penn Treebank and Text8 language modeling tasks we obtain competitive results, improving on the best reported results for non-gated networks.
jeremycochoy(2020-01-30 18:17:20):I can't figure out how to reference the attribute of the `Celu` instruction as an argument of the node `Div`.

I had a look at the helpers `FunctionBodyHelper::BuildNodes`, `FunctionBodyHelper::Const` and `MakeRefAttribute` without success.

Could you show me some documentation / example of this usage?
TMVector(2020-01-30 19:25:12):There is a comment about this on `FunctionBodyHelper::BuildNodes` [here](https://github.com/onnx/onnx/blob/121114107c38ec49bab4955c13cb82b7675bfc47/onnx/defs/function.h#L79-L80), and it's used in the `MeanVarianceNormalization` operator function [here](https://github.com/onnx/onnx/blob/121114107c38ec49bab4955c13cb82b7675bfc47/onnx/defs/nn/defs.cc#L2057-L2075). Shout if you have trouble :)
TMVector(2020-01-30 19:28:13):This should be in version 12, as the [latest released version](https://github.com/onnx/onnx/releases/tag/v1.6.0) is 11 :slightly_smiling_face: 
TMVector(2020-01-30 19:32:24):This doesn't include integer tensors.
```suggestion
           "Constrain input and output types to floating-point tensors.")
```
TMVector(2020-01-30 19:40:35):```suggestion
            [make_node('Celu', ['X'], ['Y'], alpha=2.0)],
```
jeremycochoy(2020-01-30 20:13:01):Thanks you for your answer. :)

Unfortunately I did read this line and the usage in MeanVarianceNormalization but I am still confused. I tried different syntax that compile, but since the shape inference test fail it is probably not right graph.

As I understand, I can create a graph equivalent to `Div(X, alpha=alpha)` using
```
           {{"X_alpha"},
             "Div",
             {"X"},
             {MakeRefAttribute("alpha", AttributeProto::FLOAT)}
            },
```
But it doesn't seams to be what I am lloking for, probably because `Div` have two input and 0 attribute.

How can write the equivalent of `Div(X, alpha)` (i.e. use this reference as the second argument of Div)? I would like to write
```
           {{"X_alpha"},
             "Div",
             {"X", MakeRefAttribute("alpha", AttributeProto::FLOAT)}
            }
```
but obviously this is not possible since `std::String != AttributeProtoWrapper` 😅

In the MeanVarianceNormalization it seams all the usage of axis simply forward the attribute to the underlying Ops, right ?
TMVector(2020-01-30 21:43:46):Ah okay, I see, you are quite right. I think you should be able to move from an attribute to a value by adding a `Constant` node. I'm not sure if it will be okay with providing a scalar for a tensor though :grimacing:.

Also, the current helper doesn't allow you to use different names for the attr (`value`) and the ref_attr (`alpha`), so you'd need to add that.
```
FunctionBodyHelper::BuildNodes(
           {// nodes: {outputs, op, inputs, attributes}
            {{"alpha"}, "Constant", {}, {MakeRefAttribute("value", AttributeProto::FLOAT, "alpha")}},
            {{"X_alpha"}, "Div", {"X", "alpha"}},
            {{"Y"}, "Elu", {"X_alpha"}}})
```
linkerzhang(2020-01-31 01:58:32):This function body is NOT a correct "sub-graph" representing the formula you described. Function body is actually a graph to represent the math formula you mentioned with other ops, in this case, it should be "Constant", "Div", "Elu".
wschin(2020-02-01 06:45:23):Very nice `alpha` symbol but the attribute uses plain text name `alpha`... I am also afraid of that this cool symbol can't be displayed in my VIM.
wschin(2020-02-01 07:11:20):From Pytorch, CELU equation is
```
CELU(x)=max(0,x)+min(0,α∗(exp(x/α)−1))
```
while ELU uses
```
ELU(x)=max(0,x)+min(0,α∗(exp(x)−1))
```
In addition, here the function body is doing
```
ONNX_CELU(x)=max(0,x/α)+min(0,(exp(x/α)−1))
```
which doesn't exactly match Pytorch CELU. Is this expected? Or I miss something?

Do we have a numpy reference implementation for generating tests? We should also check if that implementation matches Pytorch CELU.

[Update] I saw your numpy reference implementation. Nice! Can you please provide a short comparison to show it performs the same as Pytorch CELU? 
wschin(2020-02-01 07:14:21):Please split this line following his friends above.
wschin(2020-02-01 07:15:10):Are the comments here left intendedly?
jeremycochoy(2020-02-01 10:50:56):😱

You are completely right, the ELU implementation of Pytorch is different from ONNX Elu, and it is not possible to express CELU from ONNX's ELU. Thanks you for noticing it, I am working on a fix.
jeremycochoy(2020-02-01 12:32:08):Yes, I need some information on how to convert the `Scalar` Attribute into a Constant `Tensor`. Do you know how to accomplish this?
jeremycochoy(2020-02-01 13:06:51):Here is a code testing the differences between Pytorch CELU and the implementation (with corrected parenthesis) I provided.

```
import numpy as np
import torch

def onnx_celu(input_data, alpha=1.0):
    positive_input = np.maximum(0, input_data)
    negative_input = np.minimum(0, alpha * (np.exp(input_data / alpha) - 1))
    output_data = positive_input + negative_input
    return output_data

def torch_celu(input_data, alpha=1.0):
    return torch.nn.CELU(alpha=alpha)(torch.Tensor(input_data)).numpy()

input_data = np.random.randn(1, 2, 3).astype('float32')
alpha = 2

assert (onnx_celu(input_data, alpha) == torch_celu(input_data, alpha)).all()

```

jeremycochoy(2020-02-01 15:34:58):On Github the lines appear split but in the actual file (from current master), each op is on a sigle line. Or did I missed something?
linkerzhang(2020-02-04 02:05:12):change this function to call the overridden one added below please.
jeremycochoy(2020-02-04 09:16:17):Oh, I think you can just merge your PR and I can rebase this branch on top of it. I remember you introduced documentation, your ordering of arguments sounds more natural to me, and I have nothing against splitting PRs in smaller piece.

Edit: I rebased on top of your commit. 🙂 
linkerzhang(2020-02-04 10:07:14):Aha, I abandoned my PR this morning (realized it's duplicate with changes in your PR). Let me get it back and merge it in this way :)
jeremycochoy(2020-02-10 22:00:17):We really need to rename attribute when `ref_name != name`.
linkerzhang(2020-02-10 23:39:53):Good catch!
TMVector(2020-02-12 10:21:22):Wouldn't it be equivalent to pass Celu.alpha to Elu.alpha instead of setting Elu.alpha=1 and then multiplying by Celu.alpha?
jeremycochoy(2020-02-12 12:35:47):It is not. 😅 (Because the alpha is applied only on the second member of the + operator in the Elu equation).

Explanations from wschin: https://github.com/onnx/onnx/pull/2575#discussion_r373763604
TMVector(2020-02-12 13:48:47):Ah, quite right.

Btw should Celu be defined in `onnx/defs/math/defs.cc`? -- that's where Relu, Elu, etc. are.
jeremycochoy(2020-02-12 16:24:00):Right, it would make more sense to place it net to relu / elu / leaky relu. I will change it this evening.

Done :)
linkerzhang(2020-02-03 01:39:39):@spandantiwari @houseroad please check the pytorch test failure.
shinh(2020-02-04 00:33:28):I tried to fix the same issue (https://github.com/onnx/onnx/pull/2550/files) but in a different way. I thought this check should be done in `propagateShapeFromInputToOutput` rather than in the callers side because I found no valid usage to call this function without check the shape is available. To show my fix can solve issues in other ops, I've just created another testcase for EyeLike (https://github.com/shinh/onnx/commit/39850769779dcdcf3fe3d4724c97c8eedaae8443). That said, I admit my change was not consistent with existing code and we should probably remove redundant `hasNInputShapes` calls if we get my PR merged. What do you think?
BowenBao(2020-02-04 21:44:51):@shinh Agreed. What you proposed is a cleaner solution. However it may be backward breaking. Let's merge these two PRs, and then remove redundant `hasNInputShapes` `hasInputShape`.
spandantiwari(2020-02-04 23:11:52):cc: @houseroad @postrational 
neginraoof(2020-01-31 23:38:19): @gramalingam 
cc @houseroad @postrational
Could you please help review this PR?
Thanks
neginraoof(2020-02-18 16:35:35):cc @gramalingam @linkerzhang could you please help merge this PR?
Thanks 
linkerzhang(2020-02-02 05:20:44):for 8-bit integers, do you mean quantized integers or just regular 8 bit integer? if it's quantized, same scale/offset used for input and output?

Input and output can be always the same type?
neginraoof(2020-02-04 17:17:34):Thanks! This PR is not targeting quantized tensors. 
I added the type constraint for output (same type as input) in the spec.
spandantiwari(2020-02-04 21:46:46):What happens when the matrix is rank-deficient, or more generally, ill-conditioned with a large condition number? We should clarify what the expectation is in that case.
I would recommend that we keep it flexible for backends and treat this as an undefined behavior. Any backend can choose detect the condition and throw exception, or simply return garbage result (possibly with a warning).
neginraoof(2020-02-06 20:22:15):Thanks for pointing this out. I agree that we should expect an exception from backend. Should I mention that rank-deficient matrices are not supported?
neginraoof(2020-02-10 17:55:42):I updated the spec noting that the op only supports full-rank tensors.
gramalingam(2020-02-12 00:59:31):Can replace this loop by "propagateShapeFromInputToOutput(ctx, 0, 0);".
neginraoof(2020-02-12 17:35:00):Thanks. Replaced it.
vinitra-zz(2020-02-01 01:17:31):Comments on the proto were sparse, so feedback is much appreciated. @spandantiwari, @BowenBao 
linkerzhang(2020-02-02 05:13:39):Please also show the model example/usage of sequence data serialization in the PR.
gramalingam(2020-02-04 00:04:37):I see that the newly added message-types are not used anywhere else. So, I am not sure what the usage scenario is. Is the goal to extend "AttributeProto"? Or, is it just to support input/output values of this type (which doesn't really require a serializatio-format)?
vinitra-zz(2020-02-06 18:37:32):> I see that the newly added message-types are not used anywhere else. So, I am not sure what the usage scenario is. Is the goal to extend "AttributeProto"? Or, is it just to support input/output values of this type (which doesn't really require a serializatio-format)?

Discussed with @gramalingam in person. The purpose is to enable unit tests for the operators of Sequence and Map data types that require serialization.
gramalingam(2020-02-06 23:03:26):Thanks @vinitra for explaining the context. I think enabling unit tests with sequence/map data will require further extensions:

(a) The unit-test-case exporter seems to assume that inputs/outputs are tensors: see https://github.com/onnx/onnx/blob/8b3f7e2e7a0f2aba0e629e23d89f07c7fc0e6a5e/onnx/backend/test/case/node/__init__.py#L91 and https://github.com/onnx/onnx/blob/8b3f7e2e7a0f2aba0e629e23d89f07c7fc0e6a5e/onnx/backend/test/cmd_tools.py#L49 

(b) Extending helper.py to convert from a python representation of values to protobuf (e.g., as in make_tensor: see https://github.com/onnx/onnx/blob/8b3f7e2e7a0f2aba0e629e23d89f07c7fc0e6a5e/onnx/helper.py#L144 ) may be helpful too.

spandantiwari(2020-02-07 18:30:49):@vinitra - I agree with @gramalingam on both his points about adding necessary helper/support functionality so that a unit test involving Sequences/Dict can be written reasonably easily by an author. 

Thanks for this welcome change.
BowenBao(2020-02-11 18:30:45):@vinitra Thanks for working on this PR, this is a good start to complete the support for Sequence and Map data types. 
As others have suggested on showing an example usage of this serialization, I'd recommend adding one unit test for the Sequence/Map related operator, e.g. SequenceInsert, to demonstrate the end-to-end usage with support of serialization. 

sveta-levitan(2020-02-18 04:24:50):Should this get into ONNX 1.7? If yes, please finalize very soon. Thank you! 
vinitra-zz(2020-03-11 22:17:20):> Should this get into ONNX 1.7? If yes, please finalize very soon. Thank you!

ONNX 1.8 is fine! Sorry for the delay on updates. Should be forthcoming shortly.
lgtm-com[bot](2020-04-07 22:12:20):This pull request **introduces 1 alert** when merging ba893628527eb292aee95b0f767de5ca3f1d5a6c into 6bdac246617682f9696f0dac40362ef4f4de2cde - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-120a5e24e85872583b8339f8ad82412a3dbca5d8)

**new alerts:**

* 1 for Unused import
lgtm-com[bot](2020-04-07 22:48:23):This pull request **introduces 1 alert** when merging 8f44d4da946ccf86311175d53529aa071c52a763 into 6bdac246617682f9696f0dac40362ef4f4de2cde - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-a0b669d0e0bc1ecebd4fce7ddfe859ecd7059429)

**new alerts:**

* 1 for Unused import
lgtm-com[bot](2020-04-07 22:59:58):This pull request **introduces 1 alert** when merging d4e63cc6c39540b7166f31496940b03bacc2cbc0 into 6bdac246617682f9696f0dac40362ef4f4de2cde - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-c499ff1898096d86167ab1c110d917e0064fa044)

**new alerts:**

* 1 for Unused import
lgtm-com[bot](2020-04-10 17:58:37):This pull request **introduces 1 alert** and **fixes 2** when merging 0c05af771b68fc4674f402fe45f23cf441732a17 into e3b9383a58402d8bd8bd5cbe8cddb3429ce2520f - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-337f9a03b2ec918f9b0b3b3f76f5f2529877208e)

**new alerts:**

* 1 for Syntax error

**fixed alerts:**

* 2 for Unused local variable
lgtm-com[bot](2020-04-13 20:10:33):This pull request **introduces 1 alert** and **fixes 2** when merging acd25a939d19ebdb1e67cbe78d8f2e54ad34323e into f89f38793fd564eba7eaa0d3519619e841e27b4c - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-cc92db6c5d735a4c3e98783c46b8ba15655906f7)

**new alerts:**

* 1 for Syntax error

**fixed alerts:**

* 2 for Unused local variable
gramalingam(2020-05-08 23:03:29):@linkerzhang : since this PR involves updates to the proto definition, it would be great to have your feedback on the proposed changes. Thanks.
CLAassistant(2020-05-12 01:15:25):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=2581) <br/>All committers have signed the CLA.
BowenBao(2020-05-19 17:04:25):I think `docs/TestCoverage.md` is also updated and needs to be included in this PR.
vinitra-zz(2020-05-19 17:23:34):> I think `docs/TestCoverage.md` is also updated and needs to be included in this PR.

Thanks for the catch! Added now.
BowenBao(2020-05-21 17:12:46):Please also include the following updated files:

```
	onnx/onnx-ml.proto
	onnx/onnx-ml.proto3
	onnx/onnx.proto
	onnx/onnx.proto3
```

There seems to be another test that is failing in the CI, but I'm not sure which test it is by just looking at the log.
```
Traceback (most recent call last):
  File "/home/vsts/.conda/envs/py3.7/bin/backend-test-tools", line 11, in <module>
    load_entry_point('onnx==1.7.0', 'console_scripts', 'backend-test-tools')()
  File "/home/vsts/.conda/envs/py3.7/lib/python3.7/site-packages/onnx-1.7.0-py3.7-linux-x86_64.egg/onnx/backend/test/cmd_tools.py", line 99, in main
    args.func(args)
  File "/home/vsts/.conda/envs/py3.7/lib/python3.7/site-packages/onnx-1.7.0-py3.7-linux-x86_64.egg/onnx/backend/test/cmd_tools.py", line 79, in generate_data
    "and cannot be processed accordingly.", output)
TypeError: ('Your output is not a sequence (list), dictionary (map), or tensor (array) ', 'and cannot be processed accordingly.', 3.0)
```
BowenBao(2020-05-26 17:35:59):Please rebase with master again
lgtm-com[bot](2020-05-28 21:54:32):This pull request **introduces 1 alert** when merging c1e01b2335678575a9841907a0cfdf2bb884ec59 into cfab05a5f1cf33065a7a2ccc3df8020707e8998f - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-3b2c2142913c7d3b756729e45eec966572f44fb2)

**new alerts:**

* 1 for Unused import
lgtm-com[bot](2020-05-29 18:39:58):This pull request **fixes 1 alert** when merging 8a03c9ffc0e0ed5416d289be6163ef98c513863b into b6efcec714c4b9fb4e83945bf2f01fa746182126 - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-ead6f7088685e70f85106390a15627b7e8de994c)

**fixed alerts:**

* 1 for Unused import
BowenBao(2020-06-02 17:20:14):@vinitra please merge with master to resolve conflicts. Are we close to resolving all the CI failures?
BowenBao(2020-06-02 17:24:24):You might need to include the following files in the PR as well. They have uncaptured changes that are being reported by the CI. This might be the root cause for the `import not found` errors.

> Changes not staged for commit:
>   (use "git add <file>..." to update what will be committed)
>   (use "git restore <file>..." to discard changes in working directory)
> 	modified:   onnx/onnx-data.proto
> 	modified:   onnx/onnx-data.proto3
> 	modified:   onnx/onnx-ml.proto
> 	modified:   onnx/onnx-ml.proto3
> 	modified:   onnx/onnx.proto
> 	modified:   onnx/onnx.proto3
> 
> Untracked files:
>   (use "git add <file>..." to include in what will be committed)
> 	onnx/onnx-data-ml.proto
> 	onnx/onnx-data-ml.proto3


Edit: pasting more logs here, seems more than 2 files need to be included.
lgtm-com[bot](2020-06-03 22:57:36):This pull request **fixes 1 alert** when merging 0bfe513b33b4426f8fee246c073e83118f065b26 into 925b3657924c0c16cd20b54595f41e76159b03ab - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-0157f7c84b9ead58ae750940e0e4c692ec5b4afb)

**fixed alerts:**

* 1 for Unused import
lgtm-com[bot](2020-06-05 15:27:32):This pull request **fixes 1 alert** when merging 416ee1afb0c4b920f26b48dcffcdc5892e076f8b into 925b3657924c0c16cd20b54595f41e76159b03ab - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-c86dbc68bcce213b1b18fb7735a4f489f2736dc4)

**fixed alerts:**

* 1 for Unused import
lgtm-com[bot](2020-06-05 17:10:26):This pull request **fixes 1 alert** when merging 145980b19e946a23a65456919ca417cbf9cc507b into 925b3657924c0c16cd20b54595f41e76159b03ab - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-f387bab40a773108d3f8962369420a89596020be)

**fixed alerts:**

* 1 for Unused import
BowenBao(2020-06-05 17:55:11):BTW the import issue is still there. I don't think we should create a separate onnx-data.proto. The `TensorProto` in original onnx.in.proto is also defining data/serialization. These should not be defined in separate files.

To solve the import issue, you need to update `CMakeLists.txt` and add

```
relative_protobuf_generate_cpp(gen_onnx_data_proto
                               __tmp_srcs
                               __tmp_hdrs
                               ${ONNX_ROOT}
                               gen_onnx_proto
                               onnx/onnx-data.in.proto)
list(APPEND ONNX_PROTO_SRCS ${__tmp_srcs})
list(APPEND ONNX_PROTO_HDRS ${__tmp_hdrs})
```

then in `onnx/__init__.py` add
```
from .onnx_data_pb import * # noqa
```

at last in `onnx/helper.py` and `onnx/numpy_helper.py`, update import to be
```
from onnx import SequenceProto, MapProto, SequenceMapElement, KeyValuePair
```

Also be sure to run `sh tools/update_doc.sh` locally to run tests and generate updated docs. The above steps fix the import issue, but still encounter some type error that exists previously.

```
===> regenerate test data from node test
Traceback (most recent call last):
  File "onnx/backend/test/cmd_tools.py", line 103, in <module>
    main()
  File "onnx/backend/test/cmd_tools.py", line 99, in main
    args.func(args)
  File "onnx/backend/test/cmd_tools.py", line 79, in generate_data
    "and cannot be processed accordingly.", output)
TypeError: ('Your output is not a sequence (list), dictionary (map), or tensor (array) ', 'and cannot be processed accordingly.', 3.0)
```
lgtm-com[bot](2020-06-05 20:13:44):This pull request **introduces 1 alert** and **fixes 1** when merging 48790ca736b07322f539eadab52ad03cffcf5c1e into 925b3657924c0c16cd20b54595f41e76159b03ab - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-b6ff34fa5b98d3e250dca0d1002cc898c8ad01b8)

**new alerts:**

* 1 for &#39;import \*&#39; may pollute namespace

**fixed alerts:**

* 1 for Unused import
lgtm-com[bot](2020-06-06 22:38:06):This pull request **introduces 1 alert** and **fixes 1** when merging 7f805bdfa6cee55de6b804c0a548bb8ba27eb071 into 925b3657924c0c16cd20b54595f41e76159b03ab - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-88887fb1094b94dcb6e39eb0e16256a4c46dced5)

**new alerts:**

* 1 for &#39;import \*&#39; may pollute namespace

**fixed alerts:**

* 1 for Unused import
lgtm-com[bot](2020-06-06 23:22:24):This pull request **introduces 1 alert** and **fixes 1** when merging 19e7f7bd3d6bf4a5c8ff30f87d3e33cd4597e59a into 925b3657924c0c16cd20b54595f41e76159b03ab - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-76fb8e05f61462c68678bbc8fabd43dddd6104eb)

**new alerts:**

* 1 for &#39;import \*&#39; may pollute namespace

**fixed alerts:**

* 1 for Unused import
lgtm-com[bot](2020-06-07 03:22:23):This pull request **introduces 1 alert** and **fixes 1** when merging 456b8b96adbf0791e5a84c923d8e0a2da2e3b516 into 925b3657924c0c16cd20b54595f41e76159b03ab - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-f33a92644bc07c7b3e26f9a1352035caaed3029d)

**new alerts:**

* 1 for &#39;import \*&#39; may pollute namespace

**fixed alerts:**

* 1 for Unused import
lgtm-com[bot](2020-06-08 20:19:42):This pull request **introduces 1 alert** and **fixes 1** when merging d29016a9906d1a067f3c4de2538d9b20c7b08a81 into 925b3657924c0c16cd20b54595f41e76159b03ab - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-6421383a69268a6f57b33cca1ced1329dcf6e89a)

**new alerts:**

* 1 for &#39;import \*&#39; may pollute namespace

**fixed alerts:**

* 1 for Unused import
lgtm-com[bot](2020-06-08 20:54:01):This pull request **introduces 1 alert** and **fixes 1** when merging de4e9571bbf511eb58a7dd76674b26cc3feabbc7 into 925b3657924c0c16cd20b54595f41e76159b03ab - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-638941b3a398b4e326615b97d41359a28d095299)

**new alerts:**

* 1 for &#39;import \*&#39; may pollute namespace

**fixed alerts:**

* 1 for Unused import
lgtm-com[bot](2020-06-08 21:39:16):This pull request **introduces 1 alert** and **fixes 1** when merging 3e0b4d473fe9a496e64aea1e08a663359498f01c into 925b3657924c0c16cd20b54595f41e76159b03ab - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-9b6363124b70ddc4929256a6d5beea4761e184a2)

**new alerts:**

* 1 for &#39;import \*&#39; may pollute namespace

**fixed alerts:**

* 1 for Unused import
lgtm-com[bot](2020-06-08 22:45:52):This pull request **introduces 1 alert** and **fixes 1** when merging 62ee1d1c700058e42759becdb148abb515d6977d into 925b3657924c0c16cd20b54595f41e76159b03ab - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-aa8a6728f285925fdbce528926de7ba3ea3e137d)

**new alerts:**

* 1 for &#39;import \*&#39; may pollute namespace

**fixed alerts:**

* 1 for Unused import
lgtm-com[bot](2020-06-09 17:14:11):This pull request **introduces 1 alert** and **fixes 1** when merging 65fba620015ef87af1dea204f5d13a13756b19cd into 925b3657924c0c16cd20b54595f41e76159b03ab - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-e3c4c28c9d8815b4ed240c9603e761e6305e219b)

**new alerts:**

* 1 for &#39;import \*&#39; may pollute namespace

**fixed alerts:**

* 1 for Unused import
lgtm-com[bot](2020-06-09 22:24:39):This pull request **introduces 1 alert** and **fixes 1** when merging 647f4b2acfda6e1d69aac2aca69bd236b2aef5cc into 925b3657924c0c16cd20b54595f41e76159b03ab - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-5a935a5d21c90478294760ec4679ccfe66e60fd7)

**new alerts:**

* 1 for &#39;import \*&#39; may pollute namespace

**fixed alerts:**

* 1 for Unused import
lgtm-com[bot](2020-06-09 22:52:48):This pull request **introduces 1 alert** and **fixes 1** when merging 49718dd24801cec3cdd7fbd199e133c15299c709 into 925b3657924c0c16cd20b54595f41e76159b03ab - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-92155a9d4e12c9d12f61f076e3d91b4aeb193896)

**new alerts:**

* 1 for &#39;import \*&#39; may pollute namespace

**fixed alerts:**

* 1 for Unused import
lgtm-com[bot](2020-06-09 23:06:54):This pull request **introduces 1 alert** and **fixes 1** when merging 2e7ffa3325f51b373f2dd044a241198a30aaef59 into 925b3657924c0c16cd20b54595f41e76159b03ab - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-63b9bb1ff0b3fdd7a5e333495790a59de18d7b91)

**new alerts:**

* 1 for &#39;import \*&#39; may pollute namespace

**fixed alerts:**

* 1 for Unused import
lgtm-com[bot](2020-06-09 23:45:13):This pull request **introduces 1 alert** and **fixes 1** when merging d9611d3a6f4ea6109a040d24281a5fc7edd50d4d into 925b3657924c0c16cd20b54595f41e76159b03ab - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-16117459c03ba9b3250c5242f9510bd7128c05ef)

**new alerts:**

* 1 for &#39;import \*&#39; may pollute namespace

**fixed alerts:**

* 1 for Unused import
lgtm-com[bot](2020-06-10 19:29:11):This pull request **introduces 1 alert** and **fixes 1** when merging 2b3e94ed4782fdcab1b133320534f29af76d671c into 925b3657924c0c16cd20b54595f41e76159b03ab - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-6b0e26612c10bc4907b4be2d74827d8708ba40cc)

**new alerts:**

* 1 for &#39;import \*&#39; may pollute namespace

**fixed alerts:**

* 1 for Unused import
lgtm-com[bot](2020-06-12 19:25:04):This pull request **introduces 1 alert** and **fixes 1** when merging 5bc9bcdccdb359e580a7d24ce3d77b43802fb9c7 into cdaa8a8e87c88842062f899e2cf747e642c6b3e4 - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-2234694da693d7987fabfad98e35691935dedba8)

**new alerts:**

* 1 for &#39;import \*&#39; may pollute namespace

**fixed alerts:**

* 1 for Unused import
lgtm-com[bot](2020-06-15 15:41:13):This pull request **introduces 1 alert** and **fixes 1** when merging 8b5893ce2c801e147fdcc0d061fb91f0a50ff41e into 09a4e65bb098164491b021ffe563a559fbc1a808 - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-1169d6e6c9eda0c55d64882d234b3633d9586953)

**new alerts:**

* 1 for &#39;import \*&#39; may pollute namespace

**fixed alerts:**

* 1 for Unused import
lgtm-com[bot](2020-06-17 01:07:25):This pull request **introduces 1 alert** and **fixes 1** when merging ab94385e3c9ff0794e7f3cde0112b7e95ea7d94e into 09a4e65bb098164491b021ffe563a559fbc1a808 - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-f2dad7b9aea53ceb22969379b64baa6f5f2c16a2)

**new alerts:**

* 1 for &#39;import \*&#39; may pollute namespace

**fixed alerts:**

* 1 for Unused import
lgtm-com[bot](2020-06-18 21:07:54):This pull request **introduces 1 alert** and **fixes 1** when merging e68e22743808ebf54b511fd740454c5228894973 into cc2230603422bae893d5bc900d2d773ab34400a4 - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-052ed5e22cce0ee03a98a5ea653846b893399e9e)

**new alerts:**

* 1 for &#39;import \*&#39; may pollute namespace

**fixed alerts:**

* 1 for Unused import
lgtm-com[bot](2020-06-23 19:45:24):This pull request **introduces 1 alert** and **fixes 1** when merging 646a2739558d8a3744eb06682427be87587f5fbd into 7979c71a2ca990cad669313fd5fa52533b7dada1 - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-fc4089a6830f1667357af3538a47c2c28f48a8ea)

**new alerts:**

* 1 for &#39;import \*&#39; may pollute namespace

**fixed alerts:**

* 1 for Unused import
lgtm-com[bot](2020-06-23 22:33:53):This pull request **introduces 1 alert** and **fixes 1** when merging 7fc68cac7ae4bc59a39afb35f117aeca59f88437 into 7ee9f618ef6808b100b1d7ddd8837f1f86da8267 - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-c1204ce71979a2039dbc09e9cdd52ea433f201f8)

**new alerts:**

* 1 for &#39;import \*&#39; may pollute namespace

**fixed alerts:**

* 1 for Unused import
lgtm-com[bot](2020-06-23 23:06:38):This pull request **introduces 1 alert** and **fixes 1** when merging f2b488100f43c7472d68612c5341624eb6a1c6fe into 7ee9f618ef6808b100b1d7ddd8837f1f86da8267 - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-392ea1a4d193ad11c4ab1eddda13700431e30fbf)

**new alerts:**

* 1 for &#39;import \*&#39; may pollute namespace

**fixed alerts:**

* 1 for Unused import
lgtm-com[bot](2020-06-24 02:30:45):This pull request **introduces 1 alert** and **fixes 1** when merging 6878c26650e95308ad97f7d2990766dd8063d4b2 into 7ee9f618ef6808b100b1d7ddd8837f1f86da8267 - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-95081ffd782079256d983c94d88ecc14af7fed7d)

**new alerts:**

* 1 for &#39;import \*&#39; may pollute namespace

**fixed alerts:**

* 1 for Unused import
lgtm-com[bot](2020-06-29 21:31:14):This pull request **introduces 1 alert** and **fixes 1** when merging e922963fc22fa259465909929e33b32eb785c25b into 62d4aebfa7cb719a33267d75364a15eb7d7dcafd - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-1c1ae29b8d9e7565b5faa80b6a5ce13a72f01e55)

**new alerts:**

* 1 for &#39;import \*&#39; may pollute namespace

**fixed alerts:**

* 1 for Unused import
lgtm-com[bot](2020-06-30 00:15:26):This pull request **introduces 1 alert** and **fixes 1** when merging 3868996e673a4265b953eea221c9739d86765573 into 62d4aebfa7cb719a33267d75364a15eb7d7dcafd - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-f8328a71e792226442ad547dae3741c90153fc64)

**new alerts:**

* 1 for &#39;import \*&#39; may pollute namespace

**fixed alerts:**

* 1 for Unused import
lgtm-com[bot](2020-07-01 18:33:23):This pull request **introduces 1 alert** and **fixes 1** when merging b5a3660a54eea4de3af9295fd291830e7bbf41b9 into de8f4fc34444eea32a002a8c671863332a774a56 - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-649b440340a4d547e995ef9e2c1d597e477eae65)

**new alerts:**

* 1 for &#39;import \*&#39; may pollute namespace

**fixed alerts:**

* 1 for Unused import
lgtm-com[bot](2020-07-01 18:57:08):This pull request **introduces 1 alert** and **fixes 1** when merging 0f1f2133594ce7249f61b0d968fc645202483209 into de8f4fc34444eea32a002a8c671863332a774a56 - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-e915d06ba0a40a22df11be33b561679700711eda)

**new alerts:**

* 1 for &#39;import \*&#39; may pollute namespace

**fixed alerts:**

* 1 for Unused import
lgtm-com[bot](2020-07-01 19:22:57):This pull request **introduces 1 alert** and **fixes 1** when merging 94930d70770ec3fe98fcdfd6c197e474e0099d8b into fdfb640f26a8198eac21431cd10739c920e63aaa - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-bf7a4c71fcc07735ed4442dc1b96942a801b9588)

**new alerts:**

* 1 for &#39;import \*&#39; may pollute namespace

**fixed alerts:**

* 1 for Unused import
lgtm-com[bot](2020-07-01 19:36:46):This pull request **introduces 1 alert** and **fixes 1** when merging 759932a8a8f951b047e26b5d10db07a0a132b2c4 into fdfb640f26a8198eac21431cd10739c920e63aaa - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-c51a5bfd9ecc40b4f4b790ca998a0a2a31054a0d)

**new alerts:**

* 1 for &#39;import \*&#39; may pollute namespace

**fixed alerts:**

* 1 for Unused import
lgtm-com[bot](2020-07-01 21:10:57):This pull request **introduces 1 alert** and **fixes 1** when merging 9c1c831ccbab07949130d16e9983c2bd95b3c3aa into fdfb640f26a8198eac21431cd10739c920e63aaa - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-4f007988334423cac81f9f96b37122e5fa8a6060)

**new alerts:**

* 1 for &#39;import \*&#39; may pollute namespace

**fixed alerts:**

* 1 for Unused import
lgtm-com[bot](2020-07-01 21:29:59):This pull request **introduces 1 alert** and **fixes 1** when merging dc195dab16ef512e2d20b254a020e260a829a29f into fdfb640f26a8198eac21431cd10739c920e63aaa - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-3001499b9730e8b22277e78e44386469418fe856)

**new alerts:**

* 1 for &#39;import \*&#39; may pollute namespace

**fixed alerts:**

* 1 for Unused import
lgtm-com[bot](2020-07-01 21:55:25):This pull request **introduces 1 alert** and **fixes 1** when merging 128227db787ffae34842d88a3c0f9e8712be26f0 into fdfb640f26a8198eac21431cd10739c920e63aaa - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-c9f0fca856336b99c64fe85f0f4e5dff9c352119)

**new alerts:**

* 1 for &#39;import \*&#39; may pollute namespace

**fixed alerts:**

* 1 for Unused import
lgtm-com[bot](2020-07-01 22:24:28):This pull request **introduces 1 alert** and **fixes 1** when merging d774dafe2438d2014556cfe96229b0f2af47b9d8 into fdfb640f26a8198eac21431cd10739c920e63aaa - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-f8bd33b632ce9419c40e6fdca6b57bd9d9b36ddb)

**new alerts:**

* 1 for &#39;import \*&#39; may pollute namespace

**fixed alerts:**

* 1 for Unused import
lgtm-com[bot](2020-07-01 22:56:39):This pull request **introduces 1 alert** and **fixes 1** when merging 4077b20da38adb42b69ecb3391a1043c9e8c37df into fdfb640f26a8198eac21431cd10739c920e63aaa - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-2e5f898bbfbcd5243bfecef04b854125b9a3fb4b)

**new alerts:**

* 1 for &#39;import \*&#39; may pollute namespace

**fixed alerts:**

* 1 for Unused import
lgtm-com[bot](2020-07-02 18:05:00):This pull request **introduces 1 alert** and **fixes 1** when merging f16dfb6af9783cd649fc76fe2152e945e5f84e9e into fdfb640f26a8198eac21431cd10739c920e63aaa - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-1a54aaaac59dcaf6fa2cba12cdaf807a0bc0c63b)

**new alerts:**

* 1 for &#39;import \*&#39; may pollute namespace

**fixed alerts:**

* 1 for Unused import
lgtm-com[bot](2020-07-02 18:53:20):This pull request **introduces 1 alert** and **fixes 1** when merging 30040f7380e1819309a0d584ab70632afb7538e8 into fdfb640f26a8198eac21431cd10739c920e63aaa - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-6a0ad97122c59dffe7b6fd0e896e33f96b4125fc)

**new alerts:**

* 1 for &#39;import \*&#39; may pollute namespace

**fixed alerts:**

* 1 for Unused import
gramalingam(2020-07-02 19:43:24):@linkerzhang : do you have any further comments on this PR? Thanks!
lgtm-com[bot](2020-07-02 20:18:24):This pull request **introduces 1 alert** and **fixes 1** when merging 929bd2d6e5334e8ab65655af44d54300d5e1c441 into fdfb640f26a8198eac21431cd10739c920e63aaa - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-4af1129ffe0721e5fdabd54b98e704df2d2a851c)

**new alerts:**

* 1 for &#39;import \*&#39; may pollute namespace

**fixed alerts:**

* 1 for Unused import
lgtm-com[bot](2020-07-02 20:41:03):This pull request **introduces 1 alert** and **fixes 1** when merging 78128a379bc4fc69a542e800a0fa3d33899f1a71 into fdfb640f26a8198eac21431cd10739c920e63aaa - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-3a7655e159b438aec2ab395a9cee5265e22a2cb4)

**new alerts:**

* 1 for &#39;import \*&#39; may pollute namespace

**fixed alerts:**

* 1 for Unused import
lgtm-com[bot](2020-07-06 17:59:23):This pull request **introduces 1 alert** and **fixes 1** when merging 681e0f15ee893fee4b46f3c0ce51b3adef19b3cf into 456ba4cace5b21cbd64b579ebd48e83aa4081b76 - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-0b36b33c2ebb339700ef5788476a731cb6d4db28)

**new alerts:**

* 1 for &#39;import \*&#39; may pollute namespace

**fixed alerts:**

* 1 for Unused import
lgtm-com[bot](2020-07-10 20:40:42):This pull request **introduces 1 alert** and **fixes 1** when merging a33b5f1b47861917de631243a7c45284a6ea7d87 into 9b7c2b4f0b4a16a0cf31145eae9425abe7cbe2a9 - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-6891f53cff6d7ca1097a72156806dc10a109eca2)

**new alerts:**

* 1 for &#39;import \*&#39; may pollute namespace

**fixed alerts:**

* 1 for Unused import
lgtm-com[bot](2020-07-16 01:17:06):This pull request **introduces 1 alert** and **fixes 1** when merging 6ebe9a54f6bfccacec030b9e18d6fcb92e3eef7d into 25fa5c4c38227dc99079bebc33250202f098dd9d - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-2238d2f8d469962e6cc263c7d1038969eec68c44)

**new alerts:**

* 1 for &#39;import \*&#39; may pollute namespace

**fixed alerts:**

* 1 for Unused import
lgtm-com[bot](2020-07-25 02:55:19):This pull request **introduces 2 alerts** and **fixes 1** when merging 47371946c012b7f2c50863c59f3fc6b66f6fb342 into f8c663c6e5fe710f90381904f9f90f0c81618ce4 - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-d0e215b7d07f6321b6393f37af1d05a931f7f824)

**new alerts:**

* 1 for &#39;import \*&#39; may pollute namespace
* 1 for Suspicious unused loop iteration variable

**fixed alerts:**

* 1 for Unused import
lgtm-com[bot](2020-07-27 15:24:03):This pull request **introduces 1 alert** and **fixes 1** when merging 6b99f1c519392682a6871ee49a1cedd1ce5c7d7f into f8c663c6e5fe710f90381904f9f90f0c81618ce4 - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-d469783afdb5d939a560a424c0fddfa147a3380e)

**new alerts:**

* 1 for &#39;import \*&#39; may pollute namespace

**fixed alerts:**

* 1 for Unused import
lgtm-com[bot](2020-07-27 18:30:23):This pull request **introduces 1 alert** and **fixes 1** when merging 6692934743c2daafb6cc11e672d385eba415237f into f8c663c6e5fe710f90381904f9f90f0c81618ce4 - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-e8b0fbc7e5e4d1d1d789b72ee4204901efca779d)

**new alerts:**

* 1 for &#39;import \*&#39; may pollute namespace

**fixed alerts:**

* 1 for Unused import
lgtm-com[bot](2020-07-27 18:59:28):This pull request **introduces 1 alert** and **fixes 1** when merging 5f4dfa670e8ac90f3e686afca6dba4d73898a1a7 into f8c663c6e5fe710f90381904f9f90f0c81618ce4 - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-f1debf8db2a0d4be7e3deefbad0a7057540dc89b)

**new alerts:**

* 1 for &#39;import \*&#39; may pollute namespace

**fixed alerts:**

* 1 for Unused import
lgtm-com[bot](2020-07-27 23:54:24):This pull request **introduces 1 alert** and **fixes 1** when merging dd230ac3c502c64c72d6bc09867ad180e9b98db8 into d38a47cea4f749ebc1cf6b32b9119ee508750412 - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-6ffc975ec9da78b043e78f92141d5f0fd5bedf9b)

**new alerts:**

* 1 for &#39;import \*&#39; may pollute namespace

**fixed alerts:**

* 1 for Unused import
lgtm-com[bot](2020-07-28 18:16:51):This pull request **introduces 1 alert** and **fixes 1** when merging eb71cb52188c80577d133df3879a935eb8a6890b into d38a47cea4f749ebc1cf6b32b9119ee508750412 - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-0deda42c2e6223b784697df9c980b8b43de7b4f4)

**new alerts:**

* 1 for &#39;import \*&#39; may pollute namespace

**fixed alerts:**

* 1 for Unused import
lgtm-com[bot](2020-07-28 19:14:38):This pull request **introduces 1 alert** and **fixes 1** when merging 2f06f58be2fcff795a06d1c39d071fdd7dfb78b8 into d38a47cea4f749ebc1cf6b32b9119ee508750412 - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-467218cd2c9654da62f05b3a2a5d83f9c4416001)

**new alerts:**

* 1 for &#39;import \*&#39; may pollute namespace

**fixed alerts:**

* 1 for Unused import
lgtm-com[bot](2020-07-30 17:58:49):This pull request **introduces 1 alert** and **fixes 1** when merging 31ea28228945618f32bd93df18f96ccea7bd2f05 into 4efa6056c0f37233b5400da0ef585442a47ec532 - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-b01674cc1ef187a13e26b19956ea4d2a8774af31)

**new alerts:**

* 1 for &#39;import \*&#39; may pollute namespace

**fixed alerts:**

* 1 for Unused import
vinitra-zz(2020-07-30 18:14:23):> Please also show the model example/usage of sequence data serialization in the PR.

> btw, given this change is indeed affecting the IR, please move the operator and its test data into a separate PR.

Since we are not updating the IR version and the purpose was to enable using these data types in testing infrastructure, I think it makes sense to have an example of a Sequence operator test in this PR. Referencing the earlier comments above to indicate why it was included.

(However, if you feel strongly, I can make the change). @linkerzhang
lgtm-com[bot](2020-07-30 20:37:32):This pull request **introduces 1 alert** and **fixes 1** when merging 9d740dad575075b3a02cda9250c3ee661a9a74ce into 7baca65e03fe9ec819b397fc7a796f4f93999ae8 - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-69fabeb0abc0d60aef672ec68fcb2db0c8dff5d6)

**new alerts:**

* 1 for &#39;import \*&#39; may pollute namespace

**fixed alerts:**

* 1 for Unused import
lgtm-com[bot](2020-07-30 20:52:58):This pull request **introduces 1 alert** and **fixes 1** when merging a24339a074f21669bf8d3e8c4501e3027b117ee1 into 7baca65e03fe9ec819b397fc7a796f4f93999ae8 - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-6b78ebadbc2c76111c80f214c944bf37235c07e3)

**new alerts:**

* 1 for &#39;import \*&#39; may pollute namespace

**fixed alerts:**

* 1 for Unused import
lgtm-com[bot](2020-07-30 21:13:27):This pull request **introduces 1 alert** and **fixes 1** when merging 823f23b1a1ec5cc6f689921b02b1d3a96e8a693e into 7baca65e03fe9ec819b397fc7a796f4f93999ae8 - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-b426c7a5cc091a1cafb7a84c4d1cc02c3ddec8be)

**new alerts:**

* 1 for &#39;import \*&#39; may pollute namespace

**fixed alerts:**

* 1 for Unused import
lgtm-com[bot](2020-07-31 00:29:34):This pull request **introduces 1 alert** and **fixes 1** when merging 09cb773bd612c594914de63a23f1a305a1b15ceb into 7baca65e03fe9ec819b397fc7a796f4f93999ae8 - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-3163bd1734a48287f96006c2ad69c85bf9e5254c)

**new alerts:**

* 1 for &#39;import \*&#39; may pollute namespace

**fixed alerts:**

* 1 for Unused import
lgtm-com[bot](2020-07-31 01:22:57):This pull request **introduces 1 alert** and **fixes 1** when merging 32676ad3c466c49f195deee7e544da33769016c9 into 7baca65e03fe9ec819b397fc7a796f4f93999ae8 - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-4c65b565301ff44e65ea228e30c6e7ef9f0814eb)

**new alerts:**

* 1 for &#39;import \*&#39; may pollute namespace

**fixed alerts:**

* 1 for Unused import
lgtm-com[bot](2020-07-31 02:08:46):This pull request **introduces 1 alert** and **fixes 1** when merging 6c3a161867550f2c16d55f5404ff68d1dfce932e into 7baca65e03fe9ec819b397fc7a796f4f93999ae8 - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-e75037cbb234f879c6a96b12f3c675ccad09d46a)

**new alerts:**

* 1 for &#39;import \*&#39; may pollute namespace

**fixed alerts:**

* 1 for Unused import
lgtm-com[bot](2020-07-31 18:41:45):This pull request **introduces 1 alert** and **fixes 1** when merging 247072057ec19f5ac382ce3d6a1c4c56490ca005 into 7baca65e03fe9ec819b397fc7a796f4f93999ae8 - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-027e3666be7b02ec5d7b4e3cb6b8e94e5f58718e)

**new alerts:**

* 1 for &#39;import \*&#39; may pollute namespace

**fixed alerts:**

* 1 for Unused import
lgtm-com[bot](2020-07-31 18:54:46):This pull request **introduces 1 alert** and **fixes 1** when merging ebf7f5ca40ee990ef0672f5548e3e204b3bda6b7 into 7baca65e03fe9ec819b397fc7a796f4f93999ae8 - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-f2a7a1ee9c63e215719dc86720a9e3ca31a4c26c)

**new alerts:**

* 1 for &#39;import \*&#39; may pollute namespace

**fixed alerts:**

* 1 for Unused import
lgtm-com[bot](2020-07-31 23:10:52):This pull request **introduces 1 alert** and **fixes 1** when merging 6721c1f90e769b62d07e346ac1df45438027dcb2 into 7baca65e03fe9ec819b397fc7a796f4f93999ae8 - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-2f21650cf4d354d7ebb2f793ff8ae76c6be56acb)

**new alerts:**

* 1 for &#39;import \*&#39; may pollute namespace

**fixed alerts:**

* 1 for Unused import
lgtm-com[bot](2020-08-01 00:18:50):This pull request **introduces 1 alert** and **fixes 1** when merging fb861a178e9d662d9cbb869fd64c3a2796f20659 into 7baca65e03fe9ec819b397fc7a796f4f93999ae8 - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-181e1df3d785b4902f43aaf40e83e3f2194f05a1)

**new alerts:**

* 1 for &#39;import \*&#39; may pollute namespace

**fixed alerts:**

* 1 for Unused import
lgtm-com[bot](2020-08-03 18:26:10):This pull request **introduces 1 alert** and **fixes 1** when merging 5e83d10ea2534baf390560aa01b5726790a6d664 into 057b5136637d64b5a7c7d833ca6d5d927ec68013 - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-fc4dcc1b7860d0fb08d4fc5203d1303bd244d428)

**new alerts:**

* 1 for &#39;import \*&#39; may pollute namespace

**fixed alerts:**

* 1 for Unused import
lgtm-com[bot](2020-08-03 18:52:51):This pull request **introduces 1 alert** and **fixes 1** when merging b3e94ff16fca85d19466930108ff416381d80a30 into 057b5136637d64b5a7c7d833ca6d5d927ec68013 - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-dcd970461cfeccae0a87aa24b6e814374069fc98)

**new alerts:**

* 1 for &#39;import \*&#39; may pollute namespace

**fixed alerts:**

* 1 for Unused import
lgtm-com[bot](2020-08-03 19:43:08):This pull request **introduces 1 alert** and **fixes 1** when merging 647b41be75a2b2a2ed42a19792d0ba0afdc71b0d into 057b5136637d64b5a7c7d833ca6d5d927ec68013 - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-3001150175f8901534ed2f85eac7cf09e051a488)

**new alerts:**

* 1 for &#39;import \*&#39; may pollute namespace

**fixed alerts:**

* 1 for Unused import
lgtm-com[bot](2020-08-03 19:58:54):This pull request **introduces 1 alert** and **fixes 1** when merging 9b1b90b93a029611841d4aaa7cb4f003e6dfbb62 into 057b5136637d64b5a7c7d833ca6d5d927ec68013 - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-75776404225bc111148a0e74e1c19f8d4095227b)

**new alerts:**

* 1 for &#39;import \*&#39; may pollute namespace

**fixed alerts:**

* 1 for Unused import
lgtm-com[bot](2020-08-03 21:12:50):This pull request **introduces 1 alert** and **fixes 1** when merging abb6cc36d3e39acf34d55a468bb1a451822f2057 into 057b5136637d64b5a7c7d833ca6d5d927ec68013 - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-c341b8ba0a0e09f294d03e89a07d660587b5d1fa)

**new alerts:**

* 1 for &#39;import \*&#39; may pollute namespace

**fixed alerts:**

* 1 for Unused import
lgtm-com[bot](2020-08-03 21:33:45):This pull request **introduces 1 alert** and **fixes 1** when merging ccac1554165df46d154a27c23940ff88ea7e04ea into 057b5136637d64b5a7c7d833ca6d5d927ec68013 - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-8850398ff19d8e324f186e3aba8aa11327e8de18)

**new alerts:**

* 1 for &#39;import \*&#39; may pollute namespace

**fixed alerts:**

* 1 for Unused import
lgtm-com[bot](2020-08-03 21:54:30):This pull request **introduces 1 alert** and **fixes 1** when merging a6e77e9bf9d902fb78aa5482c94946bff6b47d44 into 057b5136637d64b5a7c7d833ca6d5d927ec68013 - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-69eedf14a7059a6daeb18e64cff93033db280335)

**new alerts:**

* 1 for &#39;import \*&#39; may pollute namespace

**fixed alerts:**

* 1 for Unused import
lgtm-com[bot](2020-08-03 22:19:50):This pull request **introduces 1 alert** and **fixes 1** when merging 6328a21270d3963732c9572c728e0259cc6556cb into 057b5136637d64b5a7c7d833ca6d5d927ec68013 - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-2f65f0bd72f175e3658891145ec23cc9499a6deb)

**new alerts:**

* 1 for &#39;import \*&#39; may pollute namespace

**fixed alerts:**

* 1 for Unused import
lgtm-com[bot](2020-08-03 22:54:07):This pull request **introduces 1 alert** and **fixes 1** when merging 8f0496bdc39e490dfe1f91df3fb6c745b8154cb9 into 057b5136637d64b5a7c7d833ca6d5d927ec68013 - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-e3e4ebeaca6d8e49d97eee7e8d9801f99a35de45)

**new alerts:**

* 1 for &#39;import \*&#39; may pollute namespace

**fixed alerts:**

* 1 for Unused import
lgtm-com[bot](2020-08-04 00:22:43):This pull request **introduces 1 alert** and **fixes 1** when merging df8af1219a68172c2def381a1e9e3be3835e2fed into 057b5136637d64b5a7c7d833ca6d5d927ec68013 - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-30e8208fff7216886e69786b03fa385025e41736)

**new alerts:**

* 1 for &#39;import \*&#39; may pollute namespace

**fixed alerts:**

* 1 for Unused import
lgtm-com[bot](2020-08-04 00:46:43):This pull request **introduces 1 alert** and **fixes 1** when merging 60efcdd5e325ee157fc25a60356ed4cb32755a66 into 057b5136637d64b5a7c7d833ca6d5d927ec68013 - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-a18b701967a646a484c1975f7a6aa0046d97b853)

**new alerts:**

* 1 for &#39;import \*&#39; may pollute namespace

**fixed alerts:**

* 1 for Unused import
vinitra-zz(2020-08-04 01:08:56):@linkerzhang, as Azure Pipelines CIs are passing, this PR looks ready to go. The requested changes to onnx-data.proto have been implemented. Can you take another look?

Minor caveats:
1. Resolved! ~CircleCI build is currently broken for all PRs and is being fixed here: https://github.com/onnx/onnx/pull/2941~ 
2. Resolved! ~TravisCI is failing because it's finding a diff in onnx/onnx-data.proto from the generated file because the onnx/onnx.proto import statement is changing across both ONNX_ML options. It's important to have the differing import statements (import onnx/onnx-ml.proto vs onnx/onnx.proto because only one is generated based on the ONNX_ML value). @askhade and I are brainstorming a way around this for the CI check, but the PR is entirely functional.~
lgtm-com[bot](2020-08-04 02:12:31):This pull request **introduces 1 alert** and **fixes 1** when merging c934a364dceafd230d9e98d0e9896d6c23ec6894 into 057b5136637d64b5a7c7d833ca6d5d927ec68013 - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-2c5e29e2f5e552b94c677b9d54bd77e86886a48b)

**new alerts:**

* 1 for &#39;import \*&#39; may pollute namespace

**fixed alerts:**

* 1 for Unused import
lgtm-com[bot](2020-08-04 03:33:50):This pull request **introduces 1 alert** and **fixes 1** when merging 1696431816ecfb3f914d86f6b782f867bd8c88ec into 057b5136637d64b5a7c7d833ca6d5d927ec68013 - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-f81a656af277026586c9c5f68babafd8e365dc86)

**new alerts:**

* 1 for &#39;import \*&#39; may pollute namespace

**fixed alerts:**

* 1 for Unused import
lgtm-com[bot](2020-08-04 17:47:00):This pull request **introduces 1 alert** and **fixes 1** when merging 27ac400100b2101c7effa06abf4b623d09b95b5d into b6e4acb146da52de1fa56cd2b0efb2fe9c5f4830 - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-46c783a27a5a3beb583e7af8618529b3d37dbdbb)

**new alerts:**

* 1 for &#39;import \*&#39; may pollute namespace

**fixed alerts:**

* 1 for Unused import
lgtm-com[bot](2020-08-05 19:49:24):This pull request **introduces 1 alert** and **fixes 1** when merging 819ff19a54da324c8395100ce3fb2ec7c665968b into b6e4acb146da52de1fa56cd2b0efb2fe9c5f4830 - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-e2aaa77fd9267e620f4129cf58e507f618444585)

**new alerts:**

* 1 for &#39;import \*&#39; may pollute namespace

**fixed alerts:**

* 1 for Unused import
lgtm-com[bot](2020-08-05 20:16:22):This pull request **introduces 1 alert** and **fixes 1** when merging b74a93947e3c4bea7055860131b32b2d5ad6b80c into b6e4acb146da52de1fa56cd2b0efb2fe9c5f4830 - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-ac637ac5ba4cab41c883748c394e1da4cd00be56)

**new alerts:**

* 1 for &#39;import \*&#39; may pollute namespace

**fixed alerts:**

* 1 for Unused import
lgtm-com[bot](2020-08-05 20:33:43):This pull request **introduces 1 alert** and **fixes 1** when merging 106af092b2bf46ed32840424cc1f10032366b7bf into b6e4acb146da52de1fa56cd2b0efb2fe9c5f4830 - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-8fca744960772e8453f06e0f12affe3e83cc6ebc)

**new alerts:**

* 1 for &#39;import \*&#39; may pollute namespace

**fixed alerts:**

* 1 for Unused import
vinitra-zz(2020-08-05 22:46:24)::shipit: 
linkerzhang(2020-02-02 05:14:56):is this needed? will it be conflict with the type in "TensorProto"?
gramalingam(2020-02-04 00:10:26):I think this and MapElement below should be the same (shared) message-type. (We could probably copy the style used in the definition of TypeProto, in case the mutual recursive dependence causes a problem.) I think we should allow a sparse-tensor here too.
gramalingam(2020-02-04 00:13:15):We need repeated KeyValuePairs, consisting of a key and a value.
gramalingam(2020-02-04 00:15:54):Yes, I was wondering the same. Why do we need the "dims" field also? (I think the edge-case where we have an empty-sequence may need it, depending on the use-case. But ideally we should not need this.)
vinitra-zz(2020-02-04 20:41:00):Yes, I added dims originally for the empty-sequence case and for ease of use, but it's perhaps overengineered. I'm removing both elem_type and dims.
vinitra-zz(2020-02-04 20:46:28):Added KeyValuePair message type. Thanks for the suggestion.
TMVector(2020-02-06 00:37:14):The key needs to support [u]int types and string.

`TypeProto.Map.key_type` says:
```
    // This field MUST have a valid TensorProto.DataType value
    // This field MUST be present for this version of the IR.
    // This field MUST refer to an integral type ([U]INT{8|16|32|64}) or STRING
    optional int32 key_type = 1;
```
vinitra-zz(2020-02-06 18:17:38):Created shared SequenceMapElement and added sparse tensor support.
vinitra-zz(2020-03-23 21:00:35):Addressed in recent update, but unsure if this is the right way to do it. 
vinitra-zz(2020-03-31 23:56:14):Rewritten as per to Bowen's suggestion of including datatype enum field and storing data in separate key fields similarly to TensorProto format in KeyValuePair. 
BowenBao(2020-04-10 00:38:10):Seems we need to extend numpy_helper to add conversion from `list[np.array]` to `SequenceProto` and vice versa. And by using `SequenceProto::SerializeToString()` we should be able to store the values to file.
vinitra-zz(2020-04-10 05:00:19):Working on this currently. Should have a PR update out soon.
vinitra-zz(2020-04-10 17:35:53):Should KeyValuePair be moved out of MapProto?
vinitra-zz(2020-04-13 20:44:29):Updated! See numpy_helper for the new functions. Also added support for Maps.
BowenBao(2020-04-13 21:05:37):this seems to be picking `test_strnorm_model_monday_casesensintive_nochangecase` as sequence as well. The input for the above test case is `array(['monday', 'tuesday', 'wednesday', 'thursday'], dtype=object)`. 
vinitra-zz(2020-04-13 21:45:24):Ah, yes, just noticed strings are treated as dtype=object in np. Will try to identify ideas for how to better specify sequences.
gramalingam(2020-04-18 04:41:42):Why not use python dictionaries and lists as the representation for ONNX's map and sequence? Won't that be more natural in Python?
vinitra-zz(2020-04-20 15:45:12):I had initially implemented it with dicts and lists, but ran into a significant number of errors in the testing infrastructure as everything was tailored to np.array formats. 
vinitra-zz(2020-04-20 15:47:32):However, I do think it is the more natural data structure within Python. I'll look into it a bit more.
gramalingam(2020-05-05 02:40:48):It would be great if we could just write sequence = [ np.array(...), np.array(...), ...] here. It also clarifies that it is a sequence of tensors.
vinitra-zz(2020-05-06 00:02:33):Addressed. Thanks!
gramalingam(2020-05-06 06:15:06):I am a bit surprised that this did not require other changes (as discussed elsewhere above about whether we should use arrays or lists for representing sequences)
vinitra-zz(2020-05-06 16:16:24):Likewise. This ended up being a point in test implementation, not test infrastructure as I had previously thought. I'll test with ORT and see if it works. Thanks!
vinitra-zz(2020-05-06 16:23:41):The changes I was talking about earlier were in the helper functions (i.e. from_array_to_sequence, etc.) Recent updates should help address these discrepancies.
gramalingam(2020-05-06 16:27:03):What happens here if "arr" is a list, instead of an array? (As an aside, the type-annotations above will no longer be valid. Unfortunately, it looks like the python type-checker is not part of the CI ... this is a separate issue, but it does complicate our task here since we can't rely on the type-checker.) I don't understand how things can work without changes here.
gramalingam(2020-05-06 16:28:56):I think changes might be required in _extract_value_info (as I have commented there) also to make this work.
vinitra-zz(2020-05-06 16:41:43):See recent updates.
vinitra-zz(2020-05-06 16:54:49):I've just addressed this change as part of the recent updates for arr->list, but some work might need to be done in sequence_value_info as well. It seems like it's only used in the shape-inference-tests: https://github.com/onnx/onnx/blob/8e9c7d57f7c5aa6f6eb7ee7abb0ba2a243781933/onnx/test/shape_inference_test.py#L2522
gramalingam(2020-05-06 17:10:02):A minor suggestion: it may be more general to use "collections.abc.Mapping" instead of "dict". Similarly, "collections.abc.iterable" or "collections.abc.Sequence" might be better than "list" below. However, this would require us to move the test for ndarray up first, since an array might also be an iterable/sequence object. (I haven't tried it out, so don't know if anything will break, so do test it out if you try this. https://stackoverflow.com/questions/35690572/what-does-isinstance-with-a-dictionary-and-abc-mapping-from-collections-doing has more info on this. (I am not fully familiar with pythonic idioms.)
vinitra-zz(2020-05-07 18:57:26):After a quick test (shown below) -- I've updated the listings with the abc mappings accordingly. Highlight: lists are abc.Sequence but arrays are not.

>>> import numpy as np
>>> from collections import abc
>>> arr = np.array([1, 2, 3])
>>> lst = [4, 5, 6]
>>> isinstance(arr, abc.Sequence)
False
>>> isinstance(lst, abc.Sequence)
True
>>> isinstance(arr, np.ndarray)
True
>>> isinstance(arr, abc.Iterable)
True
>>> isinstance(lst, abc.Iterable)
True

vinitra-zz(2020-05-13 22:01:30):Ultimately decided to not use abc functionality as collections.abc is not included in Python 2.7
linkerzhang(2020-05-15 08:30:09):can we just use a "Tensor" as a key?

Though this design does not have any function miss, it looks complex...
linkerzhang(2020-05-15 08:34:18):I'm seeing this as trying to make a complete set of data values representing any kind of sequences/maps.

Can you give some real use cases please? say, some traditional ml models. I guess, not all the compositions (also recursive) of different values (sequences/maps based on tensors/sequences/maps again) are that useful. 
vinitra-zz(2020-05-17 02:06:57):I agree, it is a bit complex. The reason for this spec was to mirror the data types in TensorProto due to the following spec for sequences in TypeProto. 
```
    // This field MUST have a valid TensorProto.DataType value
    // This field MUST be present for this version of the IR.
    // This field MUST refer to an integral type ([U]INT{8|16|32|64}) or STRING
    optional int32 key_type = 1;
```
The raw_data format for keys could be removed...
vinitra-zz(2020-05-18 23:29:09):> I'm seeing this as trying to make a complete set of data values representing any kind of sequences/maps.

Yes, defining the data type's complete possibilities is the thought process here. I don't see real-life use cases of these recursively-defined data values. There are comparative usefulness levels for multimodal data, i.e. a map with individual maps as it's own values seems less relevant than a sequence containing a sequence. Any more than 2 dimensions of recursively defining themselves don't seem to be useful as input for the current state-of-the-art in NN models.
vinitra-zz(2020-05-18 23:31:40):However, the other side of the coin is that some frameworks do allow input of this form to be expressed, even if they're not being currently used. The alternative in onnx.in.proto would be to create a similar format to key representation in MapProto or data representation in TensorProto with a data_type variable and optional input entries (Tensor, SparseTensor, Map, Sequence), and get rid of SequenceMapElement entirely. Thoughts?
vinitra-zz(2020-05-18 23:35:54):If we use a Tensor, is there any good way to type check that it only contains one element and it is not a float value? If the strict adherence to the TypeProto spec is not necessary to be enforced, then I think Tensor is the right way to go.
gramalingam(2020-05-19 23:49:35):The main concern/question I have is that this is used only to represent test-data and it is not used in a model itself. Adding this to onnx.proto means bumping the IR_VERSION, which is somewhat heavy-weight since it has significant implications (e.g., an ONNX backend might reject the model as being an unsupported version, even though the model itself has no changes). How about we define a different onnxdata.proto for this, used only by test-data generator and testing-utilities? 
gramalingam(2020-05-19 23:55:13):Yes, it is complete. But the completeness does not cost us anything (it doesn't add to the complexity in any way, right?). This seems preferable to alternative restricted versions that allow only 1 or 2 levels of nesting, because sooner or later we might find we need to use 3 levels of nesting, etc. As I mention above, the drawback/complexity is only in bumping IR_VERSION, so I think may be defining a different proto file will solve that problem. 
vinitra-zz(2020-05-21 17:46:17):I agree with this -- I don't think that updating the IR version is strictly necessary for these updates. I will define a separate proto file for MapProto, SequenceProto, etc.
BowenBao(2020-06-05 17:17:54):i wasn't aware that the onnx-data was something new added in this PR. Why create a separate proto file?
gramalingam(2020-06-05 18:04:31):This extension does not affect the IR representation used for ONNX models ... it impacts only the representation of test-data. The main goal is to avoid bumping the IR_VERSION unnecessarily. Keeping the proto files separate is helpful as it clarifies that the data-serialization does not impact the model IR or IR_VERSION. Do you see reasons for keeping them together in one proto (other than the challenges discussed below for importing etc.)?
BowenBao(2020-06-05 20:01:43):My concern is that the `TensorProto` which is also data-serialization is kept in the original proto. We also see references of it in `initializers` and `attributes`. So the data serialization is impacting the model IR right now (although for sequence and map it is currently not the case, but then we must declare that these types will not supported as initializers or attributes). 
vinitra-zz(2020-06-05 21:28:02):Discussed offline, consensus is to keep current architecture.
BowenBao(2020-06-09 21:33:29):need to be careful with python objects: the list is passed by reference, so `sequence` gets updated as well. Can use `copy` to create a new sequence to be udpated.
BowenBao(2020-06-09 21:34:59):see above comment, i think that's probably the reason why the generated test data file is not expected. The `sequence` at this point is already updated by the other branch.
BowenBao(2020-06-09 21:40:17):this sets output to `ndarray`, which is later recognized as tensor. Should be list for sequence.
vinitra-zz(2020-06-09 22:43:37):- since list doesn't have a .copy() function before python 3.3 and I didn't want to introduce the copy library, I'm using the list() casting functionality to create a new copy.
gramalingam(2020-06-18 18:55:12):If I remember right, "graph.input" is a sequence of inputs. We will have to use the i-th element of this sequence when we process the input-file corresponding to the i-th input down below.
BowenBao(2020-07-01 17:58:12):I might be missing something.. why are we removing this file?
vinitra-zz(2020-07-01 18:19:04):my mistake! that was a missed file merge.
BowenBao(2020-07-01 21:50:41):```suggestion
def _extract_value_info(input, name, ele_type=None):  # type: (Union[np.ndarray, list], Text, np.dtype) -> onnx.ValueInfoProto
```
input could be list type as well, try use Union? The suggestion may not be exactly syntactically correct.
BowenBao(2020-07-01 22:08:20):curious why are we able to determine input type through `doc_string`? Is there some part in code that we are setting 'Sequence/Tensor/Map' in `doc_string`?
BowenBao(2020-07-01 22:08:57):~Complete the checks for sequence / map :)~

done as this was put in.
vinitra-zz(2020-07-01 22:16:59):It prints out with the type in it, so I was using that as a metric, but I agree -- it's an imperfect solution since it's not actually being regulated and it's an "optional" value. ValueInfo has a type argument which has a TypeProto value. Let me see if comparing the types works better.
vinitra-zz(2020-07-01 22:38:08):Updated by comparing classes of TypeProto. Not sure if this is much better, but definitely seems less arbitrary than comparing doc strings. 
gramalingam(2020-07-01 23:32:37):I don't think we should use "typeid" here. I doubt if it would work. Protobuf has its own encoding for "oneof" : see https://developers.google.com/protocol-buffers/docs/reference/cpp-generated#oneof-embedded-message-fields ... I think we should probably be using output_info.type().has_tensor_type() ... similarly, has_sequence_type() and has_map_type() for below.
gramalingam(2020-07-01 23:36:56):It would be good to validate if the right branch is executing here. I suspect none of these branches may be executing, but maybe I am wrong.
gramalingam(2020-07-02 00:11:39):(a) enforce_has_repeated_fields requires that there be at least 1 repeated value. Do we want that condition for sequence/map here? That wouldn't allow an empty sequence to be an input, for example. May be better to remove that check?
(b) However, I think there are other sanity checks that could be done on the data: one check is to iterate through all elements of the sequence and calling the corresponding check function on each element; a second check is to verify that all elements are of the same type. 
vinitra-zz(2020-07-02 17:42:54):Updated to use .has_tensor_type() and related functions!
vinitra-zz(2020-07-02 17:49:24):After testing with the new methods, it is hitting those if branches. :)
vinitra-zz(2020-07-02 18:39:42):Added the iterative checks, as well as helper functions for key value pair and sequence map element testing. Removed the has_repeated_field checks.
gramalingam(2020-07-02 19:38:21):Add "enforce_has_field(seq_map_elem, tensor_value)" before check_tensor. Similarly for the other cases below. Similarly, for "elem_type" above.
vinitra-zz(2020-07-02 20:04:47):added! thanks.
linkerzhang(2020-07-06 00:14:15):what's the difference between onnx-data.proto and onnx-data-ml.proto?
linkerzhang(2020-07-06 00:17:52):In this way, there may be a tensor and a map in one sequence, which I don't think is correct.
linkerzhang(2020-07-06 00:20:06):A better way might be define two fields. one for keys, one for values, which should be a Sequence.
gramalingam(2020-07-06 16:12:59):What if we do something similar to AttributeProto: define the DataType here, and use "repeated TensorProto", "repeated SparseTensorProto", etc.? Would that be better?
gramalingam(2020-07-06 16:28:02):One option is to "inline" the fields in KeyValuePair here ("key_type" and all the "*_data") and make all the _data fields repeated. The value can be a SequenceProto. With the implicit constraint that both sequences have the same length. Would that work?
gramalingam(2020-07-06 16:29:12):Or, we can define a "KeySequenceProto" as above.
vinitra-zz(2020-07-06 17:59:33):No difference. It was automatically generated when ONNX_ML=1 using gen_proto.py.
vinitra-zz(2020-07-06 19:23:21):I think Rama's suggested structure makes a lot of sense. In an offline discussion, Rama explained the strongly typed nature of these data structures in ONNX and how they operate differently than in Python (where you can have {'key':'value', 1: 32} as a valid dictionary), which was the reason behind this original design.
vinitra-zz(2020-07-06 19:24:21):As per my earlier interpretation, having a tensor and a map in one sequence was a valid option. I understand now that it should not be allowed. :)
vinitra-zz(2020-07-06 19:32:39):Option 1: remove SequenceMapElement and KeyValuePair + their related helper functions. Implement the new proto structure in MapProto itself with a number of repeated key_data fields, a key_type, a value_type, and a repeated value (Sequence) field.
Option 2: Create a KeySequenceProto (instead of KeyValuePair) to not clutter the MapProto and allow us to index the dictionary pairs as separate elements, with a single element SequenceProto as the value and the same structure for the keys.

Not sure which option is better -- it might be nice to index the corresponding key and value in KeySequenceProtos if the structure exists as repeated elements instead of indexing the separate sequences in parallel, so I'm leaning towards option 2. Do you have an opinion before I implement it, @linkerzhang?
linkerzhang(2020-07-10 05:15:18):that's bad. if no difference needed, then no need having two files. "// #if ONNX-ML" should not in the file.
linkerzhang(2020-07-10 05:17:20):the map should be just one sequence of keys and one sequence of values. no?
gramalingam(2020-07-27 20:14:27):Every key has to be a scalar value. Why should we make it a tensor here? Just raising the question to understand the pros and cons. Traditionally, ONNX used a tensor of zero dimensions to represent scalars. But that is in a different context. In this context, is it useful represent the key as a tensor of zero dimensions? Or, would it be better to just use a "repeated int64" (to cover all the int cases) and a "repeated bytes" to cover the string case?
vinitra-zz(2020-07-28 17:44:37):I do think it makes more sense in terms of space to implement keys as a repeated sequence of ints / bytes. Will implement accordingly.
gramalingam(2020-07-29 20:29:52):I think we can drop this check above, since tensor_values is a repeated field and since we (presumably) want to allow for the case with zero elements in the sequence.
gramalingam(2020-07-29 20:30:49):Same here and below for other cases: drop check in repeated case where zero-elements is valid.
vinitra-zz(2020-07-30 17:49:23):Thanks for the recommendation. Removed the relevant checks.
askhade(2020-07-31 00:27:09):Add comment here to explain why you are adding this exception for onnx-data
BowenBao(2020-07-31 00:55:13):Can we extend this to allow creating from empty list? Maybe also add optional dtype parameter such that type can be specified if the list is empty.
BowenBao(2020-07-31 00:56:48):nit: remove comments
BowenBao(2020-07-31 00:57:04):should this be uncommented?
BowenBao(2020-07-31 00:57:15):nit: indent
vinitra-zz(2020-07-31 01:03:17):resolved with new fixes in latest update!
vinitra-zz(2020-07-31 01:03:35):Addressed
vinitra-zz(2020-07-31 01:38:02):Included in latest revision.
gramalingam(2020-07-31 23:21:58):The existing "make_sequence_value_info" seems to have a limitation, as it assumes that we only allow sequences of tensors and the elem-type parameter seems to be that of the type of elements in the tensor. So, I think this is not going to be consistent with that. We need to either (a) Restrict ourselves to sequences of tensors, in which case we should pick up the element-type of the numpy array here, or (b) Have a more comprehensive type-constructor (which would have to be recursive to handle sequences/maps contained inside sequences).
askhade(2020-08-01 00:57:12):this is not a python type
askhade(2020-08-03 16:28:34):this does not seem right. The elem_type here will be the type of the input 1 in the list... it is not compatible with sequence.elem_type ....
The type should be one of the following:
enum DataType {
    UNDEFINED = 0;
    TENSOR = 1;
    SPARSE_TENSOR = 2;
    SEQUENCE = 3;
    MAP = 4;
  }
vinitra-zz(2020-08-03 18:37:32):changed the name accordingly
vinitra-zz(2020-08-03 18:38:12):Right now, the implementation is option (a). A future PR will address (b).
linkerzhang(2020-08-05 00:09:40):this is not needed, right? The type has been specified in SequenceProto already.
linkerzhang(2020-08-05 00:11:04):is this file needed?
linkerzhang(2020-08-05 00:12:41):so if it's "TENSOR", the tensor's element type should also be the same, but can be different shapes, right? if yes, I'd suggest to add this clarification in the comments too.
gramalingam(2020-08-05 16:25:56):The clarification can point to https://github.com/onnx/onnx/blob/master/docs/IR.md#static-tensor-shapes for a more complete description. Whether the tensors can have different shapes or not depends on the type/shape associated with the corresponding "ValueInfo". 
gramalingam(2020-08-05 16:28:37):For example, "Sequence<Tensor<float, [M,N]>" means that all tensors have same shape. But "Sequence<Tensor<float, [omitted,omitted]>" means they can have different shapes (but all of rank 2), where the "omitted" means the corresponding dimension has no symbolic/constant value.
gramalingam(2020-08-05 16:33:51):Finally, "Sequence<Tensor<float, omitted>>" means that the different tensors can have different ranks, when the "shape" itself is omitted from the tensor-type.
vinitra-zz(2020-08-05 19:13:26):Removing this file is not trivial, a lot of infrastructure seems dependent on it -- perhaps something to investigate in a follow-on PR. onnx-operators.proto has a similar onnx-operators_pb.h as well.
vinitra-zz(2020-08-05 19:37:09):Removed! Thanks for the feedback.
vinitra-zz(2020-08-05 19:37:21):Addressed and included these points in the recent update.
linkerzhang(2020-02-04 02:04:00):this is already covered by the PR #2575 
wschin(2020-02-03 16:56:28):```suggestion
// function body node. They're using the same attribute name.
```
wschin(2020-02-03 16:57:38):```suggestion
// node.
// <type> specifies the attribute type.
```
wschin(2020-02-03 16:58:29):Not sure how but I feel it's better to have at least one test.
wschin(2020-02-05 08:56:51):Any shape inference tests for attributes?
TMVector(2020-02-05 13:02:23):Should this PR include changes to the spec?
wschin(2020-02-05 23:45:23):Not sure if we want to bump op version.
TMVector(2020-02-06 00:58:59):@gramalingam [commented](https://github.com/onnx/onnx/pull/2575#issuecomment-582581695) about compatibility. I can imagine this could definitely break some things.

I think we are trying to address a couple of issues.

### Allow an attribute to hold one of many types
I think this should include sequences and maps, which I expect will be added to `AttributeProto` in #2581).

We could address this by:
* Adding a new AttributeType `ANY_VALUE` which means any attribute type apart from `GRAPH` or `GRAPHS`. Any type restrictions (e.g. only `int` or `float`) would have to be in documentation only. This type tag would only be used in operator specifications -- node attributes would use the existing type tags.
* Adding lots of attributes (`value_int`, `value_ints`, ...) and specifying in the documentation only one should be used. This would be done ad-hoc -- it has already been done to some extent for some operators, and even `Constant` has `value` and `sparse_value`.

### Lift non-tensor attribute values in to the value level
This is the situation raised in #2575, and this could be addressed by:
* Changing `Constant` to have an attribute which can have any type, as above. Any values which are not valid graph values (`int`, `ints`, `float`, `floats`, `string`, `strings`) would be promoted to 0D or 1D tensors.
linkerzhang(2020-02-06 01:16:32):I think adding an attribute type "any" is tricky (sorry for this wording), it actually is adding a very customized type for this "Constant" requirement right now.

I got the comment from @gramalingam in PR #2575 - adding multiple attributes in "Constant", say, "value_int", "value_ints", "value_float", "value_floats", "value_string", "value_strings" to make "Constant" op be able to accept all these types' attributes. it's not that bad, as it only affects ONE operator (even though it's not that beautiful).

However, I'd still add more on why I'm still suggesting to make this change. AttributeType: INT, INTS, FLOAT, FLOATS, STRING, STRINGS were designed (instead of reusing type TENSOR), is because, they simplify things when the attribute is just a scalar or an array (They're kind of 6 shortcuts of special tensor), though they can be represented as a Tensor. This means, these 6 types are indeed "TENSOR" representable.

About version bump: This PR will ask for a IR version bump.
1. there's NO backward compatibility issue - any old models (without this PR) can still be runnable.
2. there's forward compatibility issue - a new model (with using this statement) can't be loaded and run against a runtime if it does not support this PR, so that IR version should be bumped to differentiate the new model from old models.

@gramalingam any other breakings will this PR introduce please?
linkerzhang(2020-02-10 22:58:33):abandon this PR due to agreement on adding more support on Constant op
linkerzhang(2020-02-05 01:53:08):this is the only change, all others are format changes.
wschin(2020-02-07 18:24:46):I thought we would only touch `Constant`. Why do we add these lines?

[Update] OK. I saw you use the same name for attributes in different types. I feel it's not quite ideal. It breaks strongly-typed design concept. Could we just add more attributes?
claassistantio(2020-02-05 02:38:20):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=2585) <br/>All committers have signed the CLA.
spandantiwari(2020-02-14 23:09:31):@wezuo - Could you please add some details in the description as to what scenario is this addition needed to support. Is there any existing framework op that will be supported by this?
wezuo(2020-02-18 18:12:14):@spandantiwari , could you point me the appropriate place (e.g., which file) to add those note? 
spandantiwari(2020-02-18 18:21:57):> @spandantiwari , could you point me the appropriate place (e.g., which file) to add those note?

@wezuo  - just right at the top of this PR in the opening comment.
hariharans29(2020-02-05 19:28:24):typo: batch
hariharans29(2020-02-05 19:28:57):Please fix this. Seems unintentional.
hariharans29(2020-02-05 19:33:50):nit: `b+1` (not `b` + 1), the difference in text highlighted in the documentation might be misleading
hariharans29(2020-02-05 19:34:59):should there be a statement saying the first `b` dims of both the `data` and `indices` tensor must be equal ? Maybe this can be listed as a separate bullet point below (after (1) maybe)
hariharans29(2020-02-05 20:03:09):The output rank computation here doesn't align with the new example added...
hariharans29(2020-02-05 20:05:00):nit: does first `b` -> leading `b` read better ?
gramalingam(2020-02-05 20:23:36):Change ". Where" to ", where"
gramalingam(2020-02-05 20:24:49):Change "an integer equals to the production of 1 and" to "the product of"
gramalingam(2020-02-05 20:29:27):The above two lines can be simplified to:
```
   int64_t batch_dims_data = getAttribute(ctx, "batch_dims", 0);
```
hariharans29(2020-02-05 21:02:35):production of 1 and ... -> product of all the elements of ...
hariharans29(2020-02-12 00:08:24):nit: honored
hariharans29(2020-02-12 00:08:51):the numbering order is incorrect...
lara-hdr(2020-02-14 01:23:01):nit: tabs -> spaces
claassistantio(2020-02-05 13:14:34):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=2588) <br/>All committers have signed the CLA.
postrational(2020-02-13 14:19:05):We found this issue when adding support for `MeanVarianceNormalization` using an ONNX function. I think this fills in a gap in the `FunctionExpandHelper`.
linkerzhang(2020-02-16 03:43:06):@Ewa21 please sign the CLA.
etusien(2020-02-17 09:21:00):> @Ewa21 please sign the CLA.

Done :)
linkerzhang(2020-02-18 00:45:48):the branch is out of date with the base branch. @Ewa21 please sync master and update this branch.
etusien(2020-02-18 12:14:56):> the branch is out of date with the base branch. @Ewa21 please sync master and update this branch.

Done :)
gramalingam(2020-02-19 02:40:58):Hmm, seems to have become out-of-date again! Normally, there is an option to "update branch" in the UI right here, but it seems to be missing here ... not sure why. 
linkerzhang(2020-02-20 06:14:47):yep. interesting. The "update the branch" button is not shown in this PR. @Ewa21 you'll have to update the branch by yourself again. 
prasanthpul(2020-02-05 21:04:02):can you sign the cla? then we can merge
linkerzhang(2020-02-08 02:10:03):@migueldeicaza please sign the CLA. thanks!
CLAassistant(2020-03-06 08:53:26):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=2589) <br/>All committers have signed the CLA.
gramalingam(2020-02-05 21:11:41):On this topic, may be it would be useful to point to the graphic visualizer (Netron) as well: https://github.com/lutzroeder/netron 
prasanthpul(2020-02-05 21:04:02):can you sign the cla? then we can merge
linkerzhang(2020-02-08 02:10:03):@migueldeicaza please sign the CLA. thanks!
CLAassistant(2020-03-06 08:53:26):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=2589) <br/>All committers have signed the CLA.
gramalingam(2020-02-05 21:11:41):On this topic, may be it would be useful to point to the graphic visualizer (Netron) as well: https://github.com/lutzroeder/netron 
TMVector(2020-02-06 01:48:01):I took this to refer to `AttributeProto.tensors`, but I suppose `tensors` allows heterogeneous sequences, whereas sequence values are homogenous in value type, and will probably be added in #2548.

Maybe there should be something said about allowing multiple tensors and graphs in one attribute?
gramalingam(2020-02-06 16:14:43):"a superset of" seems incorrect too. How about replacing it by "related to but slightly different from"?
gramalingam(2020-02-06 16:20:46):I also took "sequences" to refer to the "repeated" version of all attributes. I also note that there is a PR in progress (https://github.com/onnx/onnx/pull/2581 ) to define the message format for Sequences and Maps. It does not yet seem to add this to AttributeProto, but I assume that is the intention or end-goal.
gramalingam(2020-02-06 16:30:02):I am also not clear what "either variant" refers to ... sounds like a leftover error. Why not simplify this to:
```
The type system used for attributes is related to but slightly different from that used for of inputs and outputs.
Attribute-values may be a dense tensor, or sparse tensor, or a scalar numerical value, or a string, or a graph, or
repeated values of one of the abovementioned types.
```
gramalingam(2020-02-06 16:31:51):Currently, some ops encode a map as a pair of attributes (repeated keys, repeated values).
vinitra-zz(2020-02-06 18:05:06):I am in favor of the wording @gramalingam mentioned above. Definitely adds clarity.
jeremycochoy(2020-02-07 15:11:44):/cc @linkerzhang @gramalingam  🙂 
linkerzhang(2020-02-08 01:08:08):an old PR request https://github.com/onnx/onnx/pull/2516 for your reference on how to bump an op version. Thanks! @jeremycochoy 
jeremycochoy(2020-02-07 15:01:15):The constraint changed: Here at least one attribute should be defined AND no more than one attribute should be defined.
jeremycochoy(2020-02-07 15:02:00):This is indeed a bit redundant with the type definition of value_int, but maybe redundancy is a good thing? 😅 
TMVector(2020-02-07 15:26:12):I would suggest something like:
> The value for the sole element for the scalar, int64, output tensor.

> The values for the elements for the 1D, int64, output tensor.

> The value for the sole element for the scalar, float32, output tensor.

and so on.

This would make it clear what the output tensor will actually look like (shape, element type).
TMVector(2020-02-07 15:32:09):IMO the scalar attributes should produce a scalar tensor, which should have zero dimensions https://github.com/onnx/onnx/blob/master/docs/IR.md#static-tensor-shapes
TMVector(2020-02-07 15:34:35):There's also a message on line 168 that needs updating, although it should never be reached now that you check the `<= 1` part at the top.
jeremycochoy(2020-02-07 16:31:43):An alternative is simply `getOutputShape(ctx, 0)` since the shape attribute doesn't exists yet (I tried to pick the one that was semantically more sounding).
wschin(2020-02-07 18:53:06):```suggestion
This operator produces a constant tensor. Exactly one of the provided attributes, either value, sparse_value,
```
wschin(2020-02-07 22:29:34):```suggestion
              fail_shape_inference("Attribute 'value_ints' expect a list of integers.")
```
wschin(2020-02-07 22:29:44):```suggestion
              fail_shape_inference("Attribute 'value_floats' expect a list of floats.")
```
wschin(2020-02-07 22:29:54):```suggestion
              fail_shape_inference("Attribute 'value_strings' expect a list of strings.")
```
wschin(2020-02-07 22:32:08):@linkerzhang, do you think we should bump the operator's version? I feel it's better to bump operator version as long as its behavior changes (no matter if the changes are BC).
linkerzhang(2020-02-08 01:05:42):yes, version needs to be bumped.

@jeremycochoy  this Constant (Version 1) should be moved to old.cc, and new Constant (Version 12?) should be added here.
gramalingam(2020-02-09 01:52:11):Change to "updateOutputShape(ctx, 0, {})". I think "TensorShapeProto()" would represent an unknown shape (of unknown rank). It is more precise to specify output tensor is of rank 0.
gramalingam(2020-02-09 01:54:01):May be slightly simpler to write "updateOutputShape(ctx, 0, {value_ints->ints_size()})". Same comment for other statements below.
jeremycochoy(2020-02-09 16:15:45):This syntax do not compile. The compiler don't know how to create a `TensorShapeProto` from this vector.
jeremycochoy(2020-02-09 16:17:31):I think some constructors / types specifications are missing to allow this syntax to compile. :/
gramalingam(2020-02-09 19:33:58):May be the compiler is unable to infer the type of the parameter from an empty list {}. In any case, I realize that the existing code is fine. I was confusing this with a type without a defined shape ... a TensorShapeProto() should represent a shape with an empty list of dims, so that's fine. Please ignore my previous suggestion.

gramalingam(2020-02-10 04:27:02):Sorry, please ignore my suggestion. It needs to be a bit more complicated, needs a Dim, not integer value.
lgtm-com[bot](2020-02-10 21:39:23):This pull request **introduces 8 alerts** when merging add49e8fa628ee89343589b8688fc12d82461724 into f254647a3a86a689537c6fdda330bf60d2d76459 - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-87e98648167622a77932744226d825d17a5d07aa)

**new alerts:**

* 4 for Unused local variable
* 2 for Variable defined multiple times
* 1 for Syntax error
* 1 for Unreachable code
CLAassistant(2021-08-23 23:19:56):[![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/onnx/onnx?pullRequest=2595) <br/>Thank you for your submission! We really appreciate it. Like many open source projects, we ask that you sign our [Contributor License Agreement](https://cla-assistant.io/onnx/onnx?pullRequest=2595) before we can accept your contribution.<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/onnx/onnx?pullRequest=2595) it.</sub>
gramalingam(2020-02-12 05:03:14):How about changing the middle line to "A function is an operator whose semantics is formally expressed via expansion into a sub-graph (called the function body) using other operators (and functions). Functionality-wise ..."
linkerzhang(2020-02-12 07:20:13):Your wording is better, adopted. Thanks!
TMVector(2020-02-12 10:23:31):This isn't always going to be possible with context dependent functions :/
linkerzhang(2020-02-12 09:20:19):this is the non-formatting change.
linkerzhang(2020-02-12 09:20:36):this is non-formatting change.
linkerzhang(2020-02-12 09:21:06):API changes for dynamic function will be added later (maybe in a separate PR).
gramalingam(2020-02-13 01:12:31):On a different note: can we increase the line-width used for clang-format please? I feel it is too small and breaks up code into too many lines. (Nothing to do with this PR, just struck me.)
gramalingam(2020-02-13 01:14:52):I think it would be a good idea to include the function's domain/since-version in this case also (or, at least if it is not a duplicate).
linkerzhang(2020-02-13 01:27:53):it's included, it's call the other overloaded FunctionBody, which adds that. 
yinghai(2020-02-13 05:59:15):Lol 
wschin(2020-02-14 08:27:25):Could you please reference related PRs here for  people to understand the background? Thank you.
linkerzhang(2020-02-15 00:57:33):@wschin actually I may ask you to add this line in your PR please (to save this one) :) the PR merged was #2597 . Thank you!
linkerzhang(2020-02-16 00:08:41):@ebarsoum @postrational "-1" has different meanings across ONNX ops. It may be good to have a consistent definition across all of them to avoid confusion. See issue #2507 
spandantiwari(2020-02-18 20:00:03):@neginraoof  - would it be OK to use '0' instead of '-1' for keeping the dimension unchanged. I think there are other ops that are use 0 for that semantics. We should try to be consistent. 

@linkerzhang - Issue https://github.com/onnx/onnx/issues/2507 has been open for a while, and I see multiple options open for that. Do you know if any decision has been made on that? 
If yes, then we can follow the same semantics here. 
spandantiwari(2020-02-18 20:04:23):cc: @houseroad 
gramalingam(2020-02-18 21:15:26):My understanding from issue 2507 is that using 0 is problematic, and that -1 is better. (But we originally used 0 in some ops.) However, for broadcast, doesn't "1" already mean use the other-input-size? So, why do we need another special value?
neginraoof(2020-02-18 21:52:12):Just a note:
I recently updated constant of shape to create multidimensional tensors with dim value 0.
https://github.com/onnx/onnx/pull/2567

And so far, looks like at least both numpy and pytorch have support for such tensors with dimension value 0.
If we decide to allow this in onnx, I think -1 is a better choice here.
neginraoof(2020-02-21 18:26:18):@linkerzhang 
Thanks for pointing the PR. I believe -1 is a good choice, specially if we decide to allow zero.
neginraoof(2020-02-25 01:49:34):Closing this for now. As suggested by @gramalingam, we can have a workaround in export.
gramalingam(2020-02-18 21:21:18):Why can't we use new_shape = [1, 3, 6] instead?
neginraoof(2020-02-18 21:43:51):This test is for the case when we have -1 in expand shape input.
gramalingam(2020-02-18 21:54:54):I was using this to illustrate my previous question: why do we need this extension of the op, if we can use "1" instead? Am I misunderstanding something?
neginraoof(2020-02-21 01:05:09):We do not always know the dim value we want to expand to.
Let's say you have a tensor of size (?, ?, 1) and you wan to expand to (?, ?, 5) where ? could be unknown.
gramalingam(2020-02-21 03:46:11):Then, you just specify the new-shape to be (1, 1, 5) ... if you apply this to a tensor of size (M,N,1), won't you end up with the shape (M, N, 5)?
neginraoof(2020-02-21 18:18:34):No,  you cannot expand (M, N, 1) tensor to (1, 1, K)
gramalingam(2020-02-21 19:17:25):Bidirectional broadcast between the shapes (1, 1, K) and (M, N, 1) will produce the shape (M, N, K). And I believe Expand (the existing version) does bidirectional-broadcast, as far as I can see. So, what am I missing?
gramalingam(2020-02-21 19:23:30):My understanding from the existing documentation is that ONNX Expand differs from numpy's broadcast_to in that ONNX uses bidirectional broadcast, unlike numpy.
neginraoof(2020-02-21 21:10:08):This is more of an export issue, since for exporting PT or TF models that allow -1, we need to change the shape input tensor.
gramalingam(2020-02-21 23:42:13):I see. So, the semantics of "-1" will be the same as that of "1" (in ONNX)? (Not sure if PT or TF have the exact same semantics for 1 ... it does look odd to have two ways of saying the same thing.)
vinitra-zz(2020-02-15 00:36:42):Thanks for the PR!
snnn(2020-02-15 00:49:46):BTW, this is why ONNX_USE_MSVC_STATIC_RUNTIME should be OFF when ONNX_USE_PROTOBUF_SHARED_LIBS is ON.

Each copy of the CRT library has its own heap manager, allocating memory in one CRT library and passing the pointer across a DLL boundary to be freed by a different copy of the CRT library is a potential cause for heap corruption.

So it will be dangerous when you pass a protobuf object between DLL boundaries and if the ownership of the object is transferred. 
https://docs.microsoft.com/en-us/cpp/c-runtime-library/potential-errors-passing-crt-objects-across-dll-boundaries?view=vs-2019
vinitra-zz(2020-02-15 00:36:20):Is this comment needed?
snnn(2020-02-15 00:37:06):Take a look at the old line. It's wrong. It shouldn't have the mark.
snnn(2020-02-15 00:37:57):With the  quotation mark, it won't work as expected.
neginraoof(2020-02-18 16:35:18):cc @houseroad @postrational for review.
BowenBao(2020-02-26 00:51:31):LGTM, please rebase with master, and remove the [WIP] tag. Thanks
gramalingam(2020-04-09 19:42:34):The broken circle-ci adds some complexity to PR merging process as there are questions of whether it is okay to merge the PR or not. Is anyone looking at the circle-ci failures? If not, it seems like a waste to have it.
gramalingam(2020-04-10 21:55:43):@houseroad , that's useful to know. Can you clarify which text-cases does this test (models, nodes)?
neginraoof(2020-04-28 18:51:53):@gramalingam @houseroad 
We are currently only running Caffe2 backend tests as part of CI (ORT tests are ignored).
Shall we decide which backend tests to keep? Also, do we want to keep test_operators, test_models, and test_onnx_opsets?
askhade(2020-05-29 22:15:50):> If we remove this CI, then we don't have a backend test to verify the ONNX testing data. Then it's very easy to introduce wrong testing data. I am okay to switch to ONNX runtime to replace PyTorch/Caffe2.

@houseroad : Can you please elaborate what exactly is not being tested?

@neginraoof : Can you restart this PR? Let's understand what testing is missing in azurepipelines  and try to add it so that we can remove circle ci
neginraoof(2020-06-01 17:17:56):I'm rebasing this. 
linkerzhang(2020-06-08 14:26:37):I agree with Lu that having one backend (either pytorch or onnxruntime) added into ONNX CI as monitoring/verifying the correctness of testing data is a good idea. It can also help backend (pytorch or onnxruntime) to know when there's a break, though they should not be "required" CI pass. 

@askhade @neginraoof  maybe instead of removing pytorch, onnxruntime should also be added here, but both of them are not "required" CI passes.
jcwchen(2020-06-24 18:51:30):Provide another viewpoint here:
I think at least the tests for old ops in [test_pytorch_onnx_caffe2](https://github.com/pytorch/pytorch/blob/9a3e16c773496b16e6c02f6e3e020be5bb485ea0/test/onnx/test_pytorch_onnx_caffe2.py#L2497) might be unnecessary for onnx-CI.
I am enabling shape inference check in https://github.com/onnx/onnx/pull/2783, but I bumped into some test failures for op7 and op8 tests. The reason is current onnx does not support the deprecated op: `ConstantFill` which these tests are using.
vinitra-zz(2020-02-15 00:37:49):Looks like the new debug mode CI build passed in Travis -- @linkerzhang?
linkerzhang(2020-02-16 07:46:22):> Looks like the new debug mode CI build passed in Travis -- @linkerzhang?

will you also cover it in windows ci? 
vinitra-zz(2020-02-18 21:06:52):> > Looks like the new debug mode CI build passed in Travis -- @linkerzhang?
> 
> will you also cover it in windows ci?

Added windows build into Appveyor and onnx-ml linux build into Travis.
vinitra-zz(2020-02-19 01:38:04):Seeing this error in the Debug build for Windows.
```
gtest.lib(gtest-all.obj) : error LNK2038: mismatch detected for '_ITERATOR_DEBUG_LEVEL': value '0' doesn't match value '2' in test_driver.obj [C:\projects\onnx\.setuptools-cmake-build\onnxifi_test_driver_gtests.vcxproj]

gtest.lib(gtest-all.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MD_DynamicRelease' doesn't match value 'MDd_DynamicDebug' in test_driver.obj [C:\projects\onnx\.setuptools-cmake-build\onnxifi_test_driver_gtests.vcxproj]
```
Looks like a discrepancy between debug and release builds in DLLs. Investigating.
postrational(2020-03-02 17:43:32):Looks like there are some merge artifacts on this branch currently.
bddppq(2020-02-15 05:50:56):Would it be better to directly use `DEBUG=1` (instead of `DEBUG_MODE`) here?
wschin(2020-02-18 00:36:49):`DEBUG` seems a very common environment variable... Maybe `ONNX_DEBUG`?
vinitra-zz(2020-02-18 19:25:13):My logic was along the lines of @wschin's comment -- I've renamed it to `ONNX_DEBUG`. 
TMVector(2020-02-17 09:19:32):This won't handle NaNs properly, so maybe the `(a > b or a == b)` form is better?
```python
import numpy as np
np.nan >= 1 # False
not (np.nan < 1) # True!
```
jeremycochoy(2020-02-17 12:02:23):> This won't handle NaNs properly, so maybe the `(a > b or a == b)` form is better?
> 
> ```python
> import numpy as np
> np.nan >= 1 # False
> not (np.nan < 1) # True!
> ```

Good point... I wanted to avoid this spelling to prevent two call to (Tensor,Tensor)->Tensor operators, but right now it is not equivalent to its definition (indeed pretty annoying that on floats `> or ==` is different from `not >`).

So if NaNs are considered part of the domain on which operators should be defined, we would have to either change the actual definition, or use the `(a > b or a == b)` implementation. (It also make me think that there is some inconsistencies in pytorch in this case ...)

Which choice is the best?
TMVector(2020-02-17 20:31:42):I think it's fine to have not-amazing performance for functions, since backends can implement kernels if they care. However, yes, you could use the `ContextDependentFunction` functionality to have a separate function body for int types and float types

A third option could be to not provide function bodies at all, to push backends to implement it natively, since it's a fairly primitive operation.
jeremycochoy(2020-02-18 09:04:17):> ContextDependentFunction

Not sure it is worth adding complexity only for integer models (I personally do not need it 😊). But If you remember one implementation using the ContextDependentFunction feature in the code base, I'd like to have a look just for curiosity.

I changed the graph to the `(<)OR(=)`  so that we align to Numpy's behavior.
TMVector(2020-02-18 11:05:39):The `ContextDependentFunction` feature was added in https://github.com/onnx/onnx/pull/2551/files#diff-11e5546ae75100fbca62f077a8b8a904 which also has an example of using it.
linkerzhang(2020-02-21 06:26:51):@jeremycochoy can you resolved the conflicts? 
linkerzhang(2020-02-25 00:10:50):@jeremycochoy do you mind to help resolving the conflicts again? thank you! New check-ins introduce more conflicts..... 
jeremycochoy(2020-02-25 10:05:51):> @jeremycochoy do you mind to help resolving the conflicts again? thank you! New check-ins introduce more conflicts.....

Merged the master and regenerated the documentation. It should be good (just waiting for ci to complete).
jeremycochoy(2020-02-18 10:20:20):😱 Something is wrong with the Celu backend tests. I will see if I can fix it. Sorry. 😞 
TMVector(2020-02-18 10:48:29):No worries @jeremycochoy, happens to everyone :)

I think the problem might actually be with the code which creates the expanded-functions test not handling ref attrs correctly[0]. I'll give fixing it a quick try now in this PR.

[0] https://github.com/onnx/onnx/blob/master/onnx/backend/test/case/node/__init__.py#L59-L63
jeremycochoy(2020-02-18 11:06:03):Thanks. :)

I am also wondering if the lines at https://github.com/onnx/onnx/blob/807c62cf7e4c96ce49040bcf073b7e4a054f28a5/onnx/defs/function.cc#L75-L77 may require some modification too ?
jeremycochoy(2020-02-18 11:51:35):One last mistake I found that should result in test failing:
https://github.com/onnx/onnx/blob/c978d102c9fe5b3609c6d8ff87e2ab41149edd42/onnx/backend/test/case/node/celu.py#L17-L21

The attribute `alpha=2` (should be `alpha=2.`) is not forwarded to the node, so there should be a mismatch between the backend computing with `alpha=1.` and numpy computing with `alpha=2.`.
linkerzhang(2020-02-20 06:13:03):please fix Linux ci failure.


+flake8
2388
./onnx/backend/test/case/node/celu.py:22:18: E251 unexpected spaces around keyword / parameter equals
2389
./onnx/backend/test/case/node/celu.py:22:20: E251 unexpected spaces around keyword / parameter equals
jeremycochoy(2020-02-20 09:20:45):Shouldn't it be alpha=2. for the tests to pass?
TMVector(2020-02-20 09:25:17):Hmm weird -- I must have somehow missed including the actual source when I fixed it in 65fc77f7a84a3dc15d26bfd70175c2ced7289984 :man_facepalming: Good catch!
linkerzhang(2020-02-23 09:20:55):@TMVector please resolve the conflicts.
wschin(2020-02-18 00:28:39):May we have one test for each added type?
wschin(2020-02-18 00:33:05):Can we make the information to construct type constraint an input of this function? For example,
```
std::function<void(OpSchema&)> ElementwiseMultiOpDocGenerator(
    const char* name,  const vector<string> allowed_types = {"tensor(float16)", "tensor(float)", "tensor(double)"})
```
or even
```
std::function<void(OpSchema&)> ElementwiseMultiOpDocGenerator(
    const char* name,  const vector<string> allowed_types)
```
Using a flag is less explicit than passing the right information directly.
TMVector(2020-02-18 13:56:44):I didn't do that because a [previous comment](https://github.com/onnx/onnx/pull/2532#issuecomment-571787425) on a different PR said there were too many tests, but I would very happily add one for each type.
wschin(2020-02-18 16:05:34):I don't feel that comment is valid. Any code path not tested can be broken. :)
TMVector(2020-02-18 23:05:19):I removed the type constraint from that function all together, I think that's clearest :)
askhade(2020-06-19 01:16:38):@pluradj : What is the value addition here for making axis as optional? Per the op schema axis is not optional and since multiple frameworks have different value for axis can we simply fix the documentation here to remove optional from the input description for this op? This way it is clear that axis is not an optional input?
pluradj(2020-06-19 15:02:09):I'm not the original author of the CumSum spec (https://github.com/onnx/onnx/pull/2030). The original author @jeffsaremi described `axis` as optional in the [description](https://github.com/onnx/onnx/blob/2c03cff041798ec23dfe419c6567c78db59d7234/onnx/defs/math/defs.cc#L1516) but didn't declare it as optional in the schema. I don't know what the correct intentions were in this situation, and I don't have a strong preference either way. So if the project committers would rather have this PR update the doc instead, I'd be happy to make that update. Just let me know.

askhade(2020-06-22 16:07:25):@pluradj , @postrational : In the version 11 of this operator even though the intention of the author was to make axis optional the schema was not defined in such a way. In this PR the schema is being changed to make axis optional and therefore I think this warrants a opset version bump. 

This is also one reason why I feel we should simply edit the description of the attribute to remove optional from the text.


postrational(2020-07-01 14:25:59):Well, we need to make a decision, either we bump the op version, or we just remove the "optional" wording.
We should remove the "optional" wording from the current version in either case to be consistent with schema.
askhade(2020-07-01 21:34:07):> Well, we need to make a decision, either we bump the op version, or we just remove the "optional" wording.
> We should remove the "optional" wording from the current version in either case to be consistent with schema.

I am leaning for removing optional wording because the schema will remain intact and therefore we don't need any version bump. I think we should avoid unnecessary version bumps this creates a lot of overhead for onnx backends.
pluradj(2020-07-10 20:05:24):Updated doc so `axis` is required, no breaking changes. @postrational @askhade Please review and let me know if you'd like any other changes before merging.
pluradj(2020-07-28 20:32:20):Is there anything else need to be updated to get this fix merged? Please let me know. Thanks.
postrational(2020-07-29 17:19:12):@ebarsoum Are we ready to merge this? If so, please do.
pluradj(2020-08-06 14:20:15):@postrational @ebarsoum @askhade Could somebody point me at the ONNX commit process? What does it take to get a PR through review, approval, and merge to master? Where can I find a list of ONNX committers? Perhaps I'm missing more information here to help move things along, but I don't see anything mentioned in the [CONTRIBUTING.md](https://github.com/onnx/onnx/blob/master/docs/CONTRIBUTING.md).

I'm a first time contributor to ONNX, and I'm getting discouraged from contributing more because this rather small unit test case fix is taking almost 6 months to get merged. Thanks.
gramalingam(2020-08-06 15:51:02):Yes, this is ready to be merged in. Unfortunately, we need to update the branch and wait for CI to complete.
pluradj(2020-08-06 18:07:42):Appreciate your help @gramalingam and thank you for getting this merged.

Any input on the questions I listed previously would go a long way in helping other contributors in the future. Thanks.

> Could somebody point me at the ONNX commit process? What does it take to get a PR through review, approval, and merge to master? Where can I find a list of ONNX committers? Perhaps I'm missing more information here to help move things along, but I don't see anything mentioned in the CONTRIBUTING.md.
postrational(2020-07-10 22:02:16):Why are we removing the array brackets here? 

According to numpy docs this argument should be an array or an object exposing the array interface:
https://numpy.org/doc/stable/reference/generated/numpy.array.html
pluradj(2020-07-11 13:46:12):The CumSum specification documentation states that `axis` is a 0-D tensor (scalar). We are removing the array brackets so `axis` isn't a 1-D tensor.

```
>>> axis = np.array([0]).astype(np.int32)
>>> axis.ndim
1
>>> axis = np.array(0).astype(np.int32)
>>> axis.ndim
0
```
pluradj(2020-07-11 13:52:09):numpy array scalars https://numpy.org/doc/stable/reference/arrays.scalars.html
wschin(2020-02-21 19:26:29):```suggestion
|training_info|TrainingInfoProto[]|An optional extension that contains information for training.|
```
wschin(2020-02-21 19:35:58):I think we need to say `the user should also invoke all TrainingInfoProto.algorithm sequentially` because, their order affects the training result.
neginraoof(2020-02-21 02:36:54):cc @postrational @houseroad for review
neginraoof(2020-02-24 18:27:33):cc @wschin @gramalingam @BowenBao @lara-hdr for review.
Thanks a lot.
neginraoof(2020-02-25 19:45:46):All tests are passing except for a timeout from:
 Xcode: xcode9.3 PYTHON_VERSION=python2

Check the details on how to adjust your build configuration on: https://docs.travis-ci.com/user/common-build-problems/#Build-times-out-because-no-output-was-received
The build has been terminated

I'm triggering the CI again.
linkerzhang(2020-02-26 00:07:35):Looks to me:
an updated “SpaceToDepth” (with more flexible block definition) + “Squeeze” is a better idea than adding this new op. Thoughts?
neginraoof(2020-02-27 00:38:03):@linkerzhang Thanks a lot for reviewing the PR.
I have realized that the SpaceToDepth op cannot be directly replaced by UnfoldToDepth operator, and we need to perform a combination of reshape/transpose ops on the output tensor to match the exact behavior. However, we also need the UnfoldToDepth operator since it supports the more general cases. I think the best idea is to change SpaceToDepth op to a function in the next release.
Please let me know what do you think.

BowenBao(2020-02-24 19:46:40):Please update reference implementation to include strides and dilation as well. (also default value of padding should be [0,0,0,0] instead for 2d cases)
BowenBao(2020-02-24 19:48:46):Same above, pads should have 4 values for 2d case
BowenBao(2020-02-24 19:54:07):I'd suggest adding an equation here explaining how the output is computed based on block_shape, dilations, pads and strides. And also how the output shape is determined by these attributes.
BowenBao(2020-02-24 21:17:34):Discussed off-line with Negin, will update the spec to be consistent on the number of dimensions of valid input.
BowenBao(2020-02-24 21:23:21):nit: tab
BowenBao(2020-02-24 21:30:00):Any unknown dimension size in input shape will cause the `last_dim_size` to be unknown. In that case, should we set the last dim of output_shape to be dim_param?
gramalingam(2020-02-25 01:17:28):I think this also needs a has_dim_value check: if it is not known, the corresponding output-dim should be left undefined.
neginraoof(2020-02-25 01:25:37):I've updated the implementation with padding and strides.
neginraoof(2020-02-25 02:05:30):Thanks,
I think we should add the dimension with unknown value in this case. Fixed this.
neginraoof(2020-02-25 06:58:15):I added explanations for input/output shapes.
BowenBao(2020-02-25 19:49:40):nit: update this to [D1, D2, ..., Dn] as well
linkerzhang(2020-02-25 23:50:22):This op looks to me is a tensor layout change op, which is similar with SpaceToDepth, so putting it into the same folder as "SpaceToDepth" makes more sense.
linkerzhang(2020-02-25 23:51:37):the "kernel_size" is confusing ... 
linkerzhang(2020-02-25 23:52:57):Pytorch name "unfold" looks to be better than this one, since it does not have to be an image, and it's indeed unfolding a window/block.
linkerzhang(2020-02-25 23:55:11):With this new op, can "SpaceToDepth" be replaced/deprecated? looks to me this one is more general.

To replace "SpaceToDepth", a little bit more needs to be updated, as "SpaceToDepth" is not squeezing the last 2 dimensions into one (while this op does). Maybe adding all parameters used to define the block into "SpaceToDepth" is better?
neginraoof(2020-02-26 00:51:45):Thanks @linkerzhang 
I talked about this with Spandan and Emad as well. So, as you mentioned, this operator is a the more general version of SapceToDepth (except the mode attribute is not supported). I think we can modify this operator's name and functionality further (maybe not flattening the last dimension, support for CRD/DCR re-arrangemen modes, etc.), and change SapceToDepth to a function that uses this operator internally.
However,  SpaceToDepth and DepthToSpace are currently complimentary ops, and I think it's better to keep them as such. We can add an operator equivalent for ColToImage (with a more general naming), and change both SpaceToDepth and DepthToSpace operators as functions.
neginraoof(2020-02-26 00:52:32):Yes, I agree. I will update the operator name.
linkerzhang(2020-02-26 01:46:12):do you have plan to do the move? (move SpaceToDepth to function).

I am also OK to deprecate SpaceToDepth if this OP can be changed to not merge the last two dimension (split them in to sub dimensions).

ONNX op has versioning, it's definitely OK to deprecate one as long as there's new one which can do the job @ebarsoum / @postrational  may comment more on this.
ebarsoum(2020-02-26 02:20:58):Based on our discussion, this work will move to 1.8. And we will update the existing SpaceToDepth and DepthToSpace to be more general.
linkerzhang(2020-02-26 08:17:18):great! confirm: so the plan is to update SpaceToDepth/DepthToSpace more general and move this one to function?
postrational(2020-02-26 14:55:55):Should we change the Milestone to 1.8?
linkerzhang(2020-02-26 23:51:25):the name needs to be updated too.
linkerzhang(2020-02-26 23:54:07):the input_shape size can't be less than 3?  given the first dimension is batch and second is channel as asked below.
linkerzhang(2020-02-26 23:58:06):using effective_block_shape is better here.
linkerzhang(2020-02-26 23:58:56):no "convolution". 
linkerzhang(2020-02-26 23:59:44):number of output dimension should be always 3, though the dim value should be calculated by the formula. Please clarify this in the description.
linkerzhang(2020-02-27 00:02:37):again, this op should be put in the "tensor" folder instead of "nn", it belongs to the same category as "SpaceToDepth", which is used for changing a tensor's layout.
linkerzhang(2020-02-27 07:41:01):typo? block_size -> num_blocks?
linkerzhang(2020-02-27 07:42:16):for image, I think uint8 should be supported, please check whether you want to have other numeric types support.
linkerzhang(2020-02-27 07:43:18):rename to "block_size" is better? as "block_size is used the other places.
linkerzhang(2020-02-27 07:44:38):you'd rename this file too.
linkerzhang(2020-02-27 23:37:06):please clarify the default value for "block_size", given it's optional.
linkerzhang(2020-02-28 05:52:36):@postrational I think 1.7 will take this one
shinh(2020-02-21 12:36:23):@wschin Let me send a notification, thanks!
wschin(2020-02-22 17:35:52):Some mac CI fails because a missing python package `enum`. I have rerun them. If that package is still missing, we need to fix it..
TMVector(2020-02-24 11:39:19):Maybe a check could be added to CI, similar to how (I believe) it checks docs are up to date
linkerzhang(2020-02-25 00:09:18):exactly, I like the idea on adding a check on the doc update, same as proto and operator docs. @TMVector  are you interested in it? :) 
TMVector(2020-02-26 00:08:24):Unfortunately generating these tests seems to expose issues with the definition of MeanSquareDistance and SoftmaxCrossEntropyLoss, plus with function expansion. I'm looking at fixes now.
TMVector(2020-02-26 00:49:50):TODO/question -- can we remove the Python function expansion helper and use the one from C++?
gramalingam(2020-02-27 00:46:18):The changes look good to me, thanks! I guess we can import the C++ version into Python (provided there is no subtle difference between them due to bugs, etc., I suppose). I guess that could be a different PR, given that we are close to the cutoff date for the next ONNX release.
jeremycochoy(2020-02-26 21:58:59):@linkerzhang  Excuse me for the interruption, if you have 1 or 2 minutes free could you have a very quick look just to let me know if I am heading in the right direction before I add the 3 remaining Ops ?
7oud(2020-02-29 03:33:50):@jeremycochoy I am also interested in FFT related ops. Expect your commit, I had a look the code commit in your branch, but I did not find the implementation of FFT in C++, maybe it will use cuFFT?
jeremycochoy(2020-03-03 16:08:28):> @jeremycochoy I am also interested in FFT related ops. Expect your commit, I had a look the code commit in your branch, but I did not find the implementation of FFT in C++, maybe it will use cuFFT?

@7oud This question (implementation detail) will (or already have) be addressed by the ONNX backends (onnruntime, tensorflow, pytorch, etc...). This PR will contain only the description of the operators (possible inputs and expected outputs).
7oud(2020-03-04 03:04:28):@jeremycochoy I am not very clear about ONNX. Based on your reply, I think that the ONNX is a architecture of model, which contains only op name and params, but not the implementation of ops. if I wanna run ONNX model, addtional backend(runtime) is needed, just like onnx-runtime, is it right? 
I have a  practical problem, my trained tensorflow model includes fft op, I wanna convert the model to ONNX, then use TensorRT to build the ONNX model to trt engine, and finally run the model in Tensorrt. However the fft op is unsupported in both ONNX and TensorRT now. If wanna achieve this work, what steps are need? Maybe includes

- **onnx** adds fft descriptions, just like what you did
- **onnx-tensorrt** (Tensorrt backend for onnx) adds fft implementation detail, maybe cuFFT
- **tensorflow-onnx**, adds parsers to convert tensorflow fft op to onnx fft op

Could you give some advice about it？

jeremycochoy(2020-03-04 21:38:10):> @jeremycochoy I am not very clear about ONNX. Based on your reply, I think that the ONNX is a architecture of model, which contains only op name and params, but not the implementation of ops. if I wanna run ONNX model, addtional backend(runtime) is needed, just like onnx-runtime, is it right?
> I have a practical problem, my trained tensorflow model includes fft op, I wanna convert the model to ONNX, then use TensorRT to build the ONNX model to trt engine, and finally run the model in Tensorrt. However the fft op is unsupported in both ONNX and TensorRT now. If wanna achieve this work, what steps are need? Maybe includes
> 
> * **onnx** adds fft descriptions, just like what you did
> * **onnx-tensorrt** (Tensorrt backend for onnx) adds fft implementation detail, maybe cuFFT
> * **tensorflow-onnx**, adds parsers to convert tensorflow fft op to onnx fft op
> 
> Could you give some advice about it？

In your case I would first make sure TensorRT can handle FFT in their model. Once it is the case, you can probably edit the model without FFT to had them by hand to the TensorRT model. This is a short term solution. The long term solution is :
* A ONNX op for fft/ifft.
* Add to onnx-tensorrt the support for writing models with FFT from ONNX graphs which contain them.
* Add convertion from tensorflow to the ONNX op.

Also, maybe the Issue related to the FFT/iFFT/RFFT/iRFFT ops would be better suited for this conversations. 🙂 
postrational(2021-01-20 18:03:42):@jeremycochoy We discussed this PR during the Operators SIG meeting today and we would like to move forward with this proposal.

We're wondering how we could expand the description of the operation and tests to make sure we don't run into edge cases which have different behaviors on implementations such as NumPy, PyTorch and Tensorflow.
diyessi(2021-01-25 21:55:34):FFT and iFFT are implementations of DFT/iDFT. I know TensorFlow and PyTorch called it "FFT" but that's not a good reason for ONNX to continue calling it the wrong name.
There are more DFT variants in Torch; there is also the related DCT (discrete cosine transform).
masahi(2021-04-20 22:30:41):Do we want to support real value input FFT? I don't know which of real/complex valued input is more common in practice, especially for deep learning use cases.
j-paulus(2021-04-21 06:19:22):@masahi I can only talk for my needs, but for many (most?) audio applications, a real-valued input to DFT is the main use scenario. Either as a stand-alone or as a part of STFT. If supporting real-valued inputs in addition to complex-valued ones allows optimizing the computational operations, this would be beneficial for the overall efficiency.
pfeatherstone(2021-09-08 08:26:05):Is there a timeline on when this will be supported?
garymm(2021-10-21 23:16:32):Should be closed since it's a subset of https://github.com/onnx/onnx/pull/3741?
askhade(2021-10-22 16:25:29):Closing this in favor of #3741 
napohou(2021-11-18 08:51:55):Hi @jeremycochoy Does your code of https://github.com/jeremycochoy/onnx contain FFT implementation? If so, which branch should we use, master or feature/fft-ifft?
gramalingam(2021-04-13 20:21:46):Statically unknown shapes are allowed during inference. Inference should not fail in these cases, but can leave the output shapes unspecified.
gramalingam(2021-04-13 20:22:19):See comment above. Inference should not fail under this condition.
gramalingam(2021-04-13 20:33:30):Specifying the number-of-dimensions as an attribute is uncommon for ONNX ops. Will this add any value, or can we omit it? It is okay, however, for the spec to restrict the number of dimensions (to 1, 2, or 3) if that is useful. But
gramalingam(2021-04-13 20:35:23):ONNX introduced complex-numbers as a type, though no ops yet exist to support them. Seems worthwhile to make a call on this now. We could use complex as a type-specification here. If not, may be we should omit the complex-type from ONNX, if we settle on using the more basic float types.
gramalingam(2021-04-13 20:38:05):May be worth clarifying that `a_m` is the input tensor (indexed) and that `A_k` is the output tensor
gramalingam(2021-04-13 20:48:29):The numpy version appears to have some extra attributes: output-length adjustment (relative to input-length), and a normalization mode. They also support real/complex inputs. We should also specify how the output is laid out (which index corresponds to positive frequency and which for negative frequency, etc.)
xadupre(2021-07-12 14:51:07):Supported complex would make it easier to convert models into ONNX. Otherwise, the converter needs to remember which tensor is complex. But having complex means also extend operator Cast to convert the complex tensor into real. But if FFT operator does not produce complex, removing complex everywhere would be less confusing.
xadupre(2021-07-12 14:53:59):For performance, adding operator RFFT would be great too. That would reduce the computation cost by 2.
lara-hdr(2020-02-26 22:56:31):@wschin @liqunfu @linkerzhang @ebarsoum 
Can we merge this asap and include it to the ONNX release to avoid having a bug in the ONNX doc?
thanks 
lgtm-com[bot](2020-02-29 01:49:33):This pull request **introduces 1 alert** when merging 6b73f09412fae94fdb9406f53d33617863265dad into e8b33a5a2dff4e20296ecf468eef1dac0c82610a - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-84061e163ce99dda923919f2652c637424c486a2)

**new alerts:**

* 1 for Unused import
chinhuang007(2020-03-02 23:32:59):Just installed a wheel (win64 + py38) successfully. I think this should be included in release 1.7.
linkerzhang(2020-03-03 00:34:36):Thank you so much! @snnn 

Quick questions: does this mean we'll need wheel-builder for Linux packages and this for windows packages? if yes, shall we put them in one place please? 
snnn(2020-03-03 00:38:39):> Quick questions: does this mean we'll need wheel-builder for Linux packages and this for windows packages? if yes, shall we put them in one place please?

That will be better. But it will take me a few days more and I think currently the top priority thing is the 1.7 release and retiring appveyor.


linkerzhang(2020-03-03 01:19:28):I'm merging it.

@chinhuang007 please kindly document this as part of your updated release experience, so that it will help the next release manager. Hopefully, we'll fix these issues before that though. :)
chinhuang007(2020-03-03 01:34:29):@linkerzhang Sure, I will certainly document the steps we are going through for 1.7 release. It should be smoother next time!
chinhuang007(2020-03-03 17:55:46):@snnn Thanks for providing the solution! I don't see anything in Actions for the release workflow, the workflow or the files. Do I need certain permissions?
snnn(2020-03-03 18:06:16):Please try to create a rel-1.7.0 branch. The action only works on release branches. 
chinhuang007(2020-03-05 02:53:15):@snnn Thanks for the explanation. I will create a rel-1.7.0 branch tomorrow and verify.
chinhuang007(2020-03-10 17:58:52):@snnn I created the release branch last night and saw the workflow in Actions. But I don't know where to find Windows wheels, as you shared in your branch. Please help clarify if I missed anything?
snnn(2020-03-10 18:01:29):https://github.com/onnx/onnx/suites/510022829/artifacts/2715527
chinhuang007(2020-03-10 18:33:47):Thanks for providing the zip for Windows wheels. The next question is how can I generate and see the zip myself when something happens to the release branch (for ex. I am merging a couple of PRs from master to rel-1.7.0)? Should I also ask you to provide the file next time?
snnn(2020-03-10 20:11:51):See:

![image](https://user-images.githubusercontent.com/856316/76354987-ad64b780-62d0-11ea-9d9b-8067e401c870.png)

chinhuang007(2020-03-10 21:45:18):Ah, somehow I didn't get to see this before. Thanks for pointing it out.
postrational(2020-03-02 17:35:03):If we are excluding these tests in `onnx/test/test_backend_test.py`, do we need to check for `platform.architecture` here as well?
snnn(2020-03-02 17:54:44):Probably not. Thank you! I reverted this change.
postrational(2020-03-02 18:54:58):> But the spec says all the data must be little-endian on the disk.

Would checking for `system_byteorder` in `numpy_helper.to_array` interfere with that?


imaihal(2020-03-03 02:22:58):I found comments which says `raw_data` of `TensorProto` should be stored in little-endian order.
https://github.com/onnx/onnx/blob/3368834cf0b1f0ab9838cf6bdf78a27299d08187/onnx/onnx.in.proto#L538-L539

On big-endian machines, converting big to little before storing in `TensorProto`, and little to big in `numpy_helper.to_array()` works well?
postrational(2020-03-04 16:17:23):> On big-endian machines, converting big to little before storing in `TensorProto`, and little to big in `numpy_helper.to_array()` works well?

I would say that it's critical that models persisted to disk always maintain one byte order (little-endian). The tools we use to load them into memory may be adaptable, so using `numpy_helper.to_array()` for this seems logical to me.

@linkerzhang What do you think?
imaihal(2020-03-05 01:15:46):OK. I will modify `numpy_helper` to do it. 
I assume `tensor.raw_data` in `numpy_helper.to_array()`  is always little endian.
imaihal(2020-03-06 05:17:21):> But the spec says all the data must be little-endian on the disk.

> models persisted to disk always maintain one byte order (little-endian)

@postrational I couldn't find the description about byte order in the spec. Could you please tell me where they are described?  
CLAassistant(2020-03-06 06:55:31):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=2633) <br/>All committers have signed the CLA.
lgtm-com[bot](2020-03-06 07:05:34):This pull request **introduces 1 alert** when merging 3fdefa0bd3206c3715426ff5d005a5ceed4998d5 into 4cd2538df044d7637492ec5635bf47a3057749e9 - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-01fb30b3eb9bdb5018c75e54a37a895872d12553)

**new alerts:**

* 1 for Unused import
imaihal(2020-03-09 03:23:16):I updated `numpy_helper.to_array()` and `numpy_helper.from_array()`. This PR assumes `TensorProto.raw_data` in the inputs of `to_array()` and in the outputs of `from_array()` are always little endian even on big-endian machine.


imaihal(2020-03-17 00:37:15):Could @postrational, @linkerzhang, or someone review my PR?
linkerzhang(2020-04-12 11:54:36):@postrational any more comments?
imaihal(2020-05-11 07:13:55):@postrational Do you have any comments? 
postrational(2020-03-02 17:27:02):This changes the test runner, but not the underlying helper method.
With this change tests may pass, but the effect may be misleading.

Should the `byteswap()` call be moved into `numpy_helper.to_array` itself?
postrational(2020-03-02 17:28:46):If we did add the `byteswap` workaround to `numpy_helper.to_array`, would we need this increasingly complex condition here?

This condition is growing complex. It would be a good idea to add a small, well named helper function such as `_get_system_byteorder_supported()` and move the complexity there.
imaihal(2020-03-03 01:25:04):Thanks for the comments! 
At first I thought it might be better to do it, but I was not sure if it is always ok to byteswap the array in `numpy_helper.to_array()` . 
imaihal(2020-03-17 00:32:35):I removed this condition check because we can say "numpy helper is available on big-endian system" by adding endian conversion in numpy helper.
linkerzhang(2020-03-03 01:17:30):Can you sign the CLA? @xuhdev 
CLAassistant(2020-03-03 20:22:47):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=2637) <br/>All committers have signed the CLA.
chinhuang007(2020-03-04 01:15:43):I think we probably should add opset version for ai.onnx.training into docs/Versioning.md. If so, please advise whether to add it to the same table (the width might not look good with the addition) and what to put, like a dash '-'?, for previous versions that don't support training.

Please also comment on whether we should make this change in the to-be-created release branch and later merge back to the master after release is out.
kevinch-nv(2020-03-05 00:41:18):Looks good. Make sure that all 1.7 PRs are in before merging this and branching out, thanks for handling the release Chin!
chinhuang007(2020-03-05 00:46:58):@linkerzhang Can you please help review and approve? I'd like to create the release branch after this merged. Thanks!
linkerzhang(2020-03-06 08:30:11):Hmmm, merged too quickly, @take-cheeze the test data should also re-generated (updated).
postrational(2020-03-04 13:19:31):Thanks for catching this. Please sign the CLA, if you haven't yet.
take-cheeze(2020-03-05 08:27:19):I don't know why CLA check isn't working...
Other PR is OK: https://github.com/onnx/onnx/pull/2640
linkerzhang(2020-03-06 08:28:26):Indeed a good catch! 

One more question later is how to avoid this kind of bugs - validate function body's correctness. @jeremycochoy 

@take-cheeze can you create a new PR to work around the CLA issue please? this PR has to be merged before the 1.7 release - which is happening now.
linkerzhang(2020-03-06 08:28:45):@chinhuang007 
jeremycochoy(2020-03-06 08:56:46):> Indeed a good catch!
> 
> One more question later is how to avoid this kind of bugs - validate function body's correctness. @jeremycochoy
> 
> @take-cheeze can you create a new PR to work around the CLA issue please? this PR has to be merged before the 1.7 release - which is happening now.

Ouch. 🤦‍♂ 

Ideally, you'd like to "run" the function using an existing runtime and check against the function tests to confirm consistency between definition and implementation. But it my require a lots of work to setup a CI running a ONNX backend and executing tests...
lgtm-com[bot](2020-03-04 18:13:43):This pull request **introduces 2 alerts** and **fixes 1** when merging 452a4ef146a08cc59c1ac88ee742d7695a9c002f into 4cd2538df044d7637492ec5635bf47a3057749e9 - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-24a522b9ca9bbeb169207557690e18bf5c48d2fa)

**new alerts:**

* 1 for Syntax error
* 1 for Variable defined multiple times

**fixed alerts:**

* 1 for Module is imported with &#39;import&#39; and &#39;import from&#39;
CLAassistant(2020-03-05 09:40:09):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=2643) <br/>All committers have signed the CLA.
pranavsharma(2020-03-05 23:05:01):cc @linkerzhang 
pranavsharma(2020-03-05 23:05:16):Can this make it to 1.7 release?
pranavsharma(2020-03-06 07:28:31):Is there any existing issue going on with CircleCI? I see that ci/circleci: py3.6-clang7-ubuntu16.04 is failing for other PRs too that were a day or so old.
linkerzhang(2020-03-09 01:56:48):circleci is not a required ci for onnx, which may be ignored. @spandantiwari for the awareness of the failure, btw.
pranavsharma(2020-03-18 02:37:43):Any more comments that need to be addressed here?
gramalingam(2020-03-26 03:55:41):Unfortunately, there appears to be some conflicts to be resolved.
pranavsharma(2020-03-28 05:02:52):Can this be merged once CI is complete?
snnn(2020-04-21 04:52:55):This change broke raspberrypi build. 
snnn(2020-05-28 17:15:45):This change also break the build on ubuntu 14.04.

See: https://github.com/microsoft/onnxruntime/issues/4048
pranavsharma(2020-03-05 09:43:44):The addition of the output from GenerateBroadcastingDocMul() inhibits the linker from throwing away BitShift_ver11_doc. Hence the introduction of the function GetBitShiftDoc().
pranavsharma(2020-03-05 09:46:04):This kind of setup of the doc doesn't help in removing these strings from the binary. Hence the check for __ONNX_NO_DOC_STRINGS here.
pranavsharma(2020-03-05 09:47:18):Changed to const-ref. No need to pay for the copy of doc/description if we're not going to copy it. Ditto for Input() and Output() functions.
linkerzhang(2020-03-09 02:00:47):suggestion: create a macro for this pattern, so that all later added ones can reuse the pattern and we don't need to revisit this issue any more.
pranavsharma(2020-03-09 07:21:12):Currently there are 3 ways SetDoc is accomplished.
* [This](https://github.com/onnx/onnx/blob/master/onnx/defs/generator/defs.cc#L19) is easy to eliminate since it is taken care of in the SetDoc function itself. 
* [This](https://github.com/onnx/onnx/blob/master/onnx/defs/logical/defs.cc#L175) can be hidden behind a generic macro as follows:
```
.SetDoc(GET_OP_DOC_STR(std::string(BitShift_ver11_doc) + GenerateBroadcastingDocMul())

#ifndef __ONNX_NO_DOC_STRINGS
#define GET_OP_DOC_STR(doc_str) (doc_str)
#else
#define GET_OP_DOC_STR(doc_str) ("")
#endif
```
* Finally the third way is [this](https://github.com/onnx/onnx/blob/master/onnx/defs/logical/defs.cc#L28). This is a bit tricky to do in a generic way due to the way doc is constructed. Having the ```ifndef __NO_DOC_STRINGS``` inside SetDoc is not enough in this case. Surrounding SetDoc with this macro also doesn't work since it's too late. Once the 'doc' variable is used, the linker can't throw it away in the generated binary. I don't have a good solution for this one right now. Some more thought might be required.

I can change the 2nd one behind a generic macro.
linkerzhang(2020-03-09 23:32:10):how do you think of adding macro on the declaration/definition of this "replaceall" function? so that in the case of "__ONNX_NO_DOC_STRING" defined, there's no such function for developers to call. if it's still called, there will be compilation error reminding that they should not call it.
linkerzhang(2020-03-09 23:32:54):I was thinking how to avoid people keep adding more such descriptions and we'll keep doing this job of removing them :)
linkerzhang(2020-03-09 23:34:32):maybe, this mechanism should be done on the "SetDoc" API, which is more obvious and stable, as ReplaceAll is just a utility/helper function.
pranavsharma(2020-03-10 00:42:45):Yeah, I'm not super happy with this ReplaceAll setup either. We would rather not visit this issue every time :). I'll think of something. The ReplaceAll setup is also not uniform in the way the replacement is done.  
pranavsharma(2020-03-12 07:03:31):Updated.
TMVector(2020-03-06 10:09:50):I think only the files related to the operators you've updated should be changed -- it seems to have been the policy to keep old files (the only change for everything else is (should be) opset/IR version numbers).

Thanks for fixing my mistake btw :slightly_smiling_face: 
linkerzhang(2020-03-09 01:50:17):@jeremycochoy can you also help to fix the test data of these two functions please? I believe the test data checked in was also wrong because of the bug fixed by this PR.
linkerzhang(2020-03-09 01:55:03):@take-cheeze you may want to update the test data accordingly if @jeremycochoy can't offer help in time.

The function test data's generation is,
when adding a test case for a function op, there will be two sets of test data generated, with same inputs/outputs, but different models, one model is with the function op as it is in the model, the other model is having the function body graph inlineed. This is also ensuring the function body graph is defined correctly.
take-cheeze(2020-03-09 02:06:52):Sorry to forget the test data update. 
https://github.com/onnx/onnx/pull/2645/commits/7b4897daecb7f6cc9b139745078a1ef04293865d should fix it
jeremycochoy(2020-03-09 18:35:54):Seams like I read this a bit too late. 😅
vinitra-zz(2020-03-18 23:56:03):@linkerzhang, can you take a look?
gramalingam(2020-03-26 01:02:39):LGTM, thanks! I have one question though: how do we decide which combinations are worth adding CI for? There isn't a ONNX_DEBUG version of all combinations. It makes sense, as doubling the number of CIs would probably be too much, but wondering.
vinitra-zz(2020-03-26 18:46:17):> LGTM, thanks! I have one question though: how do we decide which combinations are worth adding CI for? There isn't a ONNX_DEBUG version of all combinations. It makes sense, as doubling the number of CIs would probably be too much, but wondering.

We chose one version with ONNX ML enabled, one version with it disabled. We also chose 2 OS types to vary the format and catch more errors (Linux / Mac). We were thinking of also including a Windows build, but found some errors with the ONNX Debug package on Windows that need to be resolved. These are being addressed in a separate PR: https://github.com/onnx/onnx/pull/2605

Thanks for the approval!
chinhuang007(2020-03-11 00:30:53):Close because not needed
chinhuang007(2020-03-12 21:34:57):Please ignore for now. Some issues are found during testing. Will need to fix them before going back to the official release version number.
linkerzhang(2020-03-17 15:27:52):adding @snnn for getting back to this PR with test status.
snnn(2020-03-17 15:33:29):No progress now. 
linkerzhang(2020-03-22 13:27:01):@snnn @faxu would you help to update the status please?
faxu(2020-03-23 16:13:02):The main issue is in SoftmaxCrossEntropyLoss - @codemzs is working on it and should have a fix validated this week. There are a few other minor issues that will also be fixed in the next couple days. 
chinhuang007(2020-03-27 05:00:49):@snnn @codemzs I believe we are waiting for https://github.com/onnx/onnx/pull/2680 to be approved and merged. If so, please address the comments by reviewers there. Thanks for your hard work!!
codemzs(2020-03-27 18:07:40):@chinhuang007 We plan to merge that PR by 3/30, thank you for your patience. 
chinhuang007(2020-03-27 18:47:13):@codemzs Thanks for the update. That seems to further delay the projected completion of bug fixes (by the end of this week) and the release of 3/31. The PR was approved 20 minutes ago. So @linkerzhang @snnn could you help to merge it? I'd like to start adding new PRs to the release branch and publish a candidate to TestPypi for a final test ASAP. Thanks everyone for your help!
codemzs(2020-03-27 18:49:49):@chinhuang007 The date for completion was set to 3/30 last week, what date was communicated to you?
codemzs(2020-03-27 18:52:25):@linkerzhang @snnn Please do not merge the PR without consulting me. 
chinhuang007(2020-03-27 20:24:01):@codemzs The end of week estimate is coming in a note from Prasanth cced @faxu.
@prasanthpul This is to bring to your attention. If the bug fix completion is 3/30, as @codemzs stated above, I doubt there will be enough time to go through this PR, run the builder, conduct final test, and publish release 1.7 out to Pypi on 3/31.
codemzs(2020-03-27 20:25:26):@chinhuang007 Seems like there was a miscommunication but we can get this in today afternoon. What is the latest you would like this in? 
chinhuang007(2020-03-27 20:27:31):To keep everything transparent, I already updated and announced the projected release schedule on gitter, https://gitter.im/onnx/Infra. I really think we should use it for better communication since the infra SIG owns the release process.
codemzs(2020-03-27 20:29:59):@chinhuang007 I will keep this in mind for next time and will try to communicate better. While this is not an excuse but I'm fairly new to the project and still learning how releases happen. Can you please let me know what is the latest you would want this PR merged so that we can release on as promised on 03/31?
chinhuang007(2020-03-27 20:53:00):@codemzs I am fairly new to the ONNX release process as well and really appreciate all the developers jumping in to help address critical issues. I would hope to see all release 1.7 PRs merged by this weekend. 
@linkerzhang @snnn  would you please confirm whenever we are ready for the release? I will wait for that signal to update this PR to start the final round of release verification. Thanks!
codemzs(2020-03-27 20:54:58):@chinhuang007 Thanks for the flexibility! My goal is to get this in today if possible, just validating the results to be sure what we check-in is actually useful. The spec as you see already looks good. 
snnn(2020-03-27 21:38:58):@gramalingam Do you have any comments? 
gramalingam(2020-03-27 22:08:15):Hi @snnn : do you mean comments on the other PR (softmaxcrossentropyloss)? I am fine with it, I thought it had enough approvals already.
snnn(2020-03-28 04:34:41):Is there a release candidate commit that ONNX Runtime team can use for testing?
codemzs(2020-03-28 05:06:19):@snnn https://github.com/codemzs/onnx/tree/softmaxcrossentropyloss but need to address few minor comments before check-in this weekend. 

P.S: I also have this hooked up with my private ORT for testing SoftmaxCrossEntropyLoss kernel changes.
codemzs(2020-03-30 10:01:50):@chinhuang007 #2680 is ready to be merged and shipped with ONNX 1.7. One of the sig op approves will merge it. @wschin FYI
chinhuang007(2020-03-30 16:04:41):@codemzs Thank you for taking care of this issue!

I just merged #2680 since it already has enough approvals.

@faxu @linkerzhang @snnn @prasanthpul 
I will reset the release branch, rel-1.7.0 to catch up to this commit. That means other critical PRs will be included as well. Please let me know if any objections. 

There is a discussion in infra gitter about IR version change, https://gitter.im/onnx/Infra. I am not sure if we need an IR change for release 1.7.0. Assuming no, I will create a new PR to reset the version number, 1.7.103 for the final release verification next.

linkerzhang(2020-04-04 02:32:29):@chinhuang007 About IR version, I tried to clarify things in gitter room https://gitter.im/onnx/Infra. Let's see if there're different thoughts please, if no, the IR version should be bumped. Thank you!
CLAassistant(2020-03-17 03:21:17):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=2661) <br/>All committers have signed the CLA.
CLAassistant(2020-03-17 16:26:06):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=2664) <br/>All committers have signed the CLA.
matteosal(2020-03-17 16:43:09):I have tested these changes using the adapter test suite and extra upgrade tests I wrote for myself.

My extra tests consisted in creating model files for all the operators involved, creating them at the lowest possible opset version (attaching the script I used to create them). I then ran the version converter on each of them converting to version 12, to ensure that the full stack of adapters was available for all the operators. I have focused on version upgrades because I felt they are much more critical, but a similar thing can be done with version downgrades. 

Are you interested in adding something like this to the test suite?

[gen_upgrade_test_data.zip](https://github.com/onnx/onnx/files/4344602/gen_upgrade_test_data.zip)
matteosal(2020-05-06 13:13:25):Any feedback on this?
vinitra-zz(2020-05-06 22:22:46):Thanks for the contribution, @matteosal! cc: @linkerzhang, @KsenijaS to take a look
matteosal(2020-06-04 16:02:56):Another ping on this :)
matteosal(2020-06-30 15:44:14):@vinitra @linkerzhang @KsenijaS any news on this?
jcwchen(2020-07-08 17:34:51):The adapter of `Clip` and `Dropout` looks good to me.
@matteosal Thank you for the hard work!
matteosal(2020-07-13 09:36:23):Is there something missing still? What about the tests I suggested [here](https://github.com/onnx/onnx/pull/2664#issuecomment-600175746)?
matteosal(2020-07-21 09:02:39):@jcwchen @askhade  a ping on this :)
What about the tests I've suggested [here](https://github.com/onnx/onnx/pull/2664#issuecomment-600175746)?

jcwchen(2020-07-24 14:57:49):Hi @matteosal Sorry for the late reply.
I checked your test file. Is it something like encapsulating the helper function?
IMO, to test the version_converter, maybe we can simply follow the original test [here](https://github.com/onnx/onnx/blob/master/onnx/test/version_converter_test.py).
It seems unnecessary to save the created model.

Thanks.
matteosal(2020-07-24 16:01:28):> Is it something like encapsulating the helper function?

Sorry I don't undarstand this

> IMO, to test the version_converter, maybe we can simply follow the original test [here](https://github.com/onnx/onnx/blob/master/onnx/test/version_converter_test.py).
> It seems unnecessary to save the created model.
> 
> Thanks.

Ok fine, saving the model is not necessary, but I think the testing philosophy can be improved by automatically picking the highest possible opset version for upgrades (and the lowest possible for downgrades). 

The current tests are designed to test conversions between a fixed paier of opset versions (e.g, `Sum Adapter: 5 -> 8` and `Sum Adapter: 8 -> 5`). This forces developers to continously add tests as operators are changed. On the other hand, if all tests used the current opset version as the target version the number additional tests would decrease (at least for upgrading). E.g. `Sum 5 -> 8`, is only testing a single adapter, but if the current opset version is picked instead of 8 (so currently v13), ALL the adapters for `Sum` will be tested automatically without having to add new. 

This mechanism is not perfect because it doesn't guarantee that all the upgared codepaths will be tested, so one might still have to add new tests once in a while, but I think it's still an improvement
jcwchen(2020-07-24 18:09:53):> Sorry I don't understand this

According to the file you provided, I thought the file is only for simplifying the process of graph creation.

> This mechanism is not perfect because it doesn't guarantee that all the upgraded code paths will be tested, so one might still have to add new tests once in a while, but I think it's still an improvement

Thank you for the elaboration. I think I finally get this.
The idea looks good to me. Do you have any other existing test code which can demo your idea?
It would help us to understand your proposal. Thank you!
askhade(2020-07-27 16:40:00):> > Is it something like encapsulating the helper function?
> 
> Sorry I don't undarstand this
> 
> > IMO, to test the version_converter, maybe we can simply follow the original test [here](https://github.com/onnx/onnx/blob/master/onnx/test/version_converter_test.py).
> > It seems unnecessary to save the created model.
> > Thanks.
> 
> Ok fine, saving the model is not necessary, but I think the testing philosophy can be improved by automatically picking the highest possible opset version for upgrades (and the lowest possible for downgrades).
> 
> The current tests are designed to test conversions between a fixed paier of opset versions (e.g, `Sum Adapter: 5 -> 8` and `Sum Adapter: 8 -> 5`). This forces developers to continously add tests as operators are changed. On the other hand, if all tests used the current opset version as the target version the number additional tests would decrease (at least for upgrading). E.g. `Sum 5 -> 8`, is only testing a single adapter, but if the current opset version is picked instead of 8 (so currently v13), ALL the adapters for `Sum` will be tested automatically without having to add new.
> 
> This mechanism is not perfect because it doesn't guarantee that all the upgared codepaths will be tested, so one might still have to add new tests once in a while, but I think it's still an improvement


Agree with the overall idea... Can you create a separate PR for this. Thanks!
matteosal(2020-07-27 19:17:16):> > > Is it something like encapsulating the helper function?
> > 
> > 
> > Sorry I don't undarstand this
> > > IMO, to test the version_converter, maybe we can simply follow the original test [here](https://github.com/onnx/onnx/blob/master/onnx/test/version_converter_test.py).
> > > It seems unnecessary to save the created model.
> > > Thanks.
> > 
> > 
> > Ok fine, saving the model is not necessary, but I think the testing philosophy can be improved by automatically picking the highest possible opset version for upgrades (and the lowest possible for downgrades).
> > The current tests are designed to test conversions between a fixed paier of opset versions (e.g, `Sum Adapter: 5 -> 8` and `Sum Adapter: 8 -> 5`). This forces developers to continously add tests as operators are changed. On the other hand, if all tests used the current opset version as the target version the number additional tests would decrease (at least for upgrading). E.g. `Sum 5 -> 8`, is only testing a single adapter, but if the current opset version is picked instead of 8 (so currently v13), ALL the adapters for `Sum` will be tested automatically without having to add new.
> > This mechanism is not perfect because it doesn't guarantee that all the upgared codepaths will be tested, so one might still have to add new tests once in a while, but I think it's still an improvement
> 
> Agree with the overall idea... Can you create a separate PR for this. Thanks!

Ok I will, although not in the near future
askhade(2020-07-28 16:54:27):> > > > Is it something like encapsulating the helper function?
> > > 
> > > 
> > > Sorry I don't undarstand this
> > > > IMO, to test the version_converter, maybe we can simply follow the original test [here](https://github.com/onnx/onnx/blob/master/onnx/test/version_converter_test.py).
> > > > It seems unnecessary to save the created model.
> > > > Thanks.
> > > 
> > > 
> > > Ok fine, saving the model is not necessary, but I think the testing philosophy can be improved by automatically picking the highest possible opset version for upgrades (and the lowest possible for downgrades).
> > > The current tests are designed to test conversions between a fixed paier of opset versions (e.g, `Sum Adapter: 5 -> 8` and `Sum Adapter: 8 -> 5`). This forces developers to continously add tests as operators are changed. On the other hand, if all tests used the current opset version as the target version the number additional tests would decrease (at least for upgrading). E.g. `Sum 5 -> 8`, is only testing a single adapter, but if the current opset version is picked instead of 8 (so currently v13), ALL the adapters for `Sum` will be tested automatically without having to add new.
> > > This mechanism is not perfect because it doesn't guarantee that all the upgared codepaths will be tested, so one might still have to add new tests once in a while, but I think it's still an improvement
> > 
> > 
> > Agree with the overall idea... Can you create a separate PR for this. Thanks!
> 
> Ok I will, although not in the near future

Can you create an issue with these details to track this work. This way in future someone else who has bandwidth can also pick this workitem. Thanks!
askhade(2020-07-28 17:17:31):LGTM. Once the test failures are fixed I will approve the PR.
Thanks!
matteosal(2020-07-29 09:43:26):> This mechanism is not perfect because it doesn't guarantee that all the upgared codepaths will be tested, so one might still have to add new tests once in a while, but I think it's still an improvement

Done https://github.com/onnx/onnx/issues/2931
matteosal(2020-07-29 10:19:32):@askhade some CI test is failing reporting errors about some ipynb file:
```
==================================== ERRORS ====================================
________________________ ERROR collecting test session _________________________
../../../.conda/envs/py3.6/lib/python3.6/site-packages/pluggy/hooks.py:286: in __call__
    return self._hookexec(self, self.get_hookimpls(), kwargs)
../../../.conda/envs/py3.6/lib/python3.6/site-packages/pluggy/manager.py:93: in _hookexec
    return self._inner_hookexec(hook, methods, kwargs)
../../../.conda/envs/py3.6/lib/python3.6/site-packages/pluggy/manager.py:87: in <lambda>
    firstresult=hook.spec.opts.get("firstresult") if hook.spec else False,
../../../.conda/envs/py3.6/lib/python3.6/site-packages/nbval/plugin.py:115: in pytest_collect_file
    return IPyNbFile(path, parent)
../../../.conda/envs/py3.6/lib/python3.6/site-packages/_pytest/nodes.py:95: in __call__
    warnings.warn(NODE_USE_FROM_PARENT.format(name=self.__name__), stacklevel=2)
E   pytest.PytestDeprecationWarning: Direct construction of IPyNbFile has been deprecated, please use IPyNbFile.from_parent.
E   See https://docs.pytest.org/en/stable/deprecations.html#node-construction-changed-to-node-from-parent for more details.
=========================== short test summary info ============================
ERROR  - pytest.PytestDeprecationWarning: Direct construction of IPyNbFile ha...
!!!!!!!!!!!!!!!!!!!! Interrupted: 1 error during collection !!!!!!!!!!!!!!!!!!!!
=============================== 1 error in 0.16s ===============================
```

I am not familiar with CI internals, but this really seems unrelated to my changes. I don't know what action should be taken.
jcwchen(2020-07-29 14:31:02):@matteosal
It only fails with python3 and you did not change anything on the python side. 
I think the ipynb failure should be not related to your PR. Sorry for the confusion.
https://github.com/computationalmodelling/nbval/issues/139 hopefully this library will fix this soon.
matteosal(2020-08-03 13:29:30):@askhade can this be merged now?
matteosal(2020-03-17 16:29:16):Looks like the intended version number was `target` here. 

In the change below, I found it clearer to reword the message in that way.
matteosal(2020-03-17 16:31:55):I went looking for the old broadcast behaviour [here](https://github.com/onnx/onnx/blob/master/docs/Changelog.md#Add-6), which states that dimensions could also be added to the left of the shape list. But the previous adapter was only adding them to the right.
matteosal(2020-03-17 16:33:55):I'm not sure the converters for `6 -> 5` below this comment are correct. I followed what was done for `"Add"`.
matteosal(2020-03-17 16:35:46):The three adapters for `"Max"`, `"Min"` and `"Mean"` here were pre-existing as `BroadcastBackwardCompatibility`, but it really looks they should be `CompatibleAdapter`. What is the right adapter?
jcwchen(2020-07-07 00:23:02):According to https://github.com/onnx/onnx/blob/master/docs/Changelog.md#Mean-8,
(same as Max-8 and Min-8), they started to support broadcasting from opset8. 
In that case, using `BroadcastBackwardCompatibility` here makes more sense to me.
matteosal(2020-07-07 09:37:17):Indeed. Removing the comment then
jcwchen(2020-07-07 17:34:44):Good catch
jcwchen(2020-07-07 18:52:07):I guess the reason is the old broadcast behaviour (opset 6) only supports limited broadcast (only adding dimensions to the right one). 

If the op is upgraded to opset 7, it will allow Multi-directional Broadcasting. Therefore, the version_converter needs to do Unidirectional Broadcast (only adding dimensions to the right one) for opset 6 first. Otherwise, opset 7 would misunderstand how to broadcast these old ops. 

IMO, adding dimensions to the left here looks unnecessary to me.
jcwchen(2020-07-07 19:45:36):`Add` and `Mul` started to support `int32`, `int64`, `uint32`, `uint64` since opset 6 because of supporting broadcast.
I think most of other ops like `Sum` and `Ceil`, which do not support broadcast in opset 6, do not need to use `TypeRestriction` here.
matteosal(2020-07-08 09:14:24):By "left" and "right" I don't mean the left and right inputs of the node, but left and right of the dimension list of the right (second) input, i.e. prepending (left) and appending (right) dimensions. What I added is still adding dimensions to B only, but allows adding them on both sides of the dimension list. Previously, this adapter was only adding them to the right (i.e. appending them, via `axes.emplace_back(B_sizes.size() + i)`).

The examples in the changelog I linked are both prepending and appending dimensions:
```
shape(A) = (2, 3, 4, 5), shape(B) = (,), i.e. B is a scalar tensor
shape(A) = (2, 3, 4, 5), shape(B) = (1, 1), i.e. B is an 1-element tensor
shape(A) = (2, 3, 4, 5), shape(B) = (5,)
shape(A) = (2, 3, 4, 5), shape(B) = (4, 5)
shape(A) = (2, 3, 4, 5), shape(B) = (3, 4), with axis=1
shape(A) = (2, 3, 4, 5), shape(B) = (2), with axis=0
```
matteosal(2020-07-08 10:06:36):Ah I realize that the reason why dimensions were not prepended might be that opset 7 (and higher versions) allow prepending dimensions themselves, so there's no reason to explicitly prepend them in the adapter. Is that correct? I will revert this change if yes.
matteosal(2020-07-08 13:28:03):Ok I see the point now. I went through the changelog checking which operator extended their types in opset 6, and modified the adapters accordingly
jcwchen(2020-07-08 15:36:51):Sorry for my previous misunderstanding.
But I think the answer is yes. The key point is for broadcast in opset 7, it does not need an axis to append anymore.
Besides, as you mentioned, it can handle prepending dimensions. 
So the only thing we need to do here for opset 6 to 7 is appending the dimensions by the axis.
jcwchen(2020-07-08 20:54:30):```suggestion
registerAdapter(make_unique<CompatibleAdapter>("Gemm",
        OpSetID(6), OpSetID(5)));
registerAdapter(make_unique<CompatibleAdapter>("Relu",
        OpSetID(6), OpSetID(5)));
registerAdapter(make_unique<CompatibleAdapter>("Sum",
        OpSetID(6), OpSetID(5)));
registerAdapter(make_unique<CompatibleAdapter>("Dropout",
        OpSetID(6), OpSetID(5)));
```
matteosal(2020-07-09 09:27:21):Reverted
askhade(2020-07-27 16:06:55):Do you need 3 methods? You can simply create 1 generic method which takes a few extra params for the values of min, max, default no?
askhade(2020-07-27 16:08:34):Why is this adapter needed? After opset 6 clip was only changed in opset 11 so just having a 6 -> 11 adapter should be sufficient right? Is this not the case today?
askhade(2020-07-27 16:23:10):why are these adapters needed? There is no opset 8 version for Flatten
matteosal(2020-07-27 16:28:30):Adapters are always converting between contiguous opset versions. The version converter upgrades operators making jumps of one version point at a time, and if the operatos has changed for a particular jump it goes look for an adapter. So the way Clip is upgraded from 6 to 11 is 6 -> 7 -> 8 -> 9 -> 10 -> 11, where only the last step requires an adapter.
matteosal(2020-07-27 16:31:09):There is no Flatten-8, but Flatten exists in opset version 8 anyway, as Flatten-1. The adapter 9 -> 8 is needed to downgrade Flatten from opset version 9 to any version before that.
matteosal(2020-07-27 19:36:07):Of course! I made this when I was starting getting familiar with the system and wanted to keep things very simple. Changed it to use the generic method.
askhade(2020-07-28 17:01:48):nit: Change error message to something like : Opset version 11 only supports select_last_index == 0?
matteosal(2020-07-31 11:31:26):@askhade one job is still failing because `graph` is unused here, which triggers a warning treated as error: https://travis-ci.org/github/onnx/onnx/jobs/713539738#L2049
The problem is, if I remove the `graph` argument here I get the same exact warning for `adapt(std::shared_ptr<Graph> graph, Node* node)` a few lines below, and that can't be changed because that's how adapters are supposed to be called by the converter: https://github.com/onnx/onnx/blob/master/onnx/version_converter/convert.cc#L98

So I don't know how to solve this.
matteosal(2020-07-31 11:31:41):Done
askhade(2020-07-31 16:13:50):you can simply change this to : 
void adapt_argmax_argmin_12_11(std::shared_ptr<Graph> , Node* node) const {

matteosal(2020-07-31 17:12:28):Ah of course, unnamed arguments! Done
snnn(2020-03-18 19:21:15):So, this bug fix is for test_mean_square_distance_mean_3d_expanded and some others, not for test_softmax_cross_entropy_mean_3d, right?
lara-hdr(2020-03-18 19:26:31):@snnn yes, this is the link for softmax cross entropy: https://github.com/microsoft/onnxruntime/pull/3237.
@codemzs mentioned still seeing a "Could not find an implementation for the node Max(12)" error after the fix.
snnn(2020-03-18 19:35:42):> @snnn yes, this is the link for softmax cross entropy: microsoft/onnxruntime#3237.
> @codemzs mentioned still seeing a "Could not find an implementation for the node Max(12)" error after the fix.

That's normal. Because onnxruntime doesn't have an implementation for the OP yet, that's expected. 
lara-hdr(2020-03-18 21:44:39):@ebarsoum @wschin for review
linkerzhang(2020-03-18 02:22:20):I don't think it's a good idea to have 8-bit integer support added as this, the output will overflow if it has same type as input.

What's the motivation of adding integer support right now? can adding int32/int64 support be good enough? if yes, don't add 8/16 bit integers for now.
lara-hdr(2020-03-18 17:13:19):that's a good point, thanks.
this is needed for the function ONNX::MeanSquaredDistance that executes a Pow operation with X as a float and Y as an int.
I added ints for X as well for completeness since it is a valid operation that we would need eventually. I removed 8-16 bits types for T (X and output).
spandantiwari(2020-03-18 20:14:57):The set of allowed types T is more limited than T1 for exponent. Is there a strong reason not to support other integer types?
lara-hdr(2020-03-18 20:18:02):I restricted the types of T following this comment https://github.com/onnx/onnx/pull/2666#discussion_r394071964 to avoid overflow.
Another option would be to accept any type for X as well. 
But then the output type would be impossible to infer..
wschin(2020-03-18 22:23:55):Please add tests for newly supported types.
lara-hdr(2020-03-18 22:27:19):there are 2 tests in export_types() involving int64.
Would adding 2 others for int32 be sufficient?
wschin(2020-03-18 23:10:33):Your new tests are good enough. Thanks.
CLAassistant(2020-03-18 01:51:17):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=2667) <br/>All committers have signed the CLA.
snnn(2020-03-26 21:32:39):@bddppq Could you please help take a look at the test_scalar_type failures in Caffe2Backend
 tests? I think the test case itself is wrong.
bddppq(2020-03-26 22:06:15):cc @spandantiwari @houseroad 
I believe the test case is intentionally testing comparison between float and int (as PyTorch supports it). The problem is probably that the onnx exporter does not do type promotion during conversion.

```python
class ComparisonModel(torch.nn.Module):
    def forward(self, x, y):
        return x.ge(0.5) & y.le(2)
    
x = torch.ones(2, 3, dtype=torch.int32)
y = torch.ones(2, 3, dtype=torch.float32)
self.run_model_test(ComparisonModel(), input=(x, y), train=False, batch_size=BATCH_SIZE)
...
>       inferred_model_str = C.infer_shapes(model_str, check_type)
E       onnx.onnx_cpp2py_export.checker.ValidationError: A typestr: T, has unsupported type: tensor(int32)```
snnn(2020-03-18 19:32:39):This is the major change.
shinh(2020-03-19 04:29:12):CC: @lara-hdr 
shinh(2020-03-22 11:41:03):Closing since https://github.com/onnx/onnx/pull/2674 fixes one more issue
lara-hdr(2020-03-24 19:45:27):@wschin @prasanthpul @faxu
lara-hdr(2020-03-25 18:02:46):@wschin could you review/approve please? thanks!
lara-hdr(2020-03-26 19:54:46):@wschin @prasanthpul @faxu @ebarsoum.
This PR is for fix the tests running in ORT, please review!
Thanks
shinh(2020-03-22 11:39:58):`np.ones((), dtype=bool)` or `np.array(True)` would also work, this is also fine, though?
lara-hdr(2020-03-24 19:46:35):Good to know, thanks!
Yes I tested with np.byte(1).astype(bool) and it works as well
wschin(2020-03-27 16:58:15):Is this a valid `splits`? From the [spec](https://github.com/onnx/onnx/blob/master/docs/Operators.md#splittosequence), I see
```
the sum of entries in 'split' must be equal to the dimension size of input tensor on 'axis'.
```
Maybe we can change the shape of `X` to something like `[2, 3, 4]`?
lara-hdr(2020-03-25 17:12:59):@codemzs, thanks for the PR :)
I have some comments but I cannot add them inline since the lines are not modified;

NllLoss and SoftmaxCrossEntropy are functions not operators; so they have a function body in ONNX that we will need to update, even if there is a kernel.

The functions are defined in onnx/defs/math/defs.cc, line 1831 and 2320;
We will need to update the 'if' clauses 'if (!ctx.hasInput(2))' to deal with cases with 2, 3, 4 inputs.In case ignore_iindex is specified (4 inputs), we can use ScatterElements to put a 0 in the weight on the ignored index in the weights.

The other option would be to switch to an operator.
snnn(2020-03-25 18:43:17):Is there any tests to verify if the function body works as expected? 
gramalingam(2020-03-25 20:00:50):> Is there any tests to verify if the function body works as expected?

Every "node" test-case is also expanded into a test-case for the corresponding function-body: for example, like here: https://github.com/onnx/onnx/tree/master/onnx/backend/test/data/node/test_softmax_cross_entropy_mean_3d_expanded

codemzs(2020-03-30 00:13:11):@wschin I have removed the default value for `ignore_index` but still kept it as optional because it is easier to implement the function body. In the converter we will pass an `ignore_index` only if its value provided is within the range [0, C). I did this at the suggestion of @lara-hdr. 
codemzs(2020-03-30 09:43:42):@wschin / @ebarsoum / @gramalingam Please merge this change. All of the **SoftmaxCrossEntropyLoss** & **NegativeLogLikelihoodLoss** ONNX tests on ONNX Runtime are now passing with the updated spec. @lara-hdr  has also verified the outputs match with that of PyTorch. 

![image](https://user-images.githubusercontent.com/1211949/77894860-c0610c80-722a-11ea-8c7a-ca6af768120c.png)

Thank you, @chinhuang007 for giving me time till end of weekend to wrap this change up, much appreciated! This change is ready to be shipped with ONNX 1.7 release.

CC: @SherlockNoMad 
chinhuang007(2020-03-30 15:38:27):@codemzs Thank you for the contribution!
wschin(2020-03-25 15:59:45):If we can set the `weight` of a single class to 0, why do we need `ignore_index`?
codemzs(2020-03-25 16:45:38):Hacking around is not good. You will increase the computation cost with this workaround. We are keeping this spec strictly for PyTorch. Next version will have a provision for TensorFlow. PyTorch expects weights to be a 1-D tensor only but in Tensorflow it can take the same shape as label tensor that can be of shape [N, D1, D2, .... Dk]. Hence we want to get one framework correct to begin with and add support for another incrementally.
lara-hdr(2020-03-25 17:17:12):why is this is required now?
PyTorch only has 1 output for NllLoss/CrossEntropy
lara-hdr(2020-03-25 17:19:34):maybe set ignore_index to None?
(Unlike PyTorch, the spec does not say that the default is -100.) 
lara-hdr(2020-03-25 17:21:48):can we add a case with no weights and ignore_index specified?
lara-hdr(2020-03-25 17:23:23):Should the nllloss tests be updated?
KsenijaS(2020-03-25 18:47:39):it's optional output for softmaxcrossentropy, it was a part of the original PR
gramalingam(2020-03-25 18:52:11):I don't understand this. Using weights seems the better "non-hacking" solution to me. We already have weights. Is it aligned to PyTorch or TF or does it handle both? If the weights doesn't correctly handle both PyTorch and TF, we already have a problem that needs to be fixed. 
lara-hdr(2020-03-25 18:54:24):@KsenijaS initially it was optional, but this PR makes it required.
I am curious of the reason since PyTorch does not have it as an output
lara-hdr(2020-03-25 19:05:31):why not make it an attribute?
lara-hdr(2020-03-25 19:52:13):Talked with @codemzs offline, he is ok to make this an attribute for NllLoss and SoftmaxCrossEntropy.
cc : @SherlockNoMad 
lara-hdr(2020-03-25 21:24:47):my bad, it is still optional; the "OpSchema::Optional" is after the below red lines.
wschin(2020-03-27 17:05:27):Where does this default come from? If we don't have a good default value, maybe we can make this attribute non-optional?
wschin(2020-03-27 17:06:32):Maybe you want to make sure this `labels` contain `0`, which will be ignored.
wschin(2020-03-27 17:08:40):Just want to double-check. Do you have some python code to compare pytorch and this reference implementation?
codemzs(2020-03-29 21:46:49):Resolved offline with @wschin. In his words "I understand composing Pytorch SoftmaxCrossEntropy is hard (but still doable with ops including LabelEncoder). I am fine with this change because using LabelEncoder doesn’t sound a good idea. I feel we can still keep in mind that ONNX is an IR for both of Tensorflow and Pytorch. If we only consider Pytorch, the value of ONNX may decrease."

Yes, this spec also supports TensorFlow’s tf.nn.sparse_softmax_cross_entropy_with_logits but we will need a minor change in the TensorFlow converter to permute and reshape the logits from [N, D1, D2…Dk, C] to [N, C, D1, D2…Dk]
codemzs(2020-03-29 22:13:21):It is coming from PyTorch but I can make it non-optional.
codemzs(2020-03-30 00:23:47):We spoke offline, now that ignore_index is not an input so this is not needed.
lara-hdr(2020-03-30 01:54:59):Yes, here it is.
[sce_test.zip](https://github.com/onnx/onnx/files/4400389/sce_test.zip)

codemzs(2020-03-30 02:20:21):thanks @lara-hdr for running these tests and validating them with PyTorch.
snnn(2020-03-30 05:04:50):Usually optional attribute should have a default value defined in the schema, unless the default value is too complex to be provided there. It's not required, I just want to say, if it has a default value, it would be easier for ONNXRuntime to handle it. 
codemzs(2020-03-30 05:23:37):Please read @wschin 's [comment](https://github.com/onnx/onnx/pull/2680#pullrequestreview-383098845). ORT also has a way to get default value when one is not defined, i.e `GetAttrOrDefault` 
snnn(2020-03-30 05:35:22):Thank you for clarifying this.

snnn(2020-03-30 06:28:54):Nit: please have a consistent coding style. (The position of "{" )
chinhuang007(2020-03-30 18:55:01):@onnx/sig-archinfra @onnx/sig-operators Please help review so we can start the release process. Thanks.
gramalingam(2020-04-01 03:24:34):Is this needed to be included in the 1.7 release?
lara-hdr(2020-04-01 03:54:19):@gramalingam yes that would be great.

cc: @codemzs  
lara-hdr(2020-04-01 18:01:56):@faxu @prasanthpul @wschin 

This PR is good to have in the release. 
It only updates the reference implementation and adds some tests, so it does not introduce a spec change. 
faxu(2020-04-01 18:02:58):CC @chinhuang007 
chinhuang007(2020-04-01 22:52:39):@faxu I have no problem to add this to the release 1.7 branch. However it will take another two days after this PR is merged to go through the build, publish, and verification cycle. Can we consider putting it as a post 1.7 fix? Since the release is owned by the ArchInfra SIG, so @linkerzhang please advise what we should do, in case additional PRs "should" be part of 1.7. Thanks!
gramalingam(2020-04-01 23:19:20):On the topic of 1.7 release, please note that https://github.com/onnx/onnx/pull/2691 is a bug fix to shape-inference. That PR has been merged into master. I assume that if we include this PR, we will include the other one too?
neginraoof(2020-04-01 23:24:49):@linkerzhang It would be great if we could add this other PR that Rama mentioned above (this a bug fix) to 1.7: https://github.com/onnx/onnx/pull/2691 
Thanks
chinhuang007(2020-04-02 00:04:18):@gramalingam Yes, if we decide to include this PR in 1.7 release, I assume #2691 will be included. I don't see the reason to cherry pick particular PR at this point unless told by the Archinfra SIG to do so.
wschin(2020-04-03 22:05:00):Is this PR tested with pytorch? How did you verify the correctness of the new cases?
lara-hdr(2020-04-04 21:08:55):@chinhuang007 this is ready for merge
gramalingam(2020-04-01 21:04:10):This may be a naive question: if we transpose the first two axes of input (so that C becomes the first axis) and flatten into a 2D (making all axes except first one into one axis), would it help simplify things? Of course, "target" also has to be similarly flattened. In other words, is there any difference in the treatment of axes other than C? If all those axes are treated the same way, it would simplify the logic to flatten them.
gramalingam(2020-04-01 21:07:09):I am a bit confused: it looks like "ignore_index" is being handled in two places (both in the conditions above which update neg_gather_element_input only for values other than ignore_index, and here using weight). Is that redundant?
gramalingam(2020-04-02 14:29:44):At the least, "input = input.reshape((N,C,-1))" and "target = target.reshape((N,-1))" should let us handle different D cases uniformly.
lara-hdr(2020-04-02 18:02:32):it's not really redundant. 
For example, in case the reduction is mean and ignore_index is specified, we don't want to include all the target values in the mean operation but only the non-ignored ones. This is why this if is added.
lara-hdr(2020-04-02 18:07:50):good point. I will try that.
gramalingam(2020-04-03 00:30:07):Yes, but I think this makes the earlier "if-condition" in line 22/29/37 redundant, I think. IIUC, if ignore_index is specified, then we end up introducing a weight and ensure that the weight is zero for ignore_index, and end up executing line 72 "loss = gather_weight * loss". So, why do we need the earlier if-condition in line 22 etc.? (I might be wrong. Please verify before changing it.)
wschin(2020-04-03 22:13:32):weight[ignore_index] sometime may not be valid. `ignore_idex` can be 100 while the weight is just a 10-element vector.
wschin(2020-04-03 22:15:41):humm... if you have case for `len(input_shape) == 3`, why do you need another case? We can flatten `[N, C, H, W]` to `[N, C, H*W]` and apply the 3-D branch.
wschin(2020-04-04 02:12:24):`len(input_shape) == 1` will enter this block? If yes, can we throw?
wschin(2020-04-04 02:12:55):```suggestion
            for d in range(D):
```
maybe?
wschin(2020-04-04 02:14:19):```suggestion
        // Get a dimension from the reshaped input. If the original input shape is [N, C, H, W], the D here should be H * W because we reshape [N, C, H, W] to [N, C, H * W].
        D = input.shape[2]
```
wschin(2020-04-04 02:15:54):We can reshape input shape `[N, C]` to `[N, C, 1]` (and label `[N]` to `[N, 1]`) and then all the following code become applicable to all cases without if-else. Note that we still need to throw if the input rank is less than 2.
wschin(2020-04-04 02:22:35):```suggestion
        # when the target valeu is > C or < 0, it doesn't matter which value we are
```
wschin(2020-04-04 02:22:58):```suggestion
        # taking in gather_weight, since it will be set to 0 in the following fi-block.
```
wschin(2020-04-04 02:24:37):```suggestion
        // Set `ignore_index`'s loss weight to 0. The loss tensor will be multiplied by this weight tensor, so `ingore_index`'s loss value will be eliminated.
        gather_weight = np.where(target == ignore_index, 0, 1).astype(dtype=np.float32)
```
codemzs(2020-04-04 21:22:26):This test assumes ignore_index is within range and if this is failing then we must investigate why function body definition is not working because it should handle cases where ignore_index is within range but if it is not handling that case then we must investigate and fix instead of commenting the test. Please note, the "update of function body" work item is for handling ignore_index that is **out of** range. 
neginraoof(2020-03-31 21:26:31):Would appreciate it if you could review this PR @gramalingam 
gramalingam(2020-03-31 21:27:56):Thanks for the fix. Regarding the line
```cpp
num_ellipsis_indices = rank - term.size() + 3;
```
can we add a check whether rank >= (term.size()-3)? What happens if the input rank is less than the minimum expected?
neginraoof(2020-03-31 21:49:07):> Thanks for the fix. Regarding the line
> 
> ```c++
> num_ellipsis_indices = rank - term.size() + 3;
> ```
> 
> can we add a check whether rank >= (term.size()-3)? What happens if the input rank is less than the minimum expected?

Thanks, I just added this check. Basically, if there's an ellipsis, the term will look something like: '...ij'
So the rank of each tensor should be >= the explicit indices (term.size() - 3)
hariharans29(2020-03-31 21:57:14):Thanks for the fix. 

There are still some cases to check:

1) Make sure the letters we see (inputs and outputs) are in the allowed range ['a', 'z']
2) Make sure we don't have a letter in the output that we haven't seen in the input
3) Make sure we don't have a letter that repeats in the output
4) Make sure we don't have an ellipsis in the output when we don't have one in any of the inputs

Can come in a later PR though.

neginraoof(2020-03-31 22:11:33):> Thanks for the fix.
> 
> There are still some cases to check:
> 
> 1. Make sure the letters we see (inputs and outputs) are in the allowed range ['a', 'z']
> 2. Make sure we don't have a letter in the output that we haven't seen in the input
> 3. Make sure we don't have a letter that repeats in the output
> 4. Make sure we don't have an ellipsis in the output when we don't have one in any of the inputs
> 
> Can come in a later PR though.

Actually I did not add any checks for the equation string itself. Though these tests can be considered for rank inference too. I can send a follow up PR.
neginraoof(2020-04-02 16:57:36):cc @linkerzhang Would it be possible to include this fix in 1.7 branch please?
snnn(2020-04-02 23:03:04):Do you need to regenerate the test data as well?
neginraoof(2020-04-02 23:43:02):@snnn this is merged. But should shape inference test data be re-generated? I haven't changed back-end tests.
neginraoof(2020-04-05 20:27:35):@snnn Could you please let me know if this will be part of 1.7 release?
gramalingam(2020-03-31 21:34:49):So, the inference doesn't currently infer the output-shape of (3,5,7,9), even though it has the necessary information to infer it? We don't have to do this in the current PR/release, but I think we should make the shape-inference more complete to infer the dimensions where possible.
hariharans29(2020-03-31 21:52:24):Can we add an "implicit" version of this test too ? Something like "...ij,...jk" alone ?
neginraoof(2020-03-31 22:01:35):Sure, I added this test.
neginraoof(2020-03-31 22:08:13):So actually I did not look into shape inference since Einsum could results in many different operations. I'm not sure if it's okay to have shape inference supported partially, based on the equation. If you think otherwise, we can look into supporting some cases.
gramalingam(2020-03-31 22:45:51):As I mentioned above, let us not do it for this PR or this release. I can help do this after the release.
neginraoof(2020-04-01 00:27:35):Thanks @gramalingam 
snnn(2020-04-03 18:38:12):I don't quite understand. The 3 input tensors are [2,3,4], [2,3,4], [2,3,4], why the output is not [2,9,4]?
BowenBao(2020-04-03 18:43:17):> 
> 
> I don't quite understand. The 3 input tensors are [2,3,4], [2,3,4], [2,3,4], why the output is not [2,9,4]?

The output is [2,9,4]. That should be fixed in this PR. Are you still observing [2,3,3,4]? (which should be case where `new_axis` == 1, but not this case)
snnn(2020-04-03 18:49:41):No. It's good now. Thank you
codemzs(2020-04-03 22:26:52):@chinhuang007 This is a very small **one line documentation change** that is important and I will really appreciate if you can cherry pick this into the release branch. It is extremely low risk as it is just a documentation change but it clarifies the behavior of the spec. 
spandantiwari(2020-04-03 22:35:01):Just as note for posterity - today the function subgraph for this function still assumes that the `ignore_indices` are restricted to [0, C). The subgraph will be updated in a separate PR later. 
codemzs(2020-04-03 22:37:22):@spandantiwari Thanks. I just want to add to it that updating the function body implementation is needed but it is not a ship blocker since we have the implementation of the op in ORT and ORT is the primary user of this spec. I will try to get the function body also implemented in a different PR but will leave it to folks to decide if we should take it for 1.7
linkerzhang(2020-04-04 02:00:33):@codemzs not exactly, please don't take an assumption that ORT is the "only" or primary user of this spec. that's not true. ONNX does have many partners. Per my personal investigation now, there're many mobile/edge related inference engines which are depending on ONNX. That said, making the spec correct (including function body correctness) is super important. @ebarsoum @spandantiwari @prasanthpul 
codemzs(2020-04-04 02:03:56):@linkerzhang thanks for the merge, I will try my best to get correct the function body as well for 1.7 release. A little new to this ONNX world so made the naive assumption but thanks for educating me.
spandantiwari(2020-04-03 22:30:27):nit: "It's"
codemzs(2020-04-06 05:14:27):@chinhuang007 This is the **last** commit to go with ONNX 1.7 release, it **does not** add any new features but it fixes all the gaps we found with the function body for **NegativeLogLikelihoodLoss** and re-enables all of the disabled tests. Without this change we will shipping a function body that is incomplete and as @linkerzhang pointed [here](https://github.com/onnx/onnx/pull/2696#issuecomment-608954671) function body correctness is extremely important even though ONNX Runtime has an operator implementation. We have several partners that *may* not use ONNX Runtime and instead may depend on function body definition.

Thank you @gramalingam and @skottmckay for all your help and doing the review today!!

CC: @ebarsoum @wschin @prasanthpul @spandantiwari @faxu @SherlockNoMad @linkerzhang 
gramalingam(2020-04-06 04:15:46):Minor nit: Can't we use "const_one_float" instead of "const_weights_ones_float"? Doesn't "squeeze_mask" already have the desired shape? Just wondering.
codemzs(2020-04-06 04:16:44):Good point, let me try.

---
In reply to: [403821108](https://github.com/onnx/onnx/pull/2703#discussion_r403821108) [](ancestors = 403821108)
codemzs(2020-04-06 04:24:19):This works. Let me push the commit.

---
In reply to: [403821282](https://github.com/onnx/onnx/pull/2703#discussion_r403821282) [](ancestors = 403821282,403821108)
codemzs(2020-04-06 16:44:30):>] [](start = 69, length = 1)

remove #Resolved
codemzs(2020-04-06 16:46:56):>] [](start = 61, length = 1)

remove #Resolved
neginraoof(2020-04-08 19:01:47):Failures are caffe2 backend tests:
FAILED test/onnx/test_pytorch_onnx_caffe2.py::TestCaffe2Backend_opset7::test_scalar_type
FAILED test/onnx/test_pytorch_onnx_caffe2.py::TestCaffe2BackendEmbed_opset7::test_scalar_type
FAILED test/onnx/test_pytorch_onnx_caffe2.py::TestCaffe2Backend_opset8::test_scalar_type
FAILED test/onnx/test_pytorch_onnx_caffe2.py::TestCaffe2BackendEmbed_opset8::test_scalar_type
spandantiwari(2020-04-06 22:22:23):nit: To be consistent, let's say `The matrix must be invertible.`
spandantiwari(2020-04-06 22:29:11):Would it make sense to say more here? Some something like: `The matrices must be invertible (full-rank). The behavior where one of the matrices is not invertible is undefined. The implementation can choose to throw an error or output (garbage) results as is.`
@gramalingam @neginraoof @yuslepukhin 
gramalingam(2020-04-06 22:58:01):How about "Every matrix in the batch must be invertible"?
yuslepukhin(2020-04-07 20:41:53):>ypeAndShapeInferenceFunctio [](start = 10, length = 27)

I think TypeAndShapeInferenceFunction needs to be revisited
chinhuang007(2020-04-06 22:48:08):@gramalingam can you help review, another release PR with no real changes at all, just update version number for release builds and verification? Thanks!
gramalingam(2020-04-06 22:54:58):Thanks Chin! (Looks like version number is 1.7.104.)
chinhuang007(2020-04-06 22:59:44):@gramalingam Yes, I made it 104 for verification purpose since TestPypi doesn't allow reuse version number. We will need to change back to 1.7.0 after the verification comes back clean for the real Pypi publish.
linkerzhang(2020-04-08 13:30:28):Thanks for fixing bugs!

Interesting... fixing shape inference logic should not change test models/data, right?
gramalingam(2020-04-08 16:07:42):There is a CI failure due to a flake8 complaint, needs some space character after comma or something like that.
neginraoof(2020-04-08 16:36:52):@gramalingam Thanks for review.
neginraoof(2020-04-08 16:39:20):@linkerzhang I'm just updating the ONNX IR version for models
neginraoof(2020-04-08 17:34:27):Failures here are from Caffe2 backend tests:
FAILED test/onnx/test_pytorch_onnx_caffe2.py::TestCaffe2Backend_opset7::test_scalar_type
FAILED test/onnx/test_pytorch_onnx_caffe2.py::TestCaffe2BackendEmbed_opset7::test_scalar_type
FAILED test/onnx/test_pytorch_onnx_caffe2.py::TestCaffe2Backend_opset8::test_scalar_type
FAILED test/onnx/test_pytorch_onnx_caffe2.py::TestCaffe2BackendEmbed_opset8::test_scalar_type
gramalingam(2020-04-08 23:39:33):Hi @linkerzhang : the merge of PR 2268 seems to be causing CircleCI failure (the problem is not with the PR, but the model-checker is exposing failures in some pre-existing model-export). Should that PR be reverted? Or, should we go ahead and merge this PR in without CircleCI? Thanks.
gramalingam(2020-04-08 15:59:21):I am trying to understand why the earlier version exited the while-loop at all. Anyone know? The getline spec says that it returns a reference to a stream. (Just want to double-check what the difference in behavior here.)
gramalingam(2020-04-08 16:01:36):Is the formatting produced by clang? Looks odd ... I expected some indentation of the second line.
neginraoof(2020-04-08 16:36:22):No, I just updated it
gramalingam(2020-04-08 18:54:13):Ok, I understand ... it is implicitly converting the stream object into a bool, after the "getline()" is done ... and so it wasn't processing the "empty-string" getline returns in the last iteration.
neginraoof(2020-04-08 21:21:20):Thanks for clarifying this
CLAassistant(2020-04-08 13:53:43):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=2714) <br/>All committers have signed the CLA.
gramalingam(2020-04-08 16:20:53):Curious: is this just as a matter of coding-style, or does it have some effect (on performance)?
marload(2020-04-09 01:50:22):@gramalingam 
Matter of code style.
linkerzhang(2020-04-16 03:44:11):good catch!
linkerzhang(2020-04-16 03:46:49):I'm not sure it's good to add this line so that a separate "operator sets" section is there. It may mislead readers on getting the points that "Versioning is against an operator set, instead of an operator".
natke(2020-04-16 17:08:39):That's fair. I do think a clear definition of an `Operator set` is necessary though. Especially given that users who are converting to ONNX from another format often need to specify the opset, and will need to know precisely what this means
linkerzhang(2020-04-19 12:29:28):Agree that a separate section of "OperatorSet and OperatorSet Versioning" should be there. do you want to do another review and refactor of this section "Operator Versioning" and this new "OperatorSet" section please? 

basically, things should be highlighted, 1) Versioning is against OperatorSet, instead of Operator. 2) Operator is identified by tuple <name, domain, since_version> (currently, it's saying "op_version", which is not accurate and misleading). 3) Every ONNX release will at most have 1 OperatorSet version bump (if and only if there's any breaking changes) 4) list the details of "breaking changes".
natke(2020-04-22 03:06:56):Yep sure, will do!
natke(2020-04-24 17:09:36):@linkerzhang I created a new Operator set sub-section 
neginraoof(2020-04-14 18:29:16):cc @wschin @gramalingam @hariharans29  for review. Thanks a lot.
linkerzhang(2020-04-16 03:38:48):good catch!
linkerzhang(2020-04-16 03:31:48):how about "unknown" case?
linkerzhang(2020-04-16 03:35:29):I'm wondering whether a separate "training" op doc should be generated, that will make the doc understanding easier, given ONNX may support more scenarios, other than DNN. thoughts?
wschin(2020-04-16 07:11:05):Unknown would be an empty string. Would it be better to use `differentiability unknown`? 

For the second comment, I don't feel splitting this tag from `Operator.md` is a good idea. To understand why an input/output is differentiable and how to compute its gradient, the reader must read that operator's entire document.
sveta-levitan(2020-04-18 03:11:51):I don't believe that training algorithms used for deep learning training can be applied to many traditional ML models. Hence, we probably don't need to add differentiability tags to ops in ONNX-ML.
linkerzhang(2020-05-05 23:20:10):@wschin I'm ok for "differentialbility - unknown" or "undefined" to make the 3 cases very clear - "differentiable, not differentiable, undefined", if we don't want to put the training ops into a separate doc. 
linkerzhang(2020-05-05 23:22:44):or maybe one line of "statement" - inputs without specifying "differentiability" have that as "undefined" at the beginning of operators.md?
wschin(2020-05-06 01:04:41):@linkerzhang, training ops may not be put in another MD because --- this newly added attribute will be added to ALL existing operators. 

Because each operator has their own differentiability, I am not sure how to create an one-line statement in the beginning of `operator.md`. Do you mean create one line for each operator?
linkerzhang(2020-05-11 23:52:38):yes, I mean one line in the beginning of operator.md to save clarifying it in each operator :).
wschin(2020-05-12 15:54:15):@linkerzhang, I think I get your point. Is https://github.com/onnx/onnx/pull/2723/commits/1f1fc67cd2eefc3144841859255a81280f66dff7 we want? 
wschin(2020-05-12 17:52:39):How to prove it?
1. Pytorch/Tensorflow has this differentiable.
2. Add math of backward to operator and ask reviewer to review the math.
3. Implement a backward using an existing auto-diff library.
snnn(2020-04-16 03:28:42):> you may want to have "fail early" here, as type string recognition is in a very early stage when loading the lib (it's also the same in ORT). Changing to return "nullptr" may not be a good idea as it hides the root cause of a failure, although you have a "status" returned, but it's not recorded or thrown out anywhere.

Currently, onnxruntime, almost does nothing when onnxruntime.dll is loaded. All the initialization are postponed to session creation time .
linkerzhang(2020-04-18 09:56:20):don't quite understand the point. I was saying that "early failure" should be advocated for this piece of codes. With your change, although there's a failed "status" returned, but it was swallowed and a nullptr without any error message is returned. I think this is not good? (but of course, the original code of crashing is not good neither).
snnn(2020-04-15 07:34:10):This edge case was discovered by [klee](https://klee.github.io/)
wschin(2020-05-06 21:15:45):Can we print something in this case? Maybe st.ErrorMessage(..) or something like that? I am also thinking to return `st` but not sure if it is a good idea.
wschin(2020-05-06 21:16:43):```suggestion
          Common::NONE, Common::INVALID_ARGUMENT, "invalid type string: ", type_str);
```
wschin(2020-05-06 21:18:58):```suggestion
        Common::NONE, Common::INVALID_ARGUMENT, "invalid type string: ", type_str);
```
At least user can search for the returned `type_str` in their code to find the error.
wschin(2020-05-06 21:20:51):Can we throw and print out the error message from `st` here?
snnn(2020-05-06 21:26:03):No. Unless onnx has a Logger interface, otherwise it's better to not print anything in onnx code. Because it's often applications want to take a totally control on logging, they don't want us use stdout/stderr directly.
snnn(2020-05-06 21:27:46):I prefer to change it to return Common::Status instead, to make it consist with the other function.  It look bad when mixing the use of Common::Status and exception.  
What do you think?
wschin(2020-05-06 22:05:37):I always prefer to propagate error to the surface. Let's return `Common::Status`. :)
wschin(2020-05-06 22:06:53):Makes sense. We can still return `Common::Status` as you suggested below.
codemzs(2020-04-25 06:25:59):@linkerzhang Can you please review one more time and see if your hold can be removed? I believe we have addressed your concerns. We want to ship ONNX 1.7 next week and this PR is critical for that. I will really appreciate if you can review this PR at the earliest. 
gramalingam(2020-04-27 18:00:55):Any other feedback from anyone else, before this is merged? Thanks!
gramalingam(2020-04-16 02:34:51):This PR seems to also change the op specification about the default value of ratio/drop_probability. The current spec says that default value is 0. Two questions: (a) Why change the default-value? I understand that it makes it more compatible with PyTorch/TF, but I wonder why it was made 0 in the first-place and what things would break if we change it thus, (b) If we must change the default-value, the spec also needs to be changed/updated.
codemzs(2020-04-16 02:49:56):@gramalingam The rationale is to be compatible with PyTorch/TF and even ORT has default as 0.5. Happy to update the spec but lets first understand why the default of zero was chosen, maybe @wschin might know?
linkerzhang(2020-04-16 03:53:14):@gramalingam good catch!

I don't suggest to change this default value. I believe the "drop elimination" optimizer was assuming that the default value is 0 so that in inference case it's just a copy and can be removed.
codemzs(2020-04-16 05:51:59):Thanks, @linkerzhang then in that case we need to change the "default_dropout" test to pass the probability of 0.5 because on ORT side default probability is set to 0.5 (same as PyTorch/TF). In anycase the reference implementation is not doing the scaling as it is done in PyTorch/TF and ORT. 
gramalingam(2020-04-16 15:30:28):@codemzs : it sounds like ORT implementation does not match the spec then. Either the spec needs to be changed or the ORT implementation needs to change (I don't think it is just a matter of changing the test-case).
gramalingam(2020-04-19 00:11:48):Use
```
   ctx.getAttribute("alpha") ? ctx.getAttribute("alpha")->f() : 1.0f
```
codemzs(2020-04-19 00:14:09):Thanks for the reminder I was planning to checking for missing value but doesn't; this attribute already have a default value set in the schema? Also I think we should use the default const below. 

ctx.getAttribute("alpha") ? ctx.getAttribute("alpha")->f() : celu_default_alpha


---
In reply to: [410777048](https://github.com/onnx/onnx/pull/2725#discussion_r410777048) [](ancestors = 410777048)
gramalingam(2020-04-19 00:14:43):Is that supposed to be 1.f or alpha from above?
codemzs(2020-04-19 00:19:18):I think 1.f is the default value for the attribute but we should use celu_default_alpha instead. What do you think?

---
In reply to: [410777439](https://github.com/onnx/onnx/pull/2725#discussion_r410777439) [](ancestors = 410777439)
gramalingam(2020-04-19 00:26:02):I think it is just safer. I am not sure if the check is needed (this can be called from ORT, but it can also be called from other ONNX client-code ... I recall that ORT fills in default values, but not sure if that also happens in the ONNX context).
codemzs(2020-04-19 00:27:28):Safer for sure.

---
In reply to: [410779075](https://github.com/onnx/onnx/pull/2725#discussion_r410779075) [](ancestors = 410779075)
gramalingam(2020-04-19 00:28:22):I don't know this op. But looking at the spec, it says the value is
```
   max(0,x) + min(0,alpha*(exp(x/alpha)-1))
```
So, I assume both alphas in this equation must be the attribute-value retrieved above?
gramalingam(2020-04-19 00:30:53):But I am confused about the "max(0,x)" part too. How does the above equation get rewritten in the form shown in this function-body?
codemzs(2020-04-19 00:31:09):Both alphas are attribute-value retrieved above that we put in a constant tensor by the name "alpha" 

---
In reply to: [410779367](https://github.com/onnx/onnx/pull/2725#discussion_r410779367) [](ancestors = 410779367)
codemzs(2020-04-19 00:32:11):You make a good point, the test passed so I didn't look too closely but let me think.

---
In reply to: [410779760](https://github.com/onnx/onnx/pull/2725#discussion_r410779760) [](ancestors = 410779760)
codemzs(2020-04-19 00:39:38):[https://github.com/onnx/onnx/pull/2575](https://github.com/onnx/onnx/pull/2575) that explains more on the implementation.

---
In reply to: [410779924](https://github.com/onnx/onnx/pull/2725#discussion_r410779924) [](ancestors = 410779924,410779760)
codemzs(2020-04-19 00:40:29):PyTorch implementation:

``Tensor celu(const Tensor & self, Scalar alpha) {
  double inv_alpha = 1. / alpha.to<double>();
  return at::elu(self, alpha, Scalar(1.0), Scalar(inv_alpha));
}``


---
In reply to: [410780998](https://github.com/onnx/onnx/pull/2725#discussion_r410780998) [](ancestors = 410780998,410779924,410779760)
gramalingam(2020-04-19 00:42:00):May be this is okay, since there is the multiplication by alpha down below. 
codemzs(2020-04-19 00:42:44):yep

---
In reply to: [410781381](https://github.com/onnx/onnx/pull/2725#discussion_r410781381) [](ancestors = 410781381)
linkerzhang(2020-04-19 12:20:30):I suggest not updating the spec, which costs more than the benefits it brings.
codemzs(2020-04-19 23:49:08):Thanks @linkerzhang ! lets see what other reviewers have to say? @ebarsoum @yuanbyu @wschin @gramalingam 
codemzs(2020-04-22 09:43:37):@linkerzhang Please take a look again, we added a training_mode input to Dropout so that existing inference behavior does not break while the operator can support training that can be dynamically switched with default probability of 0.5
SherlockNoMad(2020-04-22 18:56:33):We cannot match the output value in the onnx test, right? as the mask is randomly generated, and different backend will generate different mask, even with the same seed.
SherlockNoMad(2020-04-22 18:58:17):Wouldn't this test fail in ORT's onnx_test, as mask is random.
codemzs(2020-04-22 19:20:23):It is currently failing but as discussed offline I want to see it I can some how approximate the comparison but if that is not feasible I’ll skip _expect_ as you suggested.__
codemzs(2020-04-24 06:22:30):After setting the seed attribute of dropout to zero and also in the reference implementation, tests on ORT pass.
codemzs(2020-04-24 06:23:18):After setting the seed attribute of dropout to zero and also in the reference implementation, tests on ORT pass.
codemzs(2020-04-24 23:36:23):I have updated ORT test infra to count for number of zeros and make sure they are around the same count as baseline.
codemzs(2020-04-24 23:36:29):I have updated ORT test infra to count for number of zeros and make sure they are around the same count as baseline.
SherlockNoMad(2020-04-25 01:24:22):These few lines looks like LogSoftmax to me... Don't we have a LogSoftmax onnx op for this ?
SherlockNoMad(2020-04-25 01:28:07):use an Identity op should sovle this 
log_prob = Identity(X_Log) ? 
codemzs(2020-04-25 04:42:37):You are correct, however, LogSoftmax in ORT (or even ONNX) assumes input is provided with the shape [N, D1, D2....Dk, C] but in this case the input is [N, C, D1, D2...Dk]. hence we need to transpose the input first and then pass to LogSoftmax. I had earlier tried with just LogSotfmax and the results did not match. The amount of ops we will need to use just to pass the permutations to Transpose functions may cost us more than the simple solution we already have. Let me know if you disagree.


codemzs(2020-04-25 04:42:48):Perfect, thanks!
gramalingam(2020-04-27 16:37:39):Change to "... unless specified explicitly, it is false. If it is false, ratio is ignored and the operation mimics inference mode ..."?
gramalingam(2020-04-27 16:43:37):I thought the plan was to restrict the type to only float, is that right? Should line 442/473 be changed to allow T to be only tensor(float)?
gramalingam(2020-04-27 16:58:20):I suspect it might be a bug in ORT function expansion code. E.g., something similar to what is done in ONNX might be required: see: https://github.com/onnx/onnx/blob/2dd6a1ba5fcdb1d47cd485d5033e6cf804100d49/onnx/defs/function.cc#L46 

codemzs(2020-04-27 17:47:23):Agreed, I will track this separately, spoke offline and @gramalingam is fine with the workaround for now.
vinitra-zz(2020-04-27 17:59:26):> wondering where will the CI show on the page?

These CIs will become part of the PR checks / be included as badges in the READMEs. This cannot be implemented before the pipelines are enabled, which requires these YAMLs to be checked in first.
vinitra-zz(2020-05-01 22:12:02):@marload Can you merge #2719, #2718 and this contribution for deduplication into one PR? It will make it easier to review and approve. Thanks!
wschin(2020-05-05 22:11:25):> @marload Can you merge #2719, #2718 and this contribution for deduplication into one PR? It will make it easier to review and approve. Thanks!

I guess it's ok to have 3 PRs. I feel changes should be as small as possible (surely it has to be meaningful). Sounds ok?
CLAassistant(2020-04-24 01:06:15):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=2740) <br/>All committers have signed the CLA.
wschin(2020-04-24 15:01:00):> Hmmm, the "experimental" op concept was removed because it's hard to tell onnx partners to not depend on it till it becomes production quality. That brings lots of troubles on the versioning maintenance.
> 
> I may suggest to remove these ops from the repo if we think they're not in spec quality, and we can add them back when ready.
> 
> @prasanthpul @ebarsoum @postrational

It's a chicken-and-egg problem. The community already agrees to put training spec in **preview**, this PR offers a clearer way for doing it.
gramalingam(2020-04-24 19:18:34):A nit: it would be better to use a symbolic constant like AI_ONNX_EXPERIMENT_DOMAIN instead of "ai.onnx.experiment" everywhere. (But it occurs in lots of places, so probably lot of work to fix it. So, I am also okay with leaving it as is.) If this is changed, I would recommend "experimental" instead of "experiment".
wschin(2020-04-24 22:50:57):> A nit: it would be better to use a symbolic constant like AI_ONNX_EXPERIMENT_DOMAIN instead of "ai.onnx.experiment" everywhere. (But it occurs in lots of places, so probably lot of work to fix it. So, I am also okay with leaving it as is.) If this is changed, I would recommend "experimental" instead of "experiment".

It's not quick but it's done. The new domain name is `ai.onnx.experimental` now.
gramalingam(2020-04-24 23:26:10):@linkerzhang : the purpose of the experimental domain is to enable an open discussion of an op's spec and to experiment with it before finalizing the spec. I agree there was an issue with it previously and it was removed. The previous problem was that some partners took a dependence on the experimental op. It has to be made clear that users/partners should not depend on experimental ops, as they are subject to changes and even deletion. ONNX implementations are under no obligation to implement or support experimental ops. This is partly motivated by a number of issues that showed up in ops that were introduced in the last cycle as we move towards the 1.7 release. To avoid such problems, it is necessary to iterate through ONNX spec development, its use (in producers, exporters) and implementation (in backends) and this experimental domain is intended to help with that. Does that make sense? There is an Operator sig meeting next week to discuss this proposal.
codemzs(2020-04-25 07:03:15):@gramalingam Fair point but my only concern with this approach is that experimental domain _could_ become a dumping ground for specs. Who will take the responsibility to move a spec from experimental stage to production stage and if we can time bound this upgrade?  I tend to take a more conservative approach with these things and agree with @linkerzhang.
linkerzhang(2020-04-26 01:43:36):I agree with @codemzs and don't think it's a good idea to have another "experimental" concept in ONNX in any way. I am seeing two risks: 
1. How to ensure that no one is going to depend on these "experimental" stuff?
2. How to ensure that these "experimental" concepts will NOT be used as an area of putting not agreed stuff?
linkerzhang(2020-04-26 01:44:39):Adding more: are we going to allow onnx folks to add other "experimental domains" for adding their own ops?

We can put these ops in a separate .md file (say, onnx-training-preview.md as a Preview spec and don't have to follow the operator registration API to code and make them being registered as part of the official spec.
linkerzhang(2020-04-26 01:52:02):> > Hmmm, the "experimental" op concept was removed because it's hard to tell onnx partners to not depend on it till it becomes production quality. That brings lots of troubles on the versioning maintenance.
> > I may suggest to remove these ops from the repo if we think they're not in spec quality, and we can add them back when ready.
> > @prasanthpul @ebarsoum @postrational
> 
> It's a chicken-and-egg problem. The community already agrees to put training spec in **preview**, this PR offers a clearer way for doing it.

@wschin I don't agree that it's a "chicken-egg" problem here. The "egg" (preview doc) does NOT have to be coded and pushed into this repo, instead, it could be a markdown doc only there for discussion (before it turns to be chicken :)). Let's keep the ONNX repo with the "standard spec" quality.
linkerzhang(2020-04-26 01:57:12):> @linkerzhang : the purpose of the experimental domain is to enable an open discussion of an op's spec and to experiment with it before finalizing the spec. I agree there was an issue with it previously and it was removed. The previous problem was that some partners took a dependence on the experimental op. It has to be made clear that users/partners should not depend on experimental ops, as they are subject to changes and even deletion. ONNX implementations are under no obligation to implement or support experimental ops. This is partly motivated by a number of issues that showed up in ops that were introduced in the last cycle as we move towards the 1.7 release. To avoid such problems, it is necessary to iterate through ONNX spec development, its use (in producers, exporters) and implementation (in backends) and this experimental domain is intended to help with that. Does that make sense? There is an Operator sig meeting next week to discuss this proposal.

No. ONNX development should not and does not have to be in this repo. Taking ONNX runtime as an example, it's very straightforward to have all ops registered and implemented in ONNX runtime firstly and verified with releases, and then proposing them to ONNX community. Taking ONNX repo as an experimental place is NOT a good idea, which was tried and proved before.
codemzs(2020-04-26 21:59:23):If someone wants to implement an _experimental_ spec (saved as a MD file as @linkerzhang suggested and I agree) then can't they use the exact same strategy we have in place for ONNX Runtime Training where we define our own schema definition file locally as CONTRIB, see [gradient_schema_defs.cc ](https://github.com/microsoft/onnxruntime/blob/master/orttraining/orttraining/core/graph/gradient_schema_defs.cc)? We have so many gradient operators and not even one of those operators has a spec checked into ONNX, no?

As we make this important decision on the new policy to check-in specs into ONNX, I urge all of us to please also take a look at number of [issues ](https://github.com/onnx/onnx/issues/2614#issuecomment-619508755) we had to investigate and fix so that ONNX 1.7 can be released with quality. These issues were found last minute, delayed the release and were a direct result of checking-in specs that did not have a corresponding runtime implementation, inaccurate reference implementation, inadequate tests and were not validated to appropriate quality levels. 
wschin(2020-04-27 15:32:02):> > @linkerzhang : the purpose of the experimental domain is to enable an open discussion of an op's spec and to experiment with it before finalizing the spec. I agree there was an issue with it previously and it was removed. The previous problem was that some partners took a dependence on the experimental op. It has to be made clear that users/partners should not depend on experimental ops, as they are subject to changes and even deletion. ONNX implementations are under no obligation to implement or support experimental ops. This is partly motivated by a number of issues that showed up in ops that were introduced in the last cycle as we move towards the 1.7 release. To avoid such problems, it is necessary to iterate through ONNX spec development, its use (in producers, exporters) and implementation (in backends) and this experimental domain is intended to help with that. Does that make sense? There is an Operator sig meeting next week to discuss this proposal.
> 
> No. ONNX development should not and does not have to be in this repo. Taking ONNX runtime as an example, it's very straightforward to have all ops registered and implemented in ONNX runtime firstly and verified with releases, and then proposing them to ONNX community. Taking ONNX repo as an experimental place is NOT a good idea, which was tried and proved before.

It's true for a single operator. However, training is a large framework level change, which requires much more resources/time/discussion/development-iteration. It involves exporter, runtime, spec. Are we able to do all the three things in the same time? I can certainly tell you --- it's impossible as every team has their own priorities. In addition, even Pytorch doesn't have full support of auto-grad; they can do only a very limited part of it. Do you consider Pytorch should not release auto-grad APIs? Having training spec in master is also an community agreement (I remember at least 10 people explicitly approved my PR after so many meetings with much more people). If you really want to revert it, I think we need to collect opinions from all these people again before making a decision here.
wschin(2020-04-27 15:36:03):> If someone wants to implement an _experimental_ spec (saved as a MD file as @linkerzhang suggested and I agree) then can't they use the exact same strategy we have in place for ONNX Runtime Training where we define our own schema definition file locally as CONTRIB, see [gradient_schema_defs.cc ](https://github.com/microsoft/onnxruntime/blob/master/orttraining/orttraining/core/graph/gradient_schema_defs.cc)? We have so many gradient operators and not even one of those operators has a spec checked into ONNX, no?
> 
> As we make this important decision on the new policy to check-in specs into ONNX, I urge all of us to please also take a look at number of [issues ](https://github.com/onnx/onnx/issues/2614#issuecomment-619508755) we had to investigate and fix so that ONNX 1.7 can be released with quality. These issues were found last minute, delayed the release and were a direct result of checking-in specs that did not have a corresponding runtime implementation, inaccurate reference implementation, inadequate tests and were not validated to appropriate quality levels.

They CANNOT implement the spec if we don't write it in C++. How do you make sure everyone translate English into the same C++ code? I am aware of the pain of ONNX 1.7 release and I am also very aware of the year-long pain of creating a training spec where everyone had their opinion. If we don't freeze the agreement formally here. I don't know who will be willing to do this again.
codemzs(2020-04-27 15:44:36):What do you mean? How are all the gradient operator spec implemented in CONTRIB? Where is the formal C++ spec for them? these don’t even have a MD file for _experimental_ discussion.

People can implement the spec privately, experiment by implementing the op in some
ONNX RT and once they have sufficient confidence then they submit to ONNX.
wschin(2020-04-27 15:54:28):> What do you mean? How are all the gradient operator spec implemented in CONTRIB? Where is the formal C++ spec for them? these don’t even have a MD file for _experimental_ discussion.
> 
> People can implement the spec privately, experiment by implementing the op in some
> ONNX RT and once they have sufficient confidence then they submit to ONNX.

@ebarsoum already said in our meeting he will have a document for that experimental domain.

Training is not an operator. no one can implement it by herself/himself. Are saying everyone involved in this direction should take dependency on ONNXRuntime?
codemzs(2020-04-27 15:56:15):Which meeting? 
ebarsoum(2020-04-27 16:06:33):Guys, we need a staging domain to iterates on operators and gather feedbacks. This is different than the old experimental, which had FX specific operators that we needed to support.

@linkerzhang we will document that this is only to gather feedbacks and iterate on the OPs. Once it is ready and implemented in a runtime, then we can move it to core. 
codemzs(2020-04-27 18:40:23):@ebarsoum @wschin Can this _staging_ spec not be in a MD file, essentially all the same syntax but in a MD file?

What advantage do we get by having it as code in CC file and then we are creating this additional experimental domain?

If someone wants to implement they can copy the content and implement locally just like we do in [gradient_schema_defs.cc ](https://github.com/microsoft/onnxruntime/blob/master/orttraining/orttraining/core/graph/gradient_schema_defs.cc)?
chinhuang007(2020-04-27 21:19:13):If we are introducing this new 'staging domain' in release 1.7, I think we need to clearly articulate the motivation and what to expect in terms of quality, readiness, testing, docs, etc for the ops to be included in this domain so that the broader community knows exactly what to expect and what to do with the staging domain ops. Hope this will serve ONNX community better than the "experimental  ops" experience.
linkerzhang(2020-04-28 01:25:05):@wschin I may mislead you. It's very critical to have training support in ONNX. The work that have been done are very appreciated. 

This discussion now is NOT saying to revert all "training" related stuff from ONNX, but only the "OPs" which folks think are not ready (and need more iterations). All the IR related stuff are good enough with good quality.

Even for "OPs", not all operators have to be reverted, but those need more iterations/changes.

@ebarsoum @gramalingam The big reason that another kind of "experimental" area for "OPs" should not be created is that,
1. what's the bar of the quality? how do you want to control it? it's not only targeting for this specific "training" scenario, right?
2. what's the expectation of "versioning"? 
3. what's the timeline of either promoting them or removing them from ONNX?

This 3 questions were met previously, and obviously it's hard to give answers to them.

Meanwhile, after all IR related stuff introduced into ONNX (for training), "OPs" should NOT be a blocking issue for any ONNX partner (say, ORT) to try more on ONNX training. As @codemzs mentioned, there're already many "contrib" ops in ORT, and they're being verified by ORT in real biz scenarios.
ebarsoum(2020-04-28 05:56:43):@linkerzhang  and @chinhuang007 can you attend next operator meeting (This Wednesday)? 

1. The quality if experimental domain is the same as other ONNX operator currently, the bar isn't lower with . With the exception, if an OP isn't yet implemented by any framwork. This isn't only for training.
2. Experimental OPs aren't part of the release, version will be there for tracking. We will be explicit about it, no backward compatibility, it is just a preview to gather feedbacks and iterate on the OPs. If we have FooV5 in experimental and decided to promote it to core, then it will move to core as FooV1 and we will delete FooV5.
3. That is a great questions, those operators will be reviewed by the operators SIG, and operator SIG will ping people if there is no traction on some OPs. Operator SIG will decide when to remove the operators.

We will add a big banner in the md file that those OPs are for feedback only from framework developers, and they aren't part of the spec. 
linkerzhang(2020-04-28 08:53:54):> @linkerzhang and @chinhuang007 can you attend next operator meeting (This Wednesday)?
> 
> 1. The quality if experimental domain is the same as other ONNX operator currently, the bar isn't lower with . With the exception, if an OP isn't yet implemented by any framwork. This isn't only for training.
> 2. Experimental OPs aren't part of the release, version will be there for tracking. We will be explicit about it, no backward compatibility, it is just a preview to gather feedbacks and iterate on the OPs. If we have FooV5 in experimental and decided to promote it to core, then it will move to core as FooV1 and we will delete FooV5.
> 3. That is a great questions, those operators will be reviewed by the operators SIG, and operator SIG will ping people if there is no traction on some OPs. Operator SIG will decide when to remove the operators.
> 
> We will add a big banner in the md file that those OPs are for feedback only from framework developers, and they aren't part of the spec.

Haha, what time will be meeting hosted? I'll skip it if it's a mid-night time slot (after 4PM saves my life if it's doable :))

@ebarsoum, Sir, I still don't see a strong reason that "OPs" needs to be previewed, especially needs to be previewed with "c++" changes in ONNX repo.

Even stepping back a little bit, if there're a group of "OPs" which need to be previewed for discussion, then a separate .md file created and merged in ONNX repo should be good enough. Say, operators_for_preview.md.

If "Preview" ops will be promoted to "production" ops with v1 again, what's the point of tracking their versions in "Preview"? are we expecting partners will play with "Preview" ops with versioning? if yes, that will be very bad, as when "preview" ops promoted with V1 later, troubles will be introduced to partners (depending on Preview versions), if not, then no need having "versioning" for Preview ops (this is again why I think a .md document is good enough for preview instead of "c++" changes).
chinhuang007(2020-04-28 16:03:53):@ebarsoum Sure, I will attend the Operators SIG meeting this Wed to understand this new "staging domain" concept.
wschin(2020-04-28 16:49:17):We cannot stage spec in MD. It creates spec even without trying to compile it. If company A creates a operator in MD while company B tries to implement it, do you suggest company B to translate English or whatever in MD into their C++ code? Would it be possible that the C++ code in A's mind is always identical to the C++ code in B's mind? Most machine learning software has preview APIs, where `preview` means that this is the best setting given our limited knowledge and please join us to make it better. I don't see any problem with a similar concept in ONNX. I understand you worry a lot about `previous experimental` ops. Please let me share an observation: Microsoft encouraged the uses of experimental ops in production, created a lot of models with experimental ops, and finally felt painful when dropping them. The problem comes from the **promotion** of experimental ops inside Microsoft, not creating some experimental ops.

As for recent bugs in ONNX-1.7, I think they occur because some authors didn't fully follow the stages in proposing new operators while bad reviewers like me were not perfectly doing their job. If everyone follow the rules from now on, we will have a better world. But, again, the problem does not come from a domain of `ai.onnx.experimental`.
linkerzhang(2020-04-29 07:11:01):> We cannot stage spec in MD. It creates spec even without trying to compile it. If company A creates a operator in MD while company B tries to implement it, do you suggest company B to translate English or whatever in MD into their C++ code? Would it be possible that the C++ code in A's mind is always identical to the C++ code in B's mind? Most machine learning software has preview APIs, where `preview` means that this is the best setting given our limited knowledge and please join us to make it better. I don't see any problem with a similar concept in ONNX. I understand you worry a lot about `previous experimental` ops. Please let me share an observation: Microsoft encouraged the uses of experimental ops in production, created a lot of models with experimental ops, and finally felt painful when dropping them. The problem comes from the **promotion** of experimental ops inside Microsoft, not creating some experimental ops.
> 
> As for recent bugs in ONNX-1.7, I think they occur because some authors didn't fully follow the stages in proposing new operators while bad reviewers like me were not perfectly doing their job. If everyone follow the rules from now on, we will have a better world. But, again, the problem does not come from a domain of `ai.onnx.experimental`.

Sorry that I am still not convinced. Take ORT contrib ops as an example, there're bunch of ops being created/registered/verified in ORT without touching ONNX spec or experimenting/staging in ONNX spec. 

The point of having this "Preview" is not about "compilation", is about showing it to public for discussion, right? That's why I was saying a markdown file is good enough for this purpose. That means, internally, for a partner, say, ORT again, the ops' codes (registration codes) may still follow the same way as other "contrib" ops. 
wschin(2020-04-29 14:57:15):> > We cannot stage spec in MD. It creates spec even without trying to compile it. If company A creates a operator in MD while company B tries to implement it, do you suggest company B to translate English or whatever in MD into their C++ code? Would it be possible that the C++ code in A's mind is always identical to the C++ code in B's mind? Most machine learning software has preview APIs, where `preview` means that this is the best setting given our limited knowledge and please join us to make it better. I don't see any problem with a similar concept in ONNX. I understand you worry a lot about `previous experimental` ops. Please let me share an observation: Microsoft encouraged the uses of experimental ops in production, created a lot of models with experimental ops, and finally felt painful when dropping them. The problem comes from the **promotion** of experimental ops inside Microsoft, not creating some experimental ops.
> > As for recent bugs in ONNX-1.7, I think they occur because some authors didn't fully follow the stages in proposing new operators while bad reviewers like me were not perfectly doing their job. If everyone follow the rules from now on, we will have a better world. But, again, the problem does not come from a domain of `ai.onnx.experimental`.
> 
> Sorry that I am still not convinced. Take ORT contrib ops as an example, there're bunch of ops being created/registered/verified in ORT without touching ONNX spec or experimenting/staging in ONNX spec.
> 
> The point of having this "Preview" is not about "compilation", is about showing it to public for discussion, right? That's why I was saying a markdown file is good enough for this purpose. That means, internally, for a partner, say, ORT again, the ops' codes (registration codes) may still follow the same way as other "contrib" ops.

They are not contrib ops. They are decisions made by Training WG. A full training spec combines all training ops and ONNX format changes --- no one should be missed. Do you suggest Training WG use ORT to store their progress? There are many cases we need to write C++ spec to do something. Can MD solve my concern on cross-company/team development and ambiguity of text? For your concerns, it's really up to how we use this experimental domain. They may or may not happen depending on our investment to ONNX. Even without this experimental domain, all the pain in this release will show up again if we don't invest more.
codemzs(2020-04-29 15:04:55):@wschin Thanks, I think your concerns are reasonable but they don’t warrant a new domain. We can start with MD file and progress towards a domain if needed. Directly jumping into a new domain few days before a release is risky and seems reckless especially since the release was already delayed due to quality issues. 
wschin(2020-04-29 15:16:17):> @wschin Thanks, I think your concerns are reasonable but they don’t warrant a new domain. We can start with MD file and progress towards a domain if needed. Directly jumping into a new domain few days before a release is risky and seems reckless especially since the release was already delayed due to quality issues.

What you said is not very different than Ke suggestion. I have repeated many times. No, MD is not enough given we have finished their C++ spec (if Training WG already goes beyond MD stage, why do they need to move back?). I don't think new domain is a problem. We originally have a `ai.onnx.training` domain for those training ops. It works just fine and it's not a change we can't affort.
codemzs(2020-04-29 15:19:42):@wschin Your C++ argument doesn’t seem strong. Much of world’s planning happens in plain text format. Please refer to C++ standard discussions in the community, they don’t introduce “experimental” syntax, it’s all in pdfs and emails. Please don’t try to reinvent the wheel.

You also missed my point on risking the release. I’m seriously concerned.
wschin(2020-04-29 15:30:33):> @wschin Your C++ argument doesn’t seem strong. Much of world’s planning happens in plain text format. Please refer to C++ standard discussions in the community, they don’t introduce “experimental” syntax, it’s all in pdfs and emails. Please don’t try to reinvent the wheel.
> 
> You also missed my point on risking the release. I’m seriously concerned.

It's not about planning. It's a decision went through year-long discussion and must be stored in a most solid way and their next working items will have to use C++. It's not necessary for us to follow C++ community. Pytorch and Tensorflow have experimental APIs. Why can't we follow them? Taking conservative step is fine for traditional software. For deep learning, we need to move fast and iterate on it.

ONNX Training Spec is already in the master for a long while. They are stable. We have two domains for a long while and we support them very well. Why bother to add another domain?
codemzs(2020-04-29 15:34:41):@wschin Deep learning is slowly turning into a programming languages problem hence C++ was a more relevant example. This idea of a experimental domain only popped up recently, it needs more review.
wschin(2020-04-29 15:36:29):> @wschin Deep learning is slowly turning into a programming languages problem hence C++ was a more relevant example. This idea of a experimental domain only popped up recently, it needs more review.

Deep learning becoming programming language doesn't stop TF or Pytorch from using experimental APIs.

We can restrict the uses of experimental domain. It's under Operator SIG's control.
gramalingam(2020-04-29 15:51:34):A quick question: would it help to distinguish "master releases" from other "experimental branches"? In my opinion, the important question is not whether we use text or code, but whether it is part of the standard or not. I think there is clear agreement (from both sides) that the "experimental" part is not part of the "standard". The question then becomes what is the best, unambiguous way, to clarify that something is not part of the standard. Is the "experimental domain" sufficient to clarify it is not part of the standard? If not, should we exclude it from the "official master release" and have it only in other branches?
gramalingam(2020-04-29 15:55:06):(Just to clarify the previous comment, the above decision also has implications for including "training as preview" in the master-release.)
codemzs(2020-04-29 15:58:41):@gramalingam The master branch should only contain standard. May be you can keep experimental stuff in an experimental branch. Fair? If someone wants to implement experimental they can do it same way as CONTRIB.
gramalingam(2020-04-29 16:20:15):@Emad's suggestion "We will add a big banner in the md file that those OPs are for feedback only from framework developers, and they aren't part of the spec." is also one way to emphasize the distinction. We could take it one step further and make experimental.md a different file to further emphasize it is not part of the standard. It seems the question is about how heavyweight or lightweight a mechanism we want to keep the distinction between the standard and experimental clear.
codemzs(2020-04-29 16:22:22):@gramalingam I’m aware of the banner comment. Let’s start with an experimental branch and move up if needed.
gramalingam(2020-04-29 18:13:56):Speaking of C++ standards: this is what https://isocpp.org/std/status says:
```
Starting in 2012, the committee has transitioned to a “decoupled” model where major pieces of
work can progress independently from the Standard itself and be delivered in “feature branch”
TSes. Vendors can choose to implement these, and the community can gain experience with the
std::experimental version of each feature. This lets us learn and adjust each feature’s design
based on experience before it is cast in stone when merged into the “trunk” C++ Standard
itself. In the meantime, the Standard can be delivered on a more regular cadence with smaller
and more predictable batches of features. This approach also helps C++ compilers to track the
Standard more closely and add both the experimental and the draft-final C++ features in a
more consistent order.
```
wschin(2020-04-29 19:19:30):> Speaking of C++ standards: this is what https://isocpp.org/std/status says:
> 
> ```
> Starting in 2012, the committee has transitioned to a “decoupled” model where major pieces of
> work can progress independently from the Standard itself and be delivered in “feature branch”
> TSes. Vendors can choose to implement these, and the community can gain experience with the
> std::experimental version of each feature. This lets us learn and adjust each feature’s design
> based on experience before it is cast in stone when merged into the “trunk” C++ Standard
> itself. In the meantime, the Standard can be delivered on a more regular cadence with smaller
> and more predictable batches of features. This approach also helps C++ compilers to track the
> Standard more closely and add both the experimental and the draft-final C++ features in a
> more consistent order.
> ```

I guess this reflects one thing: even C++ has a stage to make progress on features in preview. Such a preview stage is natural to community-based developments. `std::experimental` sounds a mirror of `ai.onnx.experimental`. Just for reference:
```
The C++ standards committee publishes experimental C++ language and library extensions for future standardization.
```
from https://en.cppreference.com/w/cpp/experimental, which is the same as this PR tries to do.
codemzs(2020-04-29 20:38:41):@wschin First this is recent, two, it says preview features are in its own feature branch which is what I [suggested ](https://github.com/onnx/onnx/pull/2741#issuecomment-621304632)previously. No one is arguing it is bad to have preview features, just not in the same place as production feature.
gramalingam(2020-04-24 19:20:16):Change file name to use underscores consistently (it is a mix of underscore and hyphen right now)?
wschin(2020-04-24 23:02:53):Done.
codemzs(2020-04-25 09:13:04):I feel like this might be a good change to take for ONNX 1.7 release. @natke can you please fix build errors soon if possible? We plans cut the release early next week preferably on Monday.
natke(2020-04-27 16:37:50):@codemzs wrote:
> I feel like this might be a good change to take for ONNX 1.7 release. @natke can you please fix build errors soon if possible? We plans cut the release early next week preferably on Monday.

Sure, will do! On first glance, as someone who is more familiar with the build system, do you have any pointers on what is causing the break?

gramalingam(2020-04-27 17:13:18):One possibility for the build-break is that operators-ml.md also needs to be regenerated and checked in. I see only operators.md updated. (This might require setting environment variable ONNX_ML before document generation, I don't remember exact details.)
wschin(2020-05-02 06:12:28):To regenerate all documents under Windows + Anaconda, you can run
```
F:\repos\onnx> .\tools\update_doc.bat
```
codemzs(2020-05-02 06:49:10):@natke Sorry for the delay in my reply, I almost missed your message amid ONNX 1.7/ORT 1.3 release. 

Below are the steps to regenerate files from the root _assuming_ you are on Windows OS, however if you are on Linux then please use _update_doc.sh_ instead and have backward slashes for file paths instead of forward. 

1. Run `tools\update_doc.bat` 
    - After you run this command you will see bunch files being modified, that is because the command regenerates all MD, ONNX and PB files. ONNX files are generated from test files (in python), these files contain the model, PB files are also generated from test files but these contain input to the test and expected output from the test that comes from the reference implementation from the python test files. Both ONNX and PB files are placed [here](https://github.com/onnx/onnx/tree/master/onnx/backend/test/data/node) and reference implementation and test files can be found [here](https://github.com/onnx/onnx/tree/master/onnx/backend/test/case/node).
    - Our test runners on ORT side, open these test files, execute the model (ONNX file) with inputs (PB file) and compare the output with the PB (output) files to make sure results from ORT kernel matches reference implementation, which is why it is important to have the reference implementation correct.
2. `git add *.md`
3. **(Optional)** `git add */*softmax_cross_entropy_*` _[Do only if you have modified any python file that may affect test, the example depicts softmax_cross_entropy related changes, you will need to repeat this step for all sorts of files you want to keep]_
4. `git checkout */*.onnx */*.json */*.pb` (discard all other files that are regenerated but don't contain any change you want)
5. `git checkout *.proto* `(same for proto files)
6. `git status`
7. **(Optional)** `git add` any files that you may modified like *.cc files that contain spec or *.py that may contain test/reference implementation changes and make sure the updated files are also added.

**I also recommend using Anaconda and having a fresh environment that contains ONNX. Below are the packages installed in my conda environment, you _probably_ don't need all the packages but with _all_ of the below packages I can guarantee your build should work, may be install as needed and most will come pre-installed anyways**

```
(onnx) E:\github\onnx>conda list
packages in environment at e:\Continuum\anaconda2\envs\onnx:

 Name                    Version                   Build  Channel
astroid                   2.3.3                    py37_1    conda-forge
atomicwrites              1.4.0                    pypi_0    pypi
attrs                     19.3.0                   pypi_0    pypi
ca-certificates           2019.11.28           hecc5488_0    conda-forge
certifi                   2019.11.28       py37hc8dfbb8_1    conda-forge
cffi                      1.14.0           py37ha419a9e_0    conda-forge
colorama                  0.4.3                      py_0    conda-forge
cudatoolkit               10.1.168                      0    anaconda
freetype                  2.10.1               ha9979f8_0    conda-forge
importlib-metadata        1.6.0                    pypi_0    pypi
intel-openmp              2020.0                      166    anaconda
isort                     4.3.21           py37hc8dfbb8_1    conda-forge
jpeg                      9c                hfa6e2cd_1001    conda-forge
lazy-object-proxy         1.4.3            py37hfa6e2cd_1    conda-forge
libblas                   3.8.0                    15_mkl    conda-forge
libcblas                  3.8.0                    15_mkl    conda-forge
liblapack                 3.8.0                    15_mkl    conda-forge
libpng                    1.6.37               hfe6a214_1    conda-forge
libprotobuf               3.9.2                h1a1b453_0    conda-forge
libtiff                   4.1.0                h885aae3_6    conda-forge
lz4-c                     1.8.3             he025d50_1001    conda-forge
mccabe                    0.6.1                      py_1    conda-forge
mkl                       2020.0                      166    anaconda
more-itertools            8.2.0                    pypi_0    pypi
ninja                     1.10.0               h1ad3211_0    conda-forge
numpy                     1.18.1           py37hc71023c_0    conda-forge
olefile                   0.46                       py_0    conda-forge
onnx                      1.7.0                     dev_0    <develop>
openssl                   1.1.1f               hfa6e2cd_0    conda-forge
packaging                 20.3                     pypi_0    pypi
pillow                    7.1.1            py37h91e7a8d_0    conda-forge
pip                       20.0.2                     py_2    conda-forge
pluggy                    0.13.1                   pypi_0    pypi
protobuf                  3.9.2            py37he025d50_1    conda-forge
py                        1.8.1                    pypi_0    pypi
pycparser                 2.20                       py_0    conda-forge
pylint                    2.4.4                    py37_0    conda-forge
pyparsing                 2.4.7                    pypi_0    pypi
pytest                    5.4.1                    pypi_0    pypi
python                    3.7.6           h5b45d93_4_cpython    conda-forge
python_abi                3.7                     1_cp37m    conda-forge
pytorch                   1.4.0           py3.7_cuda101_cudnn7_0    pytorch
setuptools                46.0.0           py37hc8dfbb8_2    conda-forge
six                       1.14.0                     py_1    conda-forge
sqlite                    3.30.1               hfa6e2cd_0    conda-forge
tk                        8.6.10               hfa6e2cd_0    conda-forge
torchvision               0.5.0                py37_cu101    pytorch
typed-ast                 1.4.1            py37hfa6e2cd_0    conda-forge
typing-extensions         3.7.4.1                  pypi_0    pypi
vc                        14.1                 h21ff451_3    anaconda
vs2015_runtime            15.5.2                        3    anaconda
wcwidth                   0.1.9                    pypi_0    pypi
wheel                     0.34.2                     py_1    conda-forge
wincertstore              0.2                   py37_1003    conda-forge
wrapt                     1.12.1           py37h8055547_1    conda-forge
xz                        5.2.4             h2fa13f4_1002    conda-forge
zipp                      3.1.0                    pypi_0    pypi
zlib                      1.2.11            h2fa13f4_1006    conda-forge
zstd                      1.4.4                hd8a0e53_2    conda-forge
```


I have also forked your branch and updated the MD files and pushed them at [this ](https://github.com/codemzs/onnx/tree/add-version-to-ops)branch in my remote, I wanted to push them in your branch but got 403 access denied: 
```
(onnx) E:\github\onnx>git push -u https://github.com/natke/onnx.git add-version-to-ops
remote: Permission to natke/onnx.git denied to codemzs.
fatal: unable to access 'https://github.com/natke/onnx.git/': The requested URL returned error: 403
```

Would it be possible to update this branch soon so that we can take it for ONNX 1.7 release? Thanks!

codemzs(2020-05-02 08:42:20):> One possibility for the build-break is that operators-ml.md also needs to be regenerated and checked in. I see only operators.md updated. (This might require setting environment variable ONNX_ML before document generation, I don't remember exact details.)

I don't think you need to since _update.bat/sh_ does that for you, see below output from running it:

```
Using e:\continuum\anaconda2\envs\onnx\lib\site-packages
Finished processing dependencies for onnx==1.7.0

(onnx) E:\github\onnx>python onnx\backend\test\cmd_tools.py generate-data

(onnx) E:\github\onnx>python onnx\backend\test\stat_coverage.py

(onnx) E:\github\onnx>python onnx\defs\gen_doc.py

(onnx) E:\github\onnx>set ONNX_ML=0

(onnx) E:\github\onnx>python onnx\defs\gen_doc.py

(onnx) E:\github\onnx>set ONNX_ML=1
```
natke(2020-06-24 22:48:29):@codemzs, Thank you for the doc build instructions! I've updated to the latest and re-ran the build. A couple of the tests are timing out - any ideas for that?
CLAassistant(2020-04-25 00:24:34):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=2744) <br/>All committers have signed the CLA.
snnn(2020-04-25 06:26:47):See #2668

1. If you enable the check, then one of the CI build will fail. But there is nothing wrong with ONNX. PyTorch should be fixed. So you should get familiar with PyTorch codebase and fix their bug first. 

2. Then you can go back enable the check. However,  it's not just as easy as turn on a flag. The shape inference context implementation itself ignore exceptions. So even when there are inference errors,  they'll be silently ignored. see: https://github.com/onnx/onnx/blob/master/onnx/shape_inference/implementation.cc#L221

So, the flag you are changing is indeed very deceptive.  

jcwchen(2020-04-25 20:49:13):> See #2668
> 
> 1. If you enable the check, then one of the CI build will fail. But there is nothing wrong with ONNX. PyTorch should be fixed. So you should get familiar with PyTorch codebase and fix their bug first.
> 2. Then you can go back enable the check. However,  it's not just as easy as turn on a flag. The shape inference context implementation itself ignore exceptions. So even when there are inference errors,  they'll be silently ignored. see: https://github.com/onnx/onnx/blob/master/onnx/shape_inference/implementation.cc#L221
> 
> So, the flag you are changing is indeed very deceptive.

Hi @snnn, I am trying to make the onnx checker stricter so I set the flag first to see whether it could pass the CI. I will improve it according to your ideas. Thank you for the helpful instructions!
jcwchen(2020-05-11 22:36:53):After enabling full_check, it encounters the pytorch test error: Type consistency error
Might need this [PR](https://github.com/pytorch/pytorch/pull/37787) to fix the test failure in Pytorch.
jcwchen(2020-05-18 18:22:26):### Target
Make onnx-checker support large models
### Limitations
1. Protobuf file has a hard limit of 2GB
2. After loading models, onnx cannot know the original path of the loaded model
### Facts
1. Passing protobuf>2GB (model) with loaded raw_data will lose some infomration to C++ onnx-checker
2. Passing protobuf>2GB (model) without loaded raw_data will not pass C++ onnx-checker 
3. Passing protobuf<2GB (model) with loaded raw_data works well to C++ onnx-checker
4. Passing model path works well to C++ onnx-checker 
### Proposals
1. After loading those external tensors to raw_data, correct the data_location from external_data to DEFAULT.
2. For those protobuf>2GB, onnx should provide model_path to onnx-checker instead of onnx model itself. (However, I think detecting whether the model is > 2GB might be a little messy. How about we provide model_path to onnx-checker for **those models with external tensors** ?)
3. Add one more data_field in ModelProto: model_path (Prevent to modify the interface of onnx.load_model)
### Drawbacks
1. Hard to maintain tests on various platforms like the path issue. (\/ vs \\) Current solution is making the path even to various platform for matching testing results
2. Will adding one more field in ModelProto compromise some tests?
### TO-DOs:
1. Decide when to use model_path instead of model for onnx-checker
2. Add tests for checking large model (>2GB with external tensors)
3. Organize the commit history
4. Add more comments for onnx.load with external tensors 
### References
ONNX https://github.com/onnx/onnx/pull/678
ORT https://github.com/microsoft/onnxruntime/pull/520


jcwchen(2020-05-18 18:27:34):I made some modification as POC. @gramalingam Please help me to review the proposal. Thank you.
gramalingam(2020-05-19 23:26:14):It seems reasonable to add model_path to the python object representing a model. But adding it to the proto seems unnecessary. I don't think we should pollute the proto definition with features/fields needed only in a Python or C++ utility function. If the protoc-generated python-class/object doesn't allow addition of new fields/attributes, we should be able to workaround that, I think.
jcwchen(2020-05-20 15:49:01):> It seems reasonable to add model_path to the python object representing a model. But adding it to the proto seems unnecessary. I don't think we should pollute the proto definition with features/fields needed only in a Python or C++ utility function. If the protoc-generated python-class/object doesn't allow addition of new fields/attributes, we should be able to workaround that, I think.

Agree that we should not pollute the proto definition so I did tried it before. However, like you mentioned, protoc-generated python-class/object doesn't allow addition of new fields/attributes. A possible solution would be introducing a new model class in Python including the original model proto and filepath, but it will change the return type of load_model and influence the usage. Otherwise, I am afraid I don't know what else we can do here. Do you have any preference about how to solve this problem here @gramalingam ?
Options are here:
A) Modify ModelProto (same as the previous commit) 
B) Introduce a new model class in Python including ModelProto and filepath
C) Anything else? 
jcwchen(2020-05-20 17:14:39):Since the modification for enabling two functionality might conflict, it would be better to decouple different tasks from this PR. 
This PR focus on supporting large models for onnx-checker. Catching inference error has been moved to another PR https://github.com/onnx/onnx/pull/2783.  
jcwchen(2020-06-09 22:51:42):### Description

- Throw an error if the model is larger than 2GB in onnx.checker
- Fix a load_external_tensor bug (change into DEFAULT after loading)
- Add tests for 2GB models and refactor original test cases

### Motivation and Context
Current onnx.checker doest not support models larger than 2GB because they exceed the maximum of protobuf. By contrast, C++ checker works well with models with external tensors.
Therefore, when the model is larger than 2GB, users should use onnx.checker with model path instead of the model proto. 
jcwchen(2020-06-11 21:28:59):I will add some documents for using load_external_data in this PR
jcwchen(2020-06-12 01:21:52):- Add a ExternalData.md to tell users how to deal with external data
- Add example code in PythonAPIOverview
- Add a Link to ExternalData.md in IR.md
gramalingam(2020-04-30 19:19:34):Is this meant to be for debugging? 
jcwchen(2020-04-30 19:27:24):Yes, it will be removed. The current version is still in progress.  
gramalingam(2020-05-29 18:00:15):Why do we need a "loose" boundary? Can we define it exactly?
gramalingam(2020-05-29 18:00:47):Better to avoid serializing the model twice.
jcwchen(2020-05-29 18:43:11):Good catch.
jcwchen(2020-05-29 19:05:36):I was afraid that the function is approximate. Just did some experiment show the calculation is precise enough. Should set 2GB for the readability. Thank you for catching this. 
gramalingam(2020-05-30 02:13:02):Does warn throw an exception? Wouldn't it be better for the check_model to be in the else branch?
jcwchen(2020-05-30 03:45:30):Warn just throws a message without interrupting the code. 
Sounds good. Models larger than 2gb won't pass the checker anyway so there's no need to run the checker. 
codemzs(2020-04-28 21:17:11):@houseroad travis is very busy it will clear up later in the evening. Again please merge https://github.com/pytorch/pytorch/pull/37309 and I will let you know if I need you for anything else.
houseroad(2020-04-28 21:18:43):Sure, i will land pytorch/pytorch#37309 today if everything looks good.
codemzs(2020-04-28 21:19:37):thank you very much @houseroad 
codemzs(2020-04-27 17:51:41):@houseroad is merge from master enough, do we not have to regenerate the MD files? May be not, since MD file can regenerated.
KsenijaS(2020-04-27 20:09:27):looks good.
codemzs(2020-04-28 02:27:49):@houseroad It seems Caffe2 tests are [failing ](https://circleci.com/gh/onnx/onnx/4654?utm_campaign=vcs-integration-link&utm_medium=referral&utm_source=github-build-link)to build with the below error:

ninja: error: build.ninja:40966: multiple rules generate ../tools/build_variables.bzl [-w dupbuild=err]

Any idea what happened with PyTorch?
codemzs(2020-04-28 21:06:10):@houseroad I already have.
houseroad(2020-04-28 21:08:38):license/cla Expected — Waiting for status to be reported?

Why?
codemzs(2020-04-28 21:13:46):@houseroad I will appreciate if you can please merge https://github.com/pytorch/pytorch/pull/37309 ... I can take care of the rest 
codemzs(2020-04-27 06:09:50):This PR depends on https://github.com/pytorch/pytorch/pull/37309 that is causing CI failure with Caffe2 tests not being able to find BatchNorm(12)

CC: @houseroad 
neginraoof(2020-04-28 16:09:09):@houseroad Since ORT tests are not running on CI, do we still need to update ORT version?
snnn(2020-05-06 23:08:23):BTW, currently, building from onnx on Ubuntu is ok. But it doesn't work on CentOS/RHEL/manylinux/... . 
On these systems, you need to open:
https://github.com/onnx/onnx/blob/master/CMakeLists.txt#L124
and search replace all the "/lib" to "/lib64".

snnn(2020-05-06 23:09:58):Similar, if you need debug version of ONNX, search things like "NAMES protobuf-lite", append a letter 'd' at the end. 
vinitra-zz(2020-05-07 18:35:58):> BTW, currently, building from onnx on Ubuntu is ok. But it doesn't work on CentOS/RHEL/manylinux/... .
> On these systems, you need to open:
> https://github.com/onnx/onnx/blob/master/CMakeLists.txt#L124
> and search replace all the "/lib" to "/lib64".

I've included these instructions in the README. For now, I'm not including the Debug instructions because it's not used as frequently. Thanks!
wschin(2020-05-07 20:23:15):> > BTW, currently, building from onnx on Ubuntu is ok. But it doesn't work on CentOS/RHEL/manylinux/... .
> > On these systems, you need to open:
> > https://github.com/onnx/onnx/blob/master/CMakeLists.txt#L124
> > and search replace all the "/lib" to "/lib64".
> 
> I've included these instructions in the README. For now, I'm not including the Debug instructions because it's not used as frequently. Thanks!

Including Debug instructions helps new devs working on this topic. It'd be great to have it in this PR. Thank you.
gramalingam(2020-05-07 22:02:05):BTW, I am curious about the instructions for building protobuf from source for a Windows installation. I use protobuf installed from conda, and it works for me. However, I had to install a specific version of protobuf (=3.5.1) from conda. Does anyone have more concrete information? May be it will help to suggest that people installing from conda should use version 3.5.1 (even though we may officially recommend building protobuf from source)?
snnn(2020-05-07 23:54:10):> BTW, I am curious about the instructions for building protobuf from source for a Windows installation. I use protobuf installed from conda, and it works for me. However, I had to install a specific version of protobuf (=3.5.1) from conda. Does anyone have more concrete information? May be it will help to suggest that people installing from conda should use version 3.5.1 (even though we may officially recommend building protobuf from source)?

There are 3 special variables:
1. Env variable: USE_MSVC_STATIC_RUNTIME. Should be 1 or 0, not ON or OFF.
2. Cmake variables: ONNX_USE_PROTOBUF_SHARED_LIBS Protobuf_USE_STATIC_LIBS
You must set all of these 3 correctly.  If ONNX_USE_PROTOBUF_SHARED_LIBS is ON, then Protobuf_USE_STATIC_LIBS must be off and USE_MSVC_STATIC_RUNTIME must be 0.  If ONNX_USE_PROTOBUF_SHARED_LIBS is OFF then Protobuf_USE_STATIC_LIBS must be ON and USE_MSVC_STATIC_RUNTIME can be 1 or 0.
For example, one possible combination is:
```
$Env:USE_MSVC_STATIC_RUNTIME=1
$Env:CMAKE_ARGS="-DONNX_USE_PROTOBUF_SHARED_LIBS=OFF -DProtobuf_USE_STATIC_LIBS=ON
```
If you protobuf was built from source with static link to CRT and doesn't have an DLL.


If you got the libprotobuf from conda, you need to check how the conda package was built.  The package you referred changed its build command around that time(3.5.1).  

Most people don't know these things, they just use the default setting. They didn't think it's nesessary to set the 3 variables manually. 

And, all of the above are based on an assumption that you're using the latest cmake. Otherwise, depending on which cmake version you have, the instruction may differ.


vinitra-zz(2020-05-08 00:23:47):Based on @snnn's instructions above and @jacky82226's experience with installing on Windows, I've aligned the "Build with Anaconda on Windows" instructions with the new Azure Pipelines Windows CI.
wschin(2020-05-11 15:12:04):@vinitra, can you add @snnn's explanation to `common error` section? I mean this:
```
Cmake variables: ONNX_USE_PROTOBUF_SHARED_LIBS Protobuf_USE_STATIC_LIBS
You must set all of these 3 correctly. If ONNX_USE_PROTOBUF_SHARED_LIBS is ON, then Protobuf_USE_STATIC_LIBS must be off and USE_MSVC_STATIC_RUNTIME must be 0. If ONNX_USE_PROTOBUF_SHARED_LIBS is OFF then Protobuf_USE_STATIC_LIBS must be ON and USE_MSVC_STATIC_RUNTIME can be 1 or 0.
```
vinitra-zz(2020-05-11 16:11:48):> @vinitra, can you add @snnn's explanation to `common error` section? I mean this:
> 
> ```
> Cmake variables: ONNX_USE_PROTOBUF_SHARED_LIBS Protobuf_USE_STATIC_LIBS
> You must set all of these 3 correctly. If ONNX_USE_PROTOBUF_SHARED_LIBS is ON, then Protobuf_USE_STATIC_LIBS must be off and USE_MSVC_STATIC_RUNTIME must be 0. If ONNX_USE_PROTOBUF_SHARED_LIBS is OFF then Protobuf_USE_STATIC_LIBS must be ON and USE_MSVC_STATIC_RUNTIME can be 1 or 0.
> ```

Included in recent update!
vinitra-zz(2020-05-12 16:52:08):> ## Binaries
> A binary build of ONNX is available from [Conda](https://conda.io), in [conda-forge](https://conda-forge.org/):
> ```
> conda install -c conda-forge onnx
> ```

While trying these instructions, I noticed the onnx version was only 1.1.1 in a Python 3.6 environment. Should we be updating the binary release with new releases? Does it make sense to remove this if it's not up to date?
wschin(2020-05-14 20:49:23):> > ## Binaries
> > A binary build of ONNX is available from [Conda](https://conda.io), in [conda-forge](https://conda-forge.org/):
> > ```
> > conda install -c conda-forge onnx
> > ```
> 
> While trying these instructions, I noticed the onnx version was only 1.1.1 in a Python 3.6 environment. Should we be updating the binary release with new releases? Does it make sense to remove this if it's not up to date?

ONNX on Anaconda is not up-to-date. Someone needs to do the work. You can create an issue for tracking it. I guess it can be prioritized based on demand.
CLAassistant(2020-05-01 05:43:48):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=2756) <br/>All committers have signed the CLA.
orionr(2020-05-01 15:31:14):@houseroad are you able to help here? Thanks.
wschin(2020-05-05 18:49:15):This file is renamed to operator_sets_preview.h.
linkerzhang(2020-05-06 01:51:28):https://github.com/onnx/onnx/blob/673aef09ba6cb56e7c34f807255f8372f253c0ed/onnx/common/constants.h#L14
codemzs(2020-05-06 01:52:29):@linkerzhang thanks, I was searching for ONNX_PREVIEW_DOMAIN (match full word/cased), but looks like we also have AI_ONNX_PREVIEW_DOMAIN
linkerzhang(2020-05-06 01:59:48):please also change this macro's name accordingly.
codemzs(2020-05-06 02:01:27):ONNX_PREVIEW_TRAINING_OPERATOR_SET_SCHEMA look good?
codemzs(2020-05-06 15:36:28):@chinhuang007 It seems to only have last commit from master? I don’t see SoftmaxCrossEntopyLoss for example. 
chinhuang007(2020-05-06 15:51:43):@codemzs Please be specific on what is missing. My branch is 1 commit ahead of master. So it is a superset of master.
codemzs(2020-05-06 15:59:59):@chinhuang007 That clarifies. Thanks. 
wschin(2020-05-06 16:03:18):Is this change intended? Does 105 have a special meaning?
chinhuang007(2020-05-06 16:06:40):Yes, this is for the version number in test pypi. We have to use a different number each time we have a release candidate. Once the final verification is complete, we will change it back to 1.7.0 in the release branch.
chinhuang007(2020-05-06 16:09:14):There is no special meaning other than I started from 101 and increment by 1 for each release candidate.
wschin(2020-05-13 17:50:05):We need some basic shape inference tests for this new type. I am not sure how hard to add some numerical tests (like creating input and output from Tensorflow), but if it's not very hard, we should add some.
gramalingam(2020-05-13 18:44:36):@wschin : did you mean node tests? I think shape-inference should not depend on the primitive type (type-inference does, but if it correctly works for multiple types already, I don't think it is important to add new ones). But I agree that test-cases for the runtime would be helpful.
wschin(2020-05-14 20:52:47):> @wschin : did you mean node tests? I think shape-inference should not depend on the primitive type (type-inference does, but if it correctly works for multiple types already, I don't think it is important to add new ones). But I agree that test-cases for the runtime would be helpful.

Yes, I mean node tests + a shape inference test. For shape inference test, I think we just need one for this type.
gramalingam(2020-05-19 23:33:29):@souptc : thanks for the PR! As discussed above, is there any way to add some test-data for bfloat16 for at least one or two ops, to verify the representation? May be casting from float32 to bfloat16 and vice-versa?
gramalingam(2020-05-25 20:36:39):@wschin : do you have any further comments or concerns? Thanks.
gramalingam(2020-05-27 22:19:23):@linkerzhang @postrational @wschin : is this okay to merge in? Thanks!
linkerzhang(2020-05-12 23:13:40):should we also add it in the "all_tensor_types" too?
souptc(2020-05-13 00:39:19):yes i added that too.
wschin(2020-05-13 17:52:30):Not sure if bfloat16 should be added in the beginning of this list. @linkerzhang, any comment? 
wschin(2020-05-13 17:55:11):I don't think we need bfloat16 for this operator. At least, it doesn't support float16 now.
souptc(2020-05-18 17:59:40):removed
gramalingam(2020-05-19 02:36:48):Is this function used, or is it a copy-paste error?
souptc(2020-05-21 23:06:58):removed.
gramalingam(2020-05-26 20:28:22):So, I guess this trick works correctly for NaN, INF, -INF (since you have those values in the test-case)?
souptc(2020-05-26 21:43:05):yes i have the test data there, and it is done by the standard conversion between float and bfloat16. I didn't find the definition of NaN in bfloat16 as bfloat16 is not in IEEE, but as TF apply the same conversion for all the float32 numbers, i think it should be correct.
kevinch-nv(2020-05-14 01:04:53):@onnx/sig-operators-approvers please help review. I targeted this change against opset 13 since ONNX 1.7 was recently released, let me know if there are any more files I should change to reflect the opset update.
kevinch-nv(2020-05-26 23:57:58):@askhade can you help review? 

Also @onnx/sig-operators-approvers sorry for the spam, but I'm not exactly sure why CI is failing on this change. Can someone help root cause the failure? Thanks!
askhade(2020-05-27 16:53:20):FYI @tracysh @yufenglee 

askhade(2020-05-27 17:04:56):> @askhade can you help review?
> 
> Also @onnx/sig-operators-approvers sorry for the spam, but I'm not exactly sure why CI is failing on this change. Can someone help root cause the failure? Thanks!

How did you update docs like Operators.md, Changelog.md etc? I just checked the Linux CI and looks like it is failing due to formatting differences between your version and the expected version. You can check "https://github.com/onnx/onnx/blob/master/tools/update_doc.sh" to see how to auto generate these files.
kevinch-nv(2020-06-02 04:26:41):@askhade Can I get a final review / approval? Thanks!
kevinch-nv(2020-06-03 18:51:52):@askhade ping ^^
linkerzhang(2020-06-08 14:15:16):@askhade any more comments?
kevinch-nv(2020-06-09 21:31:23):@linkerzhang can someone else help review this MR if @askhade does not respond? Thanks!
linkerzhang(2020-06-12 00:10:04):the PR looks good to me. @ebarsoum @postrational to check it 
askhade(2020-06-15 15:44:35):The only open issue from my side for this PR is that why do we need axis attribute for QuantizeLinear and DequantizeLinear ops... Is the axis for per axis quantization ever going to be other than the channel axis? If no then why do we need an axis attribute? ONNX only supports NCHW data so channel axis is always known...


skyw(2020-06-15 17:09:44):> The only open issue from my side for this PR is that why do we need axis attribute for QuantizeLinear and DequantizeLinear ops... Is the axis for per axis quantization ever going to be other than the channel axis? If no then why do we need an axis attribute? ONNX only supports NCHW data so channel axis is always known...

I don't think QuantizeLinear and DequantizeLinear need to be limited by what convolution layout ONNX supports. They look independent as QuantizeLinear and DequantizeLinear can take any tensor. Even for convolution, conv and convtranspose use different layout for weight, **M x C/group x kH x kW** vs **C x M/group x kH x kW**, axis needs to be 1 and 0 respectively. 
yufenglee(2020-06-15 20:25:12):Thanks @kevinch-nv for adding the attribute! It is one feature that is in my backlog. Overall, it looks good to me. Only the bug in the QuantizeLinear inference. Though it is not introduced in this PR, it would be great if you can help to fix it. 
linkerzhang(2020-06-15 22:57:34):> Thanks @kevinch-nv for adding the attribute! It is one feature that is in my backlog. Overall, it looks good to me. Only the bug in the QuantizeLinear inference. Though it is not introduced in this PR, it would be great if you can help to fix it.

Let's merge this PR and have a separate PR to fix the bug you mentioned, either from you or Kevin :). Sounds good? @yufenglee 
kevinch-nv(2020-06-16 19:09:17):I've opened an issue (https://github.com/onnx/onnx/issues/2839) to track the type error @yufenglee mentioned, and resolved the opened conversations. @linkerzhang can you help merge? Thanks!
postrational(2020-06-17 10:48:41):LGTM
TMVector(2020-05-22 16:00:04):`1` seems like a sensible default feature dimension for computer vision models (NCHW) , but I would just like to note `-1` is common for other types of model such as time series and NLP.
askhade(2020-05-27 16:19:01):Instead of adding direct output please add a method to do the output computation.
askhade(2020-05-27 16:20:27):Same as above please add a method to compute this output. Adding a method will benefit anybody who is implementing this operator in their runtime.
askhade(2020-05-27 16:48:09):Please update the documentation to include axis. 

Other quantization ops like QLinearMatmul which allow 1-D tensors for scale and zero point do not have axis attribute, I am trying to understand whether those ops also need to be updated? Right now all quantization ops assume axis to be 1
kevinch-nv(2020-05-28 23:05:28):Good point. I've updated the MR description and schema changes to a general per-axis QDQ support, and included the fact that axis can be negative for negative indexing.
kevinch-nv(2020-05-28 23:05:58):Done!
kevinch-nv(2020-05-28 23:06:12):Done!
kevinch-nv(2020-05-28 23:08:24):Which documentation do I need to update specifically? I've added it as an attribute for these ops, and generally I've seen in the ONNX spec that attributes don't need to be explicitly explained.

I'm targeting only the QDQ Linear ops in this MR since these are the most general quantization operators. 
askhade(2020-05-28 23:19:00):I am trying to understand why do we need axis at all? We do per channel quantization and data is in format NCHW so we know where channel is?
askhade(2020-05-28 23:20:19):description not consistent. line 82 mentions per-axis dequantization and line 88 says per-channel dequantization
kevinch-nv(2020-05-28 23:40:22):Good catch, fixed.
kevinch-nv(2020-05-28 23:43:27):For QuantizeLinear and DequantizeLinear ops, they currently only support per-tensor quantization. This MR is for expanding the definition to also include per-axis quantization. 
askhade(2020-05-29 00:11:23):same here - description incosistent... line 25 vs line 31. Also add what C is for example tensor of size C where C = number of channels
kevinch-nv(2020-05-29 20:05:48):Fixed in latest commit
yufenglee(2020-06-15 20:16:49):There is a bug here. y_zero_point is optional. if it doesn't exist, we should set the output type uint8. Could you help to fix it?
kevinch-nv(2020-06-16 19:07:52):I've opened https://github.com/onnx/onnx/issues/2839 to track this issue, we can resolve this in another MR.
jcwchen(2020-06-19 17:51:45):Hi @vinitra,
I was wondering that why we don't have
```
python onnx/backend/test/stat_coverage.py
backend-test-tools generate-data
```
in [Windows-CI.yml](https://github.com/onnx/onnx/blob/master/.azure-pipelines/Windows-CI.yml)?
In fact, I cannot run backend-test-tools generate-data on my Windows...
vinitra-zz(2020-09-10 17:01:09):@askhade -- ready to merge?
jcwchen(2020-06-24 18:30:35):For Mac OS and Linux OS, set XXX=YYY is a useless command.
Should use export XXX=YYY instead.
Actually I think for Mac OS and Linux OS we don't need to modify these flags for building?
At least I can build onnx successfully without modifying these flags on my Mac. 
vinitra-zz(2020-07-01 19:15:08):> Hi @vinitra,
> I was wondering that why we don't have
> 
> ```
> python onnx/backend/test/stat_coverage.py
> backend-test-tools generate-data
> ```
> 
> in [Windows-CI.yml](https://github.com/onnx/onnx/blob/master/.azure-pipelines/Windows-CI.yml)?
> In fact, I cannot run backend-test-tools generate-data on my Windows...

Not sure. I believe it wasn't there in the original Windows build CI. I think one CI that checks these should be plenty.
vinitra-zz(2020-07-01 19:15:42):I've updated the comment above accordingly :) I suppose since it was working, I never questioned it. Thanks for the catch.
vinitra-zz(2020-05-14 20:40:30):We're moving to Apache 2.0 licensing with the shift to LFAI (perhaps in the future, if not now?). @prasanthpul might have more thoughts on this.
prasanthpul(2020-05-15 17:55:32):we'll need to update once we actually switch (maybe later this month). at this moment it MIT
jcwchen(2020-06-10 21:47:30):### Description
Enable the shape inference for each node in onnx checker
After that, fix bugs in tests
checker fails pytorch.test with ops (e.g., op7 and op8)

### Motivation and Context
Current onnx.checker ignores catching inference errors
Some tests need to be fixed after enabling this (onnx/test, pytorch/test)
jcwchen(2020-06-19 17:30:18):I think the test failures of opset7 and opset8 is because they use `ConstantFill` instead of `ConstantOfShape` for `torch.randn` before `torch.onnx._export`. The current onnx shape inference cannot inference `ConstantFill` (old experiment op) so it fails (empty input for the next node). @spandantiwari @BowenBao Do you have any comment of this? Thanks.
jcwchen(2020-06-30 22:23:46):According to https://github.com/onnx/onnx/pull/1434, while encountering deprecated operators, shape inference or checker will simply stop checking and give a warning to remind users. 
jcwchen(2020-07-02 00:07:16):If onnx.checker stops while encountering experimental operators, caffe2-onnx would bump into an error because it uses onnx.shape_inference.infer_shapes for the conversion [here](https://github.com/pytorch/pytorch/blob/b7b99ab0c8f82100177729b9751481852d83e77e/caffe2/python/onnx/backend.py#L708). And there are some models use experimental operators which can be successfully converted previously... Still thinking how to solve it.

jcwchen(2020-07-02 21:56:44):**Another issue**
Shape inference cannot work with unsupported operators like operators from `org.pytorch._caffe2`.

**My proposal**
To solve this, while shape inference encounters unsupported operators, it will throw a warning instead of throw an error.

Now all the tests can be passed on CircleCI now. This PR is ready for review.
gramalingam(2020-05-28 20:52:25):Why do we need this? The general assumption in type-inference is that input-types are known. What is the use-case / scenario where this does not hold?
jcwchen(2020-05-28 21:14:24):For [tests ](https://github.com/onnx/onnx/blob/cfab05a5f1cf33065a7a2ccc3df8020707e8998f/onnx/test/shape_inference_test.py#L42), these two inputs are set to TensorProto.UNDEFINED. I thought it should be right here because I have no idea what else tensor types we can set for them. Therefore, I chose to modify the checker part not to check the tensors type for reshape ops. 

BTW, actually there are many conflicts in tests after applying inference errors catching because the inference check is kind of general (every tensors need type definition) and there might be some exceptions. For example, ["Loop"](https://github.com/onnx/onnx/blob/cfab05a5f1cf33065a7a2ccc3df8020707e8998f/onnx/defs/controlflow/old.cc#L750). There are some unknown tensors like "M" or "cond" will trigger the inference error [here](https://github.com/onnx/onnx/blob/cfab05a5f1cf33065a7a2ccc3df8020707e8998f/onnx/defs/shape_inference.h#L165). 

My current solution is to make checker not check tensor types for these particular ops (reshape and Loop). In that case, I don't need to influence the original inference graph and break something. Or maybe we could just let inference errors become warnings? Do you have any idea about how to modify it correctly?
gramalingam(2020-05-28 21:26:47):For the first part: I think it would be better to change TestShapeInference if possible, instead of changing the shape-inference code.
gramalingam(2020-05-28 21:28:56):For the loop case: where is the test that is failing? Again, even with a Loop, the expectation is that when we analyze a complete model, we should be able to infer the types inside a Loop from the outer context.
vinitra-zz(2020-05-28 21:49:32):@jacky82226, can you remove the whitespace changes in this file of your PR?
jcwchen(2020-05-28 22:00:49):Good catch. Thank you.
jcwchen(2020-05-28 22:09:26): 

> For the first part: I think it would be better to change TestShapeInference if possible, instead of changing the shape-inference code.

If so, I am thinking to use TensorProto.String instead of TensorProto.UNDEFINED in shape_inference_test.py.
jcwchen(2020-05-28 22:44:57):> For the loop case: where is the test that is failing? Again, even with a Loop, the expectation is that when we analyze a complete model, we should be able to infer the types inside a Loop from the outer context.

Test is [here](https://github.com/pytorch/pytorch/blob/master/test/onnx/test_pytorch_onnx_caffe2.py#L2215). Tests involving forloop or while will all fail. I think the tests should be OK so I focus on modifying the checker part.  

Originally the checker will check input including "M" and "cond", but I don't know where to remove them from the graph. Do you know where to modify the input and output of graph (g_) [here](https://github.com/onnx/onnx/blob/cfab05a5f1cf33065a7a2ccc3df8020707e8998f/onnx/shape_inference/implementation.cc#L434)?  (In other words, which function calls the Loop in old.cc?) Not sure whether it is changeable. If yes, we might remove them in shape_inference because they look useless for shape inference. 
jcwchen(2020-05-29 02:03:29):> > For the first part: I think it would be better to change TestShapeInference if possible, instead of changing the shape-inference code.
> 
> If so, I am thinking to use TensorProto.String instead of TensorProto.UNDEFINED in shape_inference_test.py.

NVM. Just found a better way to do this. Should use the type from the seed_value. 
gramalingam(2020-05-29 18:31:04):Re. the loop example: I see the PyTorch code. Does this fail type-inference? If so, can you produce the corresponding ONNX model? I am trying to understand if there is any problem in the model-creation from PyTorch. Conceptually, there should not be any issue in type-inference, as it is expected to work as follows: When we analyze a Loop node, we should already know the types of the inputs to the Loop node. This allows us to infer the types of the inputs of the loop-body graph. I am not sure which part here is failing, if any, forcing you to make the above change.
gramalingam(2020-05-29 18:43:18):shape should have a a fixed type (INT64) independent of seed-value's type: see: https://github.com/onnx/onnx/blob/master/docs/Operators.md#Reshape 
gramalingam(2020-05-29 18:47:47):Better to move this into the else branch of the first if-else inside the for-loop below (line 35/37)
jcwchen(2020-05-29 20:01:07):Great catch. Set the shape correctly and then some tests don't need to be changed.
jcwchen(2020-05-30 01:13:40):It fails [here](https://github.com/onnx/onnx/blob/cfab05a5f1cf33065a7a2ccc3df8020707e8998f/onnx/defs/shape_inference.h#L165). I suspect the reason is the inference graph contains "M" and "cond" and both of them are TensorProto.UNDEFINED. And the output of Loop node is TensorProto.UNDEFINED, too. 

Yes, I can successfully create such model from PyTorch and it looks normal on Netron. 

My understanding is shape inference works normally for the inference part. Except for checking the tensors: it needs each tensor has defined tensor type. In that case, there are some inputs and outputs (like a condition) for "Loop" do not meet this requirement. 

To avoid checking this kind of tensors, I need to modify the input and output of the graph [here](https://github.com/onnx/onnx/blob/cfab05a5f1cf33065a7a2ccc3df8020707e8998f/onnx/shape_inference/implementation.cc#L434) as mentioned. However, I am still looking for where is the code for calling the Loop Node (construct the inference graph) or how can I modify the inference graph without breacking the logic. 


gramalingam(2020-05-30 02:18:41):The first question we should answer is: is there a problem in the model (this implies that the problem is with the exporter that created the model) or not (in this case the bug is in the type inference code). 
jcwchen(2020-05-31 01:43:52):Agree. I just checked the code on the Pytorch test  and found that the loop model can pass the onnx.checker.check_model(model) first but then fail on the onnx.shape_inference.infer_shapes(model). I will review the Pytorch exporter part as well. Thank you for the reminder.
jcwchen(2020-06-01 23:47:55):@gramalingam Thank you for the previous instructions for solving the input issue. 

Another problem is about checking the output. The Loop node checker fails [here](https://github.com/onnx/onnx/blob/e094e101da8cd2a10d6bf5f5ab692b50943a1470/onnx/defs/controlflow/old.cc#L588). There are two outputs for the Loop node and both of them are `TypeProto::ValueCase::VALUE_NOT_SET` [here](https://github.com/onnx/onnx/blob/e094e101da8cd2a10d6bf5f5ab692b50943a1470/onnx/shape_inference/implementation.cc#L470). It seems that there is a bug for the graph output (g_->output()) not updated as well, but the output of Loop model looks normal (has valid type) by Netron. 

Since we cannot solve this problem in the same way as the input (not providing inferred type), what would you suggest to solve this? (Maybe skip the output checker?)
gramalingam(2020-06-02 00:48:19):I think the problem is here: https://github.com/onnx/onnx/blob/e094e101da8cd2a10d6bf5f5ab692b50943a1470/onnx/shape_inference/implementation.cc#L267 ... when we infer the type of X, we need to check if X is a graph output. If it is a graph output, we should update the type of the output, instead of adding a new valueinfo. So, we need another "else if" branch to handle this.
gramalingam(2020-06-04 16:40:50):The basic idea here is good. However, the existing code has another limitation, which shows up here too: it assumes that the inputs/outputs of the graph are all tensor-types. It would be nice to update this to allow it to be of other types (like sequence, etc.) also.
gramalingam(2020-06-04 16:50:10):But may be we should make it a TODO comment and address it separately, rather than let this PR become too big. I don't think it is affecting any existing models currently.
gramalingam(2020-06-04 16:54:55):I am a bit confused by this and the lines below at line number 304: both seem to serve the same purpose. Is there a reason we need both? I think the loop at line 304 should be sufficient, and we can leave this part as before. What do you think?
jcwchen(2020-06-04 17:36:22):Thank you for pointing out. Currently, this modification can pass the test_loop, but it encounters access violation issue for some tests. Maybe encountering sequence type is the issue. I am still tracking it.

Agree that we should add that to TODO. 
jcwchen(2020-06-04 18:14:54):I think the if branch at line 276 is necessary because the original code uses `checkShapesAndTypes` and  `mergeShapesAndTypes` and they do not allow `exisitingtype` and `inferredType` use different types and cause errors. Still, I will try to simplify this if branch.
gramalingam(2020-06-04 19:44:18):I think protobuf implementation will create a type field if it does not exist when we call "mutable_type". Please see: https://developers.google.com/protocol-buffers/docs/reference/cpp-generated#embeddedmessage . I think it is safer to first check "vi.has_type()" before calling mutable_type.
jcwchen(2020-06-05 03:07:09):I did added `vi.has_type()` if outside the `has_elem_type()` if before, but it seems that the input of loop node does not `vi.has_type()` and fails test_loop (doesn't update input successfully). I agree it is dangerous here so I am trying to check something proper before getting the element type.
gramalingam(2020-06-15 23:29:52):We need to add a case for "if (inferredType.value_case() == VALUE_NOT_SET)".
gramalingam(2020-06-15 23:33:07):Please add "|| inferredTypeCase == TypeProto::ValueCase::VALUE_NOT_SET" to the if-condition. It will make it more robust, I think.
gramalingam(2020-06-15 23:42:44):I think we should replace the checks in line 243 and 248 by "if (inferredType.value_case() == VALUE_NOT_SET)".
jcwchen(2020-06-16 05:21:38):For this if case (inferredType.value_case() == VALUE_NOT_SET), do we need to do anything?

jcwchen(2020-06-16 05:21:47):Make sense
jcwchen(2020-06-16 05:29:22):In that case, the merge function can be used for kMapType, kSparseTensorType, kOpaqueType?
gramalingam(2020-06-16 15:44:34):Sorry, it was a  copy-paste error. I meant the case existingType.value_case == VALUE_NOT_SET needs to be handled. We need to copy the type from inferredType to existingType.
jcwchen(2020-06-16 15:48:36):No problem.
jcwchen(2020-06-16 15:56:35):However, in that case, it will break a few tests. I think some graphs in tests do have a valid value_case (tensor_type), but their element types are undefined.
gramalingam(2020-06-16 16:56:42):Ok, please ignore my comment. I think the following code works fine. "existingType->mutable_tensor_type()" will create the tensor_type if not already present.
gramalingam(2020-06-16 16:59:50):Great, thanks! Can we also eliminate the lines 246 to 254? It will help simplify the code and make it easier to maintain and extend in the future.
jcwchen(2020-06-16 17:37:09):It should be fine. Just removed them.
askhade(2020-06-30 22:55:01):This method as the name suggests should only check whether the given op is experimental or not... .i.e return true if it is or false if not... the calling code should decide what to do with the result. I would say move the error out of this method

Also in case of onnx checker the warning message should be something like - checker does not support models with experimental ops
askhade(2020-06-30 22:56:51):nit: change name to check_is_experimental_op
jcwchen(2020-07-11 00:01:44):```suggestion
    // nothing to check; will assign inferredType to undefined exisitingType 
```
    // nothing to check; will assign inferredType to undefined existingType 
askhade(2020-07-11 00:34:00):It is unclear which exceptions are thrown vs which ones are caught and reported at the end. 
Seems like you keep the throw statement here because if we reach this catch block then it means Type/Shape inference failed for this node and we do not want to continue right?

In the earlier cases the exceptions are caught and stored in inference_errors because there was not enough info available to infer shapes and types so instead of returning we decide to continue ?
jcwchen(2020-07-11 16:23:11):Yes. There are 2 kinds of exceptions here:
1. throw error immediately
- `CheckInputOutputType`: check unsupported type; inconsistent type
- `checkShapesAndTypes`: check type mismatch (inferred type vs existing type); shape different (rank or dimension)

2. throw errors in the end
Functions in `[operator]/def.cc` (i.e., `propagateElemTypeWithValidation`, `propagateElemTypeFromInputToOutput`, `propagateElemTypeFromDtypeToOutput`): check input is null, undefined or type mismatch (input vs output for some operations)

I remember there is a requirement that hoping shape inference can go through each node and then throw a whole error log in the end. So, I did not change the original logic here.
IMO, 1st exception looks more serious (wrong type). By contrast, 2nd exception usually occurs while producing inner graph from the original graph (produce unknown input).

I think before merging this PR, we should confirm whether current design meets our requirement. 
@gramalingam please comment here if you have other thoughts. Thank you!

gramalingam(2020-08-19 16:51:19):May be change the message to "Warning: Unsupported operator " << n.op_type() << ". No schema registered for this operator."
gramalingam(2020-08-19 16:57:10):Minor suggestion:
```cpp
  std::string op_name = n.has_name() ? (", node name: " + n.name()) : "";
  return "(op_type:" + n.op_type() + op_name + "): " + err.what();
```
gramalingam(2020-08-19 18:38:07):Sorry I missed this earlier. My opinion is that this should be very much like error-messages produced by a compiler. Ideally, we should report as many distinct errors as possible. So, even for checkShapesAndTypes, if it is possible for the checker to continue and check the remaining model, it would be great to do so. However, whatever we do, it is important to avoid the possibility of issues like null-dereferencing. (E.g., like some of the issues that Ashwini recently fixed.) This is one of the main reasons to terminate early (in case we are not sure whether the subsequent code will robustly handle missing types etc.).
jcwchen(2020-08-19 18:39:51):Both modified. Thanks for the suggestion.
gramalingam(2020-08-19 18:41:58):So, I would say that the first priority should be to make sure the code-base is robust (in terms of avoiding assumptions that an input-type is known when doing inference for a node). When we have that robustness, we can relax more of these early-termination errors and continue with checking the rest of the model (thus, this is a secondary priority).
linkerzhang(2020-05-31 09:49:12):"should new operators always come with differentiability definition?" I'd suggest not :). That's also why when adding such flag, an "unknown" option is there. Thoughts?
wschin(2020-06-02 21:29:56):> "should new operators always come with differentiability definition?" I'd suggest not :). That's also why when adding such flag, an "unknown" option is there. Thoughts?

The major motivation to force people to define differentiability is to make sure a operator is as matured as possible in its first version. But I am ok with your suggestion. I am not sure how to balance  "inference-friendly" and "completeness", and we chose "completeness".
gramalingam(2020-06-02 22:46:22):This looks nice. My gut feeling is that executable-code that can be used by ONNX frameworks would be much more valuable. In other words, an version of method 3 using a standardized interface. Doing method 1 is a good way to verify that outputY of opZ is differentiable wrt inputX, but what is the payoff? Is it worth investing a lot of effort in this, as opposed to building a version of method 3 (even if it takes longer to finish)?
gramalingam(2020-06-04 18:32:15):Overall: this seems useful advice for developers on how to figure out what's differentiable. But I am not clear what should be made mandatory in a PR. In uncomplicated cases, option (2) would seem to be the simplest, but it is not clear what sort of examples is expected. What do you need as examples to show that addition or multiplication is differentiable?
wschin(2020-06-30 16:31:14):> This looks nice. My gut feeling is that executable-code that can be used by ONNX frameworks would be much more valuable. In other words, an version of method 3 using a standardized interface. Doing method 1 is a good way to verify that outputY of opZ is differentiable wrt inputX, but what is the payoff? Is it worth investing a lot of effort in this, as opposed to building a version of method 3 (even if it takes longer to finish)?

If the author knows how Pytorch/Tensorflow operators may be converted to  ONNX, Method 1 is an easy way to make sure a differentiability tag is correct without digging into math. The goal here is to have shortest path to observe or prove that inputs or outputs are differentiable or not. Method 3 is similar to Method 1, but Method 3 requires the author to build a correct forward operator first before applying auto-diff. Forward pass could be difficult sometime.

For reuse Method 3, as you said, we need a standard interface (I guess it would be a small runtime like ONNX). Do we have such a interface today? It seems this is related to Alibaba's test framework.

[Update]
I removed Method 3 because the goal of this document is mainly for ensuring the correctness of differentiability tag.
gramalingam(2020-07-07 21:24:02):Thanks @wschin for the document. I think it will help people understand the process, especially backward-propagation. (However, I am not sure if it is realistic to expect a PR author to include this level of detail in every PR. I suspect we may have to make a decision on a case-by-case basis. Let us see how it goes.)
gramalingam(2020-06-04 18:25:54):change "define" to "verify". 
wschin(2020-06-30 16:19:04):Ok.
wschin(2020-06-30 17:41:43):Switch these two rows.
gramalingam(2020-07-07 21:00:17):"Defining Differentiability" => "the Differentiability tag"
```suggestion
# A Short Guide on The Differentiability Tag for ONNX operators
```
gramalingam(2020-07-07 21:07:02):Add an overview to provide the context. Something like "The ONNX operator schema for each operator includes a Differentiability tag for each input and output. In this document, we explain the meaning of this tag and how to ensure that a correct tag-value is provided in a PR that proposes a new operator or updates an operator. Briefly, the tag identifies the set of differentiable inputs and differentiable outputs of an operator. The meaning of the tag is that the partial derivative of each differentiable output is defined with respect to each differentiable output."
gramalingam(2020-07-07 21:14:13):I didn't quite understand. I guess this means that the PR should contain details on how the backward-prop can be implemented? The subsequent discussion seems to imply that this is not necessary? Is this optional?
gramalingam(2020-07-07 21:15:50):We present a couple of methods below to verify the differentiability tag of ONNX operators.
```suggestion
We present a couple of methods below to verify the differentiability for ONNX operator.
```
gramalingam(2020-07-07 21:17:11):"points" => "point"
```suggestion
The first way is to show that the considered operator's backward operation exists in an existing framework such as Pytorch or Tensorflow. In this case, the author should provide a runnable python script which computes the backward pass of the considered operator. The author should also point out how to map the Pytorch or Tensor code to ONNX format (for example, the author can call `torch.onnx.export` to save an ONNX model). The following script shows the differentiability of ONNX Reshape using  Pytorch.
```
wschin(2020-07-10 17:22:22):No. The author here means "author of differentiability tag", not the author of operator. I modified your `intro` message to reflect this.
wschin(2020-07-10 17:43:01):Ok.
jcwchen(2021-10-19 19:59:26):https://github.com/onnx/onnx/pull/3630 This merged PR has made to_array to handle external tensors. Thanks for the contribution!
KsenijaS(2020-05-28 03:55:24):@houseroad  please review. Thanks!
gramalingam(2020-05-29 00:26:27):So, I assume there is some issue with the previous file-rename? Does something break?
KsenijaS(2020-05-29 01:19:55):@gramalingam the first PR added all files generated by tools/update_doc.sh instead only files related to NLLLoss and SCE.
gramalingam(2020-05-29 02:45:38):Do we know why other files were modified by the update script? I assume they appeared to be different from the repo version. I think someone else faced a similar issue, so I wonder if something needs to be fixed.
CLAassistant(2020-06-04 19:44:48):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=2814) <br/>All committers have signed the CLA.
jkmarz(2020-06-04 19:45:57):There is irony in the CLA Assistant checking this PR... 
CLAassistant(2020-06-15 11:49:37):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=2830) <br/>All committers have signed the CLA.
linkerzhang(2020-06-19 08:16:05):@fumihwh you may show your usage if any of onnx optimizer as folks are trying to remove it, see https://github.com/onnx/onnx/pull/2834.
askhade(2020-10-09 04:25:28):@fumihwh : Now that optimizers are moved to a separate repo do you want to open this PR there instead? 
askhade(2021-10-08 17:05:25):Closing this PR since the optimizers are now moved to a separate repo
jcwchen(2020-06-15 23:49:24):Besides onnx/onnx itself, there are several repos using onnx.optimizer. 

1. pytorch/caffe2/onnx
https://github.com/pytorch/pytorch/pull/40061
A tricky one: use `onnx.optimize` during onnx-caffe2 conversion

2. onnx/onnx-tensorrt
https://github.com/onnx/onnx-tensorrt/blob/7ec88b96c2f03a1a0c3629ff6830decc480dcc10/main.cpp#L26

3. onnx/onnx-caffe2 (archived; moved to pytorch/caffe2)
https://github.com/onnx/onnx-caffe2/blob/e0cb48bc2d40b4e8c700e5a8c866d4ffd327457c/onnx_caffe2/backend.py#L24

4. onnx/onnx.github.io
https://github.com/onnx/onnx.github.io/blob/16242fdea5464f3412313e5e6f7f22ef8d9fd189/supported-tools.html#L251
linkerzhang(2020-06-17 13:03:50):Thank you for raising this efforts! It will be good if the optimizer can be moved to a separate repo under onnx organization. In one way,  the optimizer could still be used by partners (you listed), in another way, the optimizer will not be part of ONNX repo since it's not that actively maintained/updated.
askhade(2020-06-17 17:02:33):> Thank you for raising this efforts! It will be good if the optimizer can be moved to a separate repo under onnx organization. In one way, the optimizer could still be used by partners (you listed), in another way, the optimizer will not be part of ONNX repo since it's not that actively maintained/updated.

Moving optimizers outside onnx repo gives a false impression to the users that optimizers are  expected to work and they will be maintained. We could move them to another repo and archive the repo but again we will need to fix all the bugs, bring them to a working state and then archive  this repo. I do not think we will lose any value by getting rid of them. Today most of the optimizations they bring in are already covered by converters. So I do not see any value in maintaining them in main onnx repo or a separate repo.

BTW Most of the references to optimizers are old. I am pretty sure these repos are not dependent on these optimizers. Jacky has already created PRs to remove references to optimizers in these repos too... We will soon know whether these repos have a hard dependency on optimizers.
askhade(2020-06-17 17:18:42):> Besides onnx/onnx itself, there are several repos using onnx.optimizer.
> 
> 1. pytorch/caffe2/onnx
>    [pytorch/pytorch#40061](https://github.com/pytorch/pytorch/pull/40061)
> 2. onnx/onnx-tensorrt
>    https://github.com/onnx/onnx-tensorrt/blob/7ec88b96c2f03a1a0c3629ff6830decc480dcc10/main.cpp#L26
> 3. onnx/onnx-caffe2 (archived)
>    https://github.com/onnx/onnx-caffe2/blob/e0cb48bc2d40b4e8c700e5a8c866d4ffd327457c/onnx_caffe2/backend.py#L24
> 4. onnx/onnx.github.io
>    https://github.com/onnx/onnx.github.io/blob/16242fdea5464f3412313e5e6f7f22ef8d9fd189/supported-tools.html#L251

@dsandler-bos-msk : 
I see onnx optimizer was recently included in onnx/onnx-tensorrt (https://github.com/onnx/onnx-tensorrt/blob/7ec88b96c2f03a1a0c3629ff6830decc480dcc10/main.cpp#L26) could you elaborate on your use case? 
Which optimizations are you trying to run? Are these not covered by the converters?
spandantiwari(2020-06-17 20:44:25):> > Besides onnx/onnx itself, there are several repos using onnx.optimizer.
> > 
> > 1. pytorch/caffe2/onnx
> >    [pytorch/pytorch#40061](https://github.com/pytorch/pytorch/pull/40061)

@houseroad - can you please take a look and see if ONNX optimizer dependency can be safely removed as far as caffe2 backend is concerned?
linkerzhang(2020-06-19 08:11:52):> > Thank you for raising this efforts! It will be good if the optimizer can be moved to a separate repo under onnx organization. In one way, the optimizer could still be used by partners (you listed), in another way, the optimizer will not be part of ONNX repo since it's not that actively maintained/updated.
> 
> Moving optimizers outside onnx repo gives a false impression to the users that optimizers are expected to work and they will be maintained. We could move them to another repo and archive the repo but again we will need to fix all the bugs, bring them to a working state and then archive this repo. I do not think we will lose any value by getting rid of them. Today most of the optimizations they bring in are already covered by converters. So I do not see any value in maintaining them in main onnx repo or a separate repo.
> 
> BTW Most of the references to optimizers are old. I am pretty sure these repos are not dependent on these optimizers. Jacky has already created PRs to remove references to optimizers in these repos too... We will soon know whether these repos have a hard dependency on optimizers.

I'm not sure whether it can be that firmly guaranteed. https://github.com/onnx/onnx/pull/2830 is showing some folks are still working on it. 
fumihwh(2020-06-19 18:10:55):So, we cannot optimize onnx anymore? I don’t think it’s a good idea. 
I understand optimization is a highly experimentally feature. At least, user should use it carefully. 
Now, in my case, optimization is absolutely necessary for my project. 
So, the bottom line should be moving to an independent repo and I will try to maintain it. 
linkerzhang(2020-06-21 09:19:03):> So, we cannot optimize onnx anymore? I don’t think it’s a good idea.
> I understand optimization is a highly experimentally feature. At least, user should use it carefully.
> Now, in my case, optimization is absolutely necessary for my project.
> So, the bottom line should be moving to an independent repo and I will try to maintain it.

I think it's a good idea to move the optimizer lib out of ONNX repo, and create a separate one under ONNX org. It may be good if @jacky82226 and @fumihwh may work together on it. Removing it is not a good idea given there're indeed projects (which can't be figured out) depending on it (the optimizer does work for older IR versions I believe).
linkerzhang(2020-06-22 07:05:01):> เมื่อ อา. 21 มิ.ย. 2563 เวลา 18:17 Somchai Srisuk <sckmodifier@gmail.com> เขียนว่า:
> […](#)
> เมื่อ อา. 21 มิ.ย. 2563 เวลา 16:19 Ke Zhang ***@***.***> เขียนว่า: > So, we cannot optimize onnx anymore? I don’t think it’s a good idea. > I understand optimization is a highly experimentally feature. At least, > user should use it carefully. > Now, in my case, optimization is absolutely necessary for my project. > So, the bottom line should be moving to an independent repo and I will > try to maintain it. > > I think it's a good idea to move the optimizer lib out of ONNX repo, and > create a separate one under ONNX org. It may be good if @jacky82226 > <https://github.com/jacky82226> and @fumihwh <https://github.com/fumihwh> > may work together on it. Removing it is not a good idea given there're > indeed projects (which can't be figured out) depending on it (the optimizer > does work for older IR versions I believe). > > — > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub > <[#2834 (comment)](https://github.com/onnx/onnx/pull/2834#issuecomment-647102553)>, or > unsubscribe > <https://github.com/notifications/unsubscribe-auth/AP55P57JABSUINYMSL3HCKLRXXGCXANCNFSM4N6RZPWQ> > . >

Sorry that I don't know your language :(. It may be good if you can type english :).
linkerzhang(2020-06-22 07:07:10):@jacky82226 @fumihwh @daquexian are you guys ok to work together on moving the optimizer out of onnx repo and put it in a separate repo under onnx org please? 
daquexian(2020-06-22 08:42:51):@linkerzhang I'm glad to help maintain it
fumihwh(2020-06-22 14:09:38):@jacky82226 Let's try to move it to a new repo.
@linkerzhang Could you create an onnx/optimizer repo?
jcwchen(2020-06-22 16:09:23):> @jacky82226 @fumihwh @daquexian are you guys ok to work together on moving the optimizer out of onnx repo and put it in a separate repo under onnx org please?

Yes. Once @linkerzhang opens the new repo, could @daquexian or @fumihwh create the PR for the optimizer transfer? Thanks.
prasanthpul(2020-06-22 20:40:22):Please follow the instructions at https://github.com/onnx/onnx/blob/master/community/repo_guidelines.md to request a new repo
linkerzhang(2020-06-22 23:05:37):https://github.com/onnx/steering-committee/issues/7 was created for creating a repo @prasanthpul 
askhade(2020-06-23 00:03:39):> [onnx/steering-committee#7](https://github.com/onnx/steering-committee/issues/7) was created for creating a repo @prasanthpul

Before we create a repo @linkerzhang , @daquexian , @fumihwh  : Are you also planning to fix all the issues in the optimizers? 
@daquexian @fumihwh : Can you explain what dependencies do you have on these optimizers? Most of these optimizers are already covered by converters, are there any which are not being covered today? 
Can you evaluate the python based optimizers : https://github.com/microsoft/onnxconverter-common/blob/master/onnxconverter_common/optimizer.py
If these can cover your scenarios then its best if you use these... 

linkerzhang(2020-06-23 02:01:35):> > [onnx/steering-committee#7](https://github.com/onnx/steering-committee/issues/7) was created for creating a repo @prasanthpul
> 
> Before we create a repo @linkerzhang , @daquexian , @fumihwh : Are you also planning to fix all the issues in the optimizers?
> @daquexian @fumihwh : Can you explain what dependencies do you have on these optimizers? Most of these optimizers are already covered by converters, are there any which are not being covered today?
> Can you evaluate the python based optimizers : https://github.com/microsoft/onnxconverter-common/blob/master/onnxconverter_common/optimizer.py
> If these can cover your scenarios then its best if you use these...

I'm personally not the one using onnx-optimizer codes, and on behalf of onnx infra sig,  I have been the one keep advocating moving onnx optimizers out of onnx repo and keep onnx repo focus on onnx spec since months ago. However, I'm also trying to think of whether there're explicit or implicit partners who are depending on it, which can't be broken. 

So, in one way, I'd suggest @daquexian @fumihwh @nihui to show your own cases and let community know how you're depending on it. In another way, "Fixing all the issues in the optimizers" is a too general challenge, which I don't think is a reasonable "pushback" of having onnx optimizer moved out as a separate repo. If there's a dependency, it's not that straightforward for folks to quickly move from onnx-optimizer to the one in converter, right? At least, engineering efforts are there. 
daquexian(2020-06-23 02:35:04):> I'm personally not the one using onnx-optimizer codes, and on behalf of onnx infra sig, I have been the one keep advocating moving onnx optimizers out of onnx repo and keep onnx repo focus on onnx spec since months ago. However, I'm also trying to think of whether there're explicit or implicit partners who are depending on it, which can't be broken.
> 
> So, in one way, I'd suggest @daquexian @fumihwh @nihui to show your own cases and let community know how you're depending on it. In another way, "Fixing all the issues in the optimizers" is a too general challenge, which I don't think is a reasonable "pushback" of having onnx optimizer moved out as a separate repo. If there's a dependency, it's not that straightforward for folks to quickly move from onnx-optimizer to the one in converter, right? At least, engineering efforts are there.

@linkerzhang Thanks! I just replied in another issue, https://github.com/onnx/steering-committee/issues/7
wenbingl(2020-06-23 17:57:40):There is a python version onnx optimizer is here, which  is actively maintained  and was used by onnx converters.
And it doesn't depend on any framework, only ONNX package itself.

https://github.com/microsoft/onnxconverter-common/blob/master/onnxconverter_common/optimizer.py 



jcwchen(2021-02-22 15:31:08):Duplicate https://github.com/onnx/onnx/pull/3288. Close now.
spandantiwari(2020-06-17 20:56:45):In addition to files under `onnx/optimizer`, there are other files under `onnx/common` which we can possibly remove. For example, can we remove `onnx/common/ir.h`. I am not completely sure, but I think optimizer was the only component that was using the data structures in this files. Others that we should check are:
`#include "onnx/common/array_ref.h"`
`#include "onnx/common/assertions.h"`
`#include "onnx/common/interned_strings.h"`
`#include "onnx/common/graph_node_list.h"`
`#include "onnx/common/tensor.h"`

spandantiwari(2020-06-17 21:01:53):Given that optimizer has been removed here, do we still want to keep this API `polish_model`? All it is doing is running the checker and shape inference. strip_doc_string was there mainly for models coming from PyTorch because they had long and unwieldy doc strings. That has  been fixed. In fact, I would say removing doc strings is possibly not a good thing. Given all this, I think this API is now just an ad-hoc collection of steps with no real motivating use case anymore.
Therefore, we should consider removing it as part of this PR.
jcwchen(2020-06-17 21:33:42):Agree. I think they are for the same purpose currently.
jcwchen(2020-06-18 04:33:46):Thank you for pointing out. However, [onnx/version_converter/helper.h](https://github.com/onnx/onnx/blob/master/onnx/version_converter/helper.h) uses `onnx/common/ir.h` as well so we might still keep them. 
vinitra-zz(2020-06-18 20:54:25):Yes, some people are still using the version_converter so it makes sense to keep that functionality for now.
daquexian(2020-06-20 10:42:36):I think ir.h is also out-of-date, and it is the reason of some optimizer problems: https://github.com/onnx/onnx/pull/2247. Is there any plan to update it?
askhade(2020-06-17 18:35:56):In the same PR add templates for Feature Request and Question?
prasanthpul(2020-06-18 17:42:17):this was just a test PR
askhade(2020-06-17 18:32:42):Change the steps to something like this:
- Describe steps/code to reproduce the behavior.
- Attach the ONNX model to the issue (where applicable) to expedite investigation.

askhade(2020-06-17 18:34:56):Merge Desktop and Smartphone sections to System Information
Under this 
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- ONNX installed from (source or binary):
- ONNX version:
- Python version:


askhade(2020-06-22 16:22:16):Add a separate template for "New Operator"?
askhade(2020-06-22 16:26:01):Are you also planning to add separate templates for god-first-issue and documentation? I don't think we needs separate templates for these 2.
jcwchen(2020-06-22 16:53:03):> Are you also planning to add separate templates for god-first-issue and documentation? I don't think we needs separate templates for these 2.

I was planning to, but after carefully reviewing the existing templates we have, these 2 should be already covered. 
jcwchen(2020-06-22 16:55:25):> Add a separate template for "New Operator"?

Is it for adding a new operator? If so, hasn't it been covered by Feature Request?
askhade(2020-06-22 20:35:12):> > Add a separate template for "New Operator"?
> 
> Is it for adding a new operator? If so, hasn't it been covered by Feature Request?

Yes this is for adding new operator. I was thinking of adding a separate template for this as it requires more details and is a pretty common use case. In case of new operator you want to ask whether current op can be constructed using existing onnx ops and added as a function instead etc.  
jcwchen(2020-06-22 21:54:17):I did check some issues about new operators, organize some sections which might help users and add a new template. @askhade please review it. THANKS!
askhade(2020-06-22 16:21:20):Also add a section for - Describe the alternatives you have considered. (A clear and concise description of any alternative solutions or features you've considered.)
askhade(2020-06-22 16:23:59):Add a section for Expected Behavior 
jcwchen(2020-06-23 17:24:38):Since there are some partners using the testing data under onnx, it is not a good time to remove those test data right now.
jcwchen(2020-06-24 21:13:19):cc @ebarsoum @postrational for review. Thank you!
linkerzhang(2020-06-25 09:26:17):Quick question, what's your suggestion when releasing ONNX, should the test data included or not?
jcwchen(2020-06-25 15:55:46):IMO, a released library should not include test data because general users won't use them and it costs more effort while installing the package. 
However, it has been added to onnx for a while so there are some existing users depend on them... Ideally I hope they can additionally add a flag to install the test data. In that case, we can set excluding test data as default.
linkerzhang(2020-06-30 01:48:58):> IMO, a released library should not include test data because general users won't use them and it costs more effort while installing the package.
> However, it has been added to onnx for a while so there are some existing users depend on them... Ideally I hope they can additionally add a flag to install the test data. In that case, we can set excluding test data as default.

I agree with you!

I was thinking that "ONNX_EXCLUDE_TESTS" should be 1 by default in that way and have folks explicitly set it to 0 when needed.
askhade(2020-06-30 19:23:12):> > IMO, a released library should not include test data because general users won't use them and it costs more effort while installing the package.
> > However, it has been added to onnx for a while so there are some existing users depend on them... Ideally I hope they can additionally add a flag to install the test data. In that case, we can set excluding test data as default.
> 
> I agree with you!
> 
> I was thinking that "ONNX_EXCLUDE_TESTS" should be 1 by default in that way and have folks explicitly set it to 0 when needed.

I agree with this. However right now changing this to exclude tests by default might affect dependencies and this will probably need more time to get approval from community. To ease installation experience issues I would say let's check this in with default set to include tests (the way it is right now) and then open an issue for this to get approval from the community.
askhade(2020-06-30 19:23:54):@postrational ping :)

askhade(2020-06-30 20:57:25):> I don't think that removing test data from the package is the right solution for the long filename problem.
> It would break systems which were built to rely on this test data.
> 
> Instead of deleting the data, we should change the long filenames to shorter ones.

This PR is not proposing to delete the test data. This PR is just adding an option to exclude test data. With this PR the current behavior does not change. As I mentioned in the previous comment, removing test data will need longer discussion and approval from community members. 

@jcwchen : As part of this PR please also include the change to rename long names to short names
postrational(2020-06-30 21:06:14):> This PR is not proposing to delete the test data. This PR is just adding an option to exclude test data. 

Excluding test data will effectively delete them from user system, because after `pip install onnx` they will not have the test data installed. We could add an option like `pip install onnx[tests]` as an alternative to the users. Making users build their own packages from sources is not viable.

askhade(2020-06-30 21:08:42):> > This PR is not proposing to delete the test data. This PR is just adding an option to exclude test data.
> 
> Excluding test data will effectively delete them from user system, because after `pip install onnx` they will not have the test data installed. We could add an option like `pip install onnx[tests]` as an alternative to the users. Making users build their own packages from sources is not viable.

We should definitely rename the long names to short ones. However this PR additionally adds an option for someone to exclude test data. The onnx package will by default have the test data
jcwchen(2020-07-01 00:22:22):Thank you everyone for the feedback. I just shortened some long filenames.
There is a similar PR for renaming (shorten) tests filenames. https://github.com/onnx/onnx/pull/2803

A few file names are quite long... The longest one is `test_sce_NCd1d2d3_none_no_weight_negative_ignore_index_log_prob_expanded` which consists 72 characters.
The test names with "ignore_index" are usually long name. (5 files consist 65+ characters)
To make sure all names follow the same format, I replaced all the filenames which contain "ignore_index" into "ii". 
After modification, the longest filename is now 63 characters.

Since I am not one of the authors for this part, I am not sure whether this modification would break something (like tests in Pytorch). It's better to cc them to review this and prevent writing longer test name in the future.
jcwchen(2020-07-02 23:36:54):Since adding external flag to exclude test data is still debatable, this PR focuses on shortening those long test names to prevent the long filename issue on Windows. I just removed the modification in `setup.py`. 

@postrational please review. Thank you so much!
postrational(2020-07-02 11:45:55):```suggestion
# Exclude test data while building ONNX
```
askhade(2020-06-25 15:53:11):This pipeline has passed in the past so what changed now? If without -y install halts for user input then how did this pipeline pass before?
jcwchen(2020-06-25 16:00:38):> This pipeline has passed in the past so what changed now? If without -y install halts for user input then how did this pipeline pass before?

Since the setting here is fixed on our end, I think maybe there are some updates on conda package (need extra confirm for installing some packages). Setting yes should fix this issue.
askhade(2020-06-25 19:29:36):> > This pipeline has passed in the past so what changed now? If without -y install halts for user input then how did this pipeline pass before?
> 
> Since the setting here is fixed on our end, I think maybe there are some updates on conda package (need extra confirm for installing some packages). Setting yes should fix this issue.

Do other CIs need a similar fix? Right now only MacOS is breaking but better to add this to others as well.
jcwchen(2020-06-25 19:38:56):I did check other CIs:
For Linux-CI, the conda environment has been created initially before the scripts so it should be fine 
For Windows-CI, --yes has been added.
Thank you for pointing out.
lgtm-com[bot](2020-06-25 22:39:49):This pull request **introduces 1 alert** when merging 0867b608cf5bf5eb5817e49bc2dae262a7ca8746 into 14a7477618ec418d987fd5207178e91bb4c98dfc - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-bc7f57d7509745a1bb7c94e244e0e3fde96c2f23)

**new alerts:**

* 1 for Module is imported more than once
askhade(2020-06-30 19:28:04):How is this different than git clean?
jcwchen(2020-06-30 21:25:49):> How is this different than git clean?

Basically they are the same. The only difference would be with this PR we can keep some user-defined files like `.vs` or `.vscode`(no need to remove for reinstall), which can be defined by `.gitignore`. Therefore, this requirement is minor...maybe we can close this PR. 
jcwchen(2020-06-29 17:56:23):@vinitra could you take a look on this?
Thank you!
jcwchen(2020-07-01 23:14:10):Has been updated in https://github.com/onnx/onnx/pull/2773/files.
CLAassistant(2020-06-29 18:06:24):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=2864) <br/>All committers have signed the CLA.
askhade(2020-07-01 19:01:18):test data is also part of the onnx package. Is it required to regenerate all the node test data by running 
python onnx/backend/test/cmd_tools.py generate-data
so that all the models are of the latest opset? I don't see this step mentioned in this doc. 
chinhuang007(2020-07-01 19:36:52):I actually didn't regenerate the test data. It is a good question whether it should be part of the release process. In fact, I don't know exactly how the script works. If it always uses the latest opset version, in this case opset=13, it might cause confusion because release 1.7 officially supports opset=12.
gramalingam(2020-08-24 23:46:41):Hi, I was wondering if this requires anything else, or if it can be merged in (after the conflict is resolved)?
askhade(2021-05-06 17:21:10):Closing this PR now since a more updated version has been checked in post 1.8 release
askhade(2020-06-29 21:02:38):Can you add more details regarding where to get the creds from?
askhade(2020-06-29 21:05:06):How to make sure?
askhade(2020-06-29 21:06:00):nit: It would be nice if you can add a reference PR for this
askhade(2020-06-29 21:07:52):There is one more step below this one to create a pr in release branch to update version number... Should they be combined?
askhade(2020-06-29 21:11:16):"Test this installation with different environments and versions of protobuf binaries." this is little vague... Can you add details around which environments? and more importantly which versions of protobuf... 

askhade(2020-06-29 21:13:00):just curious: Is this created for every release? I didn't know there was a release test team in place.
chinhuang007(2020-06-29 21:25:08):This is provided to the release manager by Prasanth. I don't think we should put individual's name into the doc. So I will use "ONNX admins".
askhade(2020-06-29 21:25:23):Windows conda package release is broken right? Can you add a few details around that as a Note
askhade(2020-06-29 21:27:01):Sure I did not mean individual person... I suggest Steering Committee
chinhuang007(2020-06-29 21:28:30):This is described in https://github.com/onnx/wheel-builder. I will add the link to the release doc.
chinhuang007(2020-06-29 21:40:01):These are two different PRs for different purposes. This one is to set the new version number in the master branch. Later one is to reset version number in the release branch after verification.
chinhuang007(2020-06-29 21:47:18):Unfortunately this is done by various members from the verification/backend team and I don't know the details. The sentence here is from the previous author to provide general guidelines. If specific details are needed, @codemzs I need your help here.
chinhuang007(2020-06-29 21:52:01):This is again similar to my previous comment. I just made the release candidates available, announced it in infra gitter/github issue, and a few people would start the verification and eventually report status. That's what we need to clarify in release WG and for 1.8.
chinhuang007(2020-06-29 22:01:37):Thanks for all your good comments. I added a line to note the Windows conda packages need some work.
chinhuang007(2020-06-30 00:19:38):Sure, updated.
askhade(2020-10-07 22:37:56):what is the process for this step? In the past PRs I only see the ones to upload to test pypi. I have not seen any PR in wheel builder repo to upload to pypi
chinhuang007(2020-10-08 00:31:43):The official build and upload to pypi is currently triggered by having a new release/tag in wheel-builder. That is the reason we created release v1.7.0 there.
askhade(2020-07-24 20:33:27):Closing this in favor of #2924 
This PR is wrongly based off a branch in onnx instead of a fork. 
jcwchen(2020-06-30 16:08:24):Good idea.
Another viewpoint: how about we simply add some notes instead of creating a new section for this? (A section is expecting users to provide some context for their problem. This description is more like a reminder for users.) 

askhade(2020-06-30 18:31:25):Let's keep this is as a first section for now atleast. Right now we see a lot of issues related to model conversion and then we have to spend eng hours to redirect them to the right repo. IMO it would be best if we ask this as a question instead of note. Let's see if this reduces the number of converter issues we see in onnx. 
When the issue is not related to model conversion they can simply answer no and proceed to next section
vinitra-zz(2020-07-02 20:41:04):Quick wording suggestions! Feel free to ignore.
```suggestion
If the ONNX checker reports issues with this model then this is most probably related to the converter used to convert the original framework model to ONNX. Please create this bug in the converter GitHub repo (tensorflow-onnx, sklearn-onnx, keras-onnx, onnxmltools) to get the best assistance.
```

jcwchen(2020-06-30 22:14:58):Besides ort, pip install torchvision from their master branch is broken as well. It seems they are fixing this right now.
jcwchen(2020-07-01 18:27:12):It seems to me that the pip install torchvision problem has been solved.
With this PR, CircleCI should work normally now.
jcwchen(2020-07-02 21:52:36):@gramalingam @linkerzhang please help me to review this.
Thank you!
jcwchen(2020-07-04 14:04:46):Updated. Thank you @linkerzhang!
linkerzhang(2020-07-03 01:27:40):typo: subgrpah  -> subgraph.
linkerzhang(2020-07-03 01:28:47):suggestion: it is not in topological order --> it may not be in topological order.
linkerzhang(2020-07-06 00:23:04):It may not be not in topological order. 

Remove the redundant "not" :). 
jcwchen(2020-07-06 00:57:05):Thanks!
jcwchen(2020-07-04 19:46:56):cc @neginraoof for review. Thank you!
jcwchen(2020-07-07 23:26:31):According to https://github.com/onnx/onnx/blob/master/docs/Changelog.md#reducelogsum-13,
it's better to use `double` instead of `float64`.
jcwchen(2020-07-17 22:55:57):@ebarsoum @gramalingam could you help to review and merge this PR? Thank you!

BTW, currently please ignore the CI failures from circleCI and travisCI. They are not related to this PR and we are solving them in another PR https://github.com/onnx/onnx/pull/2890.
daquexian(2020-08-13 03:29:54):@linkerzhang @wschin @fdwr I have updated the PR according to the comments. Please review it again. Thanks!
daquexian(2020-09-02 14:01:35):@wschin @linkerzhang  I have updated this PR. Is it ok to merge it? Thanks!
wschin(2020-09-18 23:38:12):> @wschin @linkerzhang I have updated this PR. Is it ok to merge it? Thanks!

Could you please sync this branch with master again? Sorry for being late and we want this PR in for this release.
daquexian(2020-09-19 02:36:18):> Could you please sync this branch with master again? Sorry for being late and we want this PR in for this release.

Done. Do I need to sign off according to the DCO check? I tried signing off the commits according to its instructions but I found it makes git history confusing.
wschin(2020-09-19 05:39:44):> > Could you please sync this branch with master again? Sorry for being late and we want this PR in for this release.
> 
> Done. Do I need to sign off according to the DCO check? I tried signing off the commits according to its instructions but I found it makes git history confusing.

@prasanthpul, Is DCO required now? 

@daquexian, maybe you can do

1. git checkout master
2. git checkout -b new_branch
3. git merge --squash daquexian:update_softmax // Get everything from old branch as a single commit on "new_branch"
4. git branch -d daquexian:update_softmax
5. git checkout -b daquexian:update_softmax // Based on "new_branch" to recreate "daquexian:update_softmax"
6. git push -f


[Update] As discussed with @prasanthpul offline, DOC is not required now so I just merged it. Thank you for all the hard works!
linkerzhang(2020-07-08 07:42:52):The version should also be bumped, given the default value of "axis" changed.
linkerzhang(2020-07-08 07:53:03):In my opinion, if it's only default value change, I'd suggest to not change it. The benefit is not that big with fair change (version bump).
linkerzhang(2020-07-08 07:54:33):did you verify the correctness of the subgraph?  (the "expanded" and non "expanded" model generated in this PR, feeding them same inputs will get same outputs)
daquexian(2020-07-08 16:08:12):I think the current softmax op version has already been bumped since last release (current version is already 13). Is it necessary to bump it once more?
linkerzhang(2020-07-09 03:05:35):no in that way. Only one version bump in one release.
wschin(2020-07-24 16:17:22):Could  you add the math for this operator (like it had before)?
fdwr(2020-07-31 21:54:19):Yeah, changing any of these {attribute names, attribute default values, tensors meanings} is a version breaking change.
fdwr(2020-07-31 21:57:05):Here's the old decomposition I wrote down before (ignore the flattening part now, I guess):

https://fdwr.github.io/LostOnnxDocs/OperatorFormulas.html

```
function SoftMax(Input; Output; axis):
  FlattenedInput = Flatten(Input, axis) // Flatten to 2D
  NormalizedInput = SoftMax2D(FlattenedInput; ; axis)
  Output = Reshape(NormalizedInput, Shape(X))
endfunction

function SoftMax2D(Input; Output; axis):
  MaxInput = ReduceMax(Input, axes=[1], keepdims=1)
  ExpInput = Exp(Input - MaxInput)
  ReducedExpInput = ReduceSum(ExpInput, axes=[1], keepdims=1)
  Output = ExpInput / ReducedExpInput
endfunction
```
daquexian(2020-08-11 13:50:32):What it had before is not the math of the operator. It just describes how the n-d vector is coerced into a 2-d vector. However, I will still add the math in the spec.
daquexian(2020-08-13 03:07:14):@linkerzhang @fdwr I have verified the correctness of the subgraph by inferencing the subgraph in onnxruntime and comparing the result with the reference numpy implementation
wschin(2020-08-31 22:35:37):Great!
wschin(2020-08-31 22:36:30):name??
```suggestion
  indicates the dimension along which Softmax will be performed.
```
wschin(2020-08-31 22:39:11):```suggestion
  and contains the Softmax values of the corresponding input.
```
daquexian(2020-09-02 14:00:38):@wschin Thanks! I missed the `ReplaceAll` for `{name}`. It is fixed now.
jcwchen(2021-11-03 23:07:52):Close it due to inactivity.
prabhat00155(2020-07-14 12:27:18):I don't have write access to merge this PR.
askhade(2020-08-15 00:45:08):> I don't have write access to merge this PR.

Done
linkerzhang(2020-07-09 03:04:38):this is swallowing "error". An assertion should be added, rather than returning empty string.
prabhat00155(2020-07-09 10:52:14):An assertion is already there(https://github.com/onnx/onnx/blob/456ba4cace5b21cbd64b579ebd48e83aa4081b76/onnx/defs/data_type_utils.cc#L172). Assertions are for debugging(works in debug mode: Assertion failed: (t.TensorDataTypeToTypeStr().end() != iter), function ToDataTypeString, file onnx_projects/onnxruntime/cmake/external/onnx/onnx/defs/data_type_utils.cc, line 183.), we should not access `second` if the data type is not present in the hashmap. This causes seg fault(as you can see in the issue linked above).
pranavsharma(2020-07-09 23:30:04):Yes, we shouldn't swallow the error. In this case the undefined data type is coming from the model (external input). Typically assert()'s are used to check programming mistakes, not to validate user inputs. I suggest we return an error status code that can be checked by callers. I realize this can be cascading and affect many callers, but provides better semantics by setting clear expectations that this method can return an error.
linkerzhang(2020-07-10 05:05:30):If changing the API is not good (affecting many users), throwing exception/error?
prabhat00155(2020-07-10 16:23:47):Updated code to throw runtime error.
postrational(2020-07-10 22:19:42):Should we remove the `assert`? It seems redundant now.
pranavsharma(2020-07-10 23:02:24):If we're throwing an exception, throwing std::invalid_argument would probably more sense here since the failure is due to invalid input. Also, throwing an exception doesn't free us from changing the caller. If the caller doesn't catch it, std::terminate will get called.
prabhat00155(2020-07-12 22:33:18):Done.
prasanthpul(2020-07-12 17:35:30):I thought we are deprecating Travis CI?
jcwchen(2020-07-12 18:37:38):> I thought we are deprecating Travis CI?

Oh you are right. Thank you for the reminder.
prasanthpul(2020-07-15 22:00:12):but if Travis deprecation is not happening soon, then we should do this for now. @vinitra any ETA?
jcwchen(2020-07-15 22:07:52):Agree. Besides, it seems that circleCI keeps failing because the master branch of https://github.com/pytorch/vision fails... circleCI uses `pytorch/vision` library from the master branch.
vinitra-zz(2020-07-16 22:33:35):Thanks for the PR! Deprecation was happening in https://github.com/onnx/onnx/pull/2773 but work was temporarily paused. It will hopefully go in by the end of the week
askhade(2020-07-17 18:37:15):Travis failure is happening because of the following issue : https://github.com/python/mypy/issues/6545
Copying an excerpt from the above issue here : 

mypy currently supports targeting Python versions other than the version used to run mypy. That's true, and we will still do that for older Python versions. (E.g. you can use Python 3.8 to run mypy targeting Python 2.7 through 3.7.) But if you want to target the latest Python version, you will have to use that latest version to run mypy. I think it's a relatively small price to pay -- after all, if your package claims support for Python 3.8, you probably want to use Python 3.8 to run your unit tests anyway.



Right now your change works because we don't build onnx with 3.8... but once we add this to the CI matrix we will need to make another change to make sure mypy can then work with this new target too... Please add some brief comments to document the same. 

jcwchen(2020-07-17 18:54:44):> Travis failure is happening because of the following issue : [python/mypy#6545](https://github.com/python/mypy/issues/6545)
> Copying an excerpt from the above issue here :
> 
> mypy currently supports targeting Python versions other than the version used to run mypy. That's true, and we will still do that for older Python versions. (E.g. you can use Python 3.8 to run mypy targeting Python 2.7 through 3.7.) But if you want to target the latest Python version, you will have to use that latest version to run mypy. I think it's a relatively small price to pay -- after all, if your package claims support for Python 3.8, you probably want to use Python 3.8 to run your unit tests anyway.
> 
> Right now your change works because we don't build onnx with 3.8... but once we add this to the CI matrix we will need to make another change to make sure mypy can then work with this new target too... Please add some brief comments to document the same.

Thank you for the investigation. Another thing is, I tried to upgrade python to 3.8 and set it back to 3.6 or 3.7, but it still failed because `mypy` will run by the latest python on Mac OS somehow. Therefore, I chose to brew install from certain old python version (actually it's not a good way to install python on Mac) instead of brew upgrade python first then brew switch.

I agree with you that this solution is just temporary... We definitely need to set it back to the python 3.8 in the future.
jcwchen(2020-07-17 21:32:55):It seems that it encountered an accidental problem. Close and open to trigger the CI again. 
jcwchen(2020-07-17 23:35:53)::shipit: 
askhade(2020-07-17 18:37:53):add a comment here explaining why this is being done
CLAassistant(2020-07-09 23:16:14):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=2892) <br/>All committers have signed the CLA.
wschin(2020-07-14 18:09:26):>             OpSchema::Optional)

Differentiable?

---
Refers to: onnx/defs/rnn/defs.cc:239 in 772db2c. [](commit_id = 772db2c41b526b7c78f37abe69a137983673c326, deletion_comment = False)
wschin(2020-07-14 18:09:38):>             OpSchema::Optional)

Differentiable?

---
Refers to: onnx/defs/rnn/defs.cc:368 in 772db2c. [](commit_id = 772db2c41b526b7c78f37abe69a137983673c326, deletion_comment = False)
wschin(2020-07-14 18:11:18):>             OpSchema::Optional)

Differentiable?

---
Refers to: onnx/defs/rnn/defs.cc:503 in 772db2c. [](commit_id = 772db2c41b526b7c78f37abe69a137983673c326, deletion_comment = False)
wschin(2020-07-14 18:12:35):>             OpSchema::Optional)

Differentiable?

---
Refers to: onnx/defs/rnn/defs.cc:519 in 772db2c. [](commit_id = 772db2c41b526b7c78f37abe69a137983673c326, deletion_comment = False)
wschin(2020-07-14 18:17:55):>             OpSchema::Optional));

Differentiable?

---
Refers to: onnx/defs/rnn/defs.cc:527 in 772db2c. [](commit_id = 772db2c41b526b7c78f37abe69a137983673c326, deletion_comment = False)
wschin(2020-07-14 18:18:22):>         OpSchema::Optional);

Differentiable?

---
Refers to: onnx/defs/rnn/defs.cc:122 in 772db2c. [](commit_id = 772db2c41b526b7c78f37abe69a137983673c326, deletion_comment = False)
wschin(2020-07-14 18:18:30):>         OpSchema::Optional);

Differentiable?

---
Refers to: onnx/defs/rnn/defs.cc:115 in 772db2c. [](commit_id = 772db2c41b526b7c78f37abe69a137983673c326, deletion_comment = False)
wschin(2020-07-14 18:18:42):>         OpSchema::Optional);

Non-differentiable?

---
Refers to: onnx/defs/rnn/defs.cc:101 in 772db2c. [](commit_id = 772db2c41b526b7c78f37abe69a137983673c326, deletion_comment = False)
wschin(2020-07-14 18:19:00):>         "T");

Differentiable? #Closed

---
Refers to: onnx/defs/rnn/defs.cc:93 in 772db2c. [](commit_id = 772db2c41b526b7c78f37abe69a137983673c326, deletion_comment = False)
rajeevnalawadi(2020-07-15 18:14:07):> "T");
> 
> 
> Differentiable?
> 
> Refers to: onnx/defs/rnn/defs.cc:93 in 772db2c. [](commit_id = 772db2c, deletion_comment = False)


rajeevnalawadi(2020-07-15 18:14:58):Thanks Wei Sheng for catching some of the missing differentiable tags, have updated
wschin(2020-08-04 17:42:25):>         OpSchema::Optional);

non-differentiable?

---
Refers to: onnx/defs/rnn/defs.cc:115 in d150b82. [](commit_id = d150b82e3a822357243c8b96c3a95edd593ed875, deletion_comment = False)
wschin(2020-08-04 17:45:56):>             OpSchema::Optional)

non-differentiable?

---
Refers to: onnx/defs/rnn/defs.cc:532 in d150b82. [](commit_id = d150b82e3a822357243c8b96c3a95edd593ed875, deletion_comment = False)
wschin(2020-07-21 16:06:57):```suggestion
        OpSchema::Optional,
```
This should be `OpSchema::Optional`. All `optional` variables need to remain `optional`.
wschin(2020-07-21 16:07:23):```suggestion
        OpSchema::Optional,
```
wschin(2020-07-21 16:07:34):```suggestion
        OpSchema::Optional,
```
wschin(2020-07-21 16:07:47):```suggestion
            OpSchema::Optional,
```
wschin(2020-07-21 16:07:58):```suggestion
            OpSchema::Optional,
```
wschin(2020-07-21 16:08:08):```suggestion
            OpSchema::Optional,
```
wschin(2020-07-21 16:08:17):```suggestion
            OpSchema::Optional,
```
wschin(2020-07-21 16:08:24):```suggestion
            OpSchema::Optional,
```
wschin(2020-07-21 16:11:35):This should not be changed. The two differentiable inputs should also be `Optional`.
wschin(2020-07-21 16:12:47):This should not be changed. Please double-check if the two outputs are still `Optional` (instead of `Single`) in defs.cc.
rajeevnalawadi(2020-07-22 18:19:07):With the required differentiable variables highlighted as optional based on initial definition, the errors have been resolved
CLAassistant(2020-07-13 02:20:22):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=2893) <br/>All committers have signed the CLA.
ma-hei(2020-07-13 02:33:21):Differentiability of the Pooling operator is shown by Method 1, "Reuse Existing Deep Learning Frameworks", described here https://github.com/onnx/onnx/pull/2794.

Showing differentiability of MaxPool input/output and export of model from MXnet to ONNX:

```python
from mxnet import gluon, nd, autograd, context
from mxnet.contrib import onnx as onnx_mxnet
import numpy as np

net = gluon.nn.HybridSequential()
net.add(gluon.nn.MaxPool1D(pool_size=2))
net.hybridize()

x = nd.random.uniform(-1,1,(1,3,4))
labels = nd.random.uniform(-1,1,(1,3,2))
L2loss = gluon.loss.L2Loss()

# showing differentiability of input
x.attach_grad()
with autograd.record():
    output = net(x)
    loss = L2loss(output, labels)

loss.backward()
print(x.grad)

# showing differentiability of output
with autograd.record():
    output = net(x)
    output.attach_grad()
    loss = L2loss(output, labels)

loss.backward()
print(output.grad)

net.export("maxpool1d")

sym = './maxpool1d-symbol.json'
params = './maxpool1d-0000.params'
onnx_file = './maxpool1d.onnx'

converted_model_path = onnx_mxnet.export_model(sym, params, [x.shape], np.float32, onnx_file)
``` 

We can see the derivatives of the input and output from the print statements:
```
[[[ 0.          0.05175247  0.          0.1822437 ]
  [ 0.         -0.01127579  0.          0.2635129 ]
  [ 0.         -0.11336635  0.12441261  0.        ]]]
<NDArray 1x3x4 @cpu(0)>

[[[ 0.05175247  0.1822437 ]
  [-0.01127579  0.2635129 ]
  [-0.11336635  0.12441261]]]
<NDArray 1x3x2 @cpu(0)>
```

ma-hei(2020-07-13 02:42:43):Differentiability of the Conv Operator is shown similarly:
```python

net = gluon.nn.HybridSequential()
net.add(gluon.nn.Conv1D(1, 2, weight_initializer = 'ones'))
net.hybridize()
net.initialize(ctx=context.cpu(0))

net_params = net.collect_params('0.weight|0.bias')

x = nd.random.uniform(-1,1,(1, 1, 3))
x.attach_grad()
label = nd.random.uniform(-1,1,(1,1,2))

with autograd.record():
    output = net(x)
    loss = L2loss(output, label)

loss.backward()
print(x.grad)
print(net_params['0.weight'].grad())
print(net_params['0.bias'].grad())

with autograd.record():
    output = net(x)
    output.attach_grad()
    loss = L2loss(output, label)

loss.backward()
print(output.grad)

#export of model to onnx
net.export("conv1d")
sym = './conv1d-symbol.json'
params = './conv1d-0000.params'
onnx_file = './conv1d.onnx'

converted_model_path = onnx_mxnet.export_model(sym, params, [x.shape], np.float32, onnx_file)

```
We see the derivatives of the input and kernel/bias parameters as well as the output derivative from the print statements:
```
[[[-0.45106208 -0.21056682  0.24049526]]]
<NDArray 1x1x3 @cpu(0)>

[[[0.09440736 0.1604658 ]]]
<NDArray 1x1x2 @cpu(0)>

[-0.21056682]
<NDArray 1 @cpu(0)>

[[[-0.45106208  0.24049526]]]
<NDArray 1x1x2 @cpu(0)>
```
ma-hei(2020-07-13 14:28:17):@wschin could you give some advice on how to reproduce those build failures locally? I see that when you added the differentiability tag to operators in onnx/defs/tensor/defs.cc  (https://github.com/onnx/onnx/pull/2723/files#), you added the parameter "OpSchema::Single" to operators such as Transpose, Scatter, etc. Why was that necessary?

Note: I believe whats happening is that the Operator description files need to be auto generated locally and then be added to the commit!? I ran 
```
python3 onnx/defs/gen_doc.py
```
after making the changes to the defs file but docs/Operators.md doesn't get updated.

Update: I think I figured it out. I needed to run ```./tools/update_doc.sh``` to update the docs instead of running gen_doc.py directly.

wschin(2020-07-14 18:23:46):>             OpSchema::Optional)

NonDifferentiable. Because `PoolOpSchemaGenerator` is changed, we need to tag all places where `PoolOpSchemaGenerator` is used.

---
Refers to: onnx/defs/nn/defs.cc:368 in c0e1989. [](commit_id = c0e198907ce184cedba99e87e44b25e2d3fca1f2, deletion_comment = False)
ma-hei(2020-07-14 19:44:20):@wschin I revised the commit according to your comments. Thanks for catching those two issues.
wschin(2020-07-14 18:28:19):I guess it's differentiable? It's similar to one-to-one mapping.
ma-hei(2020-07-14 18:35:20):Yes, definitely. That was a mistake. Regarding the comment above (line 368): This is the indices output of MaxPool (not the actual pooling output). It's just the indices of the max value in the input window (an integer value if I understand correctly). Is it really differentiable? 
Edit: Forget the last sentence. I thought you commented that it should be differentiable. Will fix it.
wschin(2020-07-21 15:59:29):Yes, my understanding is that indexes are not differentiable but the output pooled from input is differentiable. :)
jimspohrer(2020-07-13 21:53:44):@jspisak pleases approve this, so it gets merged.   Would like to get Facebook added next if possible.
askhade(2020-07-28 00:07:16):Once https://github.com/onnx/onnx/pull/2925 is checked in, updating an op with an existing data type will not require opset version update. @gramalingam since sequence is an existing type this PR should not require version bump right?
gramalingam(2020-07-28 17:55:57):@askhade my understanding is that doesn't apply here, since the inference-code is impacted here.
BowenBao(2020-08-13 20:50:50):@gramalingam @askhade I updated the PR with test cases and extended the scope to include if operator. 
gramalingam(2020-07-22 15:59:37):I think the inference function above does not check that scan-outputs are tensors. Can we make the check stricter for scan outputs?
gramalingam(2020-07-22 16:04:58):Is there any reason (specific to the Loop op) to restrict the outputs to be of any type? E.g., why not maps? Or, sequences of sequences? (This is probably allowed by above check, but not by the type-constraint on the op below, which restricts it to sequences-of-tensors.) 

I think allowing the output-type to be unconstrained would require some extensions in the Schema implementation, which, I think, assumes that the set of possible types can be finitely enumerated and stored. It would be worth taking a look to see if this can be generalized. This would avoid having to generalize the Loop op again the next time if we want to allow other types.
gramalingam(2020-07-22 16:08:34):See comment above. If we could generalize the Schema implementation so that a type-variable "V" can be unconstrained (that is, without any TypeConstraint specified for it), we could omit this type-constraint. I think it involves some non-trivial work to extend this, but it would be really useful and great.
BowenBao(2020-07-22 20:21:09):updated
BowenBao(2020-07-22 20:49:58):Today there does not exist operators that use/produce such flexible types. I agree to support types like sequence of sequence, we need to update Schema to relax the assumption of types being a finite set. However, I'm not sure if we should start this change with Loop, instead of a real use case. Should such op be added in the future, the infrastructure will have to be updated, and I'm relatively ok to generalize Loop (and If) accordingly in that opset update.
gramalingam(2020-07-23 01:04:35):The problem is that when that happens, we will have to bump the Loop operator's version again even though a Loop implementation itself does not really depend on the types of these values. This leads to unnecessary version-updates, which impacts exporters, convertors, backends. A loop is polymorphic over this type V and the constraints on the type V are artificial. (If this had been done correctly for the earlier version, even this extension would not have been needed).
gramalingam(2020-08-24 16:29:05):Change name from Loop to If?
gramalingam(2020-08-24 16:53:09):Could we move this logic (the complete if-then-else) as a function, say "UnionTypeInfo(const TypeProto& type1, TypeProto& type2)" into the shared utility file https://github.com/onnx/onnx/blob/master/onnx/defs/shape_inference.h ? It will help if the same logic is required elsewhere and if we generalize this in the future to handle map-types, etc. as well.
gramalingam(2020-08-24 16:56:54):The if-then-else in lines 237 to 245 should also be merged into the same function. The code here needs to just say
```cpp
   *if_output = *then_output;
   UnionTypeInfo(*else_output, *if_output);
```
gramalingam(2020-08-25 20:03:49):Better to just call "UnionTypeInfo(source_type.sequence_type().elem_type(), target_type.sequence_type().elem_type())" ... that will handle the case correctly in case one is a sequence of tensors and the other is a sequence of non-tensor-type.
BowenBao(2020-08-25 23:24:57):Thanks, updated
BowenBao(2020-08-25 23:30:07):thanks! makes sense
gramalingam(2020-08-25 23:34:16):Thanks for addressing it! I think the nested condition "source_type ... has_tensor_type() && target_type ... has_tensor_type()" should also be removed.
gramalingam(2020-08-25 23:36:18):Was that meant to check "has_elem_type()"? I assumed a sequence-type will always have an "elem_type". We could check for that to be safe I suppose ... but we should probably treat it as an error if there is no elem_type.
lgtm-com[bot](2020-07-24 04:30:29):This pull request **introduces 1 alert** when merging 404fc87e7fdc610fdc083eaa5bf5c655c30f6e1b into 91bdfe47ed30379b83e8ca4416b9eabfc52d653d - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-e7ddcf5b385d27632d21421014321556d2f2ec31)

**new alerts:**

* 1 for Mismatching new/free or malloc/delete
jcwchen(2020-07-24 04:44:11):@gramalingam @askhade @skottmckay Thank you so much for the detailed review!
To conclude the discussion above:
1. There is no condition for merging input and initializer.
2. If they both exist, the only thing we need to do is to check whether these two are consistent (like rank, dimension mapping).
3. If input exists, we should always take the shape from input.
4. If input doesn't exist and ir_version>=4, we should use the shape from initializer for the input shape.

I suppose the current implementation should meet these rules.
Next, I will write more graph-level tests to test this functionality.  


lgtm-com[bot](2020-07-24 04:53:49):This pull request **introduces 1 alert** when merging 10c094fa161a58810f5e26b82850d1ddead95421 into 91bdfe47ed30379b83e8ca4416b9eabfc52d653d - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-b7eaf33fb43d7b0fd0547f38f3a2fbe9411966b4)

**new alerts:**

* 1 for Mismatching new/free or malloc/delete
lgtm-com[bot](2020-07-24 22:13:48):This pull request **introduces 1 alert** when merging 30c9ad4d417c8ccfbc5b2d927b55b23f1794b43b into f8c663c6e5fe710f90381904f9f90f0c81618ce4 - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-ace028cac05c1d2ad3b839f4fde5f28f3b2d0f66)

**new alerts:**

* 1 for Mismatching new/free or malloc/delete
lgtm-com[bot](2020-07-24 22:44:36):This pull request **introduces 1 alert** when merging c4cb4b1103697a9041e29d213e3285f0bdea9aca into f8c663c6e5fe710f90381904f9f90f0c81618ce4 - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-81057ff6a02627b568463234ebc4c76123ab84b3)

**new alerts:**

* 1 for Mismatching new/free or malloc/delete
lgtm-com[bot](2020-07-25 00:26:09):This pull request **introduces 1 alert** when merging ab9a5ac184f5ff0dfdc5d702de3e2d05b575ccd8 into f8c663c6e5fe710f90381904f9f90f0c81618ce4 - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-12f8d37e35c3ffd1e0dd20a0a4d81ca5bbc14e82)

**new alerts:**

* 1 for Mismatching new/free or malloc/delete
lgtm-com[bot](2020-07-25 17:19:17):This pull request **introduces 1 alert** when merging 797dbabb778908a02ff9dceba4215d061ab14a26 into f8c663c6e5fe710f90381904f9f90f0c81618ce4 - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-bc7f1df22d1af2cf7942d9f89693c73d55ab88ad)

**new alerts:**

* 1 for Mismatching new/free or malloc/delete
jcwchen(2020-07-31 00:36:39):Add two more changes:
1. If the model uses old ops <= 8 (corresponded IR version is <=3), the shape inference won't update the input shape from the initializer shape.
2. If the model has any opset from other doman (e.g., pytorch-caffe2), the shape inference won't update the input shape from the initializer shape either.

I suppose these two kinds of models should not follow the new IR so they don't need to update input from initializer.

@gramalingam please comment if you have other thought. Thank you!
jcwchen(2020-08-11 00:12:17):Added free memory for created `initializertype`.
jcwchen(2020-07-16 19:36:10):If such tenor has been found in input, maybe we don't need to do anything here from the initializer?


gramalingam(2020-07-23 01:14:59):I am confused. Why do we need a temp graph? Where is this eventually used?
gramalingam(2020-07-23 01:23:18):I guess you are using it for automatically allocating and freeing new ValueInfoProto below? I guess we just need a TypeProto, actually. At the least, it is worth documenting. 
gramalingam(2020-07-23 01:29:58):But it might be simpler to just use a vector of TypeProto, or something like that, to do the memory management.
gramalingam(2020-07-23 01:42:49):I think it would be useful, in this case, to check that the declared type/shape (from input lis) is compatible with the actual value in the initializer. I suggest creating a new "inferred" type (just like newTensorFromInitializer below) and calling checkShapesAndTypes on the original type and this new inferred type.
gramalingam(2020-07-23 01:44:11):I think "initializerTensorType" would be a better name for "newTensorFromInitializer" ... it is a tensor-type, not a tensor.
askhade(2020-07-23 03:04:58):this is not the right approach
ir_version 4 and above allow initializers to not be in graph inputs (this means they are constant)
if an initializer does not have a matching graph input it should be treated as constant and the shape from initializer should be used
askhade(2020-07-23 03:06:34):the behavior varies from ir_version < 4 vs ir_version >=4
In case of ir_version < 4 the check which Rama suggested makes sense
In case of ir_version >=4 the shape from input should be prefer as the value in initializer is merely a default value
gramalingam(2020-07-23 03:41:15):A couple of comments: (a) Whatever the IR version, the type of the input-variable should be consistent with the value in the initializer. It is okay if the type of the input-variable has less precise-shape than the initializer, but it should be consistent. If the type of the input-variable is a float-tensor and the initializer is an int-tensor, it is incorrect; similarly, if the shape of the input-variable is [B, 10] while the initializer-value has shape [1, 5] it is incorrect. (b) If the input is present, we should use the shape given in the input-type, and not the more-precise shape from the initializer (even though we can check that they are consistent).
jcwchen(2020-07-23 17:17:24):> I guess you are using it for automatically allocating and freeing new ValueInfoProto below? I guess we just need a TypeProto, actually. At the least, it is worth documenting.

Yes. I also felt weird about creating a useless graph... Sounds like I can simply use `valueTypesByName[tp.name()]= new TypeProto()` for creating a new TypeProto. Thank you for the good catch.
gramalingam(2020-07-23 20:13:25):Yes, that would be better ... but then you need to remember to "free" the allocated TypeProto at the end. (That is why I was referring to memory management.) As long as that happens correctly, this would be good.
skottmckay(2020-07-23 23:30:30):My understanding was that an input may override the shape of the initializer providing a default value, so when you're doing the shape inferencing you can only use shape info from a constant initializer (an initializer is constant if IR < 4, or if it has no matching input if IR >= 4). @gramalingam do you agree?
gramalingam(2020-07-23 23:35:19):We should only do the "check". We should NOT call "merge".
gramalingam(2020-07-23 23:43:46):Roughly, yes. More precisely: we should NOT do the merge below.

Case 1: If X is both an input and an initializer (regardless of IR version), we should use ONLY the shape information provided in the input type; however, it should be the case that the initializer-value should be consistent with the input-type, so it is okay to CHECK for this consistency. For example, it is okay for the input-type to have a shape [M,N] and the initializer to have shape [100, 256]. It is not okay for the input-type to have shape [M,N] and the initializer to have shape [10, 10, 256]. 

Case 2: X is only an initializer and not an input. (Here IR should be >= 4.) We can use the actual shape of the initializer in the inferencing.

Does that sound right?
gramalingam(2020-07-23 23:51:25):@skottmckay : the one point I am not clear about is your comment that an initializer is a constant if IR < 4. I need to check back to see if this was the conclusion we reached for IR < 4. 
jcwchen(2020-07-24 03:49:03):Just created a condition to handle the `ir_version` stuff. Thank you for catching this.
skottmckay(2020-07-24 04:39:59):ONNX Runtime treats initializers as constant if IR version is < 4 as the initializers had to have matching graph inputs, and to not do so would prevent constant folding running. The ONNX spec doesn't require this though, so I agree with your description of Case 1 and Case 2 handling and that an IR check shouldn't be necessary here. 
gramalingam(2020-08-04 01:11:08):I don't understand why we need to look at the opset_version here. Technically, it is okay to use IR version > 4 with opset_version < 9. In this scenario, if we have an initializer that is not an input, we can treat it as a constant and it seems okay to execute the following.
jcwchen(2020-08-04 01:30:17):The previous modification without considering the opset in this PR will break some tests in pytorch (graphs converted from `torch.onnx.export`). Those graphs are using old opsets (< 9), but it seems that the ir_version is always the latest one in `torch.onnx.export`. In that case, do we still need to update the input from the initializer? I thought old opset corresponds to old ir_version.

The failure is as follows:
https://circleci.com/gh/onnx/onnx/5368?utm_campaign=vcs-integration-link&utm_medium=referral&utm_source=github-build-link
They are using opsets from pytorch.caffe2 domain... Maybe simply considering domain is good enough for this failure fix.
gramalingam(2020-08-04 18:06:17):But what causes the checks to fail? Do those models have initializers that are not in the input-list? (Otherwise, the behavior should not be affected by this PR ... does that sound right?) If those models have initializers that are not in the input-list, how should we handle them? They should be handled as constants, right? If something breaks when we treat them as constants, then there must be some *other* issue ... either in the pytorch model exporter or within the onnx checker. Does that make sense?
gramalingam(2020-08-04 18:14:12):Looking at "test_while_cond" for example: it appears that the exporter is exporting ops (relational comparison ops) with int types, though ONNX does not have them. So, it is an invalid model, and it is correct for the checker to produce an error message. I think that we need a single general solution to the problem of incorrect ONNX models exported from these backends (for example, using some explicit list of known incorrect models from the checker in CI, or something similar).

jcwchen(2020-08-05 04:10:31):Agreed. In fact, these models keep failing in different conditions (https://github.com/onnx/onnx/pull/2783) because they have some opsets which do not come from ONNX. Then the solution will be same as what we discussed before as you mentioned above.

Therefore, I removed the opset version stuff and only consider the opset domain: if the model is using some opsets other than ONNX domain, the shape inference won't update the input from initializer. Besides, I will propose a PR about deprecating the CircleCI and consider to remove those tests (invalid models to ONNX). 

Thank you for the explanation @gramalingam
gramalingam(2020-08-05 15:57:18):I think even the opset check is not needed. The initializer semantics has nothing to do with opsets. It is related *only* to IR version. The opset check can lead to unintuitive behavior; if I simply add a new domain "my.special.domain" for one particular node-op, it will suddenly impact the inference/checking for other initializers/nodes/ops which have no connection to "my.special.domain". Thanks.
jcwchen(2020-08-05 20:53:10):Understood. Just removed other domain stuff.
gramalingam(2020-08-10 21:59:09):Dimensions are expected to have non-negative values. "-4" is not a meaningful dimension. Changing it to "4" would be better.
gramalingam(2020-08-10 22:01:03):In IR version 3, we expect every initializer to also be in input. Does this not cause an error? I think it should be better to use (None, None) as input_shape.
jcwchen(2020-08-10 23:25:18):Changed. Thanks
jcwchen(2020-08-10 23:41:54):Using ir_version=3 does not cause an error in this case with current checker.
Using `None` here is to not pass input (empty list) to `make_tensor`.
gramalingam(2020-08-11 00:11:56):Hmmm. But it will create a model with x and y as initializer, but not as inputs, in IR version 3. I think it is working because you are calling only shape-inference here ... but the model will not pass this check in checker.cc https://github.com/onnx/onnx/blob/434cf85abde5b4eb3aa537bcbe892b39eed3e9c6/onnx/checker.cc#L679 , I think. So, isn't it an invalid model?
gramalingam(2020-08-11 00:17:55):Thanks for adding this! Minor nit: "deleteCreatedTypes" would be a better name than "deleteCreatedTensors".
jcwchen(2020-08-11 00:21:01):Good idea.
jcwchen(2020-08-11 05:12:31):You are right... It will be caught by `ValidationError: x in initializer but not in graph input`.
I will use (None, None) as input to produce a valid model. Just changed this test.
Thank you for the good catch!
CLAassistant(2020-07-16 15:05:10):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=2906) <br/>All committers have signed the CLA.
askhade(2020-09-30 16:31:34):Overall looks good to me. There is a small comment I added apart from that once you sign the DCO we can check this in. Thanks!
daquexian(2020-10-25 10:50:41):@askhade @gramalingam Since it is important to onnx optimizer, I updated it myself in https://github.com/onnx/onnx/pull/3071
daquexian(2020-11-18 02:16:38):The content of this PR has been merged as #3071.
askhade(2020-09-30 16:30:25):If you do the check for found_in before checking the node outputs then you can exit early when the name is found in node inputs. 
gramalingam(2020-09-30 21:26:07):What happens when we have nested-scopes produced via control-flow operators? Don't you want to ensure uniqueness handles that too?
vinitra-zz(2020-07-17 18:26:37):As you have made a lot of the same changes implemented in https://github.com/onnx/onnx/pull/2773, I will change the focus of that PR to only deprecate Travis.

Also, as discussed offline, I will submit a PR later on addressing the ONNX_LITE_PROTO case.
jcwchen(2020-07-17 13:56:34):```suggestion
      export CMAKE_ARGS="-DONNX_USE_LITE_PROTO=ON -DONNXIFI_DUMMY_BACKEND=ON"
```
snnn(2020-07-22 21:55:00):BTW, if ONNX is dynamically link to protobuf, and if ONNX is a share lib(e.g. an python extension), then it should use the protobuf lite, otherwise if something else in the same process want to use protobuf and onnx too, it will cause conflict and won't work. The dynamic version of libprotobuf full(not lite) requires all the *.proto files be compiled in a single DLL(or .so) file.

So, ONNX should prefer to use the lite version of protobuf when possible.
snnn(2020-07-28 19:19:58):A tiny suggestion: this is getting more complex, and I feel it could be avoided. 

First,  ONNX must have a pipeline that based on manylinux docker images, because the official packages must be from there. And that one doesn't have conda.  If the CI pipeline largely differ with the packaging pipeline, it will create trouble for you. You may often need to pay extra time on repairing the packaging pipeline. 

(Please feel free to ignore this)

askhade(2020-08-04 15:33:01):> A tiny suggestion: this is getting more complex, and I feel it could be avoided.
> 
> First, ONNX must have a pipeline that based on manylinux docker images, because the official packages must be from there. And that one doesn't have conda. If the CI pipeline largely differ with the packaging pipeline, it will create trouble for you. You may often need to pay extra time on repairing the packaging pipeline.
> 
> (Please feel free to ignore this)

Agree with Changming the CI pipelines should be close to the packaging pipelines. Let's use a different PR to do this work. Goal of this PR is to bring the azurepipelines on par with the Travis pipeline so that Travis can be deprecated. 
askhade(2020-08-04 15:36:57):Based on our offline discussion following needs to be fixed/verified in this PR
1. Make sure onnx_ml=0 setting is working. Right now onnx_ml=0 is not set properly so this never gets set in the CI. More importantly do we need this at all? Can it always be just set to 1?
2. Debug setting is actually working ( it think you already fixed this)
3. Linux and Mac CIs have the "treat warning as error flag turned on" just like Travis CI
4. Right now git diff --exit-code sets the exit code but azure pipeline does not treat it s error. Add exit code check for the git diff tests
askhade(2020-07-17 20:49:37):quotation marks are required otherwise it does not recognize all the args. This is from MacOS CI for this PR
/Users/runner/work/_temp/d95fff7f-6a8a-44fd-a5d0-8d57263db9dc.sh: line 10: export: `-DONNXIFI_DUMMY_BACKEND=ON': not a valid identifier

askhade(2020-07-17 20:52:03):${onnx_lite} is empty. I think you need to put OFF or ON inside quotation marks

This is from MACOS CI for this PR
ONNX version          : 1.7.0
--   ONNX NAMESPACE        : ONNX_NAMESPACE_FOO_BAR_FOR_CI
--   ONNX_BUILD_TESTS      : ON
--   ONNX_BUILD_BENCHMARKS : OFF
--   ONNX_USE_LITE_PROTO   : 
--   ONNXIFI_DUMMY_BACKEND : OFF
--   ONNXIFI_ENABLE_EXT    : OFF

askhade(2020-07-17 20:52:33):quotation marks are needed here. without it the second arg is not recognized
vinitra-zz(2020-07-17 22:25:17):Thanks! I've fixed this in the latest update.
askhade(2020-07-17 22:41:25):this still did not take effect.... I think you need single quotation 'OFF'
vinitra-zz(2020-07-22 18:06:16):Thanks for letting me know! Will try that.
askhade(2020-07-22 21:22:06):python 2.7 is going to be deprecated soon.. can you simply add the LITE option to Py36-ml? (for both mac and linux ci)
vinitra-zz(2020-07-22 21:25:48):added!
askhade(2020-07-22 21:26:12):I dont think it is still working : 

ONNX version          : 1.7.0
--   ONNX NAMESPACE        : ONNX_NAMESPACE_FOO_BAR_FOR_CI
--   ONNX_BUILD_TESTS      : ON
--   ONNX_BUILD_BENCHMARKS : OFF
--   ONNX_USE_LITE_PROTO   : 
--   ONNXIFI_DUMMY_BACKEND : ON

askhade(2020-07-22 21:31:11):may be try something like this : 

if [ "${onnx_lite}" == "ON" ]; then
  export CMAKE_ARGS="${CMAKE_ARGS} -DONNX_USE_LITE_PROTO=ON"
fi
askhade(2020-07-27 22:14:32):Is this needed? given you are adding it in conda on line 51
I believe this is needed in non anaconda envs
vinitra-zz(2020-07-27 23:15:06):It was needed to resolve CI errors, even with the conda-forge install. The CI was failing because it could not identify the "protobuf compiler" without that line.
vinitra-zz(2020-07-27 23:34:45):Referenced run: https://dev.azure.com/onnx-pipelines/onnx/_build/results?buildId=3188&view=logs&jobId=2105a913-e5a3-5f74-1c30-4f8b7a56f2db&j=2105a913-e5a3-5f74-1c30-4f8b7a56f2db&t=c2afa4d8-3a4c-5e2d-1542-4328675067bd
snnn(2020-07-28 01:49:56):Then you could run into protobuf mismatch problem. You'll use the protoc from the operating system(protobuf-compiler package), but libprotobuf from conda(based on your description). Otherwise you don't need two. 

If the could not identify the "protobuf compiler", please check if your PATH env variable has setup correctly. Please reference conda's document to see which env vars need be updated.
askhade(2020-07-28 04:43:00):I looked at your reference run. The protobuf package is not being installed in the conda environment that is why it cant find it.
snnn(2020-07-28 17:07:47):Ideally, protoc, libprotobuf and protobuf python package, all these three components should have the same version. But it seems ok if the python one has higher version than the other two.
vinitra-zz(2020-07-28 17:15:28):I've removed the two dependencies (libprotoc-dev and protobuf-compiler) and used a conda install -c conda-forge protobuf line in the script directly (like in the CI for onnxmltools -- https://github.com/onnx/onnxmltools/pull/207). 
vinitra-zz(2020-07-28 17:16:07):It seems like the install error is a possible bug with azure pipelines' conda setup task. Thanks for the advice!
askhade(2020-08-06 16:37:38):on windows platform you dont nee quotation marks
git diff --exit-code  -- . :(exclude)onnx/onnx-data.proto :(exclude)onnx/onnx-data.proto3
should work
vinitra-zz(2020-08-07 17:44:23):Hi @askhade, @snnn   -- I can't seem to get the Linux CIs to pass without these two dependencies. It's not finding the Protobuf Library in the source package, even when I move the conda install -c conda-forge protobuf line in this block. Is it ok to install these dependencies as well?

Maybe removing them can be the work of a future PR if needed.
askhade(2020-08-10 16:00:57):As snnn explained above this might create other issues because you will end up using protoc and libprotobuf from different sources. Can you check whether protobuf compiler was indeed installed? If yes then fixing the PATH should resolve this issue otherwise can you simply get rid of conda? can you use virtualenvv for env management?
jcwchen(2020-09-03 03:32:14):```suggestion
      git diff --exit-code  -- . :(exclude)onnx/onnx-data.proto :(exclude)onnx/onnx-data.proto3
      IF NOT %ERRORLEVEL% EQU 0 (
        @echo "git diff returned failures"
        EXIT 1
      )      
```
jcwchen(2020-09-03 03:32:37):Catch the error for Windows
vinitra-zz(2020-09-03 19:31:13):thanks for the suggestion!
wschin(2020-07-21 15:45:23):Yes, your description on `Mod` is correct. I also did a small experiment for you using Pytorch.

```python
import torch
import torch.nn as nn

# A single-operator model. It's literally a Pytorch Reshape.
# Note that Pytorch Reshape can be directly mapped to ONNX Reshape.
class MyModel(nn.Module):
  def __init__(self):
    super(MyModel, self).__init__()

  def forward(self, x, t):
    y = torch.fmod(x, t)
    y.retain_grad()
    return y

model = MyModel()

x = torch.tensor([[1., -1], [1., 1.]], requires_grad=True)
t = torch.tensor([[1., 2], [3., 4.]], requires_grad=True)
y = model(x, t)
dy = torch.tensor([[1., 2.], [3., 4.]])

torch.autograd.backward([y],
  grad_tensors=[dy],
  retain_graph=True,
  create_graph=True,
  grad_variables=None)

# This example shows the input and the output in Pytorch are differentiable.
# From the exported ONNX model below, we also see that "x" is the first input
# of ONNX Reshape and "y" the output of ONNX Reshape. Therefore, we can say
# the first input and the output of ONNX Reshape are differentiable.
print(x.grad)
print(t.grad)
print(y.grad)

with open('model.onnx', 'wb') as f:
  torch.onnx.export(model, (x, t), f, opset_version=11)
```

It will throw an error
```
Exception has occurred: RuntimeError
the derivative for 'fmod: other' is not implemented
```

If we change
```
t = torch.tensor([[1., 2], [3., 4.]], requires_grad=True)
```
to
```
t = torch.tensor([[1., 2], [3., 4.]])
```
the script above works ok.
sveta-levitan(2020-09-04 20:08:57):Here is Pytorch code for checking differentiablity of Floor:
```
import torch
import torch.nn as nn

# A one-operator model for checking differentiability of Floor in Pytorch.
class MyModel(nn.Module):
  def __init__(self):
    super(MyModel, self).__init__()

  def forward(self, x):
    y = torch.floor(x)
    y.retain_grad()
 
    return y

model = MyModel()

x = torch.tensor([[1.1, -1.1], [1.2, 1.9]], requires_grad=True)
y = model(x)
y.retain_grad()
dy = torch.tensor([[1., 2.], [3., 4.]])

torch.autograd.backward([y],
  grad_tensors=[dy],
  retain_graph=True,
  create_graph=True,
  grad_variables=None)

# This example shows the input and the output in Pytorch are differentiable.
# From the exported ONNX model below, we also see that "x" is the input
# "y" the output of ONNX Floor. Therefore, we can say
# the input and the output of ONNX Floor are differentiable.
print("x.grad:", x.grad)
print("y.grad:", y.grad)

with open('modelFloor.onnx', 'wb') as f:
  torch.onnx.export(model, (x), f, opset_version=11)

Output: 
x.grad: tensor([[0., 0.],

        [0., 0.]])

y.grad: tensor([[1., 2.],

        [3., 4.]])
```
sveta-levitan(2020-09-04 21:59:46):For the operation Clip the min and max arguments must be scalars in PyTorch clamp, hence they can't have gradients, are non-differentiable. The first input and the output are differentiable.
CLAassistant(2020-10-06 04:17:38):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=2911) <br/>All committers have signed the CLA.
wschin(2020-10-11 21:19:27):It's partial differentiable. I am ok to make it non-differentiable for a more conservative setting. If needed, changing from conservative (non-differentiable) to aggressive (differentiable) won't cost us too much.
wschin(2020-10-11 21:39:58):```suggestion
            OpSchema::NonDifferentiable)
```
It should be consistent to `Ceil`'s tags.
wschin(2020-10-11 21:51:08):Let's make this op entirely non-differentiable because all its inputs and outputs are integer tensors.

There are some techniques we can use to compute the backward of quantized ops. For example, we can 
1.	first cast all inputs and outputs to float tensors,
2.	compute backward using those float tensors,  
3.	round float gradient to integer tensor. 

However, we don’t have “standard” ways to do the cast integer to float and float to integer, so I prefer to make them non-differentiable first. Once we have an agreement on the gradient computation for integer tensors, we can change those non-differentiable tags to differentiable (this won’t break backward compatibility).

wschin(2020-10-11 21:53:52):```suggestion
            OpSchema::NonDifferentiable)
```
This field contains labels in classification problems.
wschin(2020-10-11 21:55:44):I also want to make this non-differentiable because I never seen anyone trained this tensor.
```suggestion
            OpSchema::NonDifferentiable)
```
wschin(2020-10-11 21:59:18):Let's also make all its inputs and outputs non-differentiable because they are integer tensors. The differentiable of integer doesn't exist in math so I prefer to go with a conservative setting here. 
CLAassistant(2020-07-21 01:11:59):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=2913) <br/>All committers have signed the CLA.
askhade(2020-07-23 23:38:13):do comments on line 13 and 14 make sense anymore?
jcwchen(2020-07-24 17:42:48):Removed. Thank you for the reminder.
gramalingam(2020-07-23 00:58:17):> can you add a cross reference between this script and the versioning doc at https://github.com/onnx/onnx/blob/master/docs/Versioning.md#released-versions
> 
> Whenever one is updated, the other needs to be updated too.

Done, thanks.
alrevuelta(2020-07-26 13:22:26):This is very useful for #2912 thanks!. Just one concern. Regarding `VERSION_TABLE` that comes from [here](https://github.com/onnx/onnx/blob/master/docs/Versioning.md#released), is the `v1.2` entry right?

I can't find `v1.2` neither in [onnx PyPi](https://pypi.org/project/onnx/#history) nor in the [GitHub Releases](https://github.com/onnx/onnx/releases). The alternatives are `v1.2.1` or `v1.2.2`.

Since @prasanthpul mentioned this:

> can you add a cross reference between this script and the versioning doc at https://github.com/onnx/onnx/blob/master/docs/Versioning.md#released-versions
Whenever one is updated, the other needs to be updated too

I thought it would be appropriate to bring this up here.
gramalingam(2020-07-27 19:49:10):@alrevuelta : that's a good question. I merely copied the text document over. May be this is the history, and perhaps not all versions were published. @prasanthpul , do you know? Should we do something different for those versions?
askhade(2020-08-04 15:27:14):Circle CI is fixed now. Merging with master should resolve the failure

askhade(2020-08-14 17:06:12):@ebarsoum @gramalingam what is the status on this? The changes look good to me and I am ok with adding auto-generation as part of a separate PR. 
gramalingam(2020-08-14 17:17:12):I am also fine with completing this PR.
alrevuelta(2020-08-19 18:10:01):I mentioned this some weeks ago but it didn't got much attention. Can someone have a look? Is it right?

> This is very useful for #2912 thanks!. Just one concern. Regarding VERSION_TABLE that comes from here, is the v1.2 entry right?
I can't find v1.2 neither in onnx PyPi nor in the GitHub Releases. The alternatives are v1.2.1 or v1.2.2.
jcwchen(2020-07-22 23:59:53):```suggestion
        test(12, 7)
```
askhade(2020-07-23 16:45:56):can you add some tests with multiple imports... 
another questions : Is this acceptable ai.onnx 11, ai.onnx.ml 2. ai.onnx.training 1 ? Can you add such a test case with expected result
gramalingam(2020-07-27 19:45:49):I added more tests. About the second question: this will simply choose the "maximum" among the "minimum IR version" required for each opset in the list. So, it is acceptable. The rationale for this is that we should keep the IR version backward compatible as much as possible. Any model that is valid in IR version N should be a valid model in IR version n+1.
askhade(2020-07-28 21:04:49):This table needs to be updated before every release right? Can you change this to something like : This must be updated before creating release candidate packages... 
Exactly when the doc is updated does not matter as much as when this table is updated. 
askhade(2020-07-30 17:01:22):since this method is used to add custom domains to this map, not sure last_release_version  makes sense here... 

askhade(2020-07-30 17:04:16):can you also add an api to get the last release version number
gramalingam(2020-07-30 18:31:28):Isn't it possible that the same scenario arises for custom-domains? Namely, the "last official release" vs "current master branch"? Why expose a different API for onnx-domains and custom-domains? If we can keep them as similar/close to each other as possible, it seems better ... it will avoid unexpected issues later on, I think.
gramalingam(2020-07-30 19:08:50):Interesting that this does not already exist. It seems more broadly usable, so let me create a separate "version.h" in onnx/onnx/common.
askhade(2020-07-30 20:43:13):We can keep this here... I was just wondering whether this will ever be used... last release version makes sense for onnx domains because when backends pick the domain and version map thay want to be able to distinguish between the released vs yet to be released versions... I dont see how this is applicable for custom domains.

Having said that I agree with your point that keeping the apis as similar as possible is better. Can you add a comment to this method with some description for this last_release_version param.

gramalingam(2020-08-04 17:38:19):Added comment. My thinking is that if we use "my.custom.domain", it could have its own release cycle independent of ONNX. But it would still make sense to talk about the "last released opset version" of "my.custom.domain". 
gramalingam(2020-08-04 17:52:57):I am not sure I understand the distinction you want to make. It seems important that both are updated. It seems better to update both at the same time (ideally). As to the "when" question: whenever we start a new opset version (for ai.onnx or ai.onnx.ml etc.), it seems to me we should update all of these ... does that sound right?
askhade(2020-08-04 18:10:38):I got confused by the wording:
Both must be updated whenever a new version of ONNX is **released** 
This made me think that the updates need to happen before a release and that is why I made the comment that it should be clear that the update needs to happen before the release package is created (with documentation 1 can always update them after the release)

What you said now in the comment makes more sense to me - 
whenever we start a new opset version (for ai.onnx or ai.onnx.ml etc.), it seems to me we should update all of these
ebarsoum(2020-08-08 03:41:13):Why we need a text here? You should update "https://github.com/onnx/onnx/blob/master/docs/OnnxReleases.md".
ebarsoum(2020-08-08 03:41:57):Can we read it from: https://github.com/onnx/onnx/blob/master/VERSION_NUMBER?
gramalingam(2020-08-10 16:33:18):Thanks for the suggestion. I updated the OnnxReleases.md file also.
gramalingam(2020-08-10 16:37:16):We can't read that file at run-time, right? The one thing we could do is to auto-generate one file from the other. But that seems non-trivial and not worth it (at this point).
gramalingam(2020-08-12 15:59:02):(Summarizing an offline discussion.) Can we make auto-generation of these files a separate task and do it in a separate PR? It will be better to decide on a solution for all the version-numbers in multiple files, instead of just for this one file.
matteosal(2020-09-04 16:41:18):@prasanthpul @askhade can this get attention and feedback?
I have reverted the change about the position of `num_directions` in the shape list, so this PR only contains the new attribute to enable batchwise mode. I have renamed the attribute `batch_major`, defaulting to 0 for backward compat (and updated shape inference logic accordingly).
This currently only has adapters for rnn. Once the change is approved, I'll add adapters for gru and lstm as well.
postrational(2020-09-22 15:37:52):There seems to be a merge conflict in this PR, which removes the `(differentiable)` annotations from inputs and outputs.

Also, is there a reason `third_party/pybind11` was bumped?

Merge conflicts in other files need to be resolved and we should check why CI isn't passing. 
postrational(2020-09-22 16:45:58):Let's make sure we have a shape inference test for the reverse case.
matteosal(2020-09-25 12:13:54):> There seems to be a merge conflict in this PR, which removes the `(differentiable)` annotations from inputs and outputs.
> 
> Also, is there a reason `third_party/pybind11` was bumped?
> 
> Merge conflicts in other files need to be resolved and we should check why CI isn't passing.


matteosal(2020-09-25 12:15:30):> There seems to be a merge conflict in this PR, which removes the `(differentiable)` annotations from inputs and outputs.
> 
> Also, is there a reason `third_party/pybind11` was bumped?
> 
> Merge conflicts in other files need to be resolved and we should check why CI isn't passing.

The `(differentiable)` flags look good to me now. If there's still something wrong, can you point out? I have also fixed the `pybind11` version and the conflicts
matteosal(2020-12-11 18:51:36):@prasanthpul @askhade sorry for leaving this unfinished for so long. I got back to it trying to finalize it. I'm sticking with a single `batch_major` attribute for all 3 ops, moved all the changes to opset 14 and added all the adapters.

The only thing left to do is to regenerate the test files, but `backend-test-tools generate-data` is failing with a couple of errors:
```
  File "/usr/lib/python3/dist-packages/pkg_resources/__init__.py", line 792, in resolve
    raise VersionConflict(dist, req).with_context(dependent_req)
pkg_resources.VersionConflict: (onnx 1.8.0 (/usr/local/lib/python3.8/dist-packages/onnx-1.8.0-py3.8-linux-x86_64.egg), Requirement.parse('onnx==1.7.0'))
```
```
  File "/usr/lib/python3/dist-packages/pkg_resources/__init__.py", line 787, in resolve
    raise DistributionNotFound(req, requirers)
pkg_resources.DistributionNotFound: The 'onnx==1.7.0' distribution was not found and is required by the application
```

Any help with this one?
matteosal(2021-01-15 19:54:37):Commit signoff here is very impractical due to many many past conflicts that should be fixed again in the rebase. I've reopened this PR here: https://github.com/onnx/onnx/pull/3217
wschin(2020-09-22 16:37:55):I am not sure if this makes sense to everyone --- maybe we can use more general flags `batch_axis` (type: int64) and `sequence_axis` (type: int64)? For current setting, `batch_axis=1` and `sequence_axis=0`.
spandantiwari(2020-09-22 16:49:58):Maybe I missed it, but is there a shape inference test added for the bidirectional case with the new flag? If not, it would be nice to add one.
spandantiwari(2020-09-22 16:53:03):nit: If the final name of the `batch_major` or `batch_axis` flag is changed, it may be helpful to update the name of this variable.
gramalingam(2020-09-23 19:47:26):Re. the more general flag: are there uses-cases (either models or systems) where batch-axis other than 0 or 1 is used? If not, it may be reasonable to leave it in the proposed form (covering the two scenarios that are used in practice, I assume). Allowing arbitrary batch-axis and sequence-axis is conceptually simple, but there may be implementation costs in specific scenarios. 
matteosal(2020-09-25 12:38:37):In the last operator SIG meeting it was decided to adopt the more general flag `batch_axis`, but restricting its values to 0|1 and leaving extensions for later if needed.

However, putting myself back to work on this, I've realized that `batch_axis = 1|0` is not descriptive of what `batch_major = 0|1` does, because the shapes look like this:
* `batch_major = 0` (old case):
`input_shape = [seq_length, batch_size, input_size]`
`output_shape = [seq_length, num_directions, batch_size, hidden_size]`
* `batch_major = 1` (new case):
`input_shape = [batch_size, seq_length, input_size]`
`output_shape = [batch_size, seq_length, num_directions, hidden_size]`

Hence `batch_major = 1` does correspond to `batch_axis = 0`, but `batch_major = 0` corresponds to `batch_axis = 1` for the input and `batch_axis = 2` for the output. So maybe it's better to go back to `batch_major = 0|1`? The current spec for the batch axis is not even consistent between input and output, so generalizing it would require two separate flags, which might be too much for something that doesn't even have a current use case...

What should I do?
matteosal(2020-09-25 13:03:24):I've added it
wschin(2020-09-25 16:53:39):Having two int flags (`input_batch_axis` and `output_batch_axis`) sound ok to me. It doesn't hurt and help maintaining this op's extensibility. What do you think?
postrational(2020-09-28 09:46:24):We need to consider if we're not making the op too flexible, which may put unnecessary burden on backend implementors.
matteosal(2020-09-28 17:31:34):So what should I do with this? How to take a decision?
matteosal(2020-10-08 09:38:28):Can this be moved forward? Is it ok to just go for `batch_major = 0|1`?
gramalingam(2020-10-08 18:45:21):My personal opinion: it seems ok to use batch_major = 0|1.
jcwchen(2020-07-24 21:07:58):```suggestion
If the ONNX checker reports issues with this model then this is most probably related to the converter used to convert the original framework model to ONNX. Please create this bug in the appropriate converter's GitHub repo (pytorch, tensorflow-onnx, sklearn-onnx, keras-onnx, onnxmltools) to get the best help.
```
ebarsoum(2020-07-29 17:52:14):Should you add the actual link for the other repo?
askhade(2020-07-29 22:26:16):I think it is OK to not add the links. These are all git repos + if the issue is related to model conversion it is likely that the OP already knows about this converter.
prasanthpul(2020-07-25 00:11:10):@ebarsoum Please edit the the initial comment of this PR to include links to the detailed notes of the discussion and decision
ebarsoum(2020-07-29 17:53:45):@postrational, @linkerzhang and @chinhuang007 any feedback?
ebarsoum(2020-08-04 16:09:50):@chinhuang007 any other feedback?
gramalingam(2020-12-03 20:08:06):UPDATE: After further discussions, the decision was to NOT pursue this relaxation. The main motivation here was the introduction of the bfloat16 type which impacted a number of ops. But it did not seem safe enough to justify this relaxation. Closing this PR at this point. 
gramalingam(2020-07-24 23:47:45):How about adding the following? _Earlier such changes were considered breaking changes requiring an increase in the version-number. However, in practice, the cost of increasing the version-number in this situation outweighs the benefits. This is particularly so since backends, in practice, rarely guarantee complete coverage of all types that exist in the standard._
askhade(2020-07-25 00:42:49):why bump up version for removing types? do we need to treat add and remove different?
ebarsoum(2020-07-25 00:44:29):Because removing type can break existing implementation.
chinhuang007(2020-07-30 00:52:44):I agree adding a new type to inputs or outputs does not need to be considered as breaking changes. However we should find a way to communicate this kind of changes with broader community such as developers for converters. Additionally, if data type specific units tests are added, some backends will potentially fail standard backend tests although the opset version stays the same. Some clarification would help.
ebarsoum(2020-07-30 18:02:19):@chinhuang007 the way that I think about it, it will be based on ONNX release version. For example adding new types to OpSet 12 wont affect ONNX 1.7 release, but it will be part of 1.8 release.
linkerzhang(2020-07-31 02:40:23):"adding new types to OpSet 12 wont affect ONNX 1.7 release"

An model with the added (new) type can't be run against framework/runtime which supports opset12, right?
ebarsoum(2020-07-31 06:06:54):@linkerzhang that is already the case, I am not aware of any framework that support all the types. 
jcwchen(2020-07-29 03:48:39):I wrote that test... If you have not started yet, I can propose a PR and correct that error. Thank you!
jcwchen(2020-08-01 00:21:40):BTW, @gramalingam and I saw there are some errors if we run `python setup.py typecheck` on our local Windows. We are not sure that whether these errors are caused by our local machine or onnx itself. 
Since this PR will catch typecheck errors properly in Windows-CI, we need to be aware of whether it happens in the CI machine.
jcwchen(2022-04-08 20:55:13):Close it since the issue has been solved by https://github.com/onnx/onnx/pull/2935
jcwchen(2020-07-29 03:47:02):Just for my curiosity: when do we need to catch the error from last command?
I don't understand why Azure Pipelines sometimes catches the failure normally but sometimes does not.
jcwchen(2020-09-03 22:16:37):Do we need to add the error catch here too?
jcwchen(2020-09-03 22:20:24):Please note that the test has been handled (skipped) so the error catch here is ready to go. Thank you!
askhade(2020-07-31 16:23:04):For memory error add a log statement stating --> because of mem limitation this test was not executed. 
jcwchen(2020-07-31 17:05:44):Good idea. Just added.
askhade(2020-07-31 19:22:43):need to add EXIT 1 otherwise it will continue and mark pipeline as green
askhade(2020-07-31 19:23:23):take a look at windows CI - it is printing this warning but still marks the pytest as failed
jcwchen(2020-08-01 00:24:12):Thank you for catching this. Just modified.
askhade(2020-08-03 17:05:01):In the latest windows CI run there is no warning message saying this test was skipped. Can you fix the CI.
It is OK to skip this test on Windows because of hardware limitation but we need to explicitly call it out. 

jcwchen(2020-08-03 18:02:56):By default `pytest` prints stdout only if the test fails. Even though adding `-s` to `pytest` can capture the stdout from tests, it will apply to all tests. Is it acceptable? 
askhade(2020-08-03 18:46:16):instead of catching mem error can you simply skip this test on windows something like this : https://docs.pytest.org/en/stable/skipping.html#skipping-test-functions and see if pytest can add skip tests in summary
jcwchen(2020-08-03 20:30:06):@askhade do you mean something like this?
```
@pytest.mark.skipif(os.name == 'nt', reason="Because of the hardware limitation (memory) in Windows-CI, this test was not executed.")
```
Done, but the linux failed. It seems that the old problem is happening again. https://github.com/onnx/onnx/pull/2933
askhade(2020-08-03 20:48:49):did you try this :  @pytest.mark.skipif(sys.platform.startswith("win"), reason="Because of the hardware limitation (memory) in Windows-CI, this test was not executed.")

askhade(2020-08-03 20:50:43):Linux failure is because of a type check error : 
onnx/test/test_external_data.py:288: error: Untyped decorator makes function "test_check_model_by_model" untyped

jcwchen(2020-08-04 01:53:05):Thanks for the catch. Just fixed. That test has been successfully skipped on Windows.
gramalingam(2020-08-03 21:30:03):Another clarification that may be useful is that the treatment is slightly different for "model inputs/outputs" (which are inputs/outputs of top-level graphs) and "nested graph inputs/outputs" (which are inputs/outputs of graphs that specify the loop-body or body of an if-then-else).
jcwchen(2020-08-04 01:34:02):Updated. @gramalingam @skottmckay Thank you for the revision!
jcwchen(2020-07-31 19:13:02):Specify:
input -> node input
output -> node output
gramalingam(2020-08-03 21:22:56):I suggest changing the "When the shape property is absent ..." part as below:
```
When the shape property is absent in the type of a value, it indicates that the corresponding runtime value
may have any shape. However, types with a missing shape are not permitted in all contexts. In particular,
the inputs and outputs of a model are required to *have* a shape, indicating the rank of inputs and outputs,
even though the exact dimensions need not be specified.
```
gramalingam(2020-08-03 21:27:57):I suggested a reworded version which merges this statement into the previous paragraph. If you want the first part of the above, I suggest using "Note that type and shape information for intermediate values in the graph, used as inputs and outputs of nodes, are optional but not compulsory as they can be inferred, but type and rank information is compulsory for inputs and outputs of the model.
skottmckay(2020-08-03 21:59:58):This table seems to be the general description of ValueInfo, which currently is directly after this statement
> Each main (top-level) graph MUST define the names and types of its inputs and outputs, which are specified as ‘value info’ structures, having the following properties:

ValueInfo is used for multiple things and not just graph inputs/outputs so it may be better to put a general description of ValueInfo first, and in that say 'Type' may include shape information.

Following that we could have the 'MUST' statement to describe the additional requirements on the main graph inputs and outputs. I'd maybe move Rama's suggestion ("In particular,
the inputs and outputs of a model are required to *have* a shape, indicating the rank of inputs and outputs,
even though the exact dimensions need not be specified.") into that. 

Something like
> Each main (top-level) graph MUST define the names, types and shapes of its inputs and outputs, which are specified as ‘value info’ structures. The main graph inputs and outputs are required to *have* a shape, indicating the rank, even though the exact dimensions need not be specified.

It may be helpful to be more specific about what 'not specified' could equate do. Unfortunately we don't talk about dim_value vs dim_param vs unknown until much later in the document under 'Static tensor shapes'. 
skottmckay(2020-08-06 07:15:33):Being a little pedantic here as it's a spec. Can we group things together slightly better?

Specifically line 188 has the 'MUST' for a graph, then we have the ValueInfo structure (although nothing explicitly says it's the ValueInfo structure), then we have some details about nested subgraphs and other graph requirements. 

Would something like the following be slightly better organized?

'Graphs have the following properties:'
\<table with GraphProto fields>

'ValueInfo has the following properties:'
\<table with ValueInfo fields>

\<Various other requirements>
Each main (top-level) graph MUST define...
Nested subgraphs (specified as attribute values) MUST define...
Each graph MUST specify ...
etc. etc.
jcwchen(2020-08-06 21:50:31):Good idea. This flow looks clearer to me. Just moved the table.
skottmckay(2020-08-07 05:58:09):We probably don't need the info on top level graph inputs/outputs requiring a shape here as it's specified earlier. 

Are there any other known contexts where a type with a missing shape is not permitted? If not should we drop the the 'However, types with a missing shape are not permitted in all contexts.' as well? @gramalingam are you aware of other scenarios where a shape is required? 
askhade(2020-08-07 16:23:55):these 2 sentences seem redundant:
When the shape property is absent for a node input, a tensor value of any shape may be passed from the caller. When the shape property is absent in the type of a value, it indicates that the corresponding runtime value
may have any shape.

Can these 2 be combined : 
When the shape property is absent in the type of a value (including node input), it indicates that the corresponding runtime value may have any shape.


askhade(2020-08-07 17:20:43):@skottmckay , @gramalingam : Regarding the type info - it is clear from this statement that the graph inputs and outputs must have types but what about the node inputs? Are they required to have type info? 
This will determine how we handle the case of absent type info more specifically when contxt->getInputType(<input index>) is nullptr. If all inputs (graph and intermediate node inputs) require type info then the node level type inference can simply fail when context->getInputType(<input index>) is a nullptr otherwise the node level type inference should stop and return when type info is not available. 
gramalingam(2020-08-07 18:07:00):For node inputs (that are not graph inputs), the type information is optional. The model does not *need* to specify it. But in a "correct model", type-inference will be able to infer all these types automatically. (That is to answer your question. In terms of the document: I believe this is mentioned in the document, at least it was at some point in this PR; I haven't checked the latest version.)
gramalingam(2020-08-07 18:34:52):I sort of agree with the idea underlying Scott's point above. To expand on this: this section is meant to explain how to interpret a shape (or a missing-shape) in a type. This is an explanation of the "class TypeProto". It is not meant to be a statement/descriptions of (further) constraints that may be imposed on the use of a TypeProto in particular contexts ... those belong to a description of those contexts.

On the other hand, if you look at the two issues Jacky mentions as motivation, I feel some redundancy may be okay and it may be okay to repeat some information here to avoid some misunderstanding/confusion. 

What if we rephrase part of the above as below
```
This sub-section describes how to interpret a missing-shape or a shape with missing dimensions etc.
However, specific usage contexts may impose further constraints on a type and shape.
For example, the inputs and outputs of a model (top-level graph) are required to *have* a shape, ... ,
as described in section ...
```
jcwchen(2020-08-07 18:50:28):I think it is just mentioned in the next sentence?
> Nested subgraphs (specified as attribute values) MUST define the names of its inputs and outputs and MAY define the types of its inputs and outputs.

Do we need to further indicate the input/output of Nested subgraphs as the node input/output here?

Another section which is talking about this:
>  When the shape property is absent for a node input, a tensor value of any shape may be passed from the caller. When the shape property is absent in the type of a value, it indicates that the corresponding runtime value may have any shape. However, types with a missing shape are not permitted in all contexts. In particular, the inputs and outputs of a model (top-level graphs) are required to have a shape, indicating the rank of inputs and outputs, even though the exact dimensions need not be specified.

jcwchen(2020-08-07 22:00:09):Just modified. @gramalingam can you check again whether I did it the right way?
Thanks!
gramalingam(2020-08-07 22:26:06):Thanks @jcwchen. Can we add Ashwini's suggestion to the end (those two lines seem to have gotten deleted, without a replacement): namely,
```
When the shape property is absent in the type of a value (including node input),
it indicates that the corresponding runtime value may have any shape.
```
jcwchen(2020-08-08 23:16:36):Just added. Thanks for the reminder.
jcwchen(2020-08-04 01:50:59)::shipit: 
lgtm-com[bot](2020-08-05 15:54:20):LGTM pull request analysis was skipped for 14ad00152edd09163705fe8d27c0e1983cf9704e by [jcwchen](https://github.com/jcwchen). Analysis of future commits will happen as normal.
lgtm-com[bot](2020-08-05 16:13:31):LGTM pull request analysis was skipped for f406675bd1804a150172fe8e772ea9ba632f64d7 by [jcwchen](https://github.com/jcwchen). Analysis of future commits will happen as normal.
CLAassistant(2020-08-05 16:19:42):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=2946) <br/>All committers have signed the CLA.
gramalingam(2020-08-06 00:17:35):(a) I think we can just use
```cpp
   if (hasInputShape(ctx, 0))
```
which will be more consistent with the style elsewhere.

(b) Is there some case where this is failing? I am a bit concerned that the real issue might be somewhere else. This type is expected to exist (have a non-null value) at this point. If the model does not have an input here, I would have expected the earlier checker to have caught that the node is missing an input. I suppose it might be possible to have some cascading error, if the type was not inferred due to some earlier error.
shubhambhokare1(2020-08-06 03:40:54):I have changed the code such that it is similar to one the you mentioned in (a)
For (b), The case in which this is failing is if the input to the ConstantofShape node in the onnx model looks like:
  input {
      name: "9"
  }
In this particular case, the check previously tried to access type or even tensor_type which don't have any values assigned to them
jcwchen(2020-08-10 22:24:05):Duplicated to https://github.com/onnx/onnx/pull/2604.
askhade(2020-08-07 16:06:28):both changelog.md and operator.md needs to be updated. Please use python onnx/defs/gen_doc.py script to generate these files. They should not be manually updated. 
daquexian(2020-08-11 13:35:38):@HectorSVC I'm the main author of the resize op. I'm planning to submit a PR to update the resize op version, as I found the "tf_half_pixel_for_nn" coordinate_transformation_mode is indeed unnecessary and misleading. I think it is better to also make scales as an optional input in my PR. What's your opinion? Thanks!
HectorSVC(2020-08-11 17:41:13):@daquexian Make scales as an optional input is definitely the best solution. I didn't do it because I don't want to bump up the opset version. Since you need to bump up the version any way for your coming PR, I think you can do the changes for scales together. Thanks!
askhade(2020-08-07 16:10:11):Can this -> "If 'size' is needed, the user can use a nonempty string as the name of 'scales' but set it to empty data (zero shape) in this operator's input list."
be changed to ->
"If 'size' is specified, then set scales to empty data (zero shape) in this operator's input list."

Basically scales is not an optional input, adding empty data will imply that input name will be non-empty. 
BowenBao(2020-08-12 21:11:28):LG, please sync with master branch.
BowenBao(2020-08-11 21:34:33):should this part be updated to check with `(nullptr == xTensorProto) || !xTensorProto->tensor_type().has_shape()`?
askhade(2020-08-11 23:00:37):it's not required in this case. This will be checked as part of the type inference.
see line 2849 ->propagateElemTypeFromInputToOutput(ctx, 0, 0);

gramalingam(2020-08-12 22:59:10):Replace "ctx.getInputType(i)" by input_type?
wschin(2020-10-12 21:03:22):> revert changes for Upsample as it is a deprecated op

Done.
askhade(2020-10-12 21:24:50):Since the ops which are being updated in this PR has changed please update the PR description to reflect the same
wschin(2020-10-12 21:31:24):> Since the ops which are being updated in this PR has changed please update the PR description to reflect the same

Sure. Done.
wschin(2020-10-12 20:03:02):Revert changes for 'Upsample`. It is already deprecated.
askhade(2020-10-12 23:41:16):I dont see the Squeeze changes reflected in the Operators and Changelog files
wschin(2020-10-13 00:23:39):Please see line 20508 in Operators.md.
CLAassistant(2020-08-12 00:52:44):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=2955) <br/>All committers have signed the CLA.
gramalingam(2020-08-27 21:08:23):It would be helpful if someone familiar with the optimizer and convertor could also review the PR, as it has updates to those too.
gramalingam(2020-09-03 23:45:18):Can you fix the CI errors? Thanks!
askhade(2020-09-04 16:37:55):Looks good to me. Please address the CI failures and make sure to regenerate the test data before checking in.
daquexian(2020-09-19 01:54:47):Should we also update other ops (ReduceMax/Min/...) in the ReduceX family (they all have an 'axes' attribute) so that they can be consistent with ReduceSum?
gramalingam(2020-08-21 04:02:33):Check if the axes attribute is specified and produce an error-message in that case?
gramalingam(2020-08-21 04:11:48):It would be safer to use the condition
```cpp
if ( (num_inputs == 2) && ctx.getInputType(1))
```
to handle the case where the second input is specified using an empty string.
gramalingam(2020-08-21 04:15:09):Same comment as before, about adding the condition "ctx.getInputType(1)".
gramalingam(2020-08-24 16:27:01):(In the general case, a node may have a missing optional input followed by another input that is present; so, it is insufficient to check the number of inputs to figure out if an optional input is present or not. For the special case of the last input, as in this case, the above may work, but we would need to test it in the case where the second input is specified using an empty string. So, I think it is safer to add the above extra condition.)
ashbhandare(2020-08-27 20:13:28):changed
linkerzhang(2020-09-07 01:34:16):Version bump is needed.
askhade(2020-09-08 17:23:59):@linkerzhang : The version for this op is already bumped to 13 as part of PR https://github.com/onnx/onnx/pull/2770
jcwchen(2020-08-27 20:03:24):Reopen to Rerun CI
jcwchen(2020-08-27 23:43:29):Reopen again. The model urls are sometimes invalid.
jcwchen(2020-10-09 15:19:13):The CIs in ONNX are not comprehensive enough and I afraid it is still broken in some platforms...
@snnn Do you think this PR can solve this issue https://github.com/onnx/onnx/issues/1314? Thank you
jcwchen(2020-10-14 03:36:34)::shipit: 
snnn(2020-10-14 18:45:40):I think a quick fix would be generating tools/protoc-gen-mypy.py on the fly and putting the full path of PYTHON_EXECUTABLE after the shabang. 
jcwchen(2020-10-14 23:13:27):> I think a quick fix would be generating tools/protoc-gen-mypy.py on the fly and putting the full path of PYTHON_EXECUTABLE after the shabang.

Thank you @snnn for the helpful suggestion. I have got PYTHON_EXECUTABLE in the script and recovered all shabangs.
Please help me to review this. Thanks!
jcwchen(2020-08-27 17:23:12):I don't know why it set `NOT WIN32` here. It seems that the mypy check has never been used on Windows before.
askhade(2020-09-14 17:05:03):Not sure I understand this. In windows CI this check is run... 
jcwchen(2020-10-09 15:22:59):Originally ONNX only uses `find_package(PythonInterp)` and `find_package(PythonLibs)` to find python library on Windows... Move here can help finding the right Python on Linux, but I am not sure whether this will break something in some platforms.
snnn(2020-10-14 10:09:53):But it is still python not python3?
You should always use the python executable which was used for running setup.py to run this script. 
In the cases of setup.py was not used, the path of the  python executable should be got from the find_package calls.
snnn(2020-10-14 10:11:29):I think you don't need to remove these lines. They're harmless. 
jcwchen(2020-10-14 13:23:36):Thank you @snnn for bringing this up.
Yes, I thought of this, but originally the interface of using `protoc-gen-mypy` here is fixed. The only way is to call a script for `protoc-gen-mypy` so I don't know how to use `PYTHON_EXECUTABLE` from CMake for this (Could we call a CMake environment variable from an outside script?)

Otherwise, since onnx won't support python2 since ONNX 1.8, could we just use python3 here?

jcwchen(2020-10-14 14:31:57):If we are using `PYTHON_EXECUTABLE` to call them, these shebangs should be fine. But if not, they still will be troubles, right? Why don't we just remove them to prevent future mistakes?
snnn(2020-10-15 20:24:31):See https://cmake.org/cmake/help/v3.19/command/configure_file.html
jcwchen(2020-10-15 21:49:27):Thank you for this. Updated. It is more elegant now.
snnn(2020-10-19 22:12:33):Should they be required only when BUILD_ONNX_PYTHON  is ON?
jcwchen(2020-10-20 05:14:19):Make sense. Added a condition for this. Thanks.
gramalingam(2020-08-19 18:59:32):Maybe add/change as follows?
```
An execution of the training step is performed by executing the graph obtained by combining
the inference graph (namely "ModelProto.graph") and the algorithm graph. That is, the actual ...
```
gramalingam(2020-08-19 19:03:23):Add "in that order. This combined graph must satisfy the normal ONNX conditions. For example, an input of a node in the algorithm graph may reference the output of a node in the inference graph (but not the other way round)."
wschin(2020-08-25 16:20:47):Ok. Thanks.
wschin(2020-08-25 17:41:56):```suggestion
  // and the "algorithm" graph. That is, the actual
```
wschin(2020-08-25 18:03:26):Maybe add some more comments.

Before this PR:
  Inference graph in `ModelProto.graph`:
    ModelProto.grpah
      initializer_w -> MatMul1 -> tensor_x_inf
  Algorithm graph in `TrainingInfoProto.algorithm`:
    ModelProto.training_info[i].algorithm
      initializer_w -> GraphCall(MatMul1) -> tensor_x -> MatMul2 -> tensor_y -> MatMul3 -> new_initializer_w
    ModelProto.training_info[i].update_binding
      "initializer_w", "new_initializer_w"

After this PR:
  Inference graph in `ModelProto.graph`:
    ModelProto.grpah
      initializer_w -> MatMul1 -> tensor_x

  Algorithm graph in `TrainingInfoProto.algorithm`:
    ModelProto.training_info[i].algorithm
      tensor_x -> MatMul2 -> tensor_y -> MatMul3 -> new_initializer_w
    ModelProto.training_info[i].update_binding
      "initializer_w", "new_initializer_w"

That is, we semantically divide the full training graph into inference graph and algorithm graph.
 GraphProto.input = concat(ModelProto.input, algorithm.input)
 GraphProto.node = concat(ModelProto.node, algorithm.node)
 GraphProto.output = concat(ModelProto.output, algorithm.output)

advancedwebdeveloper(2020-08-29 02:57:54):There is nothing for Clang, on Windows.

I tried to compile ONNX and caught [https://github.com/pybind/pybind11/pull/1883](https://github.com/pybind/pybind11/pull/1883) .

That's CMakeFiles/onnx_cpp2py_export.dir/onnx/cpp2py_export.cc.obj build target (for Ninja, in my case):

[1/2] Building CXX object CMakeFiles/onnx_cpp2py_export.dir/onnx/cpp2py_export.cc.obj
FAILED: CMakeFiles/onnx_cpp2py_export.dir/onnx/cpp2py_export.cc.obj
C:\PROGRA~1\LLVM\bin\CLANG_~1.EXE -DONNXIFI_ENABLE_EXT=ON -DONNX_ML=1 -DONNX_NAMESPACE=onnx -DONNX_USE_LITE_PROTO=1 -Donnx_cpp2py_export_EXPORTS -I"C:\Program Files\Python38-32\include" -IC:\Users\clang\build_onnx\pybind11\include -IC:/Users/Public/Downloads/onnx-master -I. -isystem protoc/include -Wnon-virtual-dtor -O3 -DNDEBUG -D_DLL -D_MT -Xclang --dependent-lib=msvcrt -fvisibility=hidden -std=c++1y -MD -MT CMakeFiles/onnx_cpp2py_export.dir/onnx/cpp2py_export.cc.obj -MF CMakeFiles\onnx_cpp2py_export.dir\onnx\cpp2py_export.cc.obj.d -o CMakeFiles/onnx_cpp2py_export.dir/onnx/cpp2py_export.cc.obj -c C:/Users/Public/Downloads/onnx-master/onnx/cpp2py_export.cc
In file included from C:/Users/Public/Downloads/onnx-master/onnx/cpp2py_export.cc:7:
In file included from C:/Users/Public/Downloads/onnx-master\onnx/checker.h:7:
In file included from C:/Users/Public/Downloads/onnx-master\onnx/defs/schema.h:24:
In file included from C:/Users/Public/Downloads/onnx-master\onnx/defs/shape_inference.h:4:
C:/Users/Public/Downloads/onnx-master\onnx/proto_utils.h:34:16: warning: 'SetTotalBytesLimit' is deprecated: Please use the single parameter version of SetTotalBytesLimit(). The second parameter is ignored. [-Wdeprecated-declarations]
  coded_stream.SetTotalBytesLimit((2048LL << 20) - 1, 512LL << 20);
               ^
C:/Users/Public/Downloads/onnx-master/onnx/cpp2py_export.cc:69:9: note: in instantiation of function template specialization 'onnx::ParseProtoFromPyBytes<onnx::NodeProto>' requested here
        ParseProtoFromPyBytes(&proto, bytes);
        ^
protoc/include\google/protobuf/io/coded_stream.h:394:3: note: 'SetTotalBytesLimit' has been explicitly marked deprecated here
  PROTOBUF_DEPRECATED_MSG(
  ^
protoc/include\google/protobuf/port_def.inc:156:53: note: expanded from macro 'PROTOBUF_DEPRECATED_MSG'
#define PROTOBUF_DEPRECATED_MSG(msg) __attribute__((deprecated(msg)))
                                                    ^
In file included from C:/Users/Public/Downloads/onnx-master/onnx/cpp2py_export.cc:7:
In file included from C:/Users/Public/Downloads/onnx-master\onnx/checker.h:7:
In file included from C:/Users/Public/Downloads/onnx-master\onnx/defs/schema.h:24:
In file included from C:/Users/Public/Downloads/onnx-master\onnx/defs/shape_inference.h:4:
C:/Users/Public/Downloads/onnx-master\onnx/proto_utils.h:34:16: warning: 'SetTotalBytesLimit' is deprecated: Please use the single parameter version of SetTotalBytesLimit(). The second parameter is ignored. [-Wdeprecated-declarations]
  coded_stream.SetTotalBytesLimit((2048LL << 20) - 1, 512LL << 20);
               ^
C:/Users/Public/Downloads/onnx-master\onnx/py_utils.h:16:10: note: in instantiation of function template specialization 'onnx::ParseProtoFromBytes<onnx::NodeProto>' requested here
  return ParseProtoFromBytes(proto, buffer, length);
         ^
C:/Users/Public/Downloads/onnx-master/onnx/cpp2py_export.cc:69:9: note: in instantiation of function template specialization 'onnx::ParseProtoFromPyBytes<onnx::NodeProto>' requested here
        ParseProtoFromPyBytes(&proto, bytes);
        ^
protoc/include\google/protobuf/io/coded_stream.h:394:3: note: 'SetTotalBytesLimit' has been explicitly marked deprecated here
  PROTOBUF_DEPRECATED_MSG(
  ^
protoc/include\google/protobuf/port_def.inc:156:53: note: expanded from macro 'PROTOBUF_DEPRECATED_MSG'
#define PROTOBUF_DEPRECATED_MSG(msg) __attribute__((deprecated(msg)))
                                                    ^
In file included from C:/Users/Public/Downloads/onnx-master/onnx/cpp2py_export.cc:7:
In file included from C:/Users/Public/Downloads/onnx-master\onnx/checker.h:7:
In file included from C:/Users/Public/Downloads/onnx-master\onnx/defs/schema.h:24:
In file included from C:/Users/Public/Downloads/onnx-master\onnx/defs/shape_inference.h:4:
C:/Users/Public/Downloads/onnx-master\onnx/proto_utils.h:34:16: warning: 'SetTotalBytesLimit' is deprecated: Please use the single parameter version of SetTotalBytesLimit(). The second parameter is ignored. [-Wdeprecated-declarations]
  coded_stream.SetTotalBytesLimit((2048LL << 20) - 1, 512LL << 20);
               ^
C:/Users/Public/Downloads/onnx-master\onnx/py_utils.h:16:10: note: in instantiation of function template specialization 'onnx::ParseProtoFromBytes<onnx::ValueInfoProto>' requested here
  return ParseProtoFromBytes(proto, buffer, length);
         ^
C:/Users/Public/Downloads/onnx-master/onnx/cpp2py_export.cc:216:9: note: in instantiation of function template specialization 'onnx::ParseProtoFromPyBytes<onnx::ValueInfoProto>' requested here
        ParseProtoFromPyBytes(&proto, bytes);
        ^
protoc/include\google/protobuf/io/coded_stream.h:394:3: note: 'SetTotalBytesLimit' has been explicitly marked deprecated here
  PROTOBUF_DEPRECATED_MSG(
  ^
protoc/include\google/protobuf/port_def.inc:156:53: note: expanded from macro 'PROTOBUF_DEPRECATED_MSG'
#define PROTOBUF_DEPRECATED_MSG(msg) __attribute__((deprecated(msg)))
                                                    ^
In file included from C:/Users/Public/Downloads/onnx-master/onnx/cpp2py_export.cc:7:
In file included from C:/Users/Public/Downloads/onnx-master\onnx/checker.h:7:
In file included from C:/Users/Public/Downloads/onnx-master\onnx/defs/schema.h:24:
In file included from C:/Users/Public/Downloads/onnx-master\onnx/defs/shape_inference.h:4:
C:/Users/Public/Downloads/onnx-master\onnx/proto_utils.h:34:16: warning: 'SetTotalBytesLimit' is deprecated: Please use the single parameter version of SetTotalBytesLimit(). The second parameter is ignored. [-Wdeprecated-declarations]
  coded_stream.SetTotalBytesLimit((2048LL << 20) - 1, 512LL << 20);
               ^
C:/Users/Public/Downloads/onnx-master\onnx/py_utils.h:16:10: note: in instantiation of function template specialization 'onnx::ParseProtoFromBytes<onnx::TensorProto>' requested here
  return ParseProtoFromBytes(proto, buffer, length);
         ^
C:/Users/Public/Downloads/onnx-master/onnx/cpp2py_export.cc:224:9: note: in instantiation of function template specialization 'onnx::ParseProtoFromPyBytes<onnx::TensorProto>' requested here
        ParseProtoFromPyBytes(&proto, bytes);
        ^
protoc/include\google/protobuf/io/coded_stream.h:394:3: note: 'SetTotalBytesLimit' has been explicitly marked deprecated here
  PROTOBUF_DEPRECATED_MSG(
  ^
protoc/include\google/protobuf/port_def.inc:156:53: note: expanded from macro 'PROTOBUF_DEPRECATED_MSG'
#define PROTOBUF_DEPRECATED_MSG(msg) __attribute__((deprecated(msg)))
                                                    ^
In file included from C:/Users/Public/Downloads/onnx-master/onnx/cpp2py_export.cc:7:
In file included from C:/Users/Public/Downloads/onnx-master\onnx/checker.h:7:
In file included from C:/Users/Public/Downloads/onnx-master\onnx/defs/schema.h:24:
In file included from C:/Users/Public/Downloads/onnx-master\onnx/defs/shape_inference.h:4:
C:/Users/Public/Downloads/onnx-master\onnx/proto_utils.h:34:16: warning: 'SetTotalBytesLimit' is deprecated: Please use the single parameter version of SetTotalBytesLimit(). The second parameter is ignored. [-Wdeprecated-declarations]
  coded_stream.SetTotalBytesLimit((2048LL << 20) - 1, 512LL << 20);
               ^
C:/Users/Public/Downloads/onnx-master\onnx/py_utils.h:16:10: note: in instantiation of function template specialization 'onnx::ParseProtoFromBytes<onnx::SparseTensorProto>' requested here
  return ParseProtoFromBytes(proto, buffer, length);
         ^
C:/Users/Public/Downloads/onnx-master/onnx/cpp2py_export.cc:232:9: note: in instantiation of function template specialization 'onnx::ParseProtoFromPyBytes<onnx::SparseTensorProto>' requested here
        ParseProtoFromPyBytes(&proto, bytes);
        ^
protoc/include\google/protobuf/io/coded_stream.h:394:3: note: 'SetTotalBytesLimit' has been explicitly marked deprecated here
  PROTOBUF_DEPRECATED_MSG(
  ^
protoc/include\google/protobuf/port_def.inc:156:53: note: expanded from macro 'PROTOBUF_DEPRECATED_MSG'
#define PROTOBUF_DEPRECATED_MSG(msg) __attribute__((deprecated(msg)))
                                                    ^
In file included from C:/Users/Public/Downloads/onnx-master/onnx/cpp2py_export.cc:7:
In file included from C:/Users/Public/Downloads/onnx-master\onnx/checker.h:7:
In file included from C:/Users/Public/Downloads/onnx-master\onnx/defs/schema.h:24:
In file included from C:/Users/Public/Downloads/onnx-master\onnx/defs/shape_inference.h:4:
C:/Users/Public/Downloads/onnx-master\onnx/proto_utils.h:34:16: warning: 'SetTotalBytesLimit' is deprecated: Please use the single parameter version of SetTotalBytesLimit(). The second parameter is ignored. [-Wdeprecated-declarations]
  coded_stream.SetTotalBytesLimit((2048LL << 20) - 1, 512LL << 20);
               ^
C:/Users/Public/Downloads/onnx-master\onnx/py_utils.h:16:10: note: in instantiation of function template specialization 'onnx::ParseProtoFromBytes<onnx::AttributeProto>' requested here
  return ParseProtoFromBytes(proto, buffer, length);
         ^
C:/Users/Public/Downloads/onnx-master/onnx/cpp2py_export.cc:240:9: note: in instantiation of function template specialization 'onnx::ParseProtoFromPyBytes<onnx::AttributeProto>' requested here
        ParseProtoFromPyBytes(&proto, bytes);
        ^
protoc/include\google/protobuf/io/coded_stream.h:394:3: note: 'SetTotalBytesLimit' has been explicitly marked deprecated here
  PROTOBUF_DEPRECATED_MSG(
  ^
protoc/include\google/protobuf/port_def.inc:156:53: note: expanded from macro 'PROTOBUF_DEPRECATED_MSG'
#define PROTOBUF_DEPRECATED_MSG(msg) __attribute__((deprecated(msg)))
                                                    ^
In file included from C:/Users/Public/Downloads/onnx-master/onnx/cpp2py_export.cc:7:
In file included from C:/Users/Public/Downloads/onnx-master\onnx/checker.h:7:
In file included from C:/Users/Public/Downloads/onnx-master\onnx/defs/schema.h:24:
In file included from C:/Users/Public/Downloads/onnx-master\onnx/defs/shape_inference.h:4:
C:/Users/Public/Downloads/onnx-master\onnx/proto_utils.h:34:16: warning: 'SetTotalBytesLimit' is deprecated: Please use the single parameter version of SetTotalBytesLimit(). The second parameter is ignored. [-Wdeprecated-declarations]
  coded_stream.SetTotalBytesLimit((2048LL << 20) - 1, 512LL << 20);
               ^
C:/Users/Public/Downloads/onnx-master\onnx/py_utils.h:16:10: note: in instantiation of function template specialization 'onnx::ParseProtoFromBytes<onnx::GraphProto>' requested here
  return ParseProtoFromBytes(proto, buffer, length);
         ^
C:/Users/Public/Downloads/onnx-master/onnx/cpp2py_export.cc:257:9: note: in instantiation of function template specialization 'onnx::ParseProtoFromPyBytes<onnx::GraphProto>' requested here
        ParseProtoFromPyBytes(&proto, bytes);
        ^
protoc/include\google/protobuf/io/coded_stream.h:394:3: note: 'SetTotalBytesLimit' has been explicitly marked deprecated here
  PROTOBUF_DEPRECATED_MSG(
  ^
protoc/include\google/protobuf/port_def.inc:156:53: note: expanded from macro 'PROTOBUF_DEPRECATED_MSG'
#define PROTOBUF_DEPRECATED_MSG(msg) __attribute__((deprecated(msg)))
                                                    ^
In file included from C:/Users/Public/Downloads/onnx-master/onnx/cpp2py_export.cc:7:
In file included from C:/Users/Public/Downloads/onnx-master\onnx/checker.h:7:
In file included from C:/Users/Public/Downloads/onnx-master\onnx/defs/schema.h:24:
In file included from C:/Users/Public/Downloads/onnx-master\onnx/defs/shape_inference.h:4:
C:/Users/Public/Downloads/onnx-master\onnx/proto_utils.h:34:16: warning: 'SetTotalBytesLimit' is deprecated: Please use the single parameter version of SetTotalBytesLimit(). The second parameter is ignored. [-Wdeprecated-declarations]
  coded_stream.SetTotalBytesLimit((2048LL << 20) - 1, 512LL << 20);
               ^
C:/Users/Public/Downloads/onnx-master\onnx/py_utils.h:16:10: note: in instantiation of function template specialization 'onnx::ParseProtoFromBytes<onnx::ModelProto>' requested here
  return ParseProtoFromBytes(proto, buffer, length);
         ^
C:/Users/Public/Downloads/onnx-master/onnx/cpp2py_export.cc:264:5: note: in instantiation of function template specialization 'onnx::ParseProtoFromPyBytes<onnx::ModelProto>' requested here
    ParseProtoFromPyBytes(&proto, bytes);
    ^
protoc/include\google/protobuf/io/coded_stream.h:394:3: note: 'SetTotalBytesLimit' has been explicitly marked deprecated here
  PROTOBUF_DEPRECATED_MSG(
  ^
protoc/include\google/protobuf/port_def.inc:156:53: note: expanded from macro 'PROTOBUF_DEPRECATED_MSG'
#define PROTOBUF_DEPRECATED_MSG(msg) __attribute__((deprecated(msg)))
                                                    ^
In file included from C:/Users/Public/Downloads/onnx-master/onnx/cpp2py_export.cc:1:
C:\Users\clang\build_onnx\pybind11\include\**pybind11/pybind11.h:1878:57: error: cannot compile this forwarded non-trivially copyable parameter yet**
    register_exception_translator([](std::exception_ptr p) {
                                                        ^
jcwchen(2020-08-29 04:08:55):@advancedwebdeveloper For building onnx on Windows, I suggest using `Visual Studio 16 19` as cmake generator because it is fully supported. Thanks.
jcwchen(2020-10-02 23:07:49):Two updates:
- Remove quotes for the environment setting to prevent failure on some Windows
- Mention `pip uinstall onnx` before building from source

Another TODO work item is to make sure the following setting is default:
`set CMAKE_ARGS=-DONNX_USE_PROTOBUF_SHARED_LIBS=OFF -DProtobuf_USE_STATIC_LIBS=ON`
Currently either build with shared protobuf or build with static protobuf needs to set `CMAKE_ARGS`. The default flags are somehow not working.

Thank @yuslepukhin for sharing the build experience.
CLAassistant(2020-10-08 15:14:54):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=2979) <br/>All committers have signed the CLA.
askhade(2021-01-13 18:21:34):LGTM 
suggestion: we can add a script for building onnx which sets all env variables etc... something for later
jcwchen(2020-08-27 22:08:10):I can build onnx on Windows with the latest protobuf so I updated it. Please comment if you have any concern about this. Thanks.
vinitra-zz(2020-08-27 22:19:58):```suggestion
The instructions in this README assume you are using Visual Studio to build ONNX on Windows. It is recommended that you run all the commands from a shell started from "Developer Command Prompt for VS 2019" and keep the build system generator for cmake consistent.
```

A little unclear what you mean by the "build system generator for cmake"
jcwchen(2020-08-27 22:27:36):Good catch. I will make an example. Thanks!
wschin(2020-08-28 19:30:22):Is this working with Windows Anaconda?
jcwchen(2020-08-29 03:23:42):cc @vinitra for testing with Windows Anaconda. Thanks!
jcwchen(2020-09-14 21:37:39):@wschin I just tested it. Even Protobuf 3.13 (shared library) is working with Windows conda.
jcwchen(2020-10-06 05:02:54):Defaults for `-DProtobuf_USE_STATIC_LIBS` and `-DONNX_USE_PROTOBUF_SHARED_LIBS` are not working... Thus the setting here is required.
askhade(2020-11-11 22:29:15):please use 3.11.3 (as this is the one being used in the current release package)
askhade(2020-11-11 22:31:33):if you are adding an example below then I suggest changing this to 
 cmake -G <generator> -A <arch> -Dprotobuf_MSVC_STATIC_RUNTIME=OFF -Dprotobuf_BUILD_TESTS=OFF -Dprotobuf_BUILD_EXAMPLES=OFF -DCMAKE_INSTALL_PREFIX=<protobuf_install_dir> 
askhade(2020-11-11 22:33:04):I think \include and \libs also need to be added to the path
jcwchen(2020-11-12 04:23:44):Only including the \bin path which has `protoc.exe` works in my local Windows... Are \include and \libs needed for building ONNX with static protobuf in GitHub Action CI?
askhade(2021-01-13 18:20:31):yes they are needed. Please add them
jcwchen(2021-01-13 21:24:01):Added. Thanks 
jcwchen(2020-08-28 18:27:07):The fix of resize, initializer and tests looks good to me. Thank you for proposing this @gramalingam
askhade(2020-08-28 16:34:04):I think this check -> tp.dims_size() !=0 is needed before output tensor shape is initialized?
This can cause issues because has_shape for this tensor will return true from here onwards and depending what the calling code (whether it does necessary checks or not ) it can cause segfaults

See PR : https://github.com/onnx/onnx/pull/1951
gramalingam(2020-08-28 16:37:46):The dims_size check here is not related to whether the shape is known or not known. Here, the shape is known exactly. If dims_size is 0, it means it is rank 0 (a scalar). This check is different from a "has_shape" kind of check.
jcwchen(2020-08-28 17:21:52):I added `tp.dims_size() != 0` because 3 tests in `shape_inference_test` will fail without this. My thought was same as @askhade that this can check whether the shape is initialized.
If `dims_size=0` means a scalar, there might be other potential bugs in `splitToSequence` because after applying this PR, the ranks of initializer and input are different: `RuntimeError: Inferred shape and existing shape differ in rank: (0) vs (1)
`.
gramalingam(2020-08-28 17:26:59):It is possible that there are bugs elsewhere (in requiring rank 0 or rank 1 for supposedly-scalar inputs). Or, if the spec calls for rank 1, and the model uses rank 0, it is an error in the model. Adding the check here will only mask problems elsewhere, I think.
gramalingam(2020-08-28 17:58:19):The split_to_sequence testcases had an issue. I fixed them. Hope this goes through the CI.
CLAassistant(2020-09-03 09:44:59):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=2994) <br/>All committers have signed the CLA.
zhenhuaw-me(2020-09-08 06:40:06):@linkerzhang @houseroad Would you mind to take a look at this PR if you have time? I am not sure where ONNX repo may accept such utils.
gramalingam(2020-09-23 18:29:10):Looks useful to me. A quick question/comment: I think this does not handle control-flow ops (like loop/if-then-else) correctly, since the nested scopes (sub-graphs) in those ops may contain references to other tensors from the outer scope (outer graph). If that is not fixed, it may be worth documenting it in the description as a limitation.
zhenhuaw-me(2020-09-24 10:44:43):@gramalingam Thanks for your comment! Yes, I didn't try on control-flow ops, but maybe it's insignificant:
* If the _boundary_ (defined by input and output tensors) is not cutting the subgraph, control-flow ops are just like other ops.
* If the _boundary_ is cutting the subgraph, it would be a pretty tricky scenario.  But yes, this is not supported.

I will document it clear later. Thanks for your advice!
zhenhuaw-me(2020-09-25 07:28:28):The doc issue mentioned in https://github.com/onnx/onnx/pull/2994#issuecomment-697845963 has been resolved.

Would you mind to take a look at this if you have time? @gramalingam @linkerzhang @houseroad 
zhenhuaw-me(2020-10-07 15:58:07):@askhade @gramalingam Thank you for your careful review, I have updated accordingly, please help to have a look. :)

_Sorry for the delayed response, I was on vacation._
zhenhuaw-me(2020-10-08 03:21:42):@askhade @gramalingam I accidentally clicked _Resolve conversation_ button before *Comment* button, and lost some comments in the previous update. I have added the important ones back. If any code or comment confuses you, please raise.
askhade(2020-10-08 05:15:04):Apart from the 1 open comment I have regarding input names check rest of the PR looks good to me. Once you clarify fix this I will approve the PR.

Thanks for your contribution!
zhenhuaw-me(2020-10-08 10:07:01):> Apart from the 1 open comment I have regarding input names check rest of the PR looks good to me. Once you clarify fix this I will approve the PR.
> 
> Thanks for your contribution!

Thank you @askhade, I have updated [the comment](https://github.com/onnx/onnx/pull/2994#discussion_r501600166) regarding your concern.
askhade(2020-10-12 04:42:26):@jackwish : Please merge with master again. It is best if you give permission to code maintainers to merge with master to make this more efficient. Thanks!
zhenhuaw-me(2020-10-12 04:55:17):done @askhade 
askhade(2020-09-30 18:23:56):it would be best if you add type annotations where ever possible.
askhade(2020-09-30 18:31:30):why is sorting required here?
askhade(2020-09-30 19:06:33):adapt similar naming convention as other python files, basically this should be _build_name_dict

Same comment for other methods
askhade(2020-09-30 19:08:54):if input or output names are not provided then you extract the entire graph. Why not display an error instead? 
askhade(2020-09-30 19:48:48):nit: rename originals to existing_tensors and new_names to something like to_be_filtered or to_be_extracted? 
askhade(2020-09-30 19:55:30):rename to extract_model? 
gramalingam(2020-09-30 20:32:29):I think it is to ensure that the generated model has inputs/outputs in the order specified by the user (via the parameters).
gramalingam(2020-09-30 20:36:23):Interestingly, in this case it ends up being a "model optimization" that eliminates nodes that are not used (in computing any of the output values). So, the current behavior seems to have a useful usecase. But it is worth documenting.
gramalingam(2020-09-30 21:17:08):It seems like if a graph-input X is required to compute a specified-output Y, but if X is not in the specified-inputs, we could end up producing an incorrect model. May be it would help to check for this condition?
zhenhuaw-me(2020-10-07 15:44:40):Yes, this is to make the order of inputs/outputs of the generated model to be the same as user input. This is not necessary but having this won't make user supervised. 
zhenhuaw-me(2020-10-07 15:57:02):Yes, the _missing input-X_ won't be taken as _input of the resulted model_. But I am not planning to detect such a corner case as it seems not to be easy work. Hope that the user may avoid this, and it can be caught by `onnx.check_model()` if we really encounter some...
askhade(2020-10-07 16:41:35):Looks like when input_names are not provided the the extracted sub graph uses the original inputs. This should be documented. 
askhade(2020-10-07 17:52:18):To optimize I think this can be done along with _collect_reachable_nodes. This is nice to have. 
zhenhuaw-me(2020-10-08 03:08:35):All member functions have type annotations except two which are sort complex...
zhenhuaw-me(2020-10-08 03:10:05):Remove the _fallback_ like logic - input/output names are taken as they are.
zhenhuaw-me(2020-10-08 03:12:10):For the _model optimization_ like behavior, as we are extracting sub-models - which is removing _unused_ or _unneeded_ nodes and tensors - it seems to be something as expected to me :)
zhenhuaw-me(2020-10-08 03:14:17):Original input/output names are used in the _first commit_, and the current ([change](https://github.com/onnx/onnx/pull/2994/commits/dfe238f82e10f2a1c294841838535747651f851a?file-filters%5B%5D=.py#diff-498e239a2d86d6bcf902378224869c79L114)) implementation simply takes them as they are - empty is empty.
zhenhuaw-me(2020-10-08 03:18:29):Yes, we can do that. IMO, _two passes_ is sort clearer to clarify the logic. And, as `_collect_reachable_tensors` doesn't walk the graph but iterate nodes, I guess it won't be significant :) Anyway, I can rewrite it if we really want that. What do you say?
askhade(2020-10-08 05:12:51):input names being empty is not valid right? Then shouldn't there be a check for this on line 167?
askhade(2020-10-08 05:13:18):As I mentioned this is not a must have but a nice to have. 
zhenhuaw-me(2020-10-08 10:05:08):It could be surprising but an empty input name is valid actually, though not very meaningful in practice... Empty input names are cases that the extracted model is using _constant inputs_ i.e. initializers. @askhade 

The example below is a `Transpose` node with constant input, and we are extracting the model itself - as it has only one node.
```py
import onnx
from onnx import helper, TensorProto

# generate original ONNX model
M, N, K = 2, 3, 4

wdata = [i for i in range(N*K)]
weight = helper.make_tensor('weight', TensorProto.FLOAT, [N, K], wdata)

output = helper.make_tensor_value_info('output', TensorProto.FLOAT, [K, N])

transpose = helper.make_node('Transpose', ['weight'], ['output'], name='transpose', perm=(1, 0))

graph = helper.make_graph([transpose, ], 'demo', [], [output], [weight])
opset = helper.make_operatorsetid(onnx.defs.ONNX_DOMAIN, 9)
model = helper.make_model(graph, producer_name='toy-demo', ir_version=6, opset_imports=[opset])

onnx.checker.check_model(model)
onnx.save_model(model, 'example.onnx')

# extract model
onnx.utils.extract_model('example.onnx', 'extracted.onnx', list(), ['output'])
```


zhenhuaw-me(2020-10-08 10:06:09):Let's see if we can improve this in the future :)
guoyu-wang(2020-09-03 23:02:09):The 1st iter of the previous PR #2888 was trying to return an empty string, does the "Invalid tensor data type." actually mean anything to the caller?

skottmckay(2020-09-03 23:02:35):Maybe add a comment to the header saying that if the input was invalid they will get an empty TypeProto back, and return_value.value_case() will return TypeProto::VALUE_NOT_SET. 
gramalingam(2020-09-04 00:18:52):Agree. I see pros/cons, but for code that handles this, comparing against an empty string is simpler and easier.
gramalingam(2020-09-04 00:19:28):Or, a named string constant.
vinitra-zz(2020-09-04 16:28:25):Thanks for the good suggestion! Addressed.
vinitra-zz(2020-09-04 16:28:39):Resolved with an empty string.
askhade(2020-09-04 17:04:36):Could you add this in the class header data_type_utils.h where this function is declared.
vinitra-zz(2020-09-04 17:08:04):Updated!
jcwchen(2020-09-10 21:20:12):Updating the document for ConvTranspose will move to another PR because the modification needs further review. Just removed it from this PR.
askhade(2020-09-14 16:59:20):@gramalingam @ebarsoum @postrational : This PR enforces a condition which is already part of the spec. I do not think we need a version bump here. Thoughts?
gramalingam(2020-09-14 22:11:22):> @gramalingam @ebarsoum @postrational : This PR enforces a condition which is already part of the spec. I do not think we need a version bump here. Thoughts?

Agree
askhade(2020-09-10 17:48:06):Is this intentional? Why change this ?
askhade(2020-09-10 17:49:44):same comment as above - why is this change needed?
jcwchen(2020-09-10 17:54:15):It is another old issue related to ConvTranspose which hasn't been resolved: https://github.com/onnx/onnx/issues/1609 There is also a PR for it: https://github.com/onnx/onnx/pull/2097
The original reason is the descriptions are not consistent in the document. I thought it's a good chance to push this forward so I added this fix in this PR as well.
gramalingam(2020-09-14 22:08:20):Just a minor nit: this is neither a shape_inference error nor a type_inference error. Unfortunately, we have only "fail_type_inference" and "fail_shape_inference". This is okay, but it might be good to have a consistent categorization of errors (may be adding another category of errors) ... not necessarily in this PR, could be later also.
jcwchen(2020-09-09 16:07:39)::shipit: 
askhade(2020-09-09 04:54:29):nit: is stored
linkerzhang(2020-09-09 09:46:28):is this CI (continuous-integration/travis-ci/pr ) still needed? @vinitra 
askhade(2020-09-14 16:56:25):> is this CI (continuous-integration/travis-ci/pr ) still needed? @vinitra

Travis is now deprecated.
jcwchen(2020-10-09 01:38:10):Still, adding initializer to input and then remove it is kind of a hacky method to work around this. 
@daquexian @fumihwh since you are working on optimizer with `common/ir`, do you have other solution to fix this IR gap issue in your case? Thanks.
daquexian(2020-10-10 03:27:22):> @daquexian @fumihwh since you are working on optimizer with common/ir, do you have other solution to fix this IR gap issue in your case? Thanks.

I have no clear idea either. IMO we can use the mlir infra (maybe based on https://github.com/onnx/onnx-mlir) instead of implementing the ir ourselves, but obviously, it cannot be done in the short term.
linkerzhang(2020-12-21 01:29:01):shall we merge this? @daquexian @jcwchen 
jcwchen(2020-12-21 18:05:17):> shall we merge this? @daquexian @jcwchen

We are still looking for a better way to do this... This PR is more like a hacky workaround (Add initializer to input then remove it).
jcwchen(2021-08-24 01:43:01):Create a new PR to solve DCO issue: https://github.com/onnx/onnx/pull/3676 Close it now and the work will be continued there.
linkerzhang(2020-09-11 08:26:25):confusing function call...  popoutput from input :).
jcwchen(2020-09-11 14:52:55):I also felt confused for the original `addInput`...but the behavior is correct according to the original design.
I just added some comments for these two functions and hope it can resolve the confusion.
Thank you for bringing this up!
fumihwh(2020-10-06 07:19:57):Could this part be replaced by using `addInitializerAndInput` in `ir.h`?
jcwchen(2020-10-06 16:23:23):Good suggestion. Updated. Thank you
wschin(2020-09-15 17:49:51):After this line, `print (model)` prints this model in text format. It should show inference graph + training graph.
wschin(2020-09-15 17:50:29):You can also `print (full_training_model)` to see a full training algorithm graph.
askhade(2020-10-02 20:33:38):This will create an np array [2, 2] with shape [2] whereas on line 27 when creating B_value_info the shape is set as [2,2]. What am I missing here?
wschin(2020-10-02 20:53:52):My bad. I should create a random function here. 

My mistake here is a good example to support the need of having a runtime test (either ORT or a small runtime) in ONNX repo.
rajeevnalawadi(2020-09-18 22:00:05):@Wei_sheng: 
Pytorch has 3 specific modes for quantization
For training case the “Quantization aware training” QAT is applicable as described in the below linked page
https://pytorch.org/docs/stable/quantization.html#quantization-workflows
Snippet from the page under Quantization workflows & QAT (#3) “…Computations will take place in FP32 but with values clamped and rounded to simulate the effects of INT8 quantization…”

Code for checking QAT
https://pytorch.org/tutorials/advanced/static_quantization_tutorial.html

wschin(2020-09-15 15:46:03):I feel it's not differentiable. Do you have a Pytorch example to double-check?
wschin(2020-09-15 15:46:18):Shape shouldn't be differentiable. Here is a corresponding Pytorch op, https://pytorch.org/docs/stable/generated/torch.nn.MaxUnpool2d.html.
wschin(2020-09-15 15:56:53):The tags for quantized ops make sense. Just out of curiosity, are they also differentiable in Pytorch?
wschin(2020-09-15 16:01:38):This one I am not sure. Conv itself is differentiable, but its type conflicts with my mental assumption --- integer tensors are not differentiable.
wschin(2020-09-15 16:29:37):It's differentiable in math. However, I checked Pytorch and TF batchnorm but none of them outputs "mean", "var", "saved_mean", "saved_var". May we make them non-differentiable?
wschin(2020-09-15 16:31:52):```suggestion
            OpSchema::NonDifferentiable)
```
wschin(2020-09-15 16:32:47):```suggestion
            OpSchema::NonDifferentiable)
```
wschin(2020-09-15 16:40:14):This op is similar to scikit-learn [TfidfVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html), which is not a differentiable function.
```suggestion
            OpSchema::NonDifferentiable)
```
wschin(2020-09-15 16:40:24):```suggestion
            OpSchema::NonDifferentiable)
```
rajeevnalawadi(2020-09-16 22:09:39):Updated with suggestion
rajeevnalawadi(2020-09-16 22:10:00):Updated as NonDifferentiable
rajeevnalawadi(2020-09-16 22:10:20):Updated as NonDifferentiable
rajeevnalawadi(2020-09-16 22:10:43):Updated as NonDifferentiable
rajeevnalawadi(2020-09-16 22:11:34):Made the Batchnorm variables NonDifferentiable
rajeevnalawadi(2020-09-16 22:12:36):Torch indicates these quantized variables as Differentiable
jcwchen(2020-09-11 18:45:42):cc @yufenglee @YUNQIUGUO
jcwchen(2020-09-22 05:39:23):Updated. This PR is ready for review. Thanks
lgtm-com[bot](2020-09-22 17:22:01):This pull request **introduces 1 alert** when merging 9e7bf3e2d456e2391f0671f911b8251a5c9355e4 into 689c4e3a34b1d977fd656a920eabc50bc957b4fd - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-74df3c1414d001c05cc14dd51dd05d7b7aa1d964)

**new alerts:**

* 1 for Unused import
jcwchen(2020-10-09 00:35:24):Hi @gramalingam,
Do you have other concern about this PR? I hope to merge it for ONNX 1.8 Release. Thank you! 
gramalingam(2020-10-09 04:19:58):Hi @jcwchen : do you need to signoff on this PR?
jcwchen(2020-10-09 04:21:39):> Hi @jcwchen : do you need to signoff on this PR?

Thanks for the approval. I am still looking for a good way to sign-off old commits excluding merge commits... Simply use `git rebase HEAD~24 --signoff` will mess the commit log
jcwchen(2020-10-09 05:50:36):Signed. After all CIs pass, let's :shipit: . Thank you @gramalingam 
gramalingam(2020-09-11 20:34:38):It would be nice to add a comment explaining this flag "check_type". I don't recall and the name itself doesn't clarify it.
gramalingam(2020-09-11 20:35:39):Would it be better to support a separate input_model_path and a separate output_model_path?
jcwchen(2020-09-17 05:39:31):I was also confused about this... I think it is related to the full_check for checker. I will add some comments for it. Thank you for the suggestion!
jcwchen(2020-09-17 05:49:39):Sounds good. We can add an optional arguments, `output_model_path`, for Python API. And I think the `output_model_path` needs to be in the same directory as the original model_path because this output only works with the main onnx model file... We should state this limitation for the `output_model_path`.
jcwchen(2020-09-20 15:44:57):This PR will fail in Linux-CI because LITE_PROTO on Linux does not support `SerializeToOstream`(Windows and Mac do support)... No sure whether LITE_PROTO is a must for building onnx.
jcwchen(2020-09-22 05:38:53):Solved by directly using output string to file stream.
neginraoof(2020-09-15 06:20:07):@askhade 
Looks like you have updated the Loop shape inference for opset8, but not 11 (LoopInferenceFunctionOpset11).
I think we need a similar logic to check whether shape ino is available in line:
https://github.com/onnx/onnx/tree/master/onnx/defs/controlflow/old.cc#L886
askhade(2020-09-14 16:51:49):Please add a similar message in the Optimizer documentation as well.
daquexian(2020-09-19 03:01:50):@askhade Thanks! I have updated it.
askhade(2020-09-22 20:46:27):@daquexian : Please fix the DCO failure. 
daquexian(2020-09-23 14:37:28):> @daquexian : Please fix the DCO failure.

Done
askhade(2020-09-14 16:41:59):Also add a statement stating all further enhancements and fixes to optimizers will be done in this new repo.
jcwchen(2020-09-14 23:23:28):cc @gramalingam @ebarsoum @postrational. Thank you!
gramalingam(2021-01-26 21:56:10):@jcwchen : is this still relevant (given the PR https://github.com/onnx/onnx/pull/3188 )? Thanks!
jcwchen(2021-01-27 15:55:08):> @jcwchen : is this still relevant (given the PR #3188 )? Thanks!

Yes. I believe so. This issue is only related to ConvTranspose that its SAME_UPPER/SAME_LOWER has contradicted definition in the document. There is a related fix in ONNXRuntime as well. I will go back to deal with this when I have more bandwidth. Thank you!
CLAassistant(2020-09-15 07:34:44):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=3021) <br/>All committers have signed the CLA.
lgtm-com[bot](2020-09-15 07:47:06):This pull request **introduces 1 alert** when merging e332ae793c894c41b51aa55bde907caf2d16eedb into f421b68b9bfcd4992e398ca2db54897260909c6a - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-f6ff913c118b0ff72897baf4cde555a71c9b5b07)

**new alerts:**

* 1 for Inconsistent definition of copy constructor and assignment \(&#39;Rule of Two&#39;\)
daquexian(2020-09-25 04:53:05):@462630221 Could you please sign the CLA and merge the latest master branch?
daquexian(2020-09-16 03:01:58):typo, ohther->other
daquexian(2020-09-16 03:03:15):inconsistent indent
daquexian(2020-09-16 03:06:46):This copy assignment implementation doesn't guarantee strong exception safety. Maybe [copy-and-swap](https://en.wikibooks.org/wiki/More_C%2B%2B_Idioms/Copy-and-swap) like [here](https://github.com/onnx/onnx/issues/3004#issuecomment-692487726) is a better implementation.
daquexian(2020-09-16 03:10:50):> 1 for Inconsistent definition of copy constructor and assignment ('Rule of Two')

LGTM analysis also complains about this
daquexian(2020-09-24 07:03:24):uint64_data_ is missing
askhade(2020-10-02 18:11:34):@daquexian : Marking this PR for 1.8 release. Let's try and get this change checked in before 1.8 to avoid another version bump for resize.
daquexian(2020-10-05 03:58:05):@askhade @hariharans29 I have updated the spec to "one of the two MUST be specified and it is an error if both are specified" and made the input "roi" also optional. When it comes to "input (2-4)" and the checker problem, maybe we need to update the infra to support the case "one of these optional inputs must be specified" first?
hariharans29(2020-10-07 19:29:29):> @askhade @hariharans29 I have updated the spec to "one of the two MUST be specified and it is an error if both are specified" and made the input "roi" also optional. When it comes to "input (2-4)" and the checker problem, maybe we need to update the infra to support the case "one of these optional inputs must be specified" first?

Thanks for adding the line to the spec. It leaves no place for ambiguity atleast with the spec description. It does a look a little weird to see (2-4) when actually the correct line there must read (2-3), but looks like ONNX doesn't have the infrastructure to deal with "inter-dependence" of optional inputs/outputs (i.e.) we are not able to say - 'scales' and 'sizes' are individually optional both they are NOT both optional and both can't co-exist. So, I guess we can leave it for now and enhance the infrastructure for later.

For the second aspect of the PR - deprecating `tf_half_pixel_for_nn`- does this cause any issue for a model that is using this attribute and trying to upgrade to opset-13 ? It seems like (from your comment in the PR) that all the converter (or opset upgrader) would have to do is replace `tf_half_pixel_for_nn` with `half_pixel' + 'round_prefer_ceil` and the upgrade will be seamless. Is this correct or do you foresee any other upgrade issues faced by models already using this attribute ?
hariharans29(2020-10-07 19:38:59):Overall, I am good with this PR - left a couple of minor comments 
daquexian(2020-10-12 01:25:40):> For the second aspect of the PR - deprecating tf_half_pixel_for_nn- does this cause any issue for a model that is using this attribute and trying to upgrade to opset-13 ? It seems like (from your comment in the PR) that all the converter (or opset upgrader) would have to do is replace tf_half_pixel_for_nn with half_pixel' + 'round_prefer_ceil and the upgrade will be seamless. Is this correct or do you foresee any other upgrade issues faced by models already using this attribute ?

@hariharans29 Yes, it is correct. All the converter only have to replace replace 'tf_half_pixel_for_nn' with 'half_pixel' + 'round_prefer_ceil'.
daquexian(2020-10-13 08:04:36):> @daquexian : It seems the tests were not updated after roi was made optional. Can you update the test script and regenerate the tests again. Thanks!

Thanks. Updated.
askhade(2020-10-13 16:46:20):@daquexian : The tests are checked in with the test script. So you need to update the generated tests as well. I think you only updated the test script. 
FYI: We are cutting the release branch tomorrow morning and as much as possible I want to check this change in otherwise we will need another version bump for resize op. 
gramalingam(2020-10-13 18:46:14):It is true that the documentation-generator does not have infrastructure to fix the number of inputs expected. However, the type-and-shape-inference checker _can_ be augmented to verify that exactly one of two inputs is specified. (Well, sort of ... we can do the check using both getNumInputs and getInputType; we will need to use getInputType as a proxy for whether a particular input is specified; it is not perfect, but it is reasonable. The only minor issue would be a misleading error-message in the case where an input is specified, but type-inference could not infer its type.
gramalingam(2020-10-13 19:05:40):Specifically: a utility function like below should help:
```cpp
   inline bool hasInput(InferenceContext& ctx, size_t n) {
       return ctx.getNumInputs() > static_cast<size_t>(n) && ctx.getInputType(n);
}
```
We can then add a check one of the two inputs is specified. 
askhade(2020-10-13 19:09:11):Shape inference code for resize needs fixes. https://github.com/onnx/onnx/blob/53057782d19a59548705802fca756d87dee032e2/onnx/defs/tensor/utils.cc#L73
Current shape inference code will pick sizes input only if number of inputs is 4 but with roi and scales being optional inputs shape inference will fail. 
daquexian(2020-10-14 02:25:47):> @daquexian : The tests are checked in with the test script. So you need to update the generated tests as well. I think you only updated the test script.

@askhade Thanks! Updated.

> Current shape inference code will pick sizes input only if number of inputs is 4 but with roi and scales being optional inputs shape inference will fail.

I don't think so. 
https://github.com/onnx/onnx/blob/88e5d3e39c11c79a38f43af2a23147895f6f226b/onnx/shape_inference/implementation.h#L59-L65
https://github.com/onnx/onnx/blob/88e5d3e39c11c79a38f43af2a23147895f6f226b/onnx/shape_inference/implementation.h#L87-L89

The `allInputTypes_` will be `[input_type, nullptr, nullptr, sizes_type]` and the number of inputs will still be 4.
daquexian(2020-10-14 02:47:29):> It is true that the documentation-generator does not have infrastructure to fix the number of inputs expected. However, the type-and-shape-inference checker _can_ be augmented to verify that exactly one of two inputs is specified. (Well, sort of ... we can do the check using both getNumInputs and getInputType; we will need to use getInputType as a proxy for whether a particular input is specified; it is not perfect, but it is reasonable. The only minor issue would be a misleading error-message in the case where an input is specified, but type-inference could not infer its type.

Good advice! Let's merge this PR for 1.8 release and I can submit another pr for this in the future.
hariharans29(2020-10-01 22:43:27):Since both scales and sizes are now "optional" per schema - should we go ahead and include a line that says - one of the two optional inputs MUST be specified. We already have a line that says "only one can be specified...", maybe we can say - "one of the two MUST be specified and it is an error if both are specified."

Also does the onnx checker catch cases where the Resize node has only two inputs - which means it is missing one of the two ? Also, `Inputs (2-4) ` - is that correct ? 
askhade(2020-10-02 18:05:38):why not make roi optional as well? since it only takes effect when coordinate_transformation_mode is "tf_crop_and_resize"
askhade(2020-10-02 18:06:41):Should this be 2 to 3? Because Sizes and Scales should not be specified together right.
daquexian(2020-10-03 12:33:58):@askhade @hariharans29 I agree that here should be 2-3, but this line is generated automatically. Is there any way to overwrite it?
daquexian(2020-10-05 03:53:04):> Also does the onnx checker catch cases where the Resize node has only two inputs - which means it is missing one of the two ? Also, Inputs (2-4) - is that correct ?

Agree. But it seems like the infra (checker and doc generator) cannot support "one of these optional inputs must be specified".
hariharans29(2020-10-07 19:33:59):should we add a test where scales exists and sizes does not ?
hariharans29(2020-10-07 19:35:18):nit: If 'sizes' is needed (not if 'size' is needed) for consistency
daquexian(2020-10-12 01:20:35):Thanks. Fixed
daquexian(2020-10-12 01:24:13):Thanks. There are already such tests before this PR submitted (e.g., https://github.com/onnx/onnx/blob/master/onnx/backend/test/case/node/resize.py#L341) . 
askhade(2020-09-23 18:38:54):@wschin  please sign the DCO

wschin(2020-09-23 19:56:37):> @wschin please sign the DCO

Done.
wschin(2020-09-30 23:32:20):Why is this removed? You can manually add those changes back in the next commit; no need to go through the entire history again.
rajeevnalawadi(2020-10-01 00:09:55):Manual add
wschin(2020-10-06 16:37:38):Why is it non-differentiable? To speed up the merge, we probably should split this PR into two PRs (one for normal ops and the other one for quantized ops)?
wschin(2020-10-06 16:59:26):This is differentiable but in `ConvInteger` the input `X` isn't. This enhances my feeling to split quantization ops from this PR.
rajeevnalawadi(2020-10-06 22:41:37):Removed the differentiable variables for QlinearConv and ConvInteger operators
rajeevnalawadi(2020-10-06 22:41:52):Removed the differentiable variables for QlinearConv and ConvInteger operators
CLAassistant(2020-09-30 18:15:02):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=3036) <br/>All committers have signed the CLA.
Yukigaru(2020-09-30 18:47:06):Okay, I've read about DCO and will do it soon
Yukigaru(2020-10-01 06:26:07):DCO done.
Yukigaru(2020-10-10 11:14:31):@yuslepukhin, thank you for the review! Will update the PR soon.
Yukigaru(2020-10-10 19:18:45):Done, updated. 
Yukigaru(2020-10-12 11:49:51):Done, removed hard-coded .reserve(4). Tests - passed.
yuslepukhin(2020-10-12 18:22:13):@Yukigaru You will need to to rebase/merge off master.
yuslepukhin(2020-10-09 18:26:59):Nice! :)
yuslepukhin(2020-10-09 18:28:11):Pls, check if u is better be a const ref
yuslepukhin(2020-10-09 18:48:04):add assert(num_inputs >=2);
yuslepukhin(2020-10-09 18:49:05):Let's not do hardcoded reserve(), it is hard to maintain
yuslepukhin(2020-10-09 18:56:16):This creates a hard to debug case when dim_size() returns a value that is less than 2. We don't want that.
yuslepukhin(2020-10-09 18:56:41):Ditto if dim_size() is somehow less than 2.
yuslepukhin(2020-10-09 19:05:18):Assuming we will not find many inconsistent duplicate registrations you can also combine insert/find and eliminate double lookup if you want to be real efficient. Something like:
`auto p = type_constraints.emplace(type_str, *type_proto);
if(!p.second) {
// failed to insert due to a duplicate, check consistency
if(p.first.second != *type_proto) fail()
}
`
yuslepukhin(2020-10-09 19:08:13):count() usage widespread, probably more readable. Make it count(n) > 0;
yuslepukhin(2020-10-09 19:15:56):We do not want hardcoded reserve(). 
In general, while reserve() usage may be beneficial, it will not make most of the code vastly more efficient. There are two reasons for that 1) the types we often use are primitive types, cheap to copy 2) in addition, the amount of elements is very small, it would fit into a single allocation block if such an allocation is necessary at all, because std::vector as well std::string now has some inline space, which eliminates a need for any allocations/re-allocations for many cases.
yuslepukhin(2020-10-09 19:16:50):Ditto, hardcoded reserve()
yuslepukhin(2020-10-09 19:17:32):Same thing
yuslepukhin(2020-10-09 19:25:09):No C-casts. Also, please, keep sizes in size_t type.
set size to zero if B_sizes.size() is > A_sizes.size().
Yukigaru(2020-10-10 10:53:09):As I know std::vector implementations does not have SVO/SBO. It's explained here: https://stackoverflow.com/questions/48669413/is-sso-used-in-any-other-standard-library-containers-other-than-stdstring
But I will remove that for the same reason as before: it's hard-coded number.
Yukigaru(2020-10-10 11:02:35):Checked, uses() returns vector of Use objects, Use is a struct of size 16 bytes. I think it can be easily copied. 
Yukigaru(2020-10-10 11:06:05):nice trick!
linkerzhang(2020-10-12 00:46:48):why "4"?
Yukigaru(2020-10-12 11:42:56):The logic was that 4 is better than 0 (because that incurs 3 reallocs to get to capacity=4), but not high enough to waste memory. But I will remove that diff.
askhade(2020-10-06 19:47:03):Linux CI is failing due to type check failure
onnx/test/checker_test.py:306: error: Incompatible default for argument "name" (default has type "None", argument has type "str")
type check failed

gramalingam(2020-10-05 21:45:37):One question: TensorProtos and SparseTensorProtos can appear in two contexts. (a) In initializers, and (b) As an attribute-value in a node. Is the name compulsorily needed in both contexts or only when they are used in initializers? This is more a question about how to interpret the existing standard/spec. My interpretation is that the name is required in case (a), not necessarily in case (b). If so, I think we can have this check when checking initializers, instead of at this place. In either case, I think we should have the same requirement/check for dense initializers as well as sparse initializers.
gramalingam(2020-10-05 21:48:59):I agree that the spec is not very explicit about this. I am inferring it indirectly, since onnx.proto specifies "field MUST be present" if it is compulsory for all instances. Further, AttributeProto has its own name.
yuslepukhin(2020-10-06 16:46:10):If there are no other opinions, I will make the name optional for sparse tensor but enforce it for sparse initializers.
askhade(2020-10-06 16:58:22):I think that is best. I also inferred the spec the similar way. It seems like the name is required for initializers and not in the case of attribute value

gramalingam(2020-10-06 18:57:04):change to "Each TensorProto and SparseTensorProto entry"?
askhade(2020-10-06 19:40:07):no need for enforce_has_field(sparse_init, name); here?
askhade(2020-10-06 19:43:37):can you add one more tests to test the enforce_has_field behavior for attribute and initializer. This will make the intent explicit.
gramalingam(2020-10-06 23:16:17):On an unrelated point: can we increase the line-width used in clang-format? I think we would have more readable code then!
askhade(2020-10-07 04:24:26):> On an unrelated point: can we increase the line-width used in clang-format? I think we would have more readable code then!

I updated it to 120. (ORT also used 120)
jcwchen(2020-10-10 17:22:50):Space error can be solved by `git lfs prune` and this PR is ready for review. Another thing is `git lfs pull` models from ONNX Model Zoo in onnx/onnx repo is much slower than `git lfs pull` in onnx/models. (1h 43m vs 34m)
prasanthpul(2020-10-10 17:56:53):Should this test live in onnx/onnx or onnx/models?
jcwchen(2020-10-10 20:14:09):> Should this test live in onnx/onnx or onnx/models?

Good question. I think this one should exist in onnx repo.
- In onnx/onnx: To test functionality of the latest ONNX, weekly test all models from ONNX Model Zoo with the latest onnx (main branch)
- In onnx/models: Weekly test all models and test dataset from ONNX Model Zoo with the latest onnxruntime package (ORT relies on certain ONNX's commit). And for new submitted Models in ONNX Model Zoo, they will be tested by the latest ONNX package (i.e., 1.7 for now) so we don't need to test all models weekly in onnx/models. 

askhade(2020-10-14 03:29:55):why are the 15 models failing? Is it something with the pipeline? Can you repro the failure locally? If the issue is with the model then let's create a list of expected failing models and ignore the failures for them to get the CI green otherwise we will have to look through the logs to figure whether the CI failed because of a known failure or a new failure.
jcwchen(2020-10-14 04:56:22):> why are the 15 models failing? Is it something with the pipeline? Can you repro the failure locally? If the issue is with the model then let's create a list of expected failing models and ignore the failures for them to get the CI green otherwise we will have to look through the logs to figure whether the CI failed because of a known failure or a new failure.

The checker is stricter (with shape_inference) than the one which the old CI in onnx/models repo was using... Therefore it is possible that the current checker are failing with some models.

Add the list to skip for now. There are four models (vgg16-7.onnx, vgg19-7.onnx, vgg16-bn-7.onnx,  vgg19-bn-7.onn) which cannot pass on weekly-CI can pass on my local. Still figuring out why...

jcwchen(2020-10-19 05:38:45):VGG related models only fail on Windows-CI (They can pass onnx.checker locally). In addition, other models fail because some operators of opset3 do not support `TypeAndShapeInferenceFunction` for shape inference. I have sent a PR to deprecate those olde models using opset3 which cannot pass onnx.checker:
https://github.com/onnx/models/pull/389
askhade(2020-11-30 17:43:07):remove this
jcwchen(2020-12-01 16:54:02):They are failing because loading too many large ONNX models will cause MemoryError on Windows... To workaround this, move the weekly-CI from Windows to Linux.
jcwchen(2020-10-12 03:24:16):Remove this in this PR to pass pytest. It will be removed by https://github.com/onnx/onnx/pull/3048.
askhade(2020-10-12 17:59:07):"actions/upload-artifact@v1" does not upload to pypi. Check : https://github.com/actions/upload-artifact
We want this action but the comment is wrong

jcwchen(2020-10-12 18:05:42):Yes... Sorry that I forgot to mention it is TODO
sveta-levitan(2020-10-09 22:33:34):@wschin Please add documentation and label this for 1.8 release. Thank you!
wschin(2020-10-11 18:21:14):> @wschin Please add documentation and label this for 1.8 release. Thank you!

I generated the new docs using this commit but I found one place we need to fix. `ArgMax` and `ArgMin` should be non-differentiable at input and output.

wschin(2020-10-11 18:21:26):```suggestion
        OpSchema::NonDifferentiable);
```
wschin(2020-10-11 18:21:36):```suggestion
        OpSchema::NonDifferentiable);
```
jimspohrer(2020-10-18 21:52:54):thanks for updating.
CLAassistant(2020-10-12 04:22:55):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=3055) <br/>All committers have signed the CLA.
askhade(2020-10-12 04:39:56):Please add tests for this too.
pranav-prakash(2020-10-12 05:58:06):Added a test for the slice operator adapter. 

I assume that no tests are needed for those ops which used the `CompatibleAdapter`? The ones for `10 -> 11` were all of the form where opset 11 added support for negative indices, which has no impact on the forward conversion from 10 to 11.
askhade(2020-10-12 22:55:41):Please sign the DCO
pranav-prakash(2020-10-12 23:20:44):Done.
askhade(2020-10-12 17:09:44):take a const reference?
pranav-prakash(2020-10-12 20:41:57):Fixed.
CLAassistant(2020-10-15 17:17:25):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=3059) <br/>All committers have signed the CLA.
gramalingam(2020-10-15 18:46:39):Adding context to this PR: this is a follow-up to the previous PR https://github.com/onnx/onnx/pull/2955 which updated the function-body of NLL Loss but missed the version-bump.
jcwchen(2020-10-16 23:24:16):cc @yufenglee
askhade(2020-10-16 23:42:20):please remove changes to all the tests... only include tests related to the op\function you are changing

jcwchen(2020-10-19 05:33:50):> please remove changes to all the tests... only include tests related to the op\function you are changing

Just manually removed. I was thinking whether there is a good way to detect those mattered models automatically but failed... I suppose `cmd_tools.py` should be corrected to only update the mattered models.

jcwchen(2020-10-20 21:28:11):After discussion, we decide to extend reducemax and reducemin by attribute "complement_axis" to add axis attribute for dynamicQuantLinear:
1. Update ReduceMax and ReduceMin with an optional boolean attribute, complement_axis (default is False).
2. Go back to DynamicQuantizeLinear and add an optional axis attribute just like QuantizeLinear
3. Update FunctionBody of DynamicQuantizeLinear to use ReduceMax and ReduceMin with complement_axis=True and target axis (added attribute from 2.)
jcwchen(2020-10-21 16:19:24):Thank @askhade for all the helpful comments. I have just updated. Right now I am working on fixing the original test, `test_dynamicquantizelinear_min_adjusted_expanded_cpu`, which is failing because `RuntimeError: Inferred shape and existing shape differ in rank: (1) vs (0)`.
yufenglee(2020-10-21 19:30:33):> <dd>Output scale. It's a scalar, which means a per-tensor/layer quantization.</dd>

It can be either a scalar for per-tensor or a 1D dimension tensor with size equal to x on axis specifiedfor per-channel, 

---
Refers to: docs/Operators.md:4818 in 223343b. [](commit_id = 223343ba0cae5086c39974ce6d8d7c0318121bdd, deletion_comment = False)
yufenglee(2020-10-21 19:30:58):> <dd>Output zero point. It's a scalar, which means a per-tensor/layer quantization.</dd>

Similar as y_scale. It can also be a tensor with size equal to x on axis specified.

---
Refers to: docs/Operators.md:4820 in 223343b. [](commit_id = 223343ba0cae5086c39974ce6d8d7c0318121bdd, deletion_comment = False)
jcwchen(2020-10-21 20:07:06):According to @yufenglee 's feedback, this PR needs more discussion about the per-channel thing (will introduce more attribute to indicate the usage). We won't forward this before ONNX 1.8. I will send another new PR for simply bump DynamicQuantizeLinear to opset 13. Close this PR now.
yufenglee(2020-10-16 23:27:28):We'd better to make DynamicQuantizeLinear to support per-channel too.
yufenglee(2020-10-16 23:52:25):It looks hard to implement with FunctionBody because there is no way to generate axes attribute for ReduceMin correctly.
askhade(2020-10-20 22:54:11):This the wrong method. You are adding this to ArgReduceDocGenerator. It should be added to ReduceDocGenerator.
Also all reduction ops use ReduceDocGenerator so you need to add a conditional whether complementary axis should be added or not
gramalingam(2020-10-20 23:41:53):Suggested rewording: "The attribute complement_axis is used in conjunction with the axis attribute, and has a default value of False. If complement_axis is True, all axes except the specified axis will be reduced."
gramalingam(2020-10-20 23:45:59):Nit: Suggest changing "needs to be used simultaneously" to "is used in conjunction with". This is just to avoid creating the impression that the attribute must be specified if axis is specified.
jcwchen(2020-10-21 00:51:55):The attribute which DynamicQuantizeLinear is using was AttributeProto::INT, but the attribute which Reduce is using is AttributeProto::INTS... In that case, it seems that we cannot use MakeRefAttribute directly here and the conversion from INT to INTS is needed. Does current FunctionBodyHelper support it?
jcwchen(2020-10-21 01:09:06):If not, I am thinking maybe we can introduce other attribute (single integer) to represent the specified single quant dimension in reduce function instead of "complement_axis" (boolean).
jcwchen(2020-10-21 01:09:22):Updated. Thanks
jcwchen(2020-10-21 01:09:42):All updated. Thanks.
gramalingam(2020-10-21 03:19:25):If we use a ContextDependentFunctionBody, we can do it. See https://github.com/onnx/onnx/blob/a7a0fec7f25cae567429af62b7eaaee1c3f0e247/onnx/defs/math/defs.cc#L2830 for an example. The context allows us to get the attribute-value, and use it to construct new attribute-values.
jcwchen(2020-10-21 03:29:07):@gramalingam I have just sent an email and haven't seen you replied here... Thank you so much and I will give this a try.
askhade(2020-10-21 03:44:16):Add attribute to QuantizeLinear node as well
askhade(2020-10-21 04:30:58):this should be axis and not axes. In the context of this function axis should be singular.
askhade(2020-10-21 04:33:05):same comment as above. axis here should be same as axis in QuantizeLinear
askhade(2020-10-21 04:47:46):Doc for all reduction ops will be updated. complement_axes related documentaiton should only be added for ReduceMax and ReduceMin.
You can create 2 vars with original doc and one with modified doc ( which contains text for complement_axes and then choose which one to use based on value of complement_axes
askhade(2020-10-21 04:50:41):I suggest rewording this a little bit : 

The above behavior is similar to numpy, with the following exceptions
1. numpy defaults keepdims to False instead of True
2. This op uses complement _axes attribute which defaults to false but when set to true indicates all axes except the specified axes will be reduced.
The attribute complement_axis is used in conjunction with the axis attribute, and has a default value of False.

askhade(2020-10-21 05:01:58):Whether to use complement of axes for reduction
askhade(2020-10-21 05:03:43):rename as complement_axes?
askhade(2020-10-21 05:04:59):in conjunction with "axes" attribute. In context of reduction ops axes is plural so use the axes instead of axis
jcwchen(2020-10-21 06:12:01):Using axes here was just for experiment. Updated it as axis
jcwchen(2020-10-21 06:12:53):What do you mean by adding attribute to QuantizeLinear node?
askhade(2020-10-21 14:29:05):axis should also be passed to QuantizeLinear
askhade(2020-10-21 14:29:54):nit: DEFAULT_VALUE
askhade(2020-10-21 16:18:53):This line and the next is present in GenerateGeneralRedunctionDoc as well. I suggest keep it simple just keep 2 separate strings no need for concatenation
jcwchen(2020-10-21 16:23:30):OPTIONAL_VALUE is a global variable (`false`) which is the default value of "axes" attribute in Reduction functions. Actually I don't know what the proper default value here is if the "axis" attribute does not specify in `DynamicQuantizeLinear`. I believe this is why `test_dynamicquantizelinear_min_adjusted_expanded_cpu` is failing. 
askhade(2020-10-21 16:38:19):why do you need replace all? initially it was used to replace all occurrences of name with the op name... 
gramalingam(2020-10-21 17:30:02):The quantize op spec says that the default value is 1, doesn't it? (Line 159 above.)
gramalingam(2020-10-21 17:32:45):We should be using the default (1) for DynamicQuantizeLinear (not from Reduction ops). 
jcwchen(2020-10-21 18:57:53):If that is the case, it seems that it will break 3 existing test cases... I want to make sure one thing:
1. (Original) ReduceMax and ReduceMin (keepdims=0, axes is not specified; which means reduce all dimensions)
2. (Updated) ReduceMax and ReduceMin (axes=1, keepdims=0, complement_axes=1)

Will these default cases, 1 and 2, produce the same shape?

yufenglee(2020-10-21 19:40:40):Please update to reflect that they can be a tensor for per-channel quantization.
yufenglee(2020-10-21 19:43:07):There is an issue here. In QuantizeLinear, we use the size of scale to determine if it is per-tensor or per-channel, i.e., if size of scale is 1, then it is per-tensor; if size of scale is same the x.shape[axis], then it is a per-channel. Here we can not just use the axis to differentiate it.
askhade(2020-11-02 20:24:27):Can you add a test case for this too.
askhade(2020-11-02 19:56:18):rename i to say input_index for better readability
askhade(2020-11-02 20:23:45):I dont understand what this is exactly trying to do. Can you please elaborate... 

For example when node_inputs does not exist why return [] and not [TypeProto()] 
Also if (node_inputs[0] != ''): what does this check really accomplish? 
gramalingam(2020-11-02 21:39:20):I added comments to the beginning of the function to clarify what the inputs/outputs look like. This is needed because of the existing style of creating test-cases which omit values for optional-inputs when they are missing. So, we need to adjust for missing-optional-parameters when creating the list of input-types.
gramalingam(2020-11-02 21:48:11):I felt it would be simplest to maintain compatibility with the existing methods like hasInput/hasOutput. If we change it, we should do it for all simultaneously. (A related note: if we do change it, it would be more useful to change the type to unsigned as well; but then it starts becoming a more significant change ... something to think about.) Any thoughts on what exactly we should do?
askhade(2020-11-02 23:21:14):I did not realize other methods used the same convention. For this PR I suggest updating this and other methods to use a readable name instead of i ... and we can update the type to uint as part of another PR. 
gramalingam(2020-11-03 17:52:19):Done
wschin(2020-11-03 21:58:24):Maybe we should throw if a type is missing? We can write another helper function `virtual bool hasInputType(int inputIndex)` for checking if input type exists before calling `getInputType(int inputIndex)`.
wschin(2020-11-03 22:03:45):I feel it's better to have a test for this function.
gramalingam(2020-11-03 22:11:52):I think this is simpler than throwing an exception. We already have optional inputs to handle anyway. It is good to make the caller be aware and handle the possibility explicitly. (It is easier to ignore/forget to handle exceptions.)
gramalingam(2020-11-03 22:14:48):Agree with test-case. But I don't think we can do it from python since we need to define a new dummy function for this purpose, which needs to be done in C++. Let me look into this separately because some test-infrastructure is required I think.
jcwchen(2020-10-21 22:07:10):cc @yufenglee Thank you!
jcwchen(2020-10-22 17:42:50):We have verified that `QuantizeLinear` opset 13 bump won't influence `DynamicQuantizeLinear`. Close because this opset bump is not needed anymore.
gramalingam(2020-10-22 23:39:24):A few independent comments:
(a) Can we just do an assignment instead of calling mergeShapesAndTypes? "* iter->second = *inferredType"?
(b) Can we move this into the else-branch in line 323?
askhade(2020-10-22 23:46:13):agree with (a)

the assignment still needs to happen outside the else branch after inferedType is determined on line 330.


jcwchen(2020-10-23 00:07:27):Thank you all for the comments. Both (a) and (b) are workable.
For (b), I think `inferredType` is actually determined on line 311 and this update (update original undefined output) only occurs in the else-branch on line 323. So it looks good to move there.
askhade(2020-10-23 04:08:03):nit: either remove this comment or explain why this condition and undefinedValueTypesByName  needs to be removed. My suggestion remove this comment from here and create an issue to track this item. The first step is to add the clarification in the spec and then update the code here. 
askhade(2020-10-23 04:10:21):nit: line 191 and 192 can be reworded as Saving names of outputs with undefined types to allow assigning inferred types to them. 
jcwchen(2020-10-23 04:41:34):Sounds good. I have removed it and tracked here: https://github.com/onnx/onnx/issues/3069
daquexian(2020-11-05 11:09:22):@okdshin I signed off your commits to make this PR mergeable. Please contact me if you want to sign it off yourself and this PR is not merged.
okdshin(2020-11-06 09:24:08):No problem. Thank you.
daquexian(2020-11-12 00:47:22):> Added a few comments. Overall looks good to me

Thanks! Updated
askhade(2020-11-11 19:35:04):where applicable use const iterators cbegin() and cend()
askhade(2020-11-11 20:10:43):nit: isExistingName -> doesNameExist

even better would be isNameUnique
askhade(2020-11-18 00:26:07):const iterators can be used here too and on line line 896
askhade(2020-11-18 00:27:56):isNameUnique name logically suggests that the function is going to return true if name is unique and false otherwise but the return value of this function is opposite of this and is confusing. 
askhade(2020-11-18 00:29:11):isNameUnique should return false when name is not unique and this check should be changed to while(!isNameUnique(next_unique_name))
daquexian(2020-11-18 00:32:40):node->inputs/outputs() returns an ArrayRef object which doesn't have a cbegin() or cend() method
daquexian(2020-11-18 00:33:41):my bad 😂
askhade(2020-11-07 18:57:33):Closing this in favor of https://github.com/onnx/onnx/pull/3083
snnn(2020-11-05 07:59:42):You can also set protobuf_BUILD_TESTS to OFF, then this step will not be needed.
askhade(2020-11-07 18:55:02):good point. It is already set to OFF. I can remove this. Closing this PR in favor of the one we checked into rel branch. 
jcwchen(2020-11-02 17:47:24):LGTM. Thanks
askhade(2020-11-02 17:58:00):> LGTM.
> 
> But setup.py still need be changed. It should say ONNX requires protobuf python package >=3.11.3

Because we are pinning protobuf to 3.11.3 right now the package works with older versions too... for this release I prefer not to change this... after this release we will move protobuf to 3.13 and then add "ONNX requires protobuf python package >=3.13.0
" in setup.py
CLAassistant(2020-11-03 17:35:08):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=3087) <br/>All committers have signed the CLA.
gramalingam(2020-11-04 00:48:25):Hi @jcwchen : thanks. But for documentation purpose, can you clarify what exactly is the requirement about exceptions? We do internally use various classes of exceptions, and it is unclear to me what the caller is assuming in this case, or what the contract is.
jcwchen(2020-11-04 19:46:58):> Hi @jcwchen : thanks. But for documentation purpose, can you clarify what exactly is the requirement about exceptions? We do internally use various classes of exceptions, and it is unclear to me what the caller is assuming in this case, or what the contract is.

Currently throwing shape_inference error here causes some converters crashing on Windows with certain python version... Even though the exception here might be not related to the crash, it is safer to use the native exception (std::runtime_error) here instead of custom exception.

And yes, I agree with you we should discuss what shape_inference should throw in different scenarios so this modification has not been settled down yet, but that would be something we need to do after ONNX 1.8 Release. Thank you.


   
gramalingam(2020-11-04 20:30:37):Ok. But it sounds like there may be some other underlying issue relating to python-versions that needs to be fixed too, even though it may not be a priority right now.
jcwchen(2020-11-05 21:58:46):Another PR has fixed this.
gramalingam(2020-11-04 20:35:10):Sorry, saw this just now. Is this temporary debugging? 
askhade(2020-11-07 18:53:33):This was to test some fixes for the segfault which we were seeing when exception are thrown from infer shapes mtd. The solution was to update the version of pybind11
jcwchen(2020-11-05 03:31:45):```suggestion
        cmake -A $arch -DCMAKE_INSTALL_PREFIX="../../protobuf_install" -DCMAKE_BUILD_TYPE=Release -Dprotobuf_MSVC_STATIC_RUNTIME=ON -DProtobuf_USE_STATIC_LIBS=ON -Dprotobuf_BUILD_TESTS=OFF -Dprotobuf_BUILD_EXAMPLES=OFF .
```
jcwchen(2020-11-05 03:32:28):Don't forget to remove 2019.  Also for release_win.yml
snnn(2020-11-05 07:54:33):What's the bug? 
jcwchen(2020-11-05 13:21:04):> What's the bug? 

If onnx throws any kind of error from c++ to python (only if onnx builds with vs2019 and pybind11 2.4.3), converter using this onnx will occur segmentation fault. Therefore either building release onnx with vs2017- or pybind11     2.6.0 can resolve this issue... Will upgrading pybind11 bring any side effect? Otherwise using lower vs version is also  a solution. Thanks.
jcwchen(2020-11-05 17:49:51):We decide to update pybind11 (2.6.0) to fix this segmentation bug, because we can keep using the latest compiler (ONNX 1.7 used vs2019 as well).
snnn(2020-11-05 18:13:05):"converter using this onnx will occur segmentation fault"

Then what if onnx was used alone without the converter, will it fail?

Is it a problem of ONNX or a compatibility issue of these two python packages?



jcwchen(2020-11-05 18:19:12):> "converter using this onnx will occur segmentation fault"
> 
> Then what if onnx was used alone without the converter, will it fail?
> 
> Is it a problem of ONNX or a compatibility issue of these two python packages?

ONNX itself works normally with the failed model from converter without involving the converter (tensorflow-onnx). 

- ONNX build with (vs2017 + pybind 2.4.3) worked
- ONNX build with (vs2019 + pybind 2.6.0) worked
- ONNX build with  (vs2019 + pybind 2.4.3) failed

Therefore, we suppose it's a compatibility issue between VS Compiler and pybind11.
jcwchen(2020-11-05 18:29:54):We have tested between the workable and unworkable environment, and VS Compiler and pybind11 are the only variances which matter... PyPI library versions and environment variables did not matter in our experiments.
snnn(2020-11-05 19:17:12):I highly suggest we should find the root cause, instead of trying all the different combinations to get a working one. And it's very likely the other people may entercounter the same issue again, no everyone is using our prebuilt packages, many of the users build onnx from source.  Even ORT is using the source form of ONNX, and vs2019 which seems won't work. 
snnn(2020-11-05 19:21:03):> Will upgrading pybind11 bring any side effect? 

I think it should be ok. But if you don't know what went wrong, how will you be sure upgrading pybind will solve the issue? Would it happen again in a different build setting?
CLAassistant(2020-11-05 16:02:59):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=3097) <br/>All committers have signed the CLA.
prasanthpul(2020-11-19 20:27:15):Merged content into #3117 
askhade(2020-11-10 19:04:36):Can you use https://github.com/onnx/onnx/blob/master/onnx/defs/tensor_proto_util.cc#L10 ?
gramalingam(2020-11-10 19:14:38):No, this is a generalization of that. The difference is that this determines what to construct dynamically, based on the dynamic value of elem_type, while the other one has to be statically determined based on a static type.
gramalingam(2020-11-10 19:27:07):This is worth pushing into the other utility (as commented in the above code), but I don't think it is ready yet. It works for the test-case. But there are issues like how to convert a float into a float16 value which need to be resolved for a fully-general solution.
jcwchen(2020-11-12 04:27:58):All updated. @askhade Thank you so much for all the helpful detailed reviews!
jcwchen(2020-11-12 18:42:00):I added a TODO list which would be helpful for the next release manager.
jcwchen(2020-11-24 00:48:40)::shipit: 
chinhuang007(2020-11-11 16:35:26):Is there any standard test scripts we can reuse for future release?
chinhuang007(2020-11-11 16:37:19):Howe is this communicated and concluded? Is there any github issue to track it?
jcwchen(2020-11-11 17:36:52):Create a typical onnx/onnxruntime usage scenario like this (Thanks to @askhade for providing this example):
```
import onnx
import numpy
import onnxruntime as rt
from onnxruntime.datasets import get_example
import numpy.random
 
example1 = get_example("sigmoid.onnx")
model = onnx.load(example1)
onnx.checker.check_model(model)
onnx.checker.check_model(model, True)
inferred_model = onnx.shape_inference.infer_shapes(model, True)
# maybe also test with onnx.version_converter
temp_filename = "temp.onnx"
onnx.save(inferred_model, temp_filename)
print(onnx.__version__)
 
print(rt.__version__)
sess = rt.InferenceSession(temp_filename)
 
input_name = sess.get_inputs()[0].name
print("input name", input_name)
input_shape = sess.get_inputs()[0].shape
print("input shape", input_shape)
input_type = sess.get_inputs()[0].type
print("input type", input_type)
 
output_name = sess.get_outputs()[0].name
print("output name", output_name)
output_shape = sess.get_outputs()[0].shape
print("output shape", output_shape)
output_type = sess.get_outputs()[0].type
print("output type", output_type)
 
x = numpy.random.random((3,4,5))
x = x.astype(numpy.float32)
res = sess.run([output_name], {input_name: x})
print(res)
```
It's an easy one so I hope the following release managers can improve it. Thanks.
jcwchen(2020-11-11 17:39:46):This happened to MSFT internally this time... but I think in the future creating a GitHub issue to track it would be a good idea, because all ONNX users can be aware of possible issues from there. 
chinhuang007(2020-11-11 17:56:14):Is it possible to add the release verification scripts into onnx git repo for reuse purpose?
askhade(2020-11-11 18:28:56):rel-* branches are automatically marked protected right? SO may not need this explicit step
askhade(2020-11-11 18:29:41):Once the pypi packages are ready the coda package publish is automatic right? You did not have to submit a PR right?
Can you update this to reflect what happened in this release
askhade(2020-11-11 18:30:04):Can you add your PR as an example
askhade(2020-11-11 18:31:34):Is this for github release? If so can you change "submit the release summary "to something like
Create release in github with the right tag and upload the release summary along with .tar.gz and .zip
askhade(2020-11-11 18:33:36):nit: reword

Similar to TestPyPI. In wheel-builder repo, merge `pypi-test` branch to main branch and create a new Release with main branch and tag to trigger Travis CI. This will automatically upload PyPI packages after successful CI run. 
askhade(2020-11-11 18:40:38):updating release version happens on master before release branch is created right? Can you move this step above Create release branch and mention this needs to happen in master
askhade(2020-11-11 18:41:20):Do you know what this means? 
askhade(2020-11-11 18:42:07):I don't think this is needed here. 
askhade(2020-11-11 18:44:31):nit: wording
In release branch update the version number in file [VERSION_NUMBER] to something like `1.x.0rc1` as release candidate for verification before finally using the targeted version number for this release.
askhade(2020-11-11 18:45:31):nit wording: produce wheels for Windows.
change wheel files to wheels every where
askhade(2020-11-11 18:48:34):Add your PR as an example PR
askhade(2020-11-11 18:50:51):Suggestion: This is for line 50

Since everywhere else for manual twine upload we are already using just the command line for adding url as well as username password we can change the twine upload command for sdist upload as well so that you can remove the step to update the .pypirc file
askhade(2020-11-11 18:52:25):I think we can simplify testing this by just testing in 1 env... I dont think testing on different platforms with muktiple protobuf versions adds much value here.
askhade(2020-11-11 19:12:39):Can you add the test matrix that we used for verification.
Something like this:

Python versions : Applicable python versions for the release
Protobuf versions : Latest protobuf version at the time of the release + protobuf version used for previous release
  | 3.5 | 3.6 | 3.7 | 3.8 | 
-- | -- | -- | -- | -- |
Linux |   |   |   |   |
Windows |   |   |   |   |
Mac |   |   |   |   |


askhade(2020-11-11 19:15:25):pytest should be able to test all the above mentioned basic functions. I don't it needs to be explicitly stated here.
askhade(2020-11-11 19:17:59):Change this to: 
Test with onnxruntime package
I suggest checking in the script you added below in onnx repo and linking it here saying test with ort package by running this script. 


remove the submodule section from here. this needs to be added near release branch creation
askhade(2020-11-11 19:19:52):change this to 
Create github issue in converters repo to have them test the pypi test package. and add the links to converter repos. 
askhade(2020-11-11 19:20:35):Add a step for merging release branch into master
askhade(2020-11-11 19:22:17):After this step add a step to have the ort folks test the release branch commit with ort
Create an issue in ort to update onnx commit in ort to the release branch commit and run all the CI and packaging pipelines. 
askhade(2020-11-11 19:22:28):remove this. 
askhade(2020-11-11 19:26:04):nit: Test PyPi package verification
askhade(2020-11-11 19:26:38):Test with ort and converters can be under Partner Validation
askhade(2020-11-11 19:27:23):This can be moved under Test step where you add instructions for tetsing the pypi test package
askhade(2020-11-11 19:28:09):nit: Upload to Test PyPi
jcwchen(2020-11-11 23:02:42):Yes, but `rel-*` is not a required release branch naming, right? So it might be safer to keep this section unless we set `rel-*` as a required branch naming.
jcwchen(2020-11-11 23:10:45):Actually I did submit a PR... I don't know why it did not render automatically this time. I am confirming with conda-forge people now and I will update here. Thanks.
jcwchen(2020-11-12 00:17:27):I guess it says something like the operators need to be consistent? but it also looks very confusing to me...
@chinhuang007 Do you have any idea what it is? If it does not make any sense, we should remove this line. Thanks.
jcwchen(2020-11-12 00:28:10):Good idea. I also considered about doing this... Just removed and updated. It should look clearer now. Thank you!
jcwchen(2020-11-12 00:30:39):Good catch... I agreed test in 1 env is enough for testing source distribution.
jcwchen(2020-11-12 00:55:40):I don't quite understand about this line... Is this something like the following sentence?
Create GitHub issues in converters repos to provide them the package links and have them test the TestPyPI packages. 
askhade(2020-11-12 01:01:04):right... and add the links to all the converter repos... you can also add an option to announce it on the converter channel on slack
askhade(2020-11-12 01:02:17):you can add a note here saying if it does not happen automatically then use this PR as an example and add your PR
askhade(2020-11-12 01:03:13):we should set rel-* as required naming (release pipelines like windows also has triggers for master and rel-*)

jcwchen(2020-11-12 02:34:32):Make sense. Updated in line#12:
Create a release branch (please use rel-* as the branch name)...
jcwchen(2020-11-12 18:47:11):Added. Thanks for the reminder
jcwchen(2020-11-12 18:50:04):Remove this line for now.
askhade(2020-11-18 00:50:49):what does this mean "or certain private repo under onnx."

Can change to Get onnx pypi account creds from steering committee or from previous release manager. 
askhade(2020-11-18 00:52:17):I meant can you move updating release number in all files listed below near line 11 (before creating release branch)
CLAassistant(2020-11-10 22:17:46):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=3105) <br/>All committers have signed the CLA.
jcwchen(2020-11-11 22:38:39):> * Make PYTHON_INCLUDE_DIRS public (and use the official output "S" form)
> * Quote directories to protect against directories with spaces

It seems that it only can find `Python.h` under `PYTHON_INCLUDE_DIR`... I encountered the same error as CIs in my local.
Even I used the latest commit from pybind11 (>2.6.0) but the problem still persists. @henryiii  Do you have any idea what happened? Thank you.

jcwchen(2020-11-11 22:58:49):Maybe because ONNX does not build pybind11 while installing? (It only uses pybind11 as headers) Therefore, the produced Python.h cannot be found from uninstalled pybind11 anyway?
henryiii(2020-11-12 02:01:47):CMake should _really_ be using targets (see [Modern CMake](https://cliutils.gitlab.io/modern-cmake/)), but this fixes the current issues (commit 1), then updates to 2.6.1 for the built in pybind11 (commit 2). 
henryiii(2020-11-12 13:52:35):PS: This is fine except for one failing check that seems to be unrelated.
jcwchen(2020-11-12 22:22:53):circleCI can be ignored because it will be deprecated soon. Originally the usage of pybind11 in CMakeLists.txt was quite outdated... @henryiii Thank you so much for the modification and explanation. One question: so the commit ID for pybind11 in this PR will be pybind11 official v.2.6.1 release commit, right? 
henryiii(2020-11-12 22:41:06):Yes, commit one only fixes the usage of variables. Commit 2 updates to 2.6.1. For either commit, they will still prioritize an "installed" pybind11 over the local one, and they will still allow 2.2+ to be found (I can obviously change that if you want).
henryiii(2022-03-29 02:05:22):Using 2.9.1, same changes as before.
garymm(2022-03-31 20:29:55):Please add `Fixes #3084` to the PR description.
henryiii(2020-11-12 01:56:35):Python has not been found yet, so this was doing nothing. `${PYTHON_INCLUDE_DIR}` is an empty (undefined) variable at this point.
henryiii(2020-11-12 01:57:12):Before 2.6 we did not provide a unified interface in both modes. Once the submodule is updated (next commit), this is simpler.
henryiii(2020-11-12 01:58:18):This path never found Python, so not sure how it was working...
henryiii(2020-11-12 01:58:45):These are already found as part of pybind11's Config or subdirectory modes.
henryiii(2020-11-12 01:59:47):Including the word CONFIG here gives a better message. You could also add QUIET, since it's okay if it's not found.
lgtm-com[bot](2020-11-11 17:13:08):This pull request **introduces 1 alert** when merging b21317c0b81d758393b1cef0d8048299d7f450e7 into b605a6be9bb7fb88c6c2e9a3455da81eddacbb56 - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-59fce1c24faae095565e1545cc5eb3de19d1f0c2)

**new alerts:**

* 1 for Module is imported with &#39;import&#39; and &#39;import from&#39;
lgtm-com[bot](2020-11-11 17:48:20):This pull request **introduces 1 alert** when merging d31757f025efa3064523b47ede379a528b121e37 into b605a6be9bb7fb88c6c2e9a3455da81eddacbb56 - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-9fb57f02850170bb7eaedac64c51af8fc103e074)

**new alerts:**

* 1 for Module is imported with &#39;import&#39; and &#39;import from&#39;
annajung(2020-11-11 18:10:30):looks like it doesn't work on Linux, closing
askhade(2020-11-11 18:21:37):@annajung : Linux CI is failing because of type check error:
onnx/helper.py:11: error: Name 'collections_abc' already defined
type check failed
(https://dev.azure.com/onnx-pipelines/onnx/_build/results?buildId=7251&view=logs&j=2105a913-e5a3-5f74-1c30-4f8b7a56f2db&t=c2afa4d8-3a4c-5e2d-1542-4328675067bd&l=1758)

One more thing... since we have deprecated support for python 2.7 for onnx packages we can go ahead and deprecate this in  CIs as well

annajung(2020-11-11 18:30:07):I think the problem is that my solution supports both python 2 and 3 which then is causing the error that `collections_abc` is already defined. 

and thank you for the information! I can remove the line for the second import for python 2 and hopefully, that will resolve it. Also, I can look into removing 2.7 tests from the CI! 
annajung(2020-11-11 22:16:50):Build errors are the following
```
onnx/helper.py:6: error: No library stub file for standard library module 'collections.abc'
onnx/helper.py:6: note: (Stub files are from https://github.com/python/typeshed)
```

According to [mypy docs](https://mypy.readthedocs.io/en/stable/running_mypy.html#missing-type-hints-for-standard-library-module),
```
If you are getting a “No library stub file for standard library module” error, this means that you are attempting to import something from the standard library which has not yet been annotated with type hints.
```

However, [typeshed]() does support Iterable as seen [here](https://github.com/python/typeshed/blob/2949a9289ebdf2d3af427f9b5f5a4d4dbd0005a0/stdlib/3/collections/abc.pyi#L14)

I will close out the PR and create an issue to get more feedback on how to approach this problem. 
askhade(2020-11-11 22:19:16):have you checked this: https://github.com/python/mypy/issues/3905

sorry did not mean to reopen the PR
annajung(2020-11-11 22:27:54):Hi @askhade, I did see the option to hide the error using `–ignore-missing-imports`, but I didn't think that was the right approach. Is this applicable here? I assumed that any error produced by this line `python setup.py --quiet typecheck` should result in a fail build. 
annajung(2020-11-11 23:23:17):looks like using the latest `mypy` worked with changing the import for `collections`. Maybe I can hold off on this until `mypy` version can be changed. I can also look into the effort required to change the `mypy` version if there are not objections :) 
annajung(2020-11-17 19:28:43):Closing, details here https://github.com/onnx/onnx/pull/3106#discussion_r525429910
askhade(2020-11-17 18:40:01):what is the intention behind this change?
pip install --no-use-pep517 -e .[mypy] --> will install onnx from current directory along with dependencies listed in the requirement group [] .i.e mypy in this case

whereas pip install mypy will install mypy



annajung(2020-11-17 19:28:23):Hi @askhade thanks for following up. sorry, I thought I closed out this pr.

The change here is a wip, just to test to see if the latest `mypy` would work with Linux builds. 
It's not the change I want to commit/merge. I will close out this PR and either clean this up or open up a new PR based on work https://github.com/onnx/onnx/issues/3110

Thanks!
askhade(2020-11-11 19:40:53):why remove this? The check is for !2.7
annajung(2020-11-11 19:55:18):ah thank you for pointing that out. misread the code, will update!
annajung(2020-11-11 19:56:37):well, actually since python version will never equal to 2.7, is the if statement still needed here?
annajung(2020-11-11 19:59:16):I will put back the logic, but no longer wrap it in if statement :) 
annajung(2020-11-11 20:22:09):Updated! PTAL when you can, thanks again for the review!
jcwchen(2020-11-12 18:06:36):Close because the rel-* branch in main repo is protected and cannot be merged with conflict... Sending another PR of same branch in my forked repo.
jcwchen(2020-11-12 18:04:51):Since the release was done, do we still need to run the release-CI for master branch?
askhade(2020-11-17 17:20:35):we should run the release CI for master branch pushes (not pull requests)... this is a good way of making sure the CI pipelines are running fine 
jcwchen(2020-11-17 18:01:25):Make sense. Updated. Thanks
BowenBao(2020-11-14 00:44:58):cc @gramalingam, @askhade, @spandantiwari, @postrational 
gramalingam(2020-12-02 00:28:44):LGTM thanks ... I had just one minor comment about one example.
gramalingam(2020-12-10 23:15:50):Any idea why the LGTM:C/C++ analysis CI is failing?
gramalingam(2020-12-10 23:16:58):@postrational @askhade : any further comments on this extension before we merge this in?
BowenBao(2020-12-10 23:33:48):> 
> 
> Any idea why the LGTM:C/C++ analysis CI is failing?

I checked that other PR which include C++ modifications also fail this test https://github.com/onnx/onnx/pull/3141/checks?check_run_id=1533692794 with the same error Failed with bad exit code during 'Checkout'. Not sure what's the root cause. 
gramalingam(2021-01-26 21:53:12):@BowenBao : sorry, it looks like some conflicts need to be resolved now before we can merge this
BowenBao(2021-01-27 20:50:40):> 
> 
> @BowenBao : sorry, it looks like some conflicts need to be resolved now before we can merge this

Will take a look
BowenBao(2021-01-28 00:36:47):@gramalingam rebased and all tests passed. Could you help merge the PR?
askhade(2020-11-16 23:09:27):this should be 13... this refers to the released opset version. 
BowenBao(2020-11-17 21:42:11):thanks, do i need to change line 886 above back to 13?
gramalingam(2020-12-02 00:25:02):Can we make the shape of 'x' (1,1,1) ? Otherwise, this example is not a valid example, since the input and output don't have same size. Better to avoid invalid examples in our test-suite.
gramalingam(2020-12-02 00:26:49):In the future, we could technically extend the shape-inference to reject such a test-case.
BowenBao(2020-12-10 19:51:51):Thx for catching, updated.
jcwchen(2020-11-16 23:43:23):```suggestion
git commit -m 'type your own commit msg' -s  # signoff that single commit
```
jcwchen(2020-11-16 23:43:34):Wrong character
prasanthpul(2020-11-19 19:13:22):updated
askhade(2021-01-13 19:15:27):Per offline discussion
this can be simplified to just provide an option to user to specify ops for which they want the data to be regenerated. Just this fix is not enough to resolve the current issue because the test generator creates the model for latest opset... therefore even though these diffs are fixed, the script will still modify a lot of models because the opset would have changed to the latest opset
jcwchen(2021-08-05 18:26:44):For now, just simply use --op_type to specify the target operator while generating data.
jcwchen(2020-12-08 04:31:08):Don't remove existing protos for the following comparison.
gramalingam(2020-11-23 17:27:24):"have" => "has"
jcwchen(2020-11-25 07:02:33):Besides removing these files, circleCI setting needs to be changed.
daquexian(2020-12-23 02:23:31):In this PR I also updated `replaceAllUsesWith` to propagate `sizes`, `data_type` and `unique_name` if needed.

Update: I also mark optimizer tests in this repo as skipped, since they don't align with the updated ir
daquexian(2021-01-07 02:48:55):@linkerzhang @jcwchen could you please merge this?
daquexian(2020-11-27 13:12:49):The names from outer scope are imported as a `kCaptured` node, https://github.com/onnx/onnx/blob/master/onnx/common/ir_pb_converter.cc#L277
jcwchen(2020-11-30 18:56:05):Maybe add some comments to explain what this function (`uses`) is?
daquexian(2020-12-23 02:19:14):Thanks! Updated.
jcwchen(2020-12-24 17:49:01):Is `getNextUnique` a const function? Did you move it to public because optimizer wants to use it?
daquexian(2020-12-24 23:22:20):> Is getNextUnique a const function?

I think it is not const because `next_unique_` may change

> Did you move it to public because optimizer wants to use it?

Yes. Since the methods like `addInitializer(Tensor initializer, std::string name)` is public, `getNextUnique` should also be public to get a unique name as the argument.
askhade(2021-01-08 17:47:29):I am confused why do you need a const and a non const version of these methods? cant the invoker const cast and directly call the non const version?
askhade(2021-01-08 17:54:50):use auto where applicable
askhade(2021-01-08 17:56:02):this can be moved above line 1238
daquexian(2021-01-09 01:26:34):For const `forSelfAndEachSubGraph` method, the argument of `fn` is `const Graph`, that means `fn` cannot (and should not) modify the graph. For the non-const version there is not such a constraint
jcwchen(2020-12-01 22:39:14):cc @hariharans29 for review. Thank you!
gramalingam(2020-12-01 23:57:25):I think that the best approach is to get the types of inputs from the model (read above) and use the types to determine how to load the data. See, for example, here: https://github.com/onnx/onnx/blob/b353e8ba02539ace81dcb762dc78ff4df27cd9b8/onnx/backend/test/cpp/driver/test_driver.cc#L208 
jcwchen(2020-12-02 16:30:04):I cannot find a good way to detect the type of TypeProto in Python so I use a sort of hacky way here...
jcwchen(2020-12-02 17:50:43):Makes sense. However, it's hard to match the model.input/output to protobuf files before knowing the right type and loading... So I assume the order of model.input/output is same as the order of input/output proto files. 
gramalingam(2020-12-02 18:40:51):I don't understand your concern. The code looks fine to me, since for the i-th input/output we generate the corresponding file name, and get the type from graph.input and graph.output.
gramalingam(2020-12-02 18:47:16):I think you can use ```model_type_proto.HasField("sequence_type")``` and, similarly, for the other cases. See https://developers.google.com/protocol-buffers/docs/reference/python-generated#oneof for more information.
hariharans29(2020-12-02 19:17:08):Do we want to leave a message in the else branch saying : loading proto of that specific type (Map/Sparse Tensor) is currently not supported ?
hariharans29(2020-12-02 19:17:36):Need to check-in tests to cover this function ?
hariharans29(2020-12-02 19:19:53):Just thinking out aloud - does this logic of iterating through input/output proto files handle optional inputs/outputs ? Let us say we have input_0.pb and input_2.pb (with input 1 optional), does it break then ?
jcwchen(2020-12-02 22:50:15):https://github.com/onnx/onnx/pull/3136#discussion_r534397071
Just like what @gramalingam said above, the input/output protobuf files are generated by the model.graph in the same order so I think it should be fine? If the input or output is optional, then it should not show in the model.graph either.
jcwchen(2020-12-02 22:51:40):Sounds good. Added. Thanks
jcwchen(2020-12-02 22:52:27):Good idea. Updated. Thank you
jcwchen(2020-12-03 00:03:50):Since it's an inner test utility function and it won't be used by others, it should be OK not to test it? Besides, any issue of it will be caught by the original tests anyway.
hariharans29(2020-12-03 00:49:43):A comment is nice, but I was suggesting printing something for the user
gramalingam(2020-12-03 01:37:37):(As discussed offline), it would be great to have a spec/design level discussion and documentation of the underlying issues and proposed resolution, since there seem to be quite a few concerns/issues relating to versioning here. 
askhade(2020-12-03 18:12:07):> (As discussed offline), it would be great to have a spec/design level discussion and documentation of the underlying issues and proposed resolution, since there seem to be quite a few concerns/issues relating to versioning here.

Agree. I created an issue to track this: https://github.com/onnx/onnx/issues/3139
gramalingam(2020-12-02 23:41:55):Should there be an error/exception here?
gramalingam(2020-12-02 23:50:16):Nit: The qualifier "OpSchema::" is unnecessary, I assume.
pranav-prakash(2020-12-03 23:33:15):Pipeline failure seems to be due to diff in onnx operator docs, which is expected as we update the types. Not sure if any action is needed on my end for that.
askhade(2020-12-10 18:35:49):> Pipeline failure seems to be due to diff in onnx operator docs, which is expected as we update the types. Not sure if any action is needed on my end for that.

Operators doc should not be manually updated it is auto generated. Please use 
onnx/backend/test/stat_coverage.py
ONNX_ML=0 python onnx/defs/gen_doc.py
to generate these files

pranav-prakash(2020-12-10 22:04:23):Done, auto generated docs have been updated.
askhade(2021-01-08 17:27:35):@pranav-prakash : please sign the DCO and resolve conflicts with master
pranav-prakash(2021-01-08 21:45:20):Done
askhade(2020-12-11 00:10:46):CIs are failing because you manually updated some auto generated files... 
You will have to update and run the following scripts 
For TestCoverage.md --> onnx/backend/test/stat_coverage.py
For Operators.md and ChangeLog.md --> onnx/defs/gen_doc.py

And then run 
python onnx/backend/test/stat_coverage.py
python onnx/defs/gen_doc.py (generates the ml versions of the doc)
ONNX_ML=0 python onnx/defs/gen_doc.py 
szha(2020-12-16 22:58:02):@askhade any idea what causes the windows pipelines to fail?
szha(2020-12-21 16:30:26):Finally passed. It turns out the problem was due to the newline format in `.bat` files.

cc @onnx/sig-archinfra-approvers @onnx/sig-operators-approvers @onnx/steering-committee 
prasanthpul(2021-01-07 18:34:52):This PR updates dynamically generated files. Instead of updating those files directly, the script(s) that generates them should be updated. Otherwise checks will fail later on.

@askhade is it just operators.md and operators-ml.md? or are there other files too?
szha(2021-01-07 19:03:27):I did update [gen_doc.py](https://github.com/onnx/onnx/pull/3159/files?file-filters%5B%5D=.py#diff-97f1e96d2673b6d783466c78c3fc625b407a252f4971c766c26946477b3d2929) and [stat_coverage.py](https://github.com/onnx/onnx/pull/3159/files?file-filters%5B%5D=.py#diff-8c9869d456aaf3eb6a715eea4ad032e175d47c33d4e1bdd7dcc3b2eb95c0f8fe) and the updated related `.md` files are direct outputs from them.
prasanthpul(2021-01-07 20:59:40):@szha  thank you!!
I missed it
jcwchen(2020-12-18 00:20:49):Is it a "shape" inference? I thought using ONNX_NAMESPACE::InferenceError is more proper
gramalingam(2021-01-07 21:11:25):Is this intended to cover both shape-inference errors as well as type-errors? Wondering why module is called "shape inference", while the exception is plain "InferenceError".
askhade(2021-01-12 21:20:19):fail_shape_inference throws a InferenceError... not sure what you mean
askhade(2021-01-12 21:21:08):yes both module and exception class are intended to handle both type and shape inference.... I am simply registering the exception here... 
jcwchen(2021-01-12 22:11:28):Yes eventually they will throw the same error. Just thought this one is actually not a "shape" inference error so directly using `fail_shape_inference` looks a little weird to me.
gramalingam(2021-01-14 23:43:07):I wonder if making strict_mode into an integer (called mode, restricted to values 0 and 1 currently) will give us more flexibility in the future. For example, may be we could treat it as a compatibility_mode indicator indicating that we desire the behavior in version 1.7 or 1.8, etc. In the future, we could potentially end up with more than 2 modes from a compatibility-mode perspective.
askhade(2021-01-15 20:21:20):I am leaning towards still keeping the strict_mode as true\false in python and change the one in c++ interface to int... when we add more granularity we can still keep the python api for backward compatibility and add a new api in python for the new approach

gramalingam(2021-01-04 19:29:11):Minor nit: for the changes in defs.cc, it would be helpful to remove the clang-format changes of existing code (I think it is helpful to have pure formatting changes as a separate PR).
gramalingam(2021-01-04 19:25:33):This is okay for this PR. But just wanted to point to the issue discussed here: https://github.com/onnx/onnx/issues/2324 ... in the long run, this is going to be limiting because this does not allow for sequences of sequences of tensors or maps, etc. It would be useful to extend this as discussed in the issue above. Any thoughts about this?
gramalingam(2021-01-04 19:35:12):It would be more general to move this if-then-else logic into "propagateShape". For example, then it would work for sequence(sequence(tensor(...))) as well.
gramalingam(2021-01-26 21:50:35):@BowenBao : does the above make sense? Thanks!
BowenBao(2021-01-27 20:51:16):@gramalingam sorry for the late reply, will take a look and make updates accordingly.
BowenBao(2021-01-27 22:21:16):updated
yufenglee(2020-12-16 00:37:02):Are there any test cases that need to be updated? 
askhade(2020-12-16 00:49:22):> Are there any test cases that need to be updated?

Done
jcwchen(2020-12-16 00:57:54):Actually I don't see it has been used. Not sure why it is here.
wschin(2020-12-16 01:06:13):Maybe we should clean `tensor.raw_data` as well because it's being set to external
gramalingam(2020-12-16 04:54:06):Shouldn't multiple tensors be stored at different offsets within the same file? I don't see any offsets here. How does this work? (Alternatively, if that feature is not being used, may be we can remove it for now until we can add a proper version.)
gramalingam(2020-12-16 04:59:53):Could we just call save_model as the last step of this function and rename it "save_model_with_external_data" or something like that? What is the value in splitting it into two different calls? 
askhade(2020-12-16 05:57:13):I don't know why this was split in 2 calls... I am planning to add another method which will do all this in a single call. For now let's keep this method because there are partners who have a dependency on this and I don't want to change this before prior notification.  
askhade(2020-12-16 06:00:32):This method simply marks the tensors to use external data. The offsets are properly added when the data is actually written to the file. See save_external_data line 175

I tested writing all tensors to 1 file and it works.
askhade(2020-12-16 06:23:47):right now converting a model to external data is done in 2 steps. In the first step tensors are simply marked to use external data and in the second step the data is actually serialized. This method is called in both the steps. So we cant simply clean the raw_data here... Moreover convert_model_from_external_data also checks for raw_data filed which I am not sure why... I am planning to address this in another PR along with adding a method to convert model to external data in 1 step.
gramalingam(2020-12-16 21:30:29):This is one reason to expose a single method that does both things. Otherwise, we have a fragile dependence between the two parts, with the model being in a weird non-standard representation in between the two steps ... if the user ends up doing other things in between the two steps, the potential for bugs increases.
askhade(2020-12-16 21:46:23):yes I agree... we should have a single method to do this... I will cover it as part of a new PR
askhade(2020-12-17 16:17:36):why do we need a default here? I am not convinced we need this
jcwchen(2020-12-18 01:08:12):After discussion, the original behaviour in ONNX is correct. Instead, ORT should specify what the relied_opsets are for the function op. Close this PR now.
CLAassistant(2020-12-17 21:58:59):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=3178) <br/>All committers have signed the CLA.
jcwchen(2020-12-18 02:58:15):Hi @Adeykin,
Thank you for the contribution! Could you add a test like this: https://github.com/onnx/onnx/blob/37e84915c5426a779994b39f8f44ee5e674d99ee/onnx/test/version_converter_test.py#L958
jcwchen(2020-12-18 03:24:44):Is 2 here correct? I thought Upsample-1 (https://github.com/onnx/onnx/blob/master/docs/Changelog.md#upsample-1) needs 4D input tensor?
Adeykin(2020-12-18 21:23:21):You are right. 4D is easier. Fixed.
snnn(2021-01-22 23:04:14):Please don't forget to update https://github.com/onnx/onnx/blob/master/setup.py#L300 .
Change it to "numpy>=1.16.6".  (It can be in a separated PR).

snnn(2020-12-30 23:11:22):nit: this is the number of logical processors, not cores. 
snnn(2020-12-30 23:12:40):I suggest you may consider to add 3.9 there.
snnn(2020-12-30 23:16:02):You may consider to install protobuf python package from source. Otherwise you should be careful about the versioning. 
Here you will get the latest protobuf python package, in most of the cases, it's fine. Unless you got a newer version of protoc and libprotobuf C/C++ library.
snnn(2020-12-30 23:25:35):So, it will use a docker image built from https://github.com/ralfg/python-wheels-manylinux-build ? And we have no control on that, we don't know how often the image is updated? It is ok, but I'd rather not taking such a dependency unless it helps a lot. 
jcwchen(2020-12-31 00:21:03):Yes you are right. I am doing some experiment with that docker first. Actually many dependencies in that docker need to be updated so using our own docker image would be a better solution as you suggested. Now I am trying to make it work first then will decide what to do here. Thank you!
jcwchen(2020-12-31 00:22:07):Yes... Some of these libraries are not the latest ones and they need to be updated manually...
snnn(2020-12-31 05:42:12):No, you shouldn't change gcc. If the default one in the manylinux docker image doesn't work, it must because you did something wrong. 
jcwchen(2020-12-31 05:55:45):Actually the purpose of installing gcc here is to get the latest `libstdc++.so.6`.
While installing 3.11.3 protobuf, it encounters the following error:
```
2020-12-31T04:58:13.8924662Z  You could try using --skip-broken to work around the problem
2020-12-31T04:58:13.8925459Z            Requires: libstdc++.so.6(GLIBCXX_3.4.20)(64bit)
2020-12-31T04:58:13.8926555Z Error: Package: protobuf-3.11.2-2.el8.x86_64 (/protobuf-3.11.2-2.el8.x86_64)
2020-12-31T04:58:13.8927400Z            Requires: libstdc++.so.6(CXXABI_1.3.8)(64bit)
2020-12-31T04:58:13.8928777Z Error: Package: protobuf-compiler-3.11.2-2.el8.x86_64 (/protobuf-compiler-3.11.2-2.el8.x86_64)
2020-12-31T04:58:13.8929927Z            Requires: libstdc++.so.6(GLIBCXX_3.4.21)(64bit)
2020-12-31T04:58:13.8931307Z Error: Package: protobuf-compiler-3.11.2-2.el8.x86_64 (/protobuf-compiler-3.11.2-2.el8.x86_64)
2020-12-31T04:58:13.8932580Z            Requires: libstdc++.so.6(CXXABI_1.3.9)(64bit)
```
The original old libstdc++.so.6 does not have such things so I thought I need to get a newer libstdc++.so.6. To do that in CentOS, I only know I can get it from building gcc... `yum install` certain version of libstdC++ will encounter some issues like recursive dependencies failure. Do you have any suggestion for it? Thank you.
snnn(2020-12-31 22:03:04):The link you have is for CentOS 8, but the docker image is based on CentOS 6, so it definitely won't work.
see:
cbs.centos.org/kojifiles/packages/protobuf/3.11.2/2.el8/x86_64/protobuf-3.11.2-2.**el8**.x86_64.rpm

Why don't you just compile protobuf from source? It would be much easier than building gcc.

snnn(2021-01-01 03:36:42):https://github.com/protocolbuffers/protobuf/blob/master/src/README.md#binary-compatibility-warning

 --disable-shared and enable PIC.
snnn(2021-01-01 03:38:03):CFLAGS="-fPIC -g -O2" CXXFLAGS="-fPIC -g -O2" ./configure --disable-shared

jcwchen(2021-01-01 05:30:07):Building protobuf from source is a good idea. Thank you @snnn. Happy New Year!!
snnn(2021-01-04 22:02:44):No, don't install python-devel. All you need should be provided by manylinux. Don't get such changes from the system packages. 
snnn(2021-01-04 22:05:18):It depends on which cmake version you want to use.
For older cmake versions, please see how onnxruntime is handling how.  https://github.com/microsoft/onnxruntime/blob/master/tools/ci_build/github/azure-pipelines/templates/set-python-manylinux-variables-step.yml

For newer ones, please read https://gitlab.kitware.com/cmake/cmake/-/issues/20425
jcwchen(2021-01-05 04:52:02):https://github.com/pybind/pybind11/issues/1728
pybind11 needs such library (Python.h) which centOS doesn't have initially... I have tried to fix it from pybind11 side but failed. Do you have any idea about how to fix this without installing python-devel? Thank you!
jcwchen(2021-01-05 04:54:10):https://github.com/onnx/onnx/pull/2961
Apply this PR: Make onnx build not always depend on python2
Originally centOS will use python2 even the python_version is 3.
snnn(2021-01-06 09:20:19):it ends with manylinux2010_x86_64.whl
snnn(2021-01-07 01:03:09):Replied below. 
jcwchen(2021-01-07 05:25:34):Do you mean here: `It depends on which cmake version you want to use...` ?
But after some experiments, either using `find_package(PythonInterp` or `find_package(Python` cannot resolve the error as follow:
```
/root/onnx/third_party/pybind11/include/pybind11/detail/common.h:112:20: fatal error: Python.h: No such file or directory
 #include <Python.h>
          ^~~~~~~~~~         
```
I also can repro this in my local CentOS. It seems to me that centOS does not have such library and it needs to be installed from `python-devel`...
snnn(2021-01-07 17:35:39):You need to set numpy version as well.

If a package was built with numpy version A, then at runtime it requires numpy version >=A.

jcwchen(2021-01-07 22:37:43):Do you have any recommended version? I don't see any numpy requirement in ONNX so I don't know which version is appropriate...
snnn(2021-01-08 02:29:34):Yes. Please check how onnxruntime do this.
jcwchen(2021-01-10 01:05:58):Use `numpy==1.16.6` here for building wheel, which is same as what ORT is using
snnn(2021-01-10 05:27:50):I don't have. But you may ask the people in sigs. 
snnn(2021-01-22 23:02:28):Is there a plan to add 3.9?
jcwchen(2021-01-22 23:07:48):Since it's a patch for ONNX 1.8, we will stick the same supported version as ONNX 1.8. I think Python 3.9 would probably be added in ONNX 1.9. Thanks.
askhade(2021-01-27 05:04:48):nit: not needed
askhade(2021-01-27 05:05:32):for 1.8 did we build linux packages with lite proto ON?
jcwchen(2021-01-27 15:37:01):We didn't. Thanks for the reminder.
jiafatom(2020-12-29 04:48:03):Move to [PR](https://github.com/onnx/onnx/pull/3188), because this one has DCO issue.
askhade(2021-01-04 21:33:25):please add relevant backend tests for this change
yufenglee(2021-01-04 22:20:55):Why do you need to bump up the version? It looks to me that this PR makes the spec more clear and doesn't have a substantial change.
jiafatom(2021-01-04 22:32:56):> Why do you need to bump up the version? It looks to me that this PR makes the spec more clear and doesn't have a substantial change.

There is an internal discussion for this issue, and the current agreement is to bump up the version. You can find it in the email thread.
In short, the current doc is `SAME_UPPER or SAME_LOWER mean pad the input so that the output spatial size match the input.`
The word `match` is vague and usually means `equal`. And current onnxruntime implements the way `equal`. My change need an adjustment by multiplying/dividing the strides - so there will be an onnxruntime change accordingly.
yufenglee(2021-01-04 22:48:27):> > Why do you need to bump up the version? It looks to me that this PR makes the spec more clear and doesn't have a substantial change.
> 
> There is an internal discussion for this issue, and the current agreement is to bump up the version. You can find it in the email thread.
> In short, the current doc is `SAME_UPPER or SAME_LOWER mean pad the input so that the output spatial size match the input.`
> The word `match` is vague and usually means `equal`. And current onnxruntime implements the way `equal`. My change need an adjustment by multiplying/dividing the strides - so there will be an onnxruntime change accordingly.

ORT implements it as ceil(input/stride):
https://github.com/microsoft/onnxruntime/blob/9ec1ed42a809170b87474f5822c4557101812399/onnxruntime/core/providers/common.h#L91
yufenglee(2021-01-04 22:51:37):> > > Why do you need to bump up the version? It looks to me that this PR makes the spec more clear and doesn't have a substantial change.
> > 
> > 
> > There is an internal discussion for this issue, and the current agreement is to bump up the version. You can find it in the email thread.
> > In short, the current doc is `SAME_UPPER or SAME_LOWER mean pad the input so that the output spatial size match the input.`
> > The word `match` is vague and usually means `equal`. And current onnxruntime implements the way `equal`. My change need an adjustment by multiplying/dividing the strides - so there will be an onnxruntime change accordingly.
> 
> ORT implements it as ceil(input/stride):
>https://github.com/microsoft/onnxruntime/blob/9ec1ed42a809170b87474f5822c4557101812399/onnxruntime/core/providers/common.h#L91

Agree that match is not very clear here, but it doesn't mean 'equal' or it will use 'equal'. I don't think any runtime implementation is so careless to implement it as output[i] = input[i]. If so, they will hit mismatch for model with this setting.
jiafatom(2021-01-05 00:14:19):> Agree that match is not very clear here, but it doesn't mean 'equal' or it will use 'equal'. I don't think any runtime implementation is so careless to implement it as output[i] = input[i]. If so, they will hit mismatch for model with this setting.

Some models are get affected. I have posted an issue for this in onnxruntime, see [here](https://github.com/microsoft/onnxruntime/issues/6067), thanks.


gramalingam(2021-01-05 21:20:55):I agree that the change is best seen as a clarification of the existing spec, so we can avoid bumping up the op version.
askhade(2021-01-06 17:22:27):The PR looks good to me... @jiafatom please add some backend test for validation
jiafatom(2021-01-06 18:43:50):> The PR looks good to me... @jiafatom please add some backend test for validation

@askhade Thanks for your comments. It looks that the current added test (shape inference test) already covers this PR. 
(1) This PR is mostly a clarification, it does not change any current behavior of these operators.
(2) This PR is about auto_pad 'SAME' on how much padding we need. It does not change the way how Conv computes (which is covered in other backend tests, and not related to this PR). The added shape inference test is the way to test whether the output_shape is correct. Then it is exactly the PR about. So the added shape inference test is already enough.
(3) If we talk about backend tests -- (a) the real model issue is about dynamic input shape, but onnxruntime would not rely on onnx shape inference for this case. It has its own logic to do it on the fly. So the backend tests still cannot cover the real model issue here. (b) if we say static input shape, it is not used in real models, because converters will convert using VALID attr for this case, not SAME_X.
Please let me know if you have further questions. Thank you.

gramalingam(2021-01-06 19:02:19):@jiafatom : the shape-inference tests and backend tests serve different purposes. Backend tests are meant to test other backends (like ORT) with respect to the ONNX spec (and the shape-inference tests are not used for that purpose). The existing backend tests do not have a test-case to cover the auto-pad cases, so adding them would be useful and valuable. In fact, such a test-case would have caught this exact bug if we had already had such a test-case.
jiafatom(2021-01-06 19:11:50):> @jiafatom : the shape-inference tests and backend tests serve different purposes. Backend tests are meant to test other backends (like ORT) with respect to the ONNX spec (and the shape-inference tests are not used for that purpose). The existing backend tests do not have a test-case to cover the auto-pad cases, so adding them would be useful and valuable. In fact, such a test-case would have caught this exact bug if we had already had such a test-case.

@gramalingam As you see, auto_pad=SAME_X case only happens in dynamic input shape, but I only see static input shape in the backend tests for conv, conv_transpose (on onnx side). Adding a test for static input shape would not cover any real model cases here (when auto_pad=SAME_X). Seems that adding this test does not give benefits: it does not have use case, and does not cover the issue we encounter in ORT. For this case, it seems more reasonable to add test in ORT.


gramalingam(2021-01-06 19:30:34):The spec does not constrain the use of attributes to dynamic shape cases. Whether the shape is static or dynamic should not be relevant to the spec. My guess is that you are considering the use-cases that arise from the convertor, but the spec is broader and test-cases should cover cases permitted. We are adding this clarification because of a bug in ORT's convtranspose implementation. Why is there no test-case in the ONNX backend tests that would have caught this bug?
jiafatom(2021-01-06 22:47:41):> The spec does not constrain the use of attributes to dynamic shape cases. Whether the shape is static or dynamic should not be relevant to the spec. My guess is that you are considering the use-cases that arise from the convertor, but the spec is broader and test-cases should cover cases permitted. We are adding this clarification because of a bug in ORT's convtranspose implementation. Why is there no test-case in the ONNX backend tests that would have caught this bug?

@gramalingam I totally understand your point from spec perspective. Adding backend test is a burden for developers because of the ORT ConvTranspose bug: The backend test itself relies on ORT implementation, now I need fix ORT code to verify this change. Like I mentioned, adding test in ORT side is more convenient and also cover this. Just take a look at the two recent PRs that get committed (one PR bump onnx version, the other changes shape inference logic):
https://github.com/onnx/onnx/pull/3195
https://github.com/onnx/onnx/pull/2549
There are no backend tests added there. May I know what is the rule for adding backend test in general? Thank you!
gramalingam(2021-01-07 03:53:54):@jiafatom : Thanks, it looks like you were able to add the test-case anyway? As for the other two PRs you mention: one was fixing a bug in shape-inference (a previous bug fix which was unintentionally not duplicated for the older version of the op), but not changing/clarifying the spec in anyway; the second was adding new types, but not changing/clarifying the spec. The point was just that a test-case is useful for this op for this particular scenario. I understand your point about the difficulty of generating test-cases. If test-cases are difficult to generate, we can split the PR and do the test-case separately as needed.
jiafatom(2021-01-07 04:34:37):@askhade @gramalingam
Thank you very much for your comments, I added the related backend tests as you suggested.
When I add a backend test for ConvTranspose, I find a shape inference bug in ConvTranspose, it is in a very special case when `auto_pad=SAME_X && stride>1 && residue > 0`, so the previous tests do not catch it. The last change on this shape inference logic was written in 6/2019 [here](https://github.com/onnx/onnx/pull/2068), where the core logic was written even earlier. It is very difficult to resolve this bug, I spent several hours to fix this, and include the fix in this PR, and all tests passed.
Thanks again for the comments.
askhade(2021-01-07 04:42:01):LGTM

Regarding your comments on backend test - the idea is when anyone makes an update to an existing op or proposes a new OP, the update or the new OP should be done following some popular framework and should be model driven. If these 2 conditions are meant then it should not be very difficult to generate this test data.

If these test cases existed already then it would have saved us a lot of time because the bug in ORT would have been caught by these tests. 
jcwchen(2020-12-31 06:06:42):For this opset bump, do you only modify the document? Or you will make other updates? Thanks.
jiafatom(2020-12-31 06:41:12):(1) I modify the document (2) I added two shape inference test for Conv-14 and ConvTranspose-14. That is all.
For these two shape inference test, it already works well without any change on shape inference.
This means that the shape inference already has this Conv-14 and ConvTranspose-14 behavior.
Previously we don't have shape inference test for this scenario, so there was no validation previously.
askhade(2021-01-04 21:42:35):If behavior of Conv is changing then the behavior for corresponding quantization ops should change too
askhade(2021-01-04 21:55:05):change name to conv_auto_pad_doc ... 14 is not required here... this file contains schema for the latest version of ops... 
gramalingam(2021-01-06 18:48:29):Suggest changing the ```In case of odd number''' sentence as below for readability:
```
The padding is split between the two sides equally or almost equally (depending
on whether it is even or odd). In case the padding is an odd number, the extra
padding is added at the end for SAME_UPPER and at the beginning for SAME_LOWER.
```
gramalingam(2021-01-06 18:49:10):Same change below also
linkerzhang(2021-01-08 02:33:36):May I know the motivation of introducing the text based syntax and associated parser please? Is this for model visualization? Thank you very much!
gramalingam(2021-01-08 06:07:12):@linkerzhang : as mentioned in the above description, there are at least two motivations. One is that this serves as an extension of https://github.com/onnx/onnx/blob/8e99dcff98f32ab30fa31ec1fdbdc5eb0df11f04/onnx/defs/function.h#L29 to help simplify the definition of function bodies. Currently, it is too verbose. Second, it will help write test-cases the generate models fairly easily. For example, the value of this can be seen in how test-cases are written in MLIR/LLVM, where input and output are described in text.
gramalingam(2021-02-10 18:49:56):@linkerzhang : do you have any further comments or feedback? Does this make sense? Thanks!
askhade(2021-02-16 16:47:23):overall looks good to me... can you add a document describing the text syntax with some examples for writing test cases, functions etc...
lgtm-com[bot](2021-03-17 05:58:38):This pull request **introduces 1 alert** when merging 04e85ed56a92ebd80f0b052f83d558e9375e532b into 03e6c8f2ad599dda614a22ee494e982ffd0bccc7 - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-e84444a9f4d697f078737a9bc627ac2ec42bf023)

**new alerts:**

* 1 for Empty branch of conditional
postrational(2021-03-26 22:34:30):We'll need a review from someone in `onnx/sig-archinfra-approvers`
gramalingam(2021-03-27 00:25:11):@askhade : do you have any further feedback on this? Thanks!
askhade(2021-02-16 14:25:40):Is this needed ? Can you use data_type_utils instead?
askhade(2021-02-16 16:24:11):const auto& ?
askhade(2021-02-16 16:24:51):Case where attrproto contains a graphproto or tensorproto
askhade(2021-02-16 16:38:03):should this be supported x=0.625f ?
askhade(2021-02-16 16:41:58):can you also add checks to verify the type and shape of inputs and outputs
gramalingam(2021-02-18 19:52:55):I think this should be fine for a primitive type like int/float. There is no efficiency gain using a reference to int/float vs using the actual value.
gramalingam(2021-02-18 19:57:49):For other cases, like strings (or not-yet-handled tensors/graphs), I agree a const-reference should be used.
gramalingam(2021-02-18 20:20:24):Good question. At this point, attributes support only int or float. So, we can use decimal points to distinguish between the two, and I think that is sufficient. (If we ever have multiple precisions for floats in attributes, we could extend this, but I don't see any demand for that now.)
gramalingam(2021-02-18 20:22:27):But for tensors, it could make sense. Depends on the syntax for tensors (whether the type should be specified separately or not). May be worth thinking about.
postrational(2021-03-24 08:06:35):Should these methods be part of the `public` interface?
gramalingam(2021-03-24 22:39:04):I made some of them private methods, and left some of them as public. In general, I think code constructing parts of a model are common. Not all users are going to construct a complete-model in one go. So, I made what seems a reasonable classification for now. Thanks!
tomdol(2021-03-25 14:22:31):This macro looks kina hacky. You can achieve the same result by using a variadic template or some other construct that will be type safe.
tomdol(2021-03-25 14:24:07):I understand that usage of this macro lets you avoid the if-statements at the call site but this is not a very modern-cpp approach. You could even switch to exceptions instead.
tomdol(2021-03-25 14:26:38):In situations like this it is preferred to use `++line` rather than `line++`. The same applies to the for-loop statements increasing a counter/pointer.
mbencer(2021-03-25 16:18:46):Saving uinit64_t as int64_t can cause potential numeric overflow
gramalingam(2021-03-25 17:30:25):Done. But it is a pity if a compiler cannot optimize this in this day and age :)
gramalingam(2021-03-25 17:32:11):Yes, but ONNX also supports a build-option to build without exceptions. All things considered, it seems better to avoid exceptions for parser-errors.
gramalingam(2021-03-25 17:32:33):Replaced.
gramalingam(2021-03-25 17:34:57):Are you referring to MakeString? I am reusing what's being used elsewhere in ONNX. The outer macro was just to hide whether exceptions or status is being used to handle errors. The original implementation did use exceptions, but was switched to use Status due to the overall ONNX goals.
tomdol(2021-03-25 18:35:24):No, I meant the whole outer macro. Inside it has this construct with do..while(0), which I've seen many years ago and it was used to adress some specific problem but in a really hacky way. Can this just become a variadic template that builds an error message?
gramalingam(2021-03-26 00:43:25):Done.
postrational(2021-03-26 13:10:37):I'm still not sure why we don't hide all of these `Parse(...Proto&)` methods as `private`
gramalingam(2021-03-26 20:40:20):(a) My experience has been that developers construct the different kinds of objects (such as a TensorShapeProto, or TypeProto) frequently and a convenient way to construct them is useful.
(b) Some of the function-definitions require constructing a graph in which some parts are dependent on context-information (like an input type T); the flexibility exposed by the above interface will make this easier.
(c) A more minor point is that it is convenient to have unit-test-cases that test parsing of the different components. (We could possibly address these using friend-classes, but given the other reasons, the above seems simpler.)
In summary, I think the advantages of the above outweigh the disadvantages (if any). (Of course, we can revisit this later, but unless we expose this functionality, there is no way for users to test it or try it out and give feedback.)
gramalingam(2021-01-05 20:46:28):This looks good. Will some predefined-list like ```numeric_types_for_math_reduction_with_bfloat()``` work? That will help make sure we don't miss this op if a new type is added in the future. 
gramalingam(2021-01-05 20:47:42):Also, updating description string below will be good (since not all tensor types are allowed).
tianleiwu(2021-01-06 00:32:58):Good suggestion. I've updated it.
daquexian(2021-01-09 01:47:33):What "some issues" means is just some minor code style issues? I don't think it is a good reason to revert a pr. However, I will submit a new PR after addressing the comments.

Frankly speaking it is not a good experience for me, especially when I see this PR was merged in hours and #3131 had to be merged after two-week pushing (in slack) and waiting.
jcwchen(2021-01-09 04:32:32):Hi @daquexian,
Please understand we just came back from the holiday season so our response was late. We value your opinion and strive to make the experience better. Thank you for your contribution.
daquexian(2021-01-09 06:46:34):> Please understand we just came back from the holiday season so our response was late.

@jcwchen Thanks for your updating. I'll submit a new PR after addressing the comments. Frankly speaking, not only #3131, PRs from community contributors are sometimes forgotten by reviewers if the author do not remind reviewers actively (an example: https://github.com/onnx/onnx/pull/2923). Hope it can be better :)
buddhapuneeth(2021-01-15 00:59:12):@jcwchen can you look into this PR? This is fix for the issue mentioned in the ticket https://github.com/onnx/onnx/issues/3123.
jcwchen(2021-01-15 16:30:16):Hi @buddhapuneeth,
Thank you for proposing this PR. Could you add a simple test in `onnx/test/shape_inference_test.py`?
@onnx/sig-operators-approvers  please review this PR. Thanks!
postrational(2021-01-20 16:45:14):Please add a test to `onnx/test/shape_inference_test.py` as @jcwchen mentioned. Otherwise this looks good to me.
gramalingam(2021-04-13 20:14:22):Closing this as this is covered by PR #3364 
gramalingam(2021-01-20 18:02:08):I think that we should handle the case where the input rank is not known. Check ```if (hasInputShape(ctx,0))``` before calling ```getInputShape(ctx,0)```
gramalingam(2021-02-18 19:10:25):I made the suggested change. If someone could take a look, it would be helpful, thanks!
annajung(2021-01-12 22:53:19):Still failing on Linux builds. Not quite sure how to fix this, will leave it for someone else to pick up.
szha(2021-01-14 18:13:02):@askhade I found that there are some generated `.proto` files that need update (e.g. `onnx/*.proto` and `onnx/*.proto3`)

how do I update them?
gramalingam(2021-01-14 21:08:33):@szha : I think you need to run ```python onnx\gen_proto.py```. Now, it has some options that control the various combinations of files generated. I am not sure about all the combinations that are required, but it looks like the CI settings use ```python onnx/gen_proto.py -l``` and ```python onnx/gen_proto.py -l --ml```
szha(2021-01-15 22:46:55):All done!
askhade(2021-01-27 05:43:07):why return const string? this is a utility method to join to strings 
askhade(2021-01-27 05:47:48):this check can be simplified something like
if(origin.find_last_of(k_prefferred_path_separator) == origin.length() - k_preferred_path_separator.length()) {
return origin + k_preferred_path_separator + append ;
}
return origin + append

this way you can avoid creating multiple intermediate strings
askhade(2021-01-27 05:48:24):can this mtd be inlined?
askhade(2021-01-27 05:48:54):auto?
askhade(2021-01-27 05:55:59):!= 0 is a better check
also does this work properly on windows?
jcwchen(2021-01-27 21:52:14):Removing const function makes more sense. 
It cannot be inlined.
jcwchen(2021-01-27 21:53:11):Good idea. Updated.
jcwchen(2021-01-27 21:53:43):Yes it works well on Windows.
gramalingam(2021-01-27 23:41:06):Is that supposed to be == or != ? 
jcwchen(2021-01-27 23:43:37):Good catch. It should be !=. Thank you.
askhade(2021-01-27 23:49:43):why are you making a copy of origin string and the one to append. This should be const ref
askhade(2021-01-27 23:50:43):once you make origin const ref this wont work ... you can simply return origin + path_separator + append here
jcwchen(2021-01-28 00:19:42):Makes sense. Updated.
matteosal(2021-01-15 21:00:16):@prasanthpul @askhade @wschin @postrational this took way more than it should have, but now it should be ready. Can someone have a look please?
gramalingam(2021-01-26 21:36:56):I think that the default-value should have same behavior as previous version (giving us better compatibility). Shouldn't that be the value 0?
gramalingam(2021-01-26 21:41:00):A question about the shape of the "last" output: should batch-major affect that shape too? Should the "last" be [batch_size, num_directions, hidden_size] or [num_directions, batch_size, hidden_size]? It seems natural to expect "last" to have the same shape as the output, with seq_length omitted. So, just want to make sure that this is as expected (or as in other frameworks).
matteosal(2021-01-28 15:15:18):Right, this was a mistake. Initially the attribute was called `time_major`, so its default value did replicate the old behaviour. Then I've changed it to `batch_major` but didn't update the default here (but only in the shape inference). Fixed.
matteosal(2021-01-28 16:23:50):Right, good point. I agree to let `batch_major` control the shape of the initial/final states as well. I have implemented this change
gramalingam(2021-01-28 17:52:11):Just as a sanity check: is this consistent with other frameworks that use batch_major? Or, is it irrelevant because the other frameworks encode "num_directions" differently? Just hoping anyone who has concerns with the proposed spec will speak up!
gramalingam(2021-01-29 19:11:21):To clarify: I find the presence of num_directions in-between seq-length and batch-size a bit odd. Of course, this was a feature of the original ONNX spec. I am trying to understand whether there was a reason for this, in terms of either making the implementation easier or compatibility with other frameworks. Is there a batch-major implementation motivating this, and if so, what is the layout most convenient for that implementation (for num-directions, specifically). I went back to look at the two preceding PRs that motivate this PR, and the original one actually proposed to put num-directions as the last dimension. 
matteosal(2021-02-08 13:30:26):I agree that the position of `num_directions` is odd. However, my goal here is only to introduce a setting to swap batch and sequence dimensions. Moving the position of  `num_directions` like in the original PR without a flag controlling it was a bigger change and not crucial to our needs at Wolfram Research, so I felt like leaving it out to focus on `batch_major`.
gramalingam(2021-02-08 19:13:20):Just to clarify: we have two cases. When batch_major=0, we have the original layout, where it makes sense to retain the original specification for backward compatibility (and I am not proposing to change this at all). But when batch_major=1, this is entirely new, so I think it makes sense to make the layout as natural for this one as possible. So, just looking for any feedback from any potential user/implementor of this new layout as to what is preferred. Another possibility would be to simply rename the attribute "batch_major" to be "layout" (an enumeration instead of a boolean). This would naturally allow other layouts to be added in the future if desired, by allowing values beyond 0 and 1. Would that make sense? Thanks very much for your thoughts/feedback.
matteosal(2021-02-09 18:11:10):I went looking for how tensorflow and pytorch do it.

Tensorflow has a bidirectional wrapper in the Keras module (https://www.tensorflow.org/api_docs/python/tf/keras/layers/Bidirectional) which is strictly batch-major and produces a default shape `[batch, len, num_dir * hidden]`. They have a `merge_mode` parameter (default concat) that can combine the two directions directly by e.g. summing them, thus producing `[batch, len, hidden]`. This is also what we do at Wolfram. They also have `merge_mode = None` which produces a list of two tensors of shape `[batch, len, hidden]`, one for the forward and one for the backward. Thus Tensorflow doesn't seem to have a `num_directions` at all.

Pytorch (https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html) is time-major by default but can switch to batch-major. The two directions are concatenated by default, thus producing `[len, batch, num_dir * hidden]` (or `[batch, len, num_dir * hidden]`). They also have an "unpacked" mode which produces `[len, batch, hidden, num_dir]` (or `[batch, len, hidden, num_dir]`).

So they are all treat `num_dir` pretty differently from ONNX.
gramalingam(2021-02-09 18:40:16):I think that suggests [batch, len, num_dir, hidden] would be a better choice. The difference between [batch, len, num_dir, hidden] and [batch, len, num_dir * hidden] is minimal since it does not change the order of data. We can use an inexpensive reshape to switch between the two (without copying data). (I guess for a backend implementation that supports strides in the tensor implementation, most are equivalent; but there are some cases where consecutive data, is helpful.)
gramalingam(2021-02-09 18:41:36):[batch, len, num_dir, hidden], however, explicitly indicates the ordering of data in the last two dimensions, and seems preferable to [batch, len, num_dir * hidden].
gramalingam(2021-02-09 19:02:59):My suggestion is to (a) use an attribute-name "layout" (to indicate that the layout is changed), and (b) Preferably use [batch, len, num_dir, hidden], assuming it works better for your export. It does impact what happens (or is assumed) by the subsequent nodes, right? So, it must impact the conversion-to-onnx logic. 
matteosal(2021-02-15 14:47:09):Yes it does impact the conversion but only insofar as adding the right transposition/reshaping operators before and after the recurent op.

Anyway, I found your `layout` suggestion to be good and I have implemented it. The `batch_major` setup can be recovered by reverting the last commit, and I'm fine with both solutions. Now I'd like a final decision to be taken so that we can finally merge this PR.
gramalingam(2021-02-16 23:01:26):This looks good to me! I prefer the current version. There is an ONNX operator SIG meeting on thursday (day after tomorrow), in case anyone else has any other opinion to express. I hope we can merge it by thursday!
daquexian(2021-01-17 07:18:53):Thanks! When will this pr be merged? :) The same bug affects onnx optimizer
daquexian(2021-01-17 07:30:35):About #2961 there is another bug, I submitted a pr at https://github.com/onnx/onnx/pull/3221
jcwchen(2021-01-18 00:39:27):> Thanks! When will this pr be merged? :) The same bug affects onnx optimizer

Hopefully it will be merged next week. Thanks!
daquexian(2021-01-17 06:02:27):I also update ir_pb_converter for https://github.com/microsoft/onnxruntime/issues/6370
jcwchen(2021-01-18 23:25:57):nit: Do you keep these print functions intentionally for debugging? Thank you for the resubmitting.
daquexian(2021-01-19 02:23:30):@jcwchen Thanks! I forgot to remove them.
askhade(2021-01-19 22:29:10):nit: change name to collect_testcases_by_operator ?
askhade(2021-01-19 22:34:15):nit: we want the user to enter operator name which is referred to as op_type in rest of the code and more importantly here (https://github.com/onnx/onnx/blob/master/onnx/onnx-operators.proto#L101) 
so change from node to op_type to match the naming convention used everywhere else... 
askhade(2021-01-19 22:35:26):specify the node to exclude other testcases -> generates test data only for the specified op_type
askhade(2021-01-19 22:44:15):instead of creating all the test cases and then picking up the ones for a specific operator can you instead create the test cases only for the requested operator... basically in expect method if the node.op_type does not match the request op_type don't create the test data
gramalingam(2021-01-20 00:15:14):Agree with the suggestion to rename the option to indicate it specifies an op name (not a node). Anything like "op_type" or "op" or "op_name" would be better than "node".
jcwchen(2021-01-28 02:40:01):I changed to use` --op_type`, but I don't know what the appropriate argument abbreviation is since `-o` is occupied... I chose '-t' eventually. Please let me know if you have better idea. Thanks.
jcwchen(2021-01-28 03:44:33):Here we only have the module name (onnx.backend.test.case.node.XXX.py) for filter. And the test python file name might not include op_type name...
jcwchen(2021-01-28 03:56:56):Are you saying extending the arguments for expect to filter op_type? If so, do we need to change a lot of expect functions to add an extra argument?

Actually these testcases are created by `import_module`. So another approach might be changing `import_recursive`, but the test python script name might not include the op_type name. Please check my comments below. Thanks.
jcwchen(2021-01-28 04:56:55):(updated)
~Maybe we can introduce a global variable in expect function to filter. I will try it.~ No we cannot do this.
askhade(2021-02-04 06:16:01):why cant we filter in expect?
The problem with filter while importing the module is we have to make an assumption that python file name will contain op_type... this is very fragile. 

There is no need to change the signature of expect mtd. expect takes in node as the first argument you can get op_type from node as node.op_type and if it does not match the provided op_type then you can filter it. 
jcwchen(2021-02-10 19:21:31):I think without changing the function `expect`'s interface, we cannot get the given op_type in the `expect` function. op_type needs to be given while calling `expect`.
jcwchen(2021-02-11 00:05:58):Solved. Thanks for the proposal!
askhade(2021-03-05 17:32:56):rename: _TargetOpType? 
askhade(2021-03-05 17:39:20):reword to something like -> op_type for test case generation. (generates test data for the specified op_type only.)
askhade(2021-03-05 17:40:54):why lowercase? I dont think operator names are lower case Add is valid op_type... for simplicity you can convert both the provided op_type and node.op_type to lower case before comparison... or maybe python has a string comparison api which ignores case
jcwchen(2021-03-06 01:55:03):Good catch. I use lowercase for these two strings to achieve case insensitivity.
gramalingam(2021-03-08 22:34:21):Why do we want case-insensitivity? ONNX does not specify operator names are case-insensitive, does it? In which case, it seems better to not change the case at all, and expect the user to specify same case as ONNX.
jcwchen(2021-03-09 00:28:34):I was afraid that users will use wrong case to update test data and fail, but they should be aware of this while uploading related test data. I agree with you that we should keep it align with case-sensitive ONNX. Just removed lower() to make it case sensitive.
askhade(2021-01-21 18:19:22):Should the title for this PR change to - Add Mac release pipeline?

jcwchen(2021-01-20 22:14:08):Unlike onnx/wheel-builder, installing protobuf 3.11.3 from brew on Mac 10.15 will cause the following issue when using the latest protobuf 3.14.0 on Mac:
```
dlopen(/Users/x/.pyenv/versions/3.7.8/lib/python3.7/site-packages/onnx/onnx_cpp2py_export.cpython-37m-darwin.so, 2): Library not loaded: /usr/local/opt/protobuf/lib/libprotobuf-lite.22.dylib

  Referenced from: /Users/x/.pyenv/versions/3.7.8/lib/python3.7/site-packages/onnx/onnx_cpp2py_export.cpython-37m-darwin.so

  Reason: image not found
```  
Still investigating.
askhade(2021-01-21 18:14:01):nit: --quiet
askhade(2021-01-21 18:15:17):remove comment
askhade(2021-01-21 18:15:58):for file in dist/*.whl; do python -m pip install --upgrade $file; done
askhade(2021-01-21 18:16:35):it will look cleaner to separate the package creation and test step...
askhade(2021-01-21 18:18:05):run  git submodule update --init --recursive before line 54
jcwchen(2021-01-21 19:09:10):Isn't it same as line 25?
jcwchen(2021-01-21 22:33:25):https://github.com/actions/setup-python/issues/26
Try miniconda to obtain Python with lower MACOSX_DEPLOYMENT_TARGET. Current Python agent in GitHub Action is using Python with 10.14 (MACOSX_DEPLOYMENT_TARGET) and it will produce 10.14 wheel anyway.
askhade(2021-01-22 17:12:46):let's also run this for pushes on master. This way these pipelines will be run frequently
askhade(2021-01-22 17:13:25):for mac x86_64 is more appropriate
askhade(2021-01-22 17:15:23):What is the significance of setting the namespace to this? Isn't default namespace (onnx) ok? If we use release specific namespace then we will have to update this CI for every release. 1 more task for release manager
askhade(2021-01-22 17:17:30):nit: add a line break between every step\task to make it more readable. 
jcwchen(2021-01-22 19:06:27):However, GitHub Action does not support x86_64 Python so I keep using x64 here.
jcwchen(2021-01-22 19:08:03):We have a similar discussion before. IMO, not setting it should be fine. Just removed. Thanks for the reminder.
jcwchen(2021-01-22 21:21:41):Solved by installing static protobuf.
askhade(2021-01-19 23:41:59):right now change this to 1.8.150 or something for testing. Once our test packages are tested we can change this back to 1.8.1
askhade(2021-01-19 23:44:36):other places it is ok to keep it 1.8.1... since the package id is determined based on the version number in this file only

jcwchen(2021-01-19 23:52:53):Thanks for the reminder. I will use 1.8.0.1 because 1.8.150 is higher than 1.8.1
jcwchen(2021-01-27 16:40:30):Also remove remove `ONNX_BUILD_TESTS=1, USE_MSVC_STATIC_RUNTIME=1, CMAKE_ARGS="-DONNX_USE_LITE_PROTO=ON -DPYTHON_LIBRARY=/usr/lib64/librt.so"`
matteosal(2021-02-15 15:05:17):Any news on this? I was hoping to get this included in the next release, which in planned in March. Any hope it can make it?
matteosal(2021-03-19 15:15:24):Splitting this PR in two parts as requested:
https://github.com/onnx/onnx/pull/3343
https://github.com/onnx/onnx/pull/3344
matteosal(2021-01-27 15:25:27):I had to (hopefully temporarily) exclude some ops for various reasons:

1) Sequence-based ops can't be properly tested because the version converter replaces all sequence inputs with tensor inputs:
```
import onnx
from onnx import helper, TensorProto, version_converter

seq = helper.make_sequence_value_info('seq', TensorProto.FLOAT, [5, 2])
pos = helper.make_tensor_value_info('pos', TensorProto.INT64, [])
elem = helper.make_tensor_value_info('elem', TensorProto.FLOAT, [5, 2])

node = helper.make_node('SequenceAt', ['seq', 'pos'], ['elem'])

graph = helper.make_graph([node], 'MyGraph', [seq, pos], [elem], [])

model = helper.make_model(
    graph,
    producer_name='test', 
    ir_version=7,
    opset_imports=[helper.make_opsetid('', 11)]
)

converted = version_converter.convert_version(model, 11)
print(model.graph.input[0])
print(converted.graph.input[0])
```
```
name: "seq"
type {
  sequence_type {
    elem_type {
      tensor_type {
        elem_type: 1
        shape {
          dim {
            dim_value: 5
          }
          dim {
            dim_value: 2
          }
        }
      }
    }
  }
}

name: "seq"
type {
  tensor_type {
    elem_type: 0
    shape {
    }
  }
}
```

2) Shape inference crashes on `NegativeLogLikelihoodLoss` (`'Shape inference done'` doesn't get printed):
```
import onnx
from onnx import helper, TensorProto, shape_inference
import numpy as np

in1 = helper.make_tensor_value_info('in1', TensorProto.FLOAT, [3, 4, 5])
in2 = helper.make_tensor_value_info('in2', TensorProto.INT64, [3, 5])
out = helper.make_sequence_value_info('out', TensorProto.FLOAT, [])

node = helper.make_node('NegativeLogLikelihoodLoss', ['in1', 'in2'], ['out'])

graph = helper.make_graph([node], 'MyGraph', [in1, in2], [out], [])

model = helper.make_model(
    graph,
    producer_name='test', 
    ir_version=7,
    opset_imports=[helper.make_opsetid('', 12)]
)

onnx.checker.check_model(model)
print('Check done')
shape_inference.infer_shapes(model)
print('Shape inference done')
```

3) Scatter and Upsample are deprecated (in versions 11 and 10 respectively) and should be turned into different operators, but I couldn't find a way to do it. Is this possible somehow?
askhade(2021-03-04 16:45:33):nit: update name for this file
jcwchen(2021-01-31 23:54:14):It is caused by the latest numpy version: https://github.com/numpy/numpy/releases/tag/v1.20.0 and python 3.6 was dropped so it only fails with python3.7. 
askhade(2021-02-01 00:41:44):I don't understand the solution
jcwchen(2021-02-01 01:13:14):I think the goal here would be making mypy only check onnx's directory. ONNX should not take care of its dependencies' typecheck... I removed virtualenv because if using it, the installed numpy will be produced in the current dircetory and mypy will check that as well. 

However, even if we remove it, mypy will still catch the type error from numpy in other path... Still investigation.
jcwchen(2021-02-02 22:35:01):Changing this comment now
askhade(2021-02-03 18:31:16):@Ewa21 : Please sign the DCO
askhade(2021-02-03 18:42:49):overall looks good.
1 comment - we discussed picking the right tag for test pypi wheels so that all the test package tags are < new release tag. Can you mention that somewhere near line 13. 
jcwchen(2021-02-03 15:32:48):Not sure whether it is needed since 1.8.1 is a patch.
askhade(2021-02-03 18:42:59):it is needed
askhade(2021-02-10 07:51:18):@daquexian : Can you help review this PR.
daquexian(2021-02-10 08:09:46):It generally looks good to me. @462630221  Could you move this pr to https://github.com/onnx/optimizer repo? onnx optimizer is now maintained there. Thanks!
askhade(2021-02-10 17:00:56):> It generally looks good to me. @462630221 Could you move this pr to https://github.com/onnx/optimizer repo? onnx optimizer is now maintained there. Thanks!

I do not have a way of transferring this PR. @462630221 can you transfer this to the optimizer repo.
daquexian(2021-02-21 07:42:10):Closing it as it has been transferred to https://github.com/onnx/optimizer/pull/33
skottmckay(2021-02-10 23:48:43):Should you have flags to explicitly fail the build if there's an attempt to use exceptions? 

e.g. `-fno-exceptions` for gcc/clang, /EH-s-c for MSVC
skottmckay(2021-02-10 23:54:38):What's the reason for not using ONNX_THROW or ONNX_THROW_EX here?
skottmckay(2021-02-10 23:58:37):Instead of having an #ifdef on ONNX_NO_EXCEPTIONS here could it just be implemented by updating the original macro to use ORT_THROW_EX?

Ideally the only place you need the #ifdef on ONNX_NO_EXCEPTIONS is in common.h. Everywhere else can hopefully just use the macros defined there. 
skottmckay(2021-02-11 00:01:37):'but its domain is not'.  same on line 1003
skottmckay(2021-02-11 00:12:41):Would maybe be simpler to just put all the 'register twice' tests and infrastructure inside an #ifndef ONNX_NO_EXCEPTIONS. 
skottmckay(2021-02-11 00:16:48):Why doesn't RegisterTwiceFloatSchema need a similar update?
askhade(2021-02-16 20:53:09):This test was not written properly - RegisterTwiceFloatSchema would register the CustomFunc for the first time and then it was registered for the second time here. Anyways I updated both the test to remove a lot of common code and make the intention of the tests clear.
askhade(2021-02-16 20:54:35):I updated these macros first and then decided to add ONNX_THROW and ONNX_THROW_EX . I should have gone back to these macros again and updated them ... updated them now. 
gramalingam(2021-02-16 21:21:52):Add ```do { ... } while (false)``` ?
gramalingam(2021-02-16 21:23:29):Drop semicolon?
gramalingam(2021-02-16 21:47:29):I realize this is a pre-existing problem, but minor nit: better to add semicolon after fail_shape_inference(...) here and down below
gramalingam(2021-02-16 22:01:15):The two tests here are actually testing different aspects. The first tests a simple function where the parameter is of a fixed type (float), while the second tests a function where the parameter may be one of several types (float or double). The name "RegisterTwiceSchema" may be confusing, but it is meant to register a function called "CustomFunTwice". Does running these two tests cause a problem (such as same function name being registered twice)? If so, I suggest just renaming the functions as "CustomFunTwice1" and "CustomFunTwice2" or use a static boolean to ensure it is registered only once. Thanks!
skottmckay(2021-02-16 23:42:53):Doesn't elseif need a different condition?

Not sure about the 'else(MSVC)' on line 94 as the cmake doco doesn't mention anything about repeating the condition from the 'if' in the 'else' or 'endif'. 

[https://cmake.org/cmake/help/latest/command/if.html](https://cmake.org/cmake/help/latest/command/if.html)
askhade(2021-02-17 00:14:04):it was a typo It should be else and not elseif. Originally ONNX_WERROR did not have any effect on windows and mac platforms... with this change it should take effect on all 3 platforms... 

else does allow condition. 
"Per legacy, the else() and endif() commands admit an optional <condition> argument. If used, it must be a verbatim repeat of the argument of the opening if command."

However to keep it consistent with the rest of the document I will remove the condition
lgtm-com[bot](2021-03-04 04:01:23):This pull request **introduces 1 alert** when merging 7396ccb178b4cc3f4cf9ea963d5c6a193f576b5c into 1cbd549193661c7e9c870c4e9595144d0c6971c5 - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-7c1ec2d63aa82b56df234a5651c9429928481cef)

**new alerts:**

* 1 for Constant return type on member
jcwchen(2021-03-16 17:55:51):This is the PR for only verifying this in ORT: https://github.com/microsoft/onnxruntime/pull/6959.
askhade(2021-03-29 16:30:29):overall looks good... can you add some tests for this
jcwchen(2021-03-31 04:19:02):All updated. Thank you @askhade for all the reviews
jcwchen(2021-03-04 05:12:38):Check here to prevent repeatedly schema registration for same opset_version. If another opset_version is given, clear the previous one and load again.
askhade(2021-03-17 17:25:35):why not set the default value to 0 so that you dont need to do anything here
askhade(2021-03-17 17:42:44):i wonder whether this is really needed... it seems like you are only using it to determine whether the debug check should be done or not right? can you reuse the max_version\target version or some other way? 
askhade(2021-03-17 17:44:37):did you mean if loaded_schema_version == 0? in operator_sets.h you set this to 0 when you are loading all schemas and this debug check should be enabled when it is set to 0 right?
jcwchen(2021-03-17 17:47:55):Actually I set the default value as -1 to represent that the schema has not been loaded yet. If so, setting 0 here is required.

Regarding to a scenario in any Runtime: if Runtime wants to load specified schema during inference, it needs to know whether opset schema has been loaded or not to prevent conflict. This can be done in either ONNX or Runtime. I choose to implement here because I think this thing should be under ONNX-level and it can be reused by every Runtime framework.
askhade(2021-03-17 17:49:24):should max_version be renames to opset_version_to_load? max_version to me seems unintuitive... max_version gives a false impression thart all schemas till a max_version should be loaded but this is not true right... you only want to load schemas valid for a particular opset version
jcwchen(2021-03-17 17:49:26):For ONNX, yes it is just for DEBUG mode now, but I think it would be useful in Runtime. You can check more details in my comment above.
jcwchen(2021-03-17 17:52:18):Good catch! Should be ```if loaded_schema_version == 0``` here. Will update. Thanks
jcwchen(2021-03-17 17:53:40):Agreed. I was quite confused about `max_version` and `opset_version_to_load` looks better to me. Also, I will add some comments for it to make it clear.
askhade(2021-03-29 16:20:56):I dont understand... what is the purpose of setting the schema version to 0 ?
askhade(2021-03-29 16:22:10):can you mention in the comment that the calls to schema registration are required to be in descending order for this to work correctly
askhade(2021-03-29 16:23:27):change comment to : Other positive integer means the ONNX schemas for the specified version have been loaded
askhade(2021-03-29 16:24:44):nit change comment to : Only check if schemas for all opset versions are loaded
askhade(2021-03-29 16:28:47):change comment to: Return early if schema for the targeted opset version has already been loaded.
askhade(2021-03-29 16:29:25):you can drop this comment
gramalingam(2021-03-29 17:10:31):Yes, I agree, I was confused about the intention.
gramalingam(2021-03-29 17:13:26):How about "Check enabled only if schemas for all opset versions are loaded"?
jcwchen(2021-03-29 23:26:21):Default is -1, which means there is no any opset schema has been loaded yet. Setting 0 here to let OpSchemaRegistry know that ONNX opset schema has been **fully** loaded. It would be useful for Runtime to detect whether OpSchemaRegistry has loaded ONNX schema or not to prevent loading conflict.
askhade(2021-03-31 03:27:34):nit: change comment to : Disable static registration for onnx operator schemas
askhade(2021-03-31 03:30:42):do you need =1?
jcwchen(2021-03-31 03:33:42):I followed the same logic as ONNX_ML and ONNX_USE_LITE_PROTO, but yes I think pure `__ONNX_DISABLE_STATIC_REGISTRATION` should be also workable. Let's try it.
askhade(2021-03-31 03:35:36):why do you need this?
askhade(2021-03-31 03:37:28):why are you changing the accessibility from private to public?

jcwchen(2021-03-31 03:42:36):I was also considering whether to revert it... Just in case if someone wants to test C++ ONNX with ONNX_DISABLE_STATIC_REGISTRATION=ON and bumps into issue here. However, even if that is the case, the whole test will still fail with schema_test.cc anyway... So I will remove it. Thanks.
askhade(2021-03-31 03:44:11):nit: rename file schema_registration_test.cc
askhade(2021-03-31 03:46:04):what would be the issue in that case?

askhade(2021-03-31 03:46:31):oh you mean this test and schema_registration_test cannot go together?
askhade(2021-03-31 03:48:14):add some comments explaining why this test is run only when static registration is disabled
jcwchen(2021-03-31 04:20:18):Yes, then it will still fail so the if condition here is useless
askhade(2021-03-31 04:55:17):this needs to move back under private right?
jcwchen(2021-03-31 05:07:54):Updated. Thanks
askhade(2021-02-17 16:59:11):checked in model has int32 data type... it is best if you use int32 here... this way the model itself wont change... this way 
1. the opset version for the model will remain 10 (right now it will change to 13)
2. there will be less file updates in this PR
jcwchen(2021-02-17 19:25:19):Thank you for the reminder. Updated them with int32.

Also, just figure out that we don't need to update other testdata because they are int64 already. This PR only updates the test scripts to prevent issues while generating testdata on Windows. 
lgtm-com[bot](2021-02-12 22:07:21):This pull request **introduces 2 alerts** when merging a1e9702ead7e5581dab86993f4e8f60e3eddc971 into b4d69b49f4216ebbe8857bd76e7c1a77665ee44f - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-7a4ba1ac9943b6de7a6e1bdfc13fb4b6159641fd)

**new alerts:**

* 2 for Module is imported with &#39;import&#39; and &#39;import from&#39;
lgtm-com[bot](2021-02-12 22:23:09):This pull request **introduces 2 alerts** when merging a1e9702ead7e5581dab86993f4e8f60e3eddc971 into b4d69b49f4216ebbe8857bd76e7c1a77665ee44f - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-7a4ba1ac9943b6de7a6e1bdfc13fb4b6159641fd)

**new alerts:**

* 2 for Module is imported with &#39;import&#39; and &#39;import from&#39;
lgtm-com[bot](2021-02-13 01:16:33):This pull request **introduces 2 alerts** when merging 41b70a5ba8d9a0f6497e47706f4bb699842ad9c4 into b4d69b49f4216ebbe8857bd76e7c1a77665ee44f - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-3f50bbd7d4d91e1a6b7e11794254a2b7d5e78efd)

**new alerts:**

* 2 for Module is imported with &#39;import&#39; and &#39;import from&#39;
lgtm-com[bot](2021-02-13 02:19:58):This pull request **introduces 2 alerts** when merging 41b70a5ba8d9a0f6497e47706f4bb699842ad9c4 into b4d69b49f4216ebbe8857bd76e7c1a77665ee44f - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-3f50bbd7d4d91e1a6b7e11794254a2b7d5e78efd)

**new alerts:**

* 2 for Module is imported with &#39;import&#39; and &#39;import from&#39;
askhade(2021-03-05 17:29:36):overall looks good ... just a few nit suggestions
jcwchen(2021-03-06 01:57:11):> overall looks good ... just a few nit suggestions

All updated. Thanks for the review
jcwchen(2021-02-14 04:36:03):Still, running ORT test for generated backend testdata can prevent future test failures in ORT... May need to figure out a way to make ORT run unsupported opset_version. Or we can run this only before every ONNX release.
jcwchen(2021-02-18 03:46:24):This did not generate anything... remove it because it is not worked. Instead, we only check uploaded/generated in release CIs
jcwchen(2021-02-18 18:26:16):TODO: This duplicate code among all release CIs will be merged into a same yaml and be reused in the future PR.
https://github.com/actions/starter-workflows/issues/245 Currently GitHub Action has not supported reused yaml yet.
askhade(2021-03-05 17:27:18):call check_model only once after shape inference?
askhade(2021-03-05 17:28:49):I thought the test case changes were made in a separate PR... 
jcwchen(2021-03-06 00:50:19):Because this bug only happened in 32-bit environment and I found it in this PR while running 32-bit release CI.
gramalingam(2021-03-09 00:19:45):Sorry, I didn't understand this. What precision is this? Why don't we use explicit 32 or 64 bits like elsewhere in this PR?
jcwchen(2021-03-09 15:50:51):32-bit  machines somehow cannot handle numpy.int64 here so I used int_ (which means int32 for 32 machines and int64 for 64 machines). As you mentioned, using a fixed type here is better. I have changed it from int_ to int32. Thank you for the suggestion!
askhade(2021-02-16 13:48:02):const auto&
askhade(2021-02-16 13:48:24):const auto&
daquexian(2021-02-17 00:58:27):Fixed
askhade(2021-03-08 22:46:53):Update "https://github.com/onnx/onnx/blob/master/docs/PythonAPIOverview.md" with new API
askhade(2021-03-09 14:54:53):> Newbie question, where does this new optional parameter for convert_model_to_external_data need to be documented? Also where is it invoked, to change if we want the default behavior not to save attributes?

https://github.com/onnx/onnx/blob/master/docs/PythonAPIOverview.md and https://github.com/onnx/onnx/blob/master/docs/ExternalData.md
lgtm-com[bot](2021-03-11 17:01:09):This pull request **introduces 1 alert** when merging 4170dd35441e80ff76b9d611f8baab8c586bfe20 into 4f8ebec58c9ef1bc9aaf295847260f4f44a02e58 - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-bb1c3cf6230970b427631a18d34fc621c284ee13)

**new alerts:**

* 1 for Unused import
lgtm-com[bot](2021-03-23 22:10:57):This pull request **introduces 2 alerts** when merging b67914b261eb228e65cbaee85384e26850e51344 into d41396f5b81a5e5cc1c1f378ca02716d23e68c03 - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-81cdd7a85b4be21015dfe5b62c03316676841cae)

**new alerts:**

* 2 for Unused import
askhade(2021-03-08 22:53:15):why not update the convert_model_to_external_data to take convert_attribute... this way you dont need to duplicate code
mkbhanda(2021-03-09 09:21:50):I like this idea, does not require the new function save_model_to_external_data. Could introduce a local variable that takes value _get_all_tensors(model) or _get_initializer_tensors(model) based on convert_attribute.
My sense was the original convert_model_to_external_data did the honors for all tensors .. should that not translate to a default value of True for convert_attribute?

askhade(2021-03-09 14:53:49):The idea behind introducing external data was to enable models whose size exceeds the 2G max limit set by protobuf also using external data for tensors can help with sharing weights between models... however serializing attributes to external data defeats the purpose, usually attributes consume little memory and serializing them will require more memory as now you will be storing all the external data info in the tensor proto as opposed to the actual data which will most likely be an int/float/bool
This also causes issues in runtimes as attributes are required for shape inferencing etc... serializing attributes by default is a bug and we should fix that and make it optional. Ideally the option to serialize attributes is not required for the reasons stated above but we planned to add it to support any existing user scenarios which we may not be aware of.
annajung(2021-03-11 16:27:07):Modified to remove the duplicate code and update the existing code to take the convert_attribute and file path. 
annajung(2021-03-11 16:30:12):Not sure if specifying what each parameter means is required here
annajung(2021-03-11 16:34:43):I set `convert_attribute` to False, this will change the behavior of the end-user. But, I didn't think we wanted to set it to True 
annajung(2021-03-11 16:35:43):I didn't make model_file_path a requirement to make sure it doesn't break the behavior of end-user, however, I am not able to find the correct default value for this. Looks like setting it to `None` for `Union[IO[bytes], Text]` is causing CI to fail. 
annajung(2021-03-11 17:43:00):cc @askhade
annajung(2021-03-11 20:43:29):thinking about it more, it makes sense to remove this and make model_file_path a requirement. Why convert if not save?
annajung(2021-03-12 16:38:01):I ended up removing this check and making the file path required parameter. Let me know if this works and if we want to go back to making it optional. 
annajung(2021-03-19 16:36:15):After speaking with Ashwini, we decided to move the model_file_path out of the existing API to remove the requirements for the model file path. 
annajung(2021-03-19 16:37:33):After speaking with Ashwini, we decided that setting the `convert_attribute` value to false is okay even if that will change the current behavior as converting attribute was not an intended behavior in the first place 
askhade(2021-03-22 14:39:28):can we rename this to save_model and add it to onnx/__init__.py along with the original save_model API? So basically we will have 2 save_model APIs ... @gramalingam thoughts?
annajung(2021-03-22 17:26:26):So, remove it from `external_data_helper.py` and add to `init.py`? 
I can do that, but if we rename it to `save_model`, wouldn't there be a conflict since I'm calling the existing `save_model` in the new `save_model` API?
gramalingam(2021-03-22 22:29:00):I agree, it would be nicer to have them together. One possibility is to have a single API, with extra-parameters determining whether to convert data to external-data.
annajung(2021-03-23 14:07:35):ack, I can add an extra parmeter `convert_data=False` in addition to parameters from the `convert_and_save_model_to_external_data` in the existing `save_model` API in the `init.py` file

We can use the `convert_data` parameter to determine if convert data to external data is needed? 
I will work on this later today, let me know if you have any concerns @askhade @gramalingam thanks! 
askhade(2021-03-23 20:32:02):yes makes sense...  1 suggestion rename convert_data to save_as_external_data and set defualt value to False
Something like this: 1 single save_model api : 
save_model(proto, f, format=None, save_as_external_data=False, all_tensors_to_one_file=True, location=None, size_threshold=1024, convert_attributes=False)
annajung(2021-03-23 22:55:04):done, PTAL again when you can!
askhade(2021-03-24 15:10:03):nit: set -> serialize
yufenglee(2021-03-24 15:24:32):typo: please remove #is# 
askhade(2021-03-06 01:03:41):2 comments:
1. Use the release CIs instead of creating a new workflow
2. We need this for all platforms not just windows
jcwchen(2021-03-06 04:58:46):> 2 comments:
> 
> 1. Use the release CIs instead of creating a new workflow
> 2. We need this for all platforms not just windows

Updated. Thanks for the suggestions
jcwchen(2021-03-12 18:40:17):Updated. I have tested it in my forked repo and onnx-weekly wheel can be produced/auto-uploaded correctly. This PR is ready for review again. Thanks
jcwchen(2021-03-16 17:46:35):Thanks @prasanthpul for adding TestPyPI credentials
askhade(2021-03-08 17:25:54):where is this arg passed to setup.py... I dont see it in any yamls
askhade(2021-03-08 17:27:04):Looks like this step will run on every CI run. We want to upload only for weekly jobs not for push and pull request runs
askhade(2021-03-08 17:27:34):same comment as below for all yamls... this step should only run for weekly job
jcwchen(2021-03-09 03:10:17):I am afraid that we still need to decouple release CI and weekly release CI here... Currently GitHub Action only has limited support for reuse yaml https://github.com/actions/runner/issues/646. Since the same build code is hard to reuse, I will decouple them for now and set the merge as TODO in the future.
lgtm-com[bot](2021-02-21 03:24:04):This pull request **fixes 1 alert** when merging 9a2921e838a5c8eeaa7557c70c94a35e9cc22c6e into fde37df2dee398151f9a914c3c1a43f1b1334d89 - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-dc45411a65214258e5864815191fd07ff8bfbd46)

**fixed alerts:**

* 1 for Module is imported with &#39;import&#39; and &#39;import from&#39;
lgtm-com[bot](2021-02-23 01:40:48):This pull request **fixes 1 alert** when merging 73601f295efb24d0a9d4ab08d0055ce35bdfe554 into 722f5a477bcc2fe7d55b84c23cce3b17605e8788 - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-087a6c3d207ede6671ebad422eb40aac7d1dd745)

**fixed alerts:**

* 1 for Module is imported with &#39;import&#39; and &#39;import from&#39;
lgtm-com[bot](2021-03-01 07:17:33):This pull request **fixes 1 alert** when merging 91a10c665fab981f411c8a540e463e29b975122f into 1cbd549193661c7e9c870c4e9595144d0c6971c5 - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-7e7d7035c7d58724bb9ac1847af7eb66defdf0e5)

**fixed alerts:**

* 1 for Module is imported with &#39;import&#39; and &#39;import from&#39;
lgtm-com[bot](2021-03-09 17:18:07):This pull request **fixes 1 alert** when merging ba4ce9e675b5dfe4411be0e005d3ac150a8ee4cc into 184aec5dcff7e8a113222a9566fa245ea536f65a - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-b76f55a879b46fdc0fcd3640f75cd1236a8dfc7d)

**fixed alerts:**

* 1 for Module is imported with &#39;import&#39; and &#39;import from&#39;
lgtm-com[bot](2021-03-15 22:06:17):This pull request **fixes 1 alert** when merging 02f1fb24111a5baf20de040e0fce6eac763d2084 into 4f8ebec58c9ef1bc9aaf295847260f4f44a02e58 - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-a814d1b168405118c707343090da3f8f7175797a)

**fixed alerts:**

* 1 for Module is imported with &#39;import&#39; and &#39;import from&#39;
askhade(2021-02-22 18:23:48):you can keep this method just remove optimize and the checker call after optimize...
daquexian(2021-02-23 01:25:57):If so this method will only run strip_doc_string and shape_inference. It makes this method not so useful. What's your opinion? Thanks!
neginraoof(2021-02-23 22:33:09):@jcwchen 
Do you have other comments?
Can you please help review this?
Thanks!
neginraoof(2021-02-24 19:39:44):@jcwchen 
As for the ORT confirmation, since this is a contrib op already, the op is being tested:
https://github.com/microsoft/onnxruntime/blob/563218dcda709d0dfebadd5f2cdce37e75474929/orttraining/orttraining/test/python/orttraining_test_orttrainer_frontend.py#L851

Also, we have multiple instance of huggingface models that use this contrib op.


askhade(2021-02-24 20:35:01):> @jcwchen
> As for the ORT confirmation, since this is a contrib op already, the op is being tested:
> https://github.com/microsoft/onnxruntime/blob/563218dcda709d0dfebadd5f2cdce37e75474929/orttraining/orttraining/test/python/orttraining_test_orttrainer_frontend.py#L851
> 
> Also, we have multiple instance of huggingface models that use this contrib op.

The question is mainly towards the correctness of the test data itself. We often see cases where the test data has issues so want to make sure these test cases succeed with ORT
neginraoof(2021-02-24 21:37:15):@hariharans29 Would you be able to help us validate the test data?
Thanks a lot!
gramalingam(2021-02-25 00:49:35):LGTM. Yes, it would be good to ensure that the test-data is valid, thanks!
hariharans29(2021-02-25 05:33:56):> @hariharans29 Would you be able to help us validate the test data?
> Thanks a lot!

The spec looks clear and good to me.

Currently, the Trilu op is a contrib op in ORT and it needs shifting it out of that and consuming this ONNX commit to validate the test data. It is trivial but time consuming and I am not in a position right now to validate this as I am busy with other partner engagements. Could you just validate it by moving the existing kernel out of the contrib space ? 

neginraoof(2021-02-26 00:15:44):@gramalingam 
Please let me know if we can merge the PR.

Thanks!
neginraoof(2021-03-01 20:02:10):@postrational Would you be able to help us review this PR?
Thanks!
spandantiwari(2021-03-04 00:10:03):@gramalingam @postrational - can we merge this PR now.
jcwchen(2021-02-22 18:46:02):```suggestion
        x = np.random.randint(10, size=(4, 5)).astype(np.int64)
```
To prevent issues like this: https://github.com/onnx/onnx/pull/3271, please specify numpy integer type for all `np.random.randint`. Thank you!
gramalingam(2021-02-24 00:23:42):Rewrite to "excluding the main diagonal and (k-1) diagonals above it", if this description is correct.
gramalingam(2021-02-24 00:25:17):Change to "excludes the main diagonal and |k|-1 diagonals below it"?
gramalingam(2021-02-24 00:26:31):"above or below"?
CLAassistant(2021-03-02 02:57:03):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=3306) <br/>All committers have signed the CLA.
mkbhanda(2021-03-31 02:10:34):@SnoopJeDi -- I am a newbie, from a grep/replace perspective this looks good. I upgraded to jumpy 1.20 and just installed ONNX. How/what do I build to get the deprecation warnings?  Also since your PR looks like we have some branch conflicts.
Regards
SnoopJ(2021-03-31 02:26:15):@mkbhanda all you need to do is `import onnx` to trigger the deprecation warnings, see #3305 for more information about the issue this PR fixes.

Thanks for pointing out the conflicts, I've rectified those.
nomaddo(2021-05-28 06:38:02):I miss this PR because of many warnings...
Does somebody work for this PR?
SnoopJ(2021-05-28 16:37:07):> 
> 
> I miss this PR because of many warnings...
> Does somebody work for this PR?

The PR is ready to be merged, but needs approval from one of the special interest groups that has code ownership here. @linkerzhang @gramalingam does this PR need anything for approval from either of your groups?
gromero(2021-08-04 20:19:40):Hi folks @linkerzhang @gramalingam any plan to land it soon? @SnoopJeDi I think there is now a conflict which needs to get resolved?
 

I'm pretty much interested on that fix since I get the same warnings when using that onnx module on TVM project CI.

Thanks!
gramalingam(2021-08-04 20:45:04):LGTM, thanks!
CLAassistant(2021-03-02 18:45:08):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=3309) <br/>All committers have signed the CLA.
jcwchen(2021-11-03 23:08:00):Close it due to inactivity.
CLAassistant(2021-03-08 12:26:04):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=3317) <br/>All committers have signed the CLA.
chausner(2022-03-31 16:16:38):@linkerzhang Anything speaking against merging this?
gramalingam(2021-03-18 01:24:44):FYI: the ONNX Release 1.9 deadline is next week (https://github.com/onnx/onnx/wiki/Logistics-for-ONNX-Release-1.9 ). There will be ONNX operator SIG meeting tomorrow (10am Pacific time), in case this PR needs to be included in 1.9 and discussed. See the ONNX operators slack channel for meeting details (https://lfaifoundation.slack.com/archives/C018Y2RAY4C/p1615909210002800 )
gramalingam(2021-03-16 21:42:33):It would be helpful to relate this documentation with the function-body used below by adding an equivalent expression using HardSigmoid here. E.g., " = x * HardSigmoid<alpha=1/6, beta=0.5>(x)" or something like that.
gramalingam(2021-03-16 21:42:56):HardSigmoid => HardSwish ?
gramalingam(2021-03-16 22:02:43):Lifting these 3 lines out as a separate function "hard_swish" would be helpful to other users as a reference implementation. (Eg., like here: https://github.com/onnx/onnx/blob/03e6c8f2ad599dda614a22ee494e982ffd0bccc7/onnx/backend/test/case/node/logsoftmax.py#L15 )
etusien(2021-03-17 15:41:48):Done
etusien(2021-03-17 15:42:30):Fixed
etusien(2021-03-17 15:42:38):Done
gramalingam(2021-03-18 15:56:30):I think ```float(1/6)``` evaluates to ```0``` in C++? Could change it to ```1.0f/6.0f```.
gramalingam(2021-03-18 15:58:41):Is there some test to validate this? Or catch any such issue?
gramalingam(2021-03-18 16:52:38):What I mean is: the "expanded" test-cases generated will help test the function-body. Running ORT or some other backend with those expanded test-cases would help validate correctness of the function body. (Unfortunately, I don't think this can be done in CI currently.)
etusien(2021-03-19 22:06:16):I fixed the bug in type casting.
I'm not aware of any tests which are run on ORT to check correctness of functions' body.
However, I ran the expanded model with the bugfix on ORT by myself and it returned correct output. 
(One thing that I had to change was the opset version. Currently,  ORT provides  support for domain ai.onnx till opset 13.) 
neginraoof(2021-03-16 00:43:59):@gramalingam @askhade @jcwchen @hariharans29 
Please help us review this PR. Thanks!

neginraoof(2021-03-24 17:04:58):@pranav-prakash 
Would you please also review the documentation on statistic updates? Thanks.
neginraoof(2021-03-25 18:23:43):@postrational @linkerzhang 
Would you please help us review this PR? Thanks.
postrational(2021-03-26 15:54:57):This PR is approved. Please resolve merge conflicts.
neginraoof(2021-03-26 17:22:01):@postrational Thanks! Conflict is resolved.
neginraoof(2021-03-26 19:14:52):@postrational CI looks fine. Please help us merge the PR. Thanks again.
jcwchen(2021-03-16 04:16:51):nit: perhaps L26-L28 can be merged into 1-2 lines.
hariharans29(2021-03-16 07:35:24):Should this read: sqrt(saved_var + epsilon) ? 
hariharans29(2021-03-16 07:45:43):Is it an error if optional outputs are present and training mode input flows through as false (defaulted value) ? 
hariharans29(2021-03-16 07:49:41):Suggestion 1: Would current_mean and current_var be more appropriate than saved_mean and saved_var ? Or is this more common terminology used in other frameworks ?

Suggestion 2: By `mean` and `var` here, I assume you mean `output_mean` and `output_var` from the equations below ? If so, can we keep all the naming consistent ?
hariharans29(2021-03-16 08:00:03):Aren't inputs `B` and `scale` required too ?
hariharans29(2021-03-16 08:09:14):How about adding one test without the optional training_mode input ?
hariharans29(2021-03-16 08:10:57):Just thinking out aloud - is this safe to assume num channels is 1 here and tin the next step, suppose the channel index of the shape doesn't have a dim value, the initialized value will still remain one ?
hariharans29(2021-03-16 08:12:04):Why throw if it doesn't have a dim_value ? What if it has a dim_param ? Should we check to see if it matches the dim_param of the channel index in the input shape (if that is a dim_param too) ?
hariharans29(2021-03-16 08:13:44):Should handle dim_param for the channel index ?
hariharans29(2021-03-16 08:16:42):How about adding more tests to test robustness of the shape inference method :

1) Shapes with dim_params (along with dim_values)
2) Shapes with no dim_values and no dim_params ?
3) Missing shapes for some inputs
gramalingam(2021-03-16 20:36:19):+1 on more meaningful and consistent naming.
gramalingam(2021-03-16 20:46:03):In addition, can we rename inputs 'mean' and 'var' to 'input_mean' and 'input_var'? Agree with Hari that "current" is more understandable than "saved"
gramalingam(2021-03-16 20:52:03):+1
gramalingam(2021-03-16 20:55:37):Agree we should explicitly indicate whether these extraneous outputs are allowed or not (in the inference mode). If they are allowed, we need to specify what their values will be (or whether they have an undefined value)
gramalingam(2021-03-16 21:10:14):Agree, we need to handle unknown dimensions carefully. The utility function ```unifyInputDim``` handles all cases carefully and will help simplify the code. See https://github.com/onnx/onnx/blob/490895703c82f68e723a8b308369cac4ed0abc0c/onnx/defs/object_detection/defs.cc#L117 for an example of how it is used.
gramalingam(2021-03-16 21:14:49):Roughly speaking, I think something like below should work (I haven't checked if the input-numbers and dim-numbers are the correct ones):
```cpp
   Dim C;
   unifyInputDim(ctx, 0, 1, C);
   unifyInputDim(ctx, 1, 0, C);
   unifyInputDim(ctx, 2, 0, C);
   etc.
```
gramalingam(2021-03-16 21:15:42):Basically call ```unifyInputDim``` for every input/dim that should have the same value C
gramalingam(2021-03-16 21:22:02):And you can use C to set the corresponding output dimension. This will pick up a known-value from any of the input shapes if at least one of the input shapes has a known value for that dimension.
gramalingam(2021-03-16 21:30:08):I don't understand this either. If the inputs are required to have some shape, can we explicitly document it here? Is it required to be 2D?
neginraoof(2021-03-18 16:32:13):@gramalingam @hariharans29 
How about using runnning_mean and running_var instead of output_mean and output_var?
neginraoof(2021-03-18 16:45:10):I think this is a note about how to get the old (deprecated) behavior for the op. There was an attribute in older versions:
'Spatial': If true, compute the mean and variance across per activation. If false, compute the mean and variance across per feature over each mini-batch.
And I think this note specifies how to achieve this behavior.

Input X can have N dimensions.
I can clarify the lines above.

neginraoof(2021-03-18 16:47:45):@hariharans29 @gramalingam Do you have a suggestion here? Does it make sense to allow these, but fill with garbage?
neginraoof(2021-03-18 16:53:25):Such a test is there in line 1239 for batch_norm.
neginraoof(2021-03-18 23:01:21):What is the difference between 2 and 3? 
hariharans29(2021-03-19 04:34:22):Sure - how about "if the training_mode is false and the optional training related outputs are present, the contents of it are undefined...." ?
hariharans29(2021-03-19 04:35:55):running_mean and running_var seems okay to me and I assume the other set of mean/variance will be current_mean and current_var ?
hariharans29(2021-03-19 04:58:31):2 -> shapes are present for inputs but they contain no dim_value or dim_param.
3 -> Shapes are missing altogether for certain inputs.

Does this make sense ?
gramalingam(2021-03-22 18:27:27):I think that for new op extensions, it is preferable to restrict scalars to be of rank 0.

@postrational what do you think? This is one of the "unified op interface" items. Traditionally, there has been some divergence with some ops using or allowing rank-1 tensors of size 1 for a scalar, with some requiring rank-0 tensors.
hariharans29(2021-03-23 15:31:18):Sorry - still not sure if this should be `input_mean` and `input_var` instead of `mean` and `var` ?
gramalingam(2021-03-23 17:12:30):A note about this as it relates to ORT implementation: if the extra outputs are specified and ignored by the ORT kernel, it is possible that even the memory for these output tensors are not allocated. This can cause subsequent errors with unallocated memory usage (depending on how the memory-planner reuses these buffers). So, it may be preferable to flag the use of extra outputs as error when training_mode=False. It would be helpful to get some input from ORT side on this. (@askhade any thoughts?)
gramalingam(2021-03-23 17:15:14):We can't produce an error-message if the input-dimension is not statically known. (But as commented above, it would be even better to just allow rank-0 tensors here.)
neginraoof(2021-03-23 19:10:27):Here is the ORT implementation: https://github.com/microsoft/onnxruntime/blob/8892ee4b6d343109699ab292e66c2c7a5e41925a/onnxruntime/core/providers/cpu/nn/batch_norm.h#L78

I don't think the backend needs to access memory location of such outputs in inference.
neginraoof(2021-03-23 19:10:43):cc @pranav-prakash 
SherlockNoMad(2021-03-23 19:30:45):when training_mode is true, output 1, 2, 3 , 4 would be populated. 
pranav-prakash(2021-03-23 20:25:53):@neginraoof While the batch_norm kernel doesn't access the optional tensors in inference mode (at least as I've implemented it in the linked PR), I believe the concern was over the allocation planner, which might try to re-use a buffer that hasn't been allocated properly. I'm in favor of making it invalid to use the extra outputs when in inference mode, since it reduces edge-cases and is generally confusing anyway (why introduce the notion of "undefined behavior" when you can just prevent i t entirely)?
pranav-prakash(2021-03-23 20:28:38):Btw the discussion in https://github.com/onnx/onnx/issues/1042 might be relevant here. I think the consensus (at least for n=2, @gramalingam and I) was that there is not really any use-case for the optional outputs during inference. While this means that we could technically change the spec so that training vs. inference mode is determined purely based on presence of these optional outputs, I don't think changing the behavior of an op based on # of outputs is as clean as just having an attribute for it.
gramalingam(2021-03-23 23:49:56):One challenge is that the "mode" is an input (as opposed to an attribute). I guess this was chosen to allow a training-engine to use the same graph/node, but change the mode dynamically? If we want this ability, then we will have to allow the multiple outputs to be specified. This also means that the issue mentioned above for allocation-planner buffer-reuse is a problem and ORT will need to solve it someway. A trivial fix would be for the kernel to ensure that the output is allocated by calling "Output" even if it's value is not initialized, but this would mean some unnecessary memory allocation in some situations.
pranav-prakash(2021-03-24 00:20:06):Oh I missed that `mode` was an input instead of attribute. IIRC in the older (reverted) PR it was an attribute instead. @neginraoof can you clarify the motivation for it being an input?

> allow a training-engine to use the same graph/node, but change the mode dynamically

I don't think this is an issue because ORT already has a pre-training graph transform pass that can modify attributes or add outputs. Being able to change mode by setting a graph input doesn't seem like it provides any concrete advantage over just changing the attribute.
gramalingam(2021-03-24 00:23:49):Yes, an attribute would simplify things. It would be great to clarify the motivation to have it as an input, thanks!
neginraoof(2021-03-24 16:53:12):This is the revert PR: https://github.com/onnx/onnx/pull/2750/files
This one shows that training_mode has been an input for the op before, and I kept it based on the old implementations. Also, training_mode for dropout op is an input. So do we want to keep it consistent?
@gramalingam @pranav-prakash 
pranav-prakash(2021-03-24 21:26:18):Ah I see. If it were solely up to me, I think that having it as an attribute is cleaner – and for consistency it's dropout that should be updated to move the "training_mode" to an attribute (there have been various other ops in the past that have had inputs moved as attributes, so this is not really anything unprecedented – and seems to indicate to me that having things as attributes is what schemas generally evolve towards anyway). But I'm just a user so I'll defer to the others on whatever they think is better.

Btw dropout states that 
>If it [training_mode] is false... and if mask is requested as output it will contain all ones.

So if we do indeed choose to keep it as an input, explicitly specifying the value of the optional output might be better than leaving it as "undefined behavior"
pranav-prakash(2021-03-24 21:45:53):Do you mean `ReduceMean` instead of `ReducedMean`? (same for var)

Otherwise the formulas look good to me (I especially like the use of `running_mean` which is less confusing than the `saved_mean` that the older spec had).
pranav-prakash(2021-03-24 21:48:17):I think input `X` is expected to have at least rank 2. In the most common case input will be NCHW, but as noted in the comment to replicate the older non-spatial behavior you can flatten it to `N x CHW`.
neginraoof(2021-03-24 22:58:27):Thanks. I can update this.
I updated the part for optional outputs, mentioning this is invalid. What do you think?
pranav-prakash(2021-03-24 23:12:13):Nice! I think in addition the shape inference could be updated to enforce that no optional outputs are present if `training_mode=true`
neginraoof(2021-03-24 23:33:12):Sure. Will do. 
@pranav-prakash Do you think you can help us test this spec with you ORT implementation? Just to check spec works fine.
pranav-prakash(2021-03-25 00:57:36):Sure, I can do so (next week though, I'm a bit busy this week). Even with the change in spec I don't there would be be any changes in my  PR for ORT BatchNorm training though (aside from checking the newly-added `training_mode` attribute).

 (There's also a separate issue where the pre-existing CUDA BatchNorm training implementation in ORT deviates from the spec in outputting inverse-standard-deviation instead of variance – whose behavior I copied so the tests could remain the same – but that's a discussion for that PR).
askhade(2021-03-17 22:02:12):@yufenglee and @tracysh FYI
BowenBao(2021-03-24 17:34:28):cc @askhade @gramalingam PR updated for opset version. Can we merge the PR now?
gramalingam(2021-03-24 22:08:56):One minor comment (optional): I think, going forward, it would be good if the operator documentation included a line or two explaining the change made with respect to the previous version of the op. E.g., in this case, add a line like
```
(Opset 14 change): Extend supported types to include uint8, int8, uint16, and int16.
```
askhade(2021-03-22 18:50:56):not sure if this can be treated as a valid test case
1. max allowed value here is 24 and 24 * 24 will overflow. In this case different hardware can produce different results and it may not match the expected result in output
2. I think because we don't handle overflow we should not add a test case with expected result when there is an overflow
BowenBao(2021-03-22 20:58:07):good point, updating the test case.
yuslepukhin(2021-03-16 18:38:34):There is a problem in exporting classes that has std members in it. Exported classes MUST have members that are also exported to ensure that they are the same definitions. STD classes are, of course, not exported. I have seen CONDA builds protobuf as DLLs, there are timebombs in it.
mbencer(2021-03-17 12:59:47):> There is a problem in exporting classes that has std members in it. Exported classes MUST have members that are also exported to ensure that they are the same definitions. STD classes are, of course, not exported. I have seen CONDA builds protobuf as DLLs, there are timebombs in it.

Good point, but as I know the main problem is ABI compatibility in such a case. For example when we have 2 protobuf DLLs (compiled in different way) sharing proto data between them is very bad idea.
In my case we have 2 libs which use one protobuf DLL. What's more, we don't share proto objects between them directly, but in serialized to std::string form.

daquexian(2021-03-21 15:29:07):A possibly related pr: https://github.com/onnx/onnx/pull/1938
postrational(2021-03-24 11:25:50):@linkerzhang Could you please comment on this?

As far as we can tell, there is currently no way to export symbols from `onnx_proto` on Windows.
mbencer(2021-03-30 08:28:25):A more comprehensive solution was implemented in https://github.com/onnx/onnx/pull/3371
askhade(2021-03-22 14:31:55):can we separate this in 2 PRs... to keep things clean.... Create a new 1 for python deprecation and keep this for binary size reduction. Thanks!
jcwchen(2021-03-22 15:26:35):> can we separate this in 2 PRs... to keep things clean.... Create a new 1 for python deprecation and keep this for binary size reduction. Thanks!

Sure thing. A new PR (https://github.com/onnx/onnx/pull/3353) decouple from this PR 
jcwchen(2021-03-18 03:30:30):Mac CI fails because of this https://github.com/pypa/setuptools_scm/issues/542. Will try to reduce binary size as well.
jcwchen(2021-03-18 20:49:35):ONNX decided to deprecate python 3.5 in the following 1.9. Will remove it in this PR to prevent failure caused by Python 3.5 incompatibility
etusien(2021-03-22 14:30:21):Is this still relevant if we want to remove support for Python 3.5?
jcwchen(2021-03-22 15:24:37):Good catch. Removed. Thanks
askhade(2021-03-22 14:31:55):can we separate this in 2 PRs... to keep things clean.... Create a new 1 for python deprecation and keep this for binary size reduction. Thanks!
jcwchen(2021-03-22 15:26:35):> can we separate this in 2 PRs... to keep things clean.... Create a new 1 for python deprecation and keep this for binary size reduction. Thanks!

Sure thing. A new PR (https://github.com/onnx/onnx/pull/3353) decouple from this PR 
jcwchen(2021-03-18 03:30:30):Mac CI fails because of this https://github.com/pypa/setuptools_scm/issues/542. Will try to reduce binary size as well.
jcwchen(2021-03-18 20:49:35):ONNX decided to deprecate python 3.5 in the following 1.9. Will remove it in this PR to prevent failure caused by Python 3.5 incompatibility
etusien(2021-03-22 14:30:21):Is this still relevant if we want to remove support for Python 3.5?
jcwchen(2021-03-22 15:24:37):Good catch. Removed. Thanks
prasanthpul(2021-03-17 17:42:59):Does this require a IR version bump? @gramalingam 
gramalingam(2021-03-24 23:15:00):Hi: As discussed offline, I think this does require IR version bump. (@pranavsharma is this a concern for ORT?)
gramalingam(2021-03-24 23:17:54):@postrational : any opinion or feedback on this? No ops are being added at this stage. However, adding the type will enable experimentation with custom ops for sparse-tensors before they are promoted into ops in the ONNX standard.
pranavsharma(2021-03-25 00:18:27):> Hi: As discussed offline, I think this does require IR version bump. (@pranavsharma is this a concern for ORT?)

I don't recall updating the IR version when introducing the sequence type.
gramalingam(2021-03-25 02:44:57):```
I don't recall updating the IR version when introducing the sequence type.
```
I think it was part of either IR 6 or 7 update.
pranavsharma(2021-03-17 20:46:49):Can we also add the sparse tensor format to the type? In the absence of this, how does a user know the format she should feed the sparse input to the model?
yuslepukhin(2021-03-22 17:08:55):The problem with the sparse tensor type format is that it would be encoded into the model. However, different backends may favor different ways to implement sparsity and, thus, may have different format sparsity. In fact, depending on the libraries used, different formats may be preferable on different devices.

Thus, it becomes a runtime property. The customer will still need to know what is the optimal format under the circumstances, however, hard-coding it into the model may not have the desired effect.
askhade(2021-03-29 20:55:40):This means typeAndShapeInferenceFunction will have to check the input type and based on the type call the right method of InferenceContext getInputData or getInputSparseData?
askhade(2021-03-29 20:58:47):if existingType does not have shape info then you can simply copy the shape info from inferredType here and return... no need to enter the for loop on line 197 right?
askhade(2021-03-29 21:04:28):we don't need default implementation here...
yuslepukhin(2021-03-29 22:12:28):> This means typeAndShapeInferenceFunction will have to check the input type and based on the type call the right method of InferenceContext getInputData or getInputSparseData?

I think, each kernel would actually query what is required. Currently, all kernels know they would be recieiving Tensors both as input and initializers. The upcoming kernels would query only Sparse Data. In general, it might be both.
The issue here and through out the code, is that we initially have two different fields, initializer and sparse_initializer. And Tensor and SparseTensor. Both of these tuples have the same high level semantic meaning but different types in proto and implementation.
So we are bound to have two different types and at times we would be checking both.
yuslepukhin(2021-03-29 22:13:18):I agree, but the existing code favors this form.
yuslepukhin(2021-03-29 22:13:37):Do we care to preserve the order of vtable entries?
gramalingam(2021-04-05 23:36:50):Hmmm ... I don't know if there are users who want that level of compatibility or that ONNX guarantees that. But I guess this is safer anyway.
askhade(2021-03-22 14:30:28):@gramalingam , @postrational : This PR needs sign off from OP SIG member
postrational(2021-03-18 16:20:32):Python 3.5 has already reached end-of-life. Should we still support it with upcoming releases?
jcwchen(2021-03-18 18:50:39):> Python 3.5 has already reached end-of-life. Should we still support it with upcoming releases?

Thanks for bringing this up. According to the discussion in today's meeting, Python 3.5 deprecation  will be announced in ONNX 1.9 and be done after 1.9. This PR is still needed for now and it's ready for review.
askhade(2021-03-18 19:02:00):> > Python 3.5 has already reached end-of-life. Should we still support it with upcoming releases?
> 
> Thanks for bringing this up. According to the discussion in today's meeting, Python 3.5 deprecation will be announced in ONNX 1.9 and be done after 1.9. This PR is still needed for now and it's ready for review.

In today's meeting we reached the conclusion that we will deprecate 3.5 in 1.9 since keeping it will require us to fix the current issues
jcwchen(2021-03-18 19:26:12):
> In today's meeting we reached the conclusion that we will deprecate 3.5 in 1.9 since keeping it will require us to fix the current issues

Sorry I misunderstood. Close this PR then
matteosal(2021-06-14 15:10:17):@askhade I managed to write the adapters for the deprecated operators (`Scatter` and `Upsample`), removing the deprecated node and creating a new one as you suggested. But it required quite a bit of work, as I had to make all adapters return the current node and change how the converter loops through the nodes.

This PR should be fully ready now.
matteosal(2021-06-15 16:48:38):I have encountered another issue with the version converter: https://github.com/onnx/onnx/issues/3529

It would be nice to discuss a solution for that while this PR is under review, so that a fix could be hopefully added here.
postrational(2021-06-18 12:07:04):Please add instructions about creating a converter adapter to the docs. 
Preferably here:
https://github.com/onnx/onnx/blob/master/docs/AddNewOp.md
Or here:
https://github.com/onnx/onnx/blob/master/docs/Versioning.md#operator-versioning

matteosal(2021-06-18 13:03:54):> Please add instructions about creating a converter adapter to the docs.
> Preferably here:
> https://github.com/onnx/onnx/blob/master/docs/AddNewOp.md
> Or here:
> https://github.com/onnx/onnx/blob/master/docs/Versioning.md#operator-versioning

Sorry, I forgot about that. Done
matteosal(2021-06-22 12:05:59):Any news on this?
matteosal(2021-06-28 15:52:49):@askhade @postrational @gramalingam another ping on this
matteosal(2021-07-08 09:28:27):@askhade @postrational @gramalingam can we please move this forward? It's been more than 6 months since the original PR
jcwchen(2021-06-10 18:08:19):To strengthen the check, shall we use a stricter shape_inference? like `shape_inference.infer_shapes(converted, strict_mode=True)`
matteosal(2021-06-15 12:01:54):Good point, added it
askhade(2021-07-12 17:32:21):Is this step required for new operator?
In case of op updates, what happens if automatic upgrade is not possible?

Can you add more clarification in the doc
askhade(2021-07-12 17:33:50):I see some details are present in Versioning.md... can you add them here as well as people generally use this doc to understand all the steps involved
askhade(2021-07-12 17:39:42):does this create a new Node and returns it? Can you add comments for this mtd
askhade(2021-07-12 17:40:40):I dont understand the point of this change.... input Node* node is not const, it is mutable then why return the same node?
matteosal(2021-07-12 18:52:34):> Is this step required for new operator?

Yes because the new tests check that all operators are tested at least once (`test_ops_tested`)

> In case of op updates, what happens if automatic upgrade is not possible?

What do you mean by this? As long as the old op's functionality is a subset of the new one's, upgrade is always possible. Maybe the only case in which this got violated is `batch_normalization_13_14.h` which I added in this PR (opset 14 disabled outputs 4 and 5). Currently the automatic upgrade tests are not testing this upgrade path (they are FAR from testing all the possible upgrade paths from all possible settings for all ops anyways). In the future we can extend these new tests to also add upgrade paths which are expected to fail at some step.

> Can you add more clarification in the doc
> I see some details are present in Versioning.md... can you add them here as well as people generally use this doc to understand all the steps involved

Versioning.md says to add an adapter whenever a change is made (and don't add a test because the automatic tests will test the adapter), while this is about adding a test whenever a new op is introduced (and don't add an adapter because there's nothing to adapt), so I don't see much overlap. But I have added more information about the basic idea of the new tests.
matteosal(2021-07-12 18:55:52):I had to make this change because in case of deprecated operators we have to destroy the current node and replace it with another one. Since the new node is referenced via its pointer even after `adapt` is called, `adapt` has to return the pointer. But 99% of the time (when the node is preserved and only its attributes/inputs/outputs are changed) the returned pointer is the same as the argument.
matteosal(2021-07-12 18:58:09):See the other reply. All adapters return the same node except adapters for deprecated operators (upsample_9_10.h, scatter_10_11.h).
askhade(2021-07-13 04:30:01):thanks for the clarification.
How are we enforcing\gating PRs on these tests... Will these tests run as part of python test suite and fail if someone did not add a test for new op?
askhade(2021-07-13 04:30:37):you have graph and mutable Node* can you not create a new node and point Node* to this new node?
matteosal(2021-07-13 11:18:51):> thanks for the clarification.
> How are we enforcing\gating PRs on these tests... Will these tests run as part of python test suite and fail if someone did not add a test for new op?

Yes. The CI will fail in these 2 cases:
1) A new op is introduced but an automatic upgrade test is not added
2) An op is changed but an upgrade adapter is not added
matteosal(2021-07-13 11:26:29):Yes but why bother to create a new node, copy all the attributes/inputs/outputs of the old node to the new one and destroying the old node in all adapters when we can almost always modify the old node in place? That's how the adapters have always worked up to just before this PR. I have added the possibility of returning a different node only to deal with deprecated operators. I have added a comment to clarify this
askhade(2021-07-13 17:07:06):what is the difficulty in changing say Upsample Node to Resize Node in place? I remember you mentioned it was not possible... cant remember what is the exact reason
matteosal(2021-07-13 17:40:43):I just couldn't find a way to do it. Which doesn't mean it can't be done, but `kind_` is private in the definition of `Node` and it didn't seem designed to be changed to me. If that's possible I'll revert this change and modify the nodes in place for the deprecated ops.
gramalingam(2021-07-13 18:05:44):So, I guess it is a matter of changing kind_ to be non-constant, and adding a method to SetKind in a Node?
gramalingam(2021-07-13 18:08:42):It seems likely that they didn't anticipate a need to change the operation. But generalizing it to allow it seems reasonable, but I understand your concern whether it might cause some unanticipated problem.
askhade(2021-07-13 18:13:11):Looking at the  (existing) code it is not clear to me what NodeKind is exactly meant for... It is not exactly similar to op_type since it includes popular attribute names and such... and while it includes most of the ops it does not include all... will need to check the usage of Kind through out version converter and optimizer code to understand whether we should add a setter for kind_. 

I am OK with the current change now. There are some fundamental changes that need to happen to version converter (updates post IRv3 .i.e allow initializers not included as inputs) soon and we can reconsider this when we take up that task.

gramalingam(2021-07-13 18:20:14):I am fine with using the current API to play it safe.
gramalingam(2021-04-26 20:32:26):I think full_check and strict_mode serve slightly different purpose right now? Full_check controls whether type/shape inference check is invoked or not, while strict_mode indicates further how strict the type/shape checker should be. So, I don't think option 2 is  a good choice. Option 1 seems okay for now. Option 3 could be considered, but I think we may also want this mode flag to become an enumeration in the future, instead of boolean. BTW, what is the default-value for strict-mode? Isn't it true by default?
jcwchen(2021-04-26 21:30:55):> I think full_check and strict_mode serve slightly different purpose right now? Full_check controls whether type/shape inference check is invoked or not, while strict_mode indicates further how strict the type/shape checker should be. So, I don't think option 2 is a good choice. Option 1 seems okay for now. Option 3 could be considered, but I think we may also want this mode flag to become an enumeration in the future, instead of boolean. BTW, what is the default-value for strict-mode? Isn't it true by default?

Agreed. Option 1 is the best choice for now... The default value of strict_mode is False since enabling it will cause some failures from old models (e.g., pytorch-onnx CIs). The current behaviour with strict_mode=False is suspending shape_inference without throwing error if it bumps into a shape inference error, and the inferred shape will be kept.


askhade(2021-03-22 18:39:06):Python 3.5 deprecation is checked in to master now... just merge with master should work
postrational(2021-03-24 14:50:14):@askhade Should we merge this?
CLAassistant(2021-03-25 10:11:54):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=3361) <br/>All committers have signed the CLA.
postrational(2021-03-25 18:33:33):@jcwchen could you please verify this updated test using ONNX Runtime?
jcwchen(2021-03-25 18:44:10):> @jcwchen could you please verify this updated test using ONNX Runtime?

Yes, they pass ORT backend test and onnx.checker. Thanks.
fdwr(2021-05-04 23:50:09):@tomdol: We see some failures after updating ONNX in ONNX runtime, and I found it's because some tensors have duplicate names:

test_resize_downsample_sizes_nearest_tf_half_pixel_for_nn\test_data_set_0\

input_0.pb - Tensor "X", datatype: float32, dimensions: 1,1,4,4, (64 bytes): 1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16
input_1.pb - *__Tensor "sizes"__*, datatype: int64, dimensions: 4, (32 bytes): 1,1,3,2
input_2.pb - Tensor "scales", datatype: float32, dimensions: 0, (0 bytes)
input_3.pb - *__Tensor "sizes"__*, datatype: int64, dimensions: 4, (32 bytes): 1,1,3,2

Can this be fixed, updating the name in "input_1.pb" to "roi"? I'll just disable that test in meantime. Thanks.
tomdol(2021-05-05 07:44:51):@fdwr will do, sorry I missed that. I'll let you know when the PR is ready.
tomdol(2021-05-05 11:10:53):@fdwr I've re-checked the test and it looks correct to me. The tested model expects the "X" and "sizes" inputs to be passed in. "roi" and "scales" are set to empty strings according to the documentation to indicate that they are optional and should be disregarded for this model.

The input data in `test_resize_downsample_sizes_nearest_tf_half_pixel_for_nn/test_data_set_0` now contains 2 files instead of 4:

```
>>> onnx.TensorProto.FromString(open("input_0.pb", "rb").read())
dims: 1
dims: 1
dims: 4
dims: 4
data_type: 1
name: "X"
raw_data: "\000\000\200?\000\000\000@\000\000@@\000\000\200@\000\000\240@\000\000\300@\000\000\340@\000\000\000A\000\000\020A\000\000 A\000\0000A\000\000@A\000\000PA\000\000`A\000\000pA\000\000\200A"

>>> onnx.TensorProto.FromString(open("input_1.pb", "rb").read())
dims: 4
data_type: 7
name: "sizes"
raw_data: "\001\000\000\000\000\000\000\000\001\000\000\000\000\000\000\000\003\000\000\000\000\000\000\000\002\000\000\000\000\000\000\000"
```

The input names match the operator's input names in the model. Can you please verify on your end?
fdwr(2021-05-05 18:17:52):@tomdol : Sorry for the false alarm 😞. Our test copies the new data, but the copying script left behind the existing old tensors which were deleted in your PR (as this test case was updated inline). The test passes now after wiping the directory 😅. Thanks for your change - it was kinda silly before that the tensor was empty. 👍
CLAassistant(2021-03-29 16:39:04):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=3364) <br/>All committers have signed the CLA.
gramalingam(2021-03-30 00:10:09):This seems similar to https://github.com/onnx/onnx/pull/3207 ... but has extra test-cases, so makes sense to pick this one. The other one also has some signoff issues preventing a merge.
impactaky(2021-03-30 01:28:08):Thank you!
askhade(2021-03-30 17:41:05):running some validations as part of the release validation work... will check this in a few
mkbhanda(2021-03-31 07:57:51):What is the policy in project ONNX on branches out-of-da†e, this is of†en in busy projects. https://github.community/t/requiring-up-to-date-branches-in-a-busy-repo/1550/2
mkbhanda(2021-03-31 07:54:31):is this additional call to add_dim() necessary?
On line 2697 first occurrence of output_shape.add_dim()
mkbhanda(2021-03-31 07:55:38):ditto, a second call to add_dim()
impactaky(2021-03-31 12:08:41):Yes, the output rank should be 2 as written in test, so call add_dim function twice.
askhade(2021-03-31 16:40:56):The shape of non zero op is [rank of input, num of non zero indices] hence the 2 calls to add_dim
mkbhanda(2021-03-31 20:25:30):Thank you for explaining @impactaky and @askhade ! Perhaps add the above explanation as a comment :-)
askhade(2021-03-29 21:50:59):ready to merge - waiting on python 3.6 CI fix
mbencer(2021-03-31 08:08:55):@askhade Do you maybe know why `ONNX_BUILD_SHARED_LIBS` is needed?
I don't see a scenario when it is needed (it seems to be equivalent to `BUILD_SHARED_LIBS` in this case)
askhade(2021-05-19 16:25:51):@mbencer: Some of the cmake options are legacy and I am not sure why they were added. I need to take a thorough look at this PR before I can approve it. Will add my comments by today or tomorrow
ilyachur(2021-07-16 05:52:27):@askhade Do you have an update for this PR?
askhade(2021-07-16 23:02:41):FYI - 
https://github.com/protocolbuffers/protobuf/blob/master/cmake/README.md#dlls-vs-static-linking

If your project is itself a DLL intended for use by third-party software, we recommend that you do NOT expose protocol buffer objects in your library's public interface, and that you statically link protocol buffers into your library.
gramalingam(2021-07-16 23:17:12):There is a long thread relating to this in  a previous PR: please see: https://github.com/onnx/onnx/pull/1938 . Among other things, there is a comment towards the end of the thread that defining the preprocessors macros in the cmake breaks the pytorch build.
gramalingam(2021-07-16 23:33:16):I did not fully understand the context here. Is the goal to produce an ONNX dll that statically links onnx_proto as well as protobuf? Or, is it something else?
mbencer(2021-07-19 07:20:48):> I did not fully understand the context here. Is the goal to produce an ONNX dll that statically links onnx_proto as well as protobuf? Or, is it something else?

The goal for us was to build: shared `protobuf`, shared `onnx_proto`, and static `onnx`.
We had two separate libraries which depend on onnx-related staff (`protobuf, `onnx_proto`) in one application. More details in: https://github.com/onnx/onnx/issues/3319
askhade(2021-07-16 22:22:47):can we keep this default meaning let BUILD_SHARED_LIBS control whether we use STATIC or SHARED

what is the problem with building shared lib when build_shared_lib is set?
askhade(2021-07-16 22:24:45):what is the difference between original set vs the PR version target_compile_definitions(onnx_proto ?

is set global ?
askhade(2021-07-16 22:25:16):typo: be be
mbencer(2021-07-19 07:02:05):The features (classes/members/functions) from onnx library don't use decorators to control visibility.
It is especially required for Windows - `__declspec`.
So the idea behind it was to block building shared `onnx` if it is not supported.
mbencer(2021-07-19 07:13:20):Previous version set value on directory level - it will be visible in current directory scope.
In the current version, we match it only with the target which uses it (it will be visible for `onnx_proto` code and for targets which use `onnx_proto`).
So from a functional point of view, it's rather not a big difference.
askhade(2021-03-31 18:59:15):> There seem to be no references left to the inline function "ProtoDebugString", do we even need it regardless of whether ONNX_USE_LITE_PROTO is defined or not?

Right! but since this is part of proto utils there is a possibility that runtimes or converters may have a dependency on this. Since we are very close to the release dont want to remove this at this moment... usually we give people ample notice before deprecating any utils or apis
jcwchen(2021-03-31 15:33:55):Similar to https://github.com/onnx/onnx/pull/3309 and it can solve https://github.com/onnx/onnx/issues/3307
prasanthpul(2021-03-31 16:30:38):Thanks. Sorry I did not see #3309 before. Since the .pb files are test data, even though they are technically protobuf, it's better to exclude them rather than classify as protobuf. The .proto files are correctly classified as protobuf.
gramalingam(2021-04-01 23:15:43):Thanks very much for resolving the issue with the extra two outputs
gramalingam(2021-04-01 18:55:35):Can we drop the last two outputs "saved_tensor_1" and "saved_tensor_2" from the spec?
pranav-prakash(2021-04-02 22:55:21):@gramalingam Isn't the batch mean/var needed for the gradient calculation? If it's no longer outputted as part of the forward pass then it will have to be redundantly recomputed in the gradient op. Granted this is a backend implementation detail, but currently I think the only mechanism in ORT to pass tensors between ops is to explicitly declare them input/output?
gramalingam(2021-04-02 23:17:42):@pranav-prakash : there was an offline discussion as to what these values should be. The conclusion was that there was no unique-choice and that the actual backends compute something different from what was in the ONNX specification (either the inverse of variance or inverse of std-dev, etc.) So, the conclusion was to let backends transform the graph/node into a custom-op of their choice (for backward-propagation).
pranav-prakash(2021-05-01 04:31:00):@gramalingam Hm if the backends need to replace Batchnorm with their own custom-op anyway, then isn't having the `running_mean/var` outputs pointless? Any custom-op that the backend defines would have to duplicate the entire schema def anyway, so there doesn't seem to be any advantage to including the `running_mean` explicitly in the official spec. (By such an argument, `is_training` could be deemed redundant as well, although I suppose that might have some use purely as a sentinel to indicate that the model was exported in training mode).
gramalingam(2021-05-02 01:46:38):Technically, note that the model can use running_mean/var outputs later in the model ... backends that replace BatchNorm will then still be computing these for the subsequent usage, but using other mechanisms. @neginraoof can say more about how they plan to use this, as this was requested for some model export. 
jcwchen(2021-04-01 15:05:35):nit: We can only set `1.8.200` in `VERSION_NUMBER` and set `1.9.0` in all other places. (For instance, https://github.com/onnx/onnx/blob/f22bde0a01899a4497e98c7fc22ffccf3362b07c/onnx/common/version.h#L11)

askhade(2021-04-01 16:31:38):> We need to bump opset_version here as well:
> 
> https://github.com/onnx/onnx/blob/f22bde0a01899a4497e98c7fc22ffccf3362b07c/onnx/defs/schema.h#L902
> 
> One question: why don't we bump version number in the main branch? (Then we don't need to merge release branch back to the main branch in the end)

That is because we build onnx-weekly packages off of master branch. IF we update the version number in master then we will see onnx-weekly package version bump before the release
jcwchen(2021-04-01 16:37:42):
> That is because we build onnx-weekly packages off of master branch. IF we update the version number in master then we will see onnx-weekly package version bump before the release

That would be a pre-release (1.9.0.devXXX) before the real released 1.9.0. Still, I am OK with bumping it in the release branch. Thank you for the explanation

jcwchen(2021-04-07 20:44:37):We have merged a similar PR:https://github.com/onnx/onnx/pull/3408 to bring ONNX TestPyPI package to final verification. Thank you @postrational !
postrational(2021-04-15 14:52:38):Closed in favor of  #3408
etusien(2021-04-01 14:28:09):Reopened in another PR  
jcwchen(2022-07-21 18:26:54):@onnx/sig-operators-approvers Could you please review/approve this PR? Thank you for your time!
askhade(2021-11-17 22:21:59):what is the motivation behind generating the data in the CI pipeline? why not simply test the data which is already checked in?
jcwchen(2021-11-19 20:38:10):For now, it can only make sure models from test generation on various platforms (Windows, Linux, Mac) pass strict onnx.checker. It does not require the models generated from different platforms to be the same as the checked-in ones. We can probably achieve that after https://github.com/onnx/onnx/pull/3855 is merged.
askhade(2021-12-08 19:08:27):what is "-k OnnxBackendNodeModelTest" for?
askhade(2021-12-08 19:09:03):Also earlier test script was running both checker and shape_inference checks on models and now we are only doing the shape_inference check?
askhade(2021-12-08 19:11:27):I think we should wait for #3855 and then refine this PR. 
Once #3855 is merged we should use since_version to create all the test data and run a git diff check to verify there are no diffs. This will help us validate the PR is including all the test data that it needs to (based on the unit test cases added) and that the test data is refreshed (in the past we have run into cases where author's made changes to the test case and forgot to regenerate the test data. This resulted in bad test data being checked in)
jcwchen(2021-12-08 21:35:42):Specify OnnxBackendNodeModelTest here will make onnx\test\test_backend_test only test those models under `onnx/backend/test/case/node/`.

> Also earlier test script was running both checker and shape_inference checks on models and now we are only doing the shape_inference check?

Yes. To keep the same behavior as previous verification, I just added onnx.checker in test_backend_test.py. It can also enhance the test for backend data.

askhade(2021-12-08 21:38:58):I suggest moving check_model above infer_shapes. No point in inferring shapes for an invalid model.
askhade(2021-12-08 21:40:58):Where is "OnnxBackendNodeModelTest " defined? The name suggests it will run tests for both Node and Model
jcwchen(2021-12-08 21:54:11):I cannot find the source code about it shortly, but from my understanding the name here is collected as "OnnxBackend-[directory name under [here](https://github.com/onnx/onnx/tree/master/onnx/backend/test/data)]-ModelTest".
For instance, OnnxBackend"Node"ModelTest or OnnxBackend"PyTorchConverted"ModelTest.


gramalingam(2021-04-02 23:11:08):Thanks! May help to apply same fix to older version of Shape (in old.cc) also.
CLAassistant(2021-04-02 23:15:45):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=3394) <br/>All committers have signed the CLA.
mrry(2021-04-02 23:17:56):Thanks @gramalingam... good idea, done!
askhade(2021-04-06 17:47:00):IR_Version needs to be updated too right?
gramalingam(2021-04-06 18:30:33):A generic question: is this the same as the closed PR, or is there any significant difference in this PR?
yuslepukhin(2021-04-06 18:41:57):There should be no difference as this was created to deal with DCO issues.

---
In reply to: [814347121](https://github.com/onnx/onnx/pull/3398#issuecomment-814347121) [](ancestors = 814347121)
yuslepukhin(2021-04-06 18:46:02):> IR_Version needs to be updated too right?

Will do
askhade(2021-04-06 17:51:05):Can this be simplified by using a template? 
askhade(2021-04-06 17:52:43):do we even need a unique_ptr here? Can the vector simply hold the TypeProto object?
gramalingam(2021-04-06 18:34:14):(General comment, not specific to this PR). I wonder if this include file is worth splitting into an include-file and implementation-file (in a separate PR, doesn't have to be in this). It is growing big, and it is better to reduce unnecessary implementation-dependence some users might acquire.
yuslepukhin(2021-04-06 18:44:34):I was thinking along the same lines.
yuslepukhin(2021-04-06 18:45:42):Will do.
askhade(2021-04-06 18:48:28):I also agree
yuslepukhin(2021-04-07 01:05:39):I think this would fall under refactoring category and we better manage it under a separate PR.
jcwchen(2021-04-07 18:28:17):I think L13 (`"\n",`) needs to be removed as well. The check here is quite unfriendly for people who want to update it... I will try to improve it in another PR. Thank you for the effort.
askhade(2021-04-20 20:53:09):no we dont... you can remove this comment
askhade(2021-04-20 21:00:37):may be we can change this to published with ONNX release 1.10? This way no one will forget to update the TBD and go through the pain of finding the date and whether it was published or still under work like you had to
gramalingam(2021-04-26 21:43:41):I think a default implementation does help users who have an implementation of InferenceContext use this version without modifying their code (which seems reasonable if they do not want to handle sparse data yet).
SherlockNoMad(2021-04-09 05:33:54):During mixed precision training, input_mean, input_var and running_mean and running_var are usually kept in fp32, while X and Y can be in fp16.

see https://docs.nvidia.com/deeplearning/performance/mixed-precision-training/index.html#lossscaling 

We need to introduce a different type for input_mean, input_var and running_mean and running_var. 
askhade(2021-04-09 15:35:07):The corresponding cherry pick change has been already checked in release branch so we should not make any more changes to this PR... any changes should be made in a new PR
jcwchen(2021-04-07 16:55:17):Don't forget to consider the comment here: https://github.com/onnx/onnx/pull/3406#discussion_r608826460
IceTDrinker(2021-04-09 07:10:58):If you need a squashed commit don't hesitate to ping
IceTDrinker(2021-04-28 07:46:27):Test dependencies (protobuf) on Linux seem broken, I also see some errors related to pybind11
IceTDrinker(2021-04-28 08:09:08):gentle ping @jcwchen I saw you were working on some requirements for builds, is the failure in the Linux CI related to requirements issues ? It seems so to my untrained eye with the protobuf version apparently not being cloned properly during dependencies installation
jcwchen(2021-04-28 16:01:51):> gentle ping @jcwchen I saw you were working on some requirements for builds, is the failure in the Linux CI related to requirements issues ? It seems so to my untrained eye with the protobuf version apparently not being cloned properly during dependencies installation

Thank you for catching this. I just sent a PR to fix this
gramalingam(2021-04-28 16:30:45):LGTM, thanks!
IceTDrinker(2021-04-29 19:52:28):Anything I need to do on my end to be able to merge this PR ? I don't seem to be able to request a review from onnx/sig-archinfra-approvers
jcwchen(2021-04-30 14:21:03):Could @onnx/sig-archinfra-approvers please review this PR? Thank you.
IceTDrinker(2021-05-04 10:09:27):Hello @onnx/sig-archinfra-approvers anything else to be done for this PR ?
gramalingam(2021-04-07 16:48:54):Minor nit: I would suggest "where the denominator is N and not (N-1), where N is the number of summed elements." If we want to contrast with sample variance, suggest changing "refers to the population variance" to "refers to the population variance, and not sample variance".
gramalingam(2021-04-07 17:40:47):Sorry about the comment on the wrong PR! I should have added this to the original PR.
yuslepukhin(2021-04-08 17:58:24):Would be nice to have a general description of the PR as having TypeProto in the attribute is unusual and requires explanation.
neginraoof(2021-04-08 19:15:03):Thanks @yuslepukhin for the review. This is a WIP that I opened to get some feedback as I'm completing the work. Will add more details on why this is needed.
postrational(2021-04-15 14:45:08):@neginraoof Please add a description to this PR with a motivation for this new type and examples of how it will be used.
postrational(2021-04-15 16:39:09):What operators are we expecting to use this optional type?
neginraoof(2021-04-28 00:59:33):@gramalingam @jcwchen @postrational 
Thanks a lot for your comments. I have addressed your comments. Please help in reviewing the updates.

neginraoof(2021-05-13 17:08:19):@askhade @gramalingam 
Do you have any other comments?
Thanks for review.
neginraoof(2021-05-13 18:45:16):cc @postrational 
Please help us review this PR. Thanks.
gramalingam(2021-05-25 23:26:51):I notice that the IR version has already been incremented due to the addition of SparseTensor. Can you please add a comment there, documenting the new IR change (addition of Optional type)?
MikeRooke(2021-10-25 14:17:59):Are there any examples of how #3407 is applied / used in practice? How is the optional type used with an operator e.g could convInteger use int16 as an example?
yuslepukhin(2021-04-08 18:01:17):Changes in in.proto files require regeneration of the actual proto files. Use
`python onnx/gen_proto.py -l`
`
python onnx/gen_proto.py -l --ml
`
yuslepukhin(2021-04-08 18:04:15):I have a PR pending that introduces SparseTensor as first class citizen. Should I also add some changes here so sparse attributes are handled or it is something that the converter team would address later?
yuslepukhin(2021-04-08 18:04:38):Comments must be resolved one way or another.
neginraoof(2021-04-08 22:04:51):Check if we need oneof here.
neginraoof(2021-04-09 03:33:43):@yuslepukhin 
Thanks. I'm trying to find a method or write one to copy or clone onnx::TypeProto. Do you have a suggestion for this part?
jcwchen(2021-04-09 23:21:44):Why it is not AttributeProto.TYPE_PROTOS?
jcwchen(2021-04-09 23:23:36):nit: L490-L552 Could we sync all the bracket and break with the same coding style? It looks a little confusing.
jcwchen(2021-04-09 23:59:52):L409: Add something like `self.assertFalse(optional_none.HasField('tensor_values'))` ?
jcwchen(2021-04-09 23:59:55):nit: Shall we directly use OptionalProto.TENSOR? (which is more readable) Also see L408.
jcwchen(2021-04-10 00:20:16):Random thought:
Is it possible to create a new utility function for these if-else conditions? (L288-L295 and L150-L157 is a similar logic) `from_list` and `from_optional` have the same situation.

Since using another new function might lose some readability, honestly I am open to both options (keep the original way or introduce a new function). Thanks.


gramalingam(2021-04-12 16:50:44):Run clangformat?
gramalingam(2021-04-12 16:53:51):"Sequence" => "OptionalProto" ?
yuslepukhin(2021-04-12 17:03:04):> 
> 
> @yuslepukhin
> Thanks. I'm trying to find a method or write one to copy or clone onnx::TypeProto. Do you have a suggestion for this part?

The method is called CopyFrom() here is an example of [usage](https://github.com/onnx/onnx/pull/3398/files#diff-df8e9d76c8ada1b583a5dd877963bd651fbf76ec0cac241d05bf97af4ef8c8f4R613)
gramalingam(2021-04-12 17:29:20):A couple of related questions: (a) Does the caller have to specify elem_type, or should we infer it from the actual value? (b) What is the purpose of elem_type in the proto, is it to just indicate which variant is being used or to indicate the full-type of the contained value? I think we can only do the former, not the latter. Once the "proto" spec is clarified, I think the helper methods can be adjusted accordingly.
gramalingam(2021-04-12 17:31:09):Stated another way: how is a None value represented in the Proto? Is elem_type supposed to be UNDEFINED?
gramalingam(2021-04-12 17:31:58):"value" may be more appropriate than "values"
yuslepukhin(2021-04-12 17:56:57):@neginraoof if using Visual Studio, configure it to automatically pickup .clangformat file in this repo and other related options.
gramalingam(2021-04-12 17:57:29):(Comment meant mainly for SequenceProto, but also here for completeness) We should add "OPTIONAL = 5" to this enum, and add a field "repeated OptionalProto optional_values" (for SequenceProto) and, similarly, "optional OptionalProto optional_value" here ... even though an optional value of optional type is unlikely to be used, it would help to be complete.
gramalingam(2021-04-12 17:58:00):All these names must be singular "tensor_value", etc.
neginraoof(2021-04-22 20:41:48):I think we can have two maps and use the key of elem_type to get corresponding the transform. I think this is pretty readable and small for now. I'll leave a todo if the list gets expanded.
neginraoof(2021-04-22 20:45:51):Thanks!
neginraoof(2021-04-22 21:30:08):Yeah would be great if you can add sparse tensor attributes. Thanks!
neginraoof(2021-04-22 21:36:20):@gramalingam 
I think from what we discussed, we decided to mandate setting the elem_type even for None.
Use of UNDEFINED I think is mainly for data serialization purposes.
jcwchen(2021-04-23 18:53:13):This one should be 7?
neginraoof(2021-04-23 19:16:16):Thanks!
gramalingam(2021-05-03 21:50:21):Is this used anywhere? I think it would be better NOT to introduce this kind of "limited" set of special types or encourage its use in op schemas.
gramalingam(2021-05-03 23:29:55):I think this is not going to work, because SequenceProto.TENSOR and OptionalProto.TENSOR have same (key) value. We will have to use two different maps, one for Sequence, and one for Optional.
neginraoof(2021-05-06 15:26:40):It is not used yet, but I guess it would be needed once introducing the ops and specifying input/output types.
What is your suggestion for this use-case?
askhade(2021-05-06 16:40:28):Can you add some comments regarding some requirements... meaning acceptable/unacceptable values 
askhade(2021-05-06 17:06:40):make_optional_value_info will only work for tensors and not sequences or sparse tensors?
neginraoof(2021-05-07 02:25:38):This was replicating the behavior of make_sequence_value_info. I extended this for to support sequences as well.
askhade(2021-05-10 17:09:17):Once we introduce ops do we need to support all these types? can we remove this and simply add support for the set of types which are required for the op? Example optional(seq(tensor(complex64))) it seems unlikely that we will need support for this... 
askhade(2021-05-10 17:12:20):another question regarding the elem_type - similar to Rama's question what is the expected value for elem_type? Is it the type held by Optional say Optional <Tensor, Sequence etc> or the complete type meaning optional(Tensor(float32))
gramalingam(2021-05-13 18:20:27):I am slightly bothered by this utility: it is not fully general-purpose, and seems specialized to handle just the common cases (of the optional type). E.g., I think it won't work for creating more complex things like "optional sequence of sequence of tensor" etc., or am I mistaken? I feel it may be better to omit this utility for now (or replace with a general-purpose utility which would be extra work). Otherwise, it would be problematic to update it later on if users start taking a dependence on its current form.
neginraoof(2021-05-13 18:36:50):This is very close (and even more generic) than make_sequence_value_info.
This function would be needed for shape inference of an op with optional input, and I have the spec for the new op to send right after this PR.
I currently don't see any of the similar utility functions handling more complex types, and I don't also see a use-case for it.
Anyways, if you think so, I can remove it. But the following PR I have requires this.
gramalingam(2021-05-13 18:56:37):I agree that make_sequence_value_info has a similar problem. I am afraid that we are duplicating the problem and spreading it further. My concern is if we end up with lot of code assuming that a "sequence" is only a "sequence of tensor" or that an "optional" is only an "optional tensor" or "optional sequence of tensor" (and then we end up trying to using something more general like a "sequence of sequence of tensor" we may hit issues). When I say remove it, I mean that we can write the equivalent code in the caller. At a call-site, the caller knows that they are only creating an "optional tensor" and so write code for that, and that looks preferable. Anyway, happy to hear other opinions. I don't mind the current form if we can clean it up later, but that is tricky once users start using it widely.
gramalingam(2021-05-13 19:10:05):nit: "Python list" seems like a copy-paste error. Maybe a "Python optional"?
gramalingam(2021-05-13 19:10:32):Doesn't return a list (always).
gramalingam(2021-05-13 19:12:08):We don't really have repeated values. Why is this loop needed? 
gramalingam(2021-05-13 19:12:38):Not a list.
gramalingam(2021-05-13 19:29:34):Alternatively, we could define two specialized functions with different names like "make_optional_tensor_value_info" and "make_optional_sequence_tensor_value_info" ... at least, it makes it clear it is for a special-case.
gramalingam(2021-05-14 22:28:40):There seems to be some potential confusion between this and the passing of elem_type to ```make_sequence_value_info``` down below ... both elem_types cannot be the same, they must be different, right?
gramalingam(2021-05-14 22:31:21):Is "elem_type" supposed to be OptionalProto.TENSOR as in assert a few lines above, or is it supposed to the element type of tensor? I think the latter one. But I am unclear how a test-case would pass that assert above.
neginraoof(2021-05-14 23:27:36):Fixed this with another argument for seq_elem_type
askhade(2021-05-14 23:36:24):building sequence is a bit complicated. Theoretically sequence can contain a tensor or a sequence of tensors or a sequence of sequence of tensors etc...

Current make_sequence_value_info only supports Sequence of Tensors ... Are there use cases for Optional type to support anything other Sequence of Tensors? if so then make_sequence_valu_info needs to be fixed. 

Regardless we should add some documentation and checks around this 
askhade(2021-05-14 23:46:52):actually looking at the caller the elem_type is OptionalProto.Tensor
if ele_type == onnx.mapping.OptionalProto.TENSOR:
            return onnx.helper.make_optional_tensor_value_info(
                name=name,
                elem_type=ele_type,
                shape=None)

what this means is this type will not work for make_tensor_value_info
askhade(2021-05-14 23:55:50):add some error checking around this? as in whether dtype falls within the expected values
askhade(2021-05-14 23:57:40):a bit confused... why is this SequenceProto.Tensor?
askhade(2021-05-15 00:01:22):not sure if I am missing something - 317 to 322 set elem_type to SequenceProto.blah yet here there is no option for SequenceProto.blah 
gramalingam(2021-05-15 01:22:38):Nit: IMHO, we can just drop the "elem_type" parameter, which is used only in the assert. We only need the seq_elem_type.
neginraoof(2021-05-17 20:14:23):Thanks, yeah this part needs to be updated to OptionalProto.
gramalingam(2021-05-24 18:00:46):Seems redundant to create a ValueInfoProto and then throw it away. I think we can just do ```type_proto = TypeProto()``` and ```tensor_type_proto = type_proto.tensor_type```, right?
gramalingam(2021-05-24 18:02:51):And change this to ```return type_proto``` (see comment above).
gramalingam(2021-05-24 18:04:42):See previous comment. This seems redundant. We should just create a TypeProto() here.
gramalingam(2021-05-24 18:05:28):As above. Should just create a TypeProto.
gramalingam(2021-05-24 18:05:48):As above. Should just create a TypeProto.
askhade(2021-05-27 17:36:52):nit: move this above "make_sparse_tensor_value_info"
CLAassistant(2021-04-08 05:57:31):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=3410) <br/>All committers have signed the CLA.
prasanthpul(2021-04-08 14:36:33):Why isn't this 1.9?
etusien(2021-04-08 14:45:03):This is still a candidate for ONNX 1.9.0 release. Currently, we're testing the package on TestPyPi.
CLAassistant(2021-04-08 19:16:10):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=3412) <br/>All committers have signed the CLA.
gramalingam(2021-04-12 16:43:37):LGTM, thanks! Please note that the opset will likely need to be adjusted to 15 (unless this PR is cherry-picked for the next release).
postrational(2021-04-15 14:50:37):Yes, this PR wasn't included in our 1.9 release, so we'll need to include it in op set 15.
manbearian(2021-04-15 15:46:01):> Yes, this PR wasn't included in our 1.9 release, so we'll need to include it in op set 15.

Seems like something i should be able to fix. I'll push a change shortly
CLAassistant(2021-04-09 14:52:34):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=3414) <br/>All committers have signed the CLA.
odidev(2021-05-05 11:45:34):@jcwchen, could you please merge this PR. Thanks in advance.

jcwchen(2021-05-05 16:02:36):Could @onnx/sig-archinfra-approvers please review this PR? Thanks.
odidev(2021-06-23 10:16:44):Could you please review this PR? Thanks

askhade(2021-06-23 16:19:01):@odidev : Please fix pipeline failures:
![image](https://user-images.githubusercontent.com/6475296/123132846-098b1680-d404-11eb-977a-ffba831fdae0.png)

askhade(2021-07-12 23:20:02):@odidev : can you please resolve conflicts? 
askhade(2021-07-13 04:31:26):@odidev : we can check this PR in once the conflicts are resolved
askhade(2021-07-15 22:53:30):@odidev : FYI - The code freeze date for ONNX 1.10 is July 15th (today) since this PR is reviewed and ready we can wait until July 20th. Otherwise this PR will miss ONNX 1.10 release.
odidev(2021-07-16 09:05:00):> @odidev : can you please resolve conflicts?

Done.
jcwchen(2021-04-09 21:34:27):```suggestion
    branches: [rel-*, master]  # TODO remove master before merge
```
Could you please add master trigger for testing? Thank you.
jcwchen(2021-04-09 21:35:46):L43, L49
nit: this should be my typo: PYTHON_COMMAND 
jcwchen(2021-04-09 21:43:09):I thought the wheel name for aarch64 would be something else other than -manylinux? Also L85 might be not needed for aarch64
odidev(2021-04-12 11:56:56):Done.

odidev(2021-04-12 11:57:12):Corrected.
odidev(2021-04-12 12:00:53):It is required as before auditwheel the aarch64 wheels will be generated with name "onnx-1.8.1-cp39-cp39-linux_aarch64.whl" and  auditwheel repair creates new wheel  file with name "onnx-1.8.1-cp39-cp39-manylinux2014_aarch64.whl".
jcwchen(2021-04-12 20:52:41):Also L66 and L68 for PIP_COMMAND. Thank you!
askhade(2021-06-23 16:21:36):add workflow_dispatch: on line 11 to enable manual trigger similar to : https://github.com/onnx/onnx/blob/master/.github/workflows/release_linux_x86_64.yml#L11
askhade(2021-06-23 16:23:59):add an if here similar to https://github.com/onnx/onnx/blob/master/.github/workflows/release_linux_x86_64.yml#L15
to enable manual trigger

We enable manual triggers from master by setting a label "run release CIs" on the PR
askhade(2021-06-23 16:25:12):nit: add a space after even task for better readability
askhade(2021-06-23 16:28:28):this workflow is different from others since you are running inside a docker container because this is aarch64 platform. Please add detailed comments through out this yml to help understand this workflow.
askhade(2021-06-23 16:29:51):Can you club "Install python dependencies" and "Install dependencies" together? 
askhade(2021-06-23 16:32:35):why do you need virtualenv? 
askhade(2021-06-23 16:33:21):I think dist/*-manylinux2014_aarch64.whl should be changes to dist/*manylinux2014_aarch64.whl
askhade(2021-06-23 16:35:34):we added a few more verification steps to the pipelines to cut the manual testing effort. Please add them for this pipeline too. You can refer to: https://github.com/onnx/onnx/blob/master/.github/workflows/release_linux_x86_64.yml#L75
odidev(2021-07-09 05:34:51):By using virtual environment we are making already installed dependencies available for different tasks. 
odidev(2021-07-09 05:35:06):Done.
odidev(2021-07-09 05:35:18):Done.
odidev(2021-07-09 05:35:28):Done.
odidev(2021-07-09 05:35:39):Done.
odidev(2021-07-09 05:35:52):Done.
odidev(2021-07-09 05:36:01):Done.
askhade(2021-07-09 05:47:16):git actions does let you do this across tasks... you dont need virtual env for that
jcwchen(2021-07-09 17:17:16):nit: this #TODO comment can be removed now
jcwchen(2021-07-09 17:22:18):How about Python 3.6?
jcwchen(2021-07-09 17:42:10):I assume the protobuf version from yum package is not 3.11.3, right? I think a better way is to install protobuf from source like this: https://github.com/onnx/onnx/blob/31f0fd6103e231664d1cfd3b25264f672f5de960/.github/workflows/manylinux/entrypoint.sh#L18

BTW, I think anyway protobuf will be installed by entrypoint.sh? Is the installation here required?
odidev(2021-07-16 09:08:59):Yes, I checked it, it is not required as it is already installed. 
odidev(2021-07-16 09:10:20):Done.
odidev(2021-07-16 09:10:31):Removed.
jcwchen(2021-04-09 17:38:40):cc @SherlockNoMad 
gramalingam(2021-04-09 17:40:29):Drop ", but backend would only implement for fp32." That is not part of the spec.
jcwchen(2021-04-09 17:44:27):Dropped. Thanks
wschin(2021-04-21 23:11:47):This looks very reasonable. Pytorch (and other frameworks) has two versions of alignment. Is it possible to maintain BC by introducing an attribute like this one
```
 torchvision.ops.roi_align(aligned=False…)
```
? We also need to bump operator version because we make `ambiguous` behavior clear.
fdwr(2021-04-22 02:10:08):@wschin: Hi Wei-Sheng. We can add an attribute for compat sake if older models need it, but I recommend that either way (like was done with the resampling operator [`Resize`](https://github.com/onnx/onnx/blob/master/docs/Operators.md#Resize) attribute `coordinate_transformation_mode`) we default to the corrected one in future opsets (that is default = `half_pixel` for `RoiAlign-14`). The only reason frameworks have 2 is because they introduced the buggy one first 😅 (sad to have to encode their mistake too).

Regarding naming, the other frameworks call the attribute which controls the coordinate transforms and pixel offsets names like `aligned`, `align_corners`, or `half_pixel_centers`. For consistency of ONNX with itself, I propose using the existing attribute `coordinate_transformation_mode` with two legal values: "half_pixel" (fixed behavior), plus one (existing buggy behavior) to convey that only the *output* is offset like "output_half_pixel" (alas none of the existing `coordinate_transformation_mode`'s in the resampling operator capture this).

We *could* also include `coordinate_transformation_mode` = `align_corners` for the sake of `tf.image.crop_and_resize(…)`, but I am reluctant to propagate that resampling algorithm as it's improper from a computer graphics perspective, and TF fixed the behavior for `resize` but not yet `crop_and_resize` (https://github.com/tensorflow/tensorflow/issues/6720). So I'll defer that until someone else pushes for it.
wschin(2021-04-22 18:13:43):Sounds good to me. Extending `coordinate_transformation_mode` is general enough to cover all unknown issues and default to the correct setting is good too. Agree that we only need to focus on this misalign in this PR.
gramalingam(2021-04-27 03:28:22):On a different note: I was looking at the documentation of the operator. Why does the "rois" input have a floating type, instead of integer type? Did they use T1 instead of T2 by mistake?
fdwr(2021-04-27 03:43:50):> On a different note: I was looking at the documentation of the operator. Why does the "rois" input have a floating type, instead of integer type? Did they use T1 instead of T2 by mistake?

@gramalingam : Floating point is intentional, as objects are identified at a subpixel alignment, which is why RoiAlign has more accuracy over simple integer-aligned RoiPooling. From the Mask-RCNN paper: "RoIPool first quantizes a floating-number RoI to the discrete granularity ... These quantizations introduce misalignments between the RoI and the extracted features. While this may not impact classification, which is robust to small translations, it has a large negative effect on predicting pixel-accurate masks."

![image](https://user-images.githubusercontent.com/1809166/110403224-95ce2500-8031-11eb-80c0-92732fa247d2.png)
gramalingam(2021-04-26 19:12:09):Instead of having just the numbers, is it possible to have code that generates this? That is, some python code that can act as a reference implementation? I guess the original test-case perhaps pre-dates the requirement to use reference implementations to the extent possible.
fdwr(2021-04-26 19:27:23):I'm not sure. Maybe [scipy.signal.resample](https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.resample.html]) or [scipy.ndimage.zoom](https://docs.scipy.org/doc/scipy/reference/generated/scipy.ndimage.zoom.html) would work 🤔 - I only compared against other frameworks like Detectron/PyTorch/TF.

For the 2nd smaller test case at least, I rather like having directly visible numbers, which make the test cases clearer and more immediately comprehensible (saying this as someone who's had to implement multiple ONNX operators for DML, and frequently referred back to Operators.md 😅). For the 1st case with the larger tensors, cutting that sea of numbers and replacing it with a numpy equivalent would be nice.
gramalingam(2021-04-26 21:10:20):Sure, I agree that for the smaller example, visually matching input/output is useful. But for the larger it is harder. The code could also be specialized for different cases, if it makes the job easier.
fdwr(2021-04-27 00:49:54):We only want dependencies on pure numpy for the ONNX backend, right? I couldn't find any function in `numpy` that resamples tensors (and the numpy [interpolation functionality](https://numpy.org/doc/stable/reference/generated/numpy.interp.html) works on a series of *coordinates*, not tensors), and playing with various scipy functions, `scipy.ndimage.zoom` and `scipy.ndimage.affine_transform` were the two that worked as expected, but those are scipy dependencies. :/

```python
# pip install numpy==1.18.5 scipy==1.6.3
import numpy
import scipy
from scipy import ndimage
from scipy import signal
from scipy import misc

numpy.set_printoptions(linewidth=120)

x = numpy.array(
        [[ 0, 1, 2, 3],
         [ 4, 5, 6, 7],
         [ 8, 9,10,11],
         [12,13,14,15]],
        dtype=numpy.float32
    )
print("input:\n", x, sep='')

# ✖numpy.interp - Unsuitable. Takes an array input coordinates, not a tensor.
# https://numpy.org/doc/stable/reference/generated/numpy.interp.html

# ✖scipy.interpolate.interp2d - Unsuitable. Takes an array of input points, not a tensor or 2D image.
# https://docs.scipy.org/doc/scipy/reference/generated/scipy.interpolate.interp2d.html

# ✖scipy.misc.imresize - Unsuitable. Deprecated and removed in later version. Also has dependency on PIL anyway.
# AttributeError: module 'scipy.misc' has no attribute 'imresize'
# https://docs.scipy.org/doc/scipy-1.2.0/reference/generated/scipy.misc.imresize.html
#y = scipy.misc.imresize(x, (8,8), 'bilinear')
#print("\nscipy.misc.imresize\n", y, sep='')

# ✖scipy.signal.resample - Unsuitable. Only takes a single axis.
y = scipy.signal.resample(x, num=8, axis=1)
print("\nscipy.signal.resample:\n", y, sep='')

# ✖scipy.ndimage.zoom grid_mode = False - Unsuitable. Aligns corners rather than pixel centers.
y = scipy.ndimage.zoom(x, zoom=2, order=1, grid_mode=False)
print("\nscipy.ndimage.zoom grid_mode=False:\n", y, sep='')

# ✔scipy.ndimage.zoom grid_mode = True - Suitable. Aligns half pixels.
y = scipy.ndimage.zoom(x, zoom=2, order=1, mode='nearest', grid_mode = True)
print("\nscipy.ndimage.zoom grid_mode=True:\n", y, sep='')

# ✔Suitable.
y = scipy.ndimage.affine_transform(x, matrix=numpy.array([[0.5,0],[0,0.5]]), offset=[-0.25, -0.25], output_shape=(8, 8), order=1, mode="nearest")
print("\nscipy.ndimage.affine_transform:\n", y, sep='')
```

```
input:
[[ 0.  1.  2.  3.]
 [ 4.  5.  6.  7.]
 [ 8.  9. 10. 11.]
 [12. 13. 14. 15.]]

✖scipy.signal.resample:
[[ 0.          0.08578646  1.          1.5         2.          2.9142137   3.          1.5       ]
 [ 4.          4.0857863   5.          5.5         6.          6.9142137   7.          5.5       ]
 [ 8.          8.085787    9.          9.5        10.         10.914213   11.          9.5       ]
 [12.         12.085787   13.         13.5        14.         14.914213   15.         13.5       ]]

✖scipy.ndimage.zoom grid_mode=False:
[[ 0.          0.42857143  0.85714287  1.2857143   1.7142857   2.142857    2.5714285   3.        ]
 [ 1.7142857   2.142857    2.5714285   3.          3.4285715   3.857143    4.285714    4.714286  ]
 [ 3.4285715   3.857143    4.285714    4.714286    5.142857    5.571429    6.          6.428571  ]
 [ 5.142857    5.571429    6.          6.428571    6.857143    7.285714    7.714286    8.142858  ]
 [ 6.857143    7.285714    7.714286    8.142858    8.571428    9.          9.428572    9.857142  ]
 [ 8.571428    9.          9.428572    9.857142   10.285714   10.714286   11.142858   11.571428  ]
 [10.285714   10.714286   11.142858   11.571428   12.         12.428572   12.857142   13.285714  ]
 [12.         12.428572   12.857142   13.285714   13.714286   14.142858   14.571428   15.        ]]

✔scipy.ndimage.zoom grid_mode=True:
[[ 0.    0.25  0.75  1.25  1.75  2.25  2.75  3.  ]
 [ 1.    1.25  1.75  2.25  2.75  3.25  3.75  4.  ]
 [ 3.    3.25  3.75  4.25  4.75  5.25  5.75  6.  ]
 [ 5.    5.25  5.75  6.25  6.75  7.25  7.75  8.  ]
 [ 7.    7.25  7.75  8.25  8.75  9.25  9.75 10.  ]
 [ 9.    9.25  9.75 10.25 10.75 11.25 11.75 12.  ]
 [11.   11.25 11.75 12.25 12.75 13.25 13.75 14.  ]
 [12.   12.25 12.75 13.25 13.75 14.25 14.75 15.  ]]

✔scipy.ndimage.affine_transform:
[[ 0.    0.25  0.75  1.25  1.75  2.25  2.75  3.  ]
 [ 1.    1.25  1.75  2.25  2.75  3.25  3.75  4.  ]
 [ 3.    3.25  3.75  4.25  4.75  5.25  5.75  6.  ]
 [ 5.    5.25  5.75  6.25  6.75  7.25  7.75  8.  ]
 [ 7.    7.25  7.75  8.25  8.75  9.25  9.75 10.  ]
 [ 9.    9.25  9.75 10.25 10.75 11.25 11.75 12.  ]
 [11.   11.25 11.75 12.25 12.75 13.25 13.75 14.  ]
 [12.   12.25 12.75 13.25 13.75 14.25 14.75 15.  ]]
```
gramalingam(2021-04-27 01:18:13):@askhade : is there any issue or concern with using scipy? Having said that, I don't know if it is worthwhile introducing extra dependencies for this one utility. I took a look at the pooling operations, and they have the same issue (and just have inputs/outputs). Thanks @fdwr for the detailed response. I personally feel it may be okay to leave it as is. 
askhade(2021-04-30 21:23:05):usually we either use numpy or custom python functions to generate test data... 
CLAassistant(2021-04-16 14:02:56):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=3431) <br/>All committers have signed the CLA.
gramalingam(2021-04-20 22:24:04):It looks like it might be possible to define this as a function using RandomUniformLike? E.g., using ```Where(random < input, 0, 1)``` or something like that. If so, it would be better to define it as a function.
askhade(2021-05-12 22:20:07):How did you generate the test data? The <test case>.py should also be checked in along with this PR
askhade(2021-05-12 22:21:48):this op should be registered in operator_sets.h
askhade(2021-05-12 22:22:14):please also add shape inference test for this op
hwangdeyu(2021-05-13 09:41:56):> How did you generate the test data? The .py should also be checked in along with this PR

Thanks for the good catch.
Sorry for deleting the test and registered code in once a rebase master branch and force-pushed git behavior.
Now I added those logic again.
hwangdeyu(2021-05-20 07:58:36):@gramalingam @askhade  
All the CI failures are passed, do you have other comments?
Thanks!
hwangdeyu(2021-05-20 08:07:50):@postrational Would you also convenient to help us review this Bernoulli spec PR? Thanks!
hwangdeyu(2021-05-27 00:37:34):@gramalingam @askhade 
Please let me know if we can approve and merge the PR.
Thanks!
gramalingam(2021-04-20 22:17:56):dtype should probably be an Int, not float.
hwangdeyu(2021-04-21 12:45:15):Thanks for this comment. I tried to run a simple example of bernouli in torch as below:
 ```python
>>> a = torch.Tensor([0, 1])
>>> torch.bernoulli(a).type()
'torch.FloatTensor'
```
So the dtype should be float too? 

gramalingam(2021-04-21 16:40:32):The attribute is used to indicate what the type of the output should be (e.g., float, or double, or float16, etc.) But the attribute-value is an integer value from the enumeration TensorProto::DataType  (see https://github.com/onnx/onnx/blob/08afa047e90c33357bb6a13b7313105859bdd2c4/onnx/onnx.in.proto#L461 ). See, for example, this: https://github.com/onnx/onnx/blob/08afa047e90c33357bb6a13b7313105859bdd2c4/onnx/defs/generator/defs.cc#L411 
gramalingam(2021-05-11 16:55:55):Change "01" to "O1"? Actually, alternative names like X1 or T1 may be better, since it is less likely to be confused with numbers like 01.
gramalingam(2021-05-11 16:56:43):May need to add a cast from the boolean O1 to integer value as in spec. Also, need an assignment to "output".
gramalingam(2021-05-11 17:00:24):Please clarify if the input-tensor value specifies the probability of a 1 (I assume) or 0.
gramalingam(2021-05-11 17:03:29):Since the output type depends on the attribute value, you may need to use ```FunctionBodyBuildContext``` (see: https://github.com/onnx/onnx/blob/299f83fe6900706c2c798def517566ce058846f1/onnx/defs/math/defs.cc#L671 )
hwangdeyu(2021-05-13 09:28:52):This means the output of Bernoulli is 0 or 1, input tensor is a values from range [0, 1].
As [PyTorch docs description](https://pytorch.org/docs/stable/generated/torch.bernoulli.html).
I completed the description in latest commit.
hwangdeyu(2021-05-13 09:31:20):Thanks for very useful comments!
Added the dtype attribute and cast output as well in FunctionBodyBuilde.

gramalingam(2021-05-20 16:58:44):Sorry I was unclear. I suggest changing the second sentence to
```
The input tensor should be a tensor containing probabilities p (a value in the range [0,1]) to be
used for drawing the binary random number, where an output of 1 is produced with probability
p and an output of 0 is produced with probability (1-p).
```
hwangdeyu(2021-05-24 10:24:31):Done, this description is more reasonable and clearer.
gramalingam(2021-06-15 15:41:44):Let us add a comment here, something like:
```
This example and test-case is for informational purpose. The operator is non-deterministic and may not produce
the same values in different implementations (even if a seed is specified).
```
gramalingam(2021-06-15 15:43:14):I suggest adding a line as below here:
```
This operator is non-deterministic and may not produce the same values in different
implementations (even if a seed is specified).
```
hwangdeyu(2021-06-16 09:47:21):I'm little bit confused. If we specified the seed, the RandomUniformLike op will generate the same value between 0 to 1. Then we can use Greater op to compare input values to product same results right?
So this operator may not produce the same values in different implementations only when seed is not specified?
gramalingam(2021-06-16 17:46:09):I think the same comment applies to RandomUniformLike. The point is: is there a guarantee that different implementations of random number generation (in GPUs vs CPUs, in different OS platforms, etc.) all use the same pseudo-random-number generator algorithm? That seems hard to guarantee. So, it is better to add the above comment. If you know that all backends use the same pseudo-random-number-generator (PRNG), please let us know what this PRNG is.
gramalingam(2021-04-16 17:22:15):Shouldn't scale and b be same type as x?
jcwchen(2021-04-16 17:25:05):(Updated) Yes. I am also thinking about this... This leads to another question: for shape_inference of BatchNorm, do we need to add a strict type check for the consistency among x, scale and b?
gramalingam(2021-04-16 17:28:38):Yes. Feel free to add it, but I think we need a more general solution since this issue is much more widespread problem: eg., same problem exists with all arithmetic ops like Add, Mul, etc. (There is a more general solution, used inside ORT. We just need to port it to ONNX.) So, we can also solve that as a separate PR later.
jcwchen(2021-04-16 17:44:01):Good point. I was also checking other operators to seek for similar implementation but failed. I agreed with you we should do it globally. I will keep this PR simple and let's solve this general issue in another PR. Thank you @gramalingam!
jcwchen(2021-04-19 18:29:16):Reopen this PR to trigger CI
postrational(2021-04-22 13:09:47):Superseded by https://github.com/onnx/onnx/pull/3445
CLAassistant(2021-04-19 18:21:33):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=3438) <br/>All committers have signed the CLA.
garymm(2021-04-21 18:00:45):@gramalingam could you PTAL?
gramalingam(2021-04-20 22:11:39):While we are updating this, can we clarify the documentation for padding a bit more? I find it difficult to understand all the combinations of attributes that are allowed and their combined effect. Can we explicitly indicate the order in which the attributes are considered to determine the output-size? (Which attributes are ignored or forbidden if some other attribute is specified?)
jcwchen(2021-04-21 00:05:05):> While we are updating this, can we clarify the documentation for padding a bit more? I find it difficult to understand all the combinations of attributes that are allowed and their combined effect. Can we explicitly indicate the order in which the attributes are considered to determine the output-size? (Which attributes are ignored or forbidden if some other attribute is specified?)

I agreed since the combined behaviour in current document is quite ambiguous. However, have this ambiguous been well-defined yet? If not, it will take some time to organize the whole behaviour and actually I would like to forward this PR faster because this bug fix in ORT wants to consume this update (updated test data). In that case, perhaps the document clarification can be done by another PR. Thanks. 
gramalingam(2021-04-20 21:56:56):Thanks for the fix. I think you need to generate the ONNX-ML versions of the proto files as well.
garymm(2021-04-21 00:47:34):> Thanks for the fix. I think you need to generate the ONNX-ML versions of the proto files as well.

Done.

As an aside: running `gen_proto.py` with no flags created a bunch of unexpected diffs, which made it a bit confusing to figure out how to regenerate things properly.
Should we change the default flag values to match what is actually checked in at master?
gramalingam(2021-04-20 21:54:54):LGTM (with just one minor nit mentioned above).
gramalingam(2021-04-20 21:46:48):The wording here is a bit confusing in the formal sense (though understandable). The earlier explanatory test seems to clarify the meaning of "increasing" better.
jcwchen(2021-04-20 22:03:50):https://github.com/onnx/onnx/pull/3376 ONNX recently updated these URLs as absolute links for PyPI description (https://pypi.org/project/onnx/) to access it as well
gramalingam(2021-04-21 00:23:30):Thanks, good catch!
gramalingam(2021-04-21 00:26:39):Is there some way of meeting both goals?
jcwchen(2021-04-22 21:38:23):I think we might need more code to achieve (like string convert here https://github.com/onnx/onnx/blob/e3900651f056d559a72211c86f561f70d4c9f2e9/setup.py#L341), which would be quite complicated. 
fdwr(2021-04-20 19:51:51):@jcwchen : Welcome. Do editorial changes need review from a specific sig reviewer, or anyone? I'd propose also that `python onnx/defs/gen_doc.py` just take an *explicit* parameter, rather than using a wonky environment variable `ONNX_ML`, but I'm unsure who to propose that idea too.
jcwchen(2021-04-20 20:14:01):> @jcwchen : Welcome. Do editorial changes need review from a specific sig reviewer, or anyone? I'd propose also that `python onnx/defs/gen_doc.py` just take an _explicit_ parameter, rather than using a wonky environment variable `ONNX_ML`, but I'm unsure who to propose that idea too.

Since it is related to infra, you can propose it to @onnx/sig-archinfra-approvers . Thanks
askhade(2021-04-20 21:07:58):> @jcwchen : Welcome. Do editorial changes need review from a specific sig reviewer, or anyone? I'd propose also that `python onnx/defs/gen_doc.py` just take an _explicit_ parameter, rather than using a wonky environment variable `ONNX_ML`, but I'm unsure who to propose that idea too.

you can create a PR and add arch infra sig
jcwchen(2021-04-28 15:39:14):Missing the following code?
```
except onnx.checker.ValidationError as e:
    print('The model is invalid: %s' % e)
```
garymm(2021-04-28 15:47:31):Previous to this change, and now, the behavior is the same: if the model is invalid, an exception will be raised that makes that obvious.
So I didn't see a need to catch the exception.
If you prefer I can add the catch, but could you please explain why?
jcwchen(2021-04-28 15:53:55):Thank you for the explanation. Actually the CIs will validate .ipynb and so a try needs a except.
annajung(2021-06-25 21:02:00):I noticed that pybind11 was removed, is this no longer necessary here?
jcwchen(2021-06-25 21:04:53):Yes, actually ONNX uses pybind11 as a submodule so basically it should not need to manually install pybind11 for ONNX
rajeevsrao(2021-07-16 17:19:02):Based on discussion with @askhade @jcwchen to remove the minimum version requirement here for consistency with past releases. Also check protobuf version requirements of tf/torch.
jcwchen(2021-07-20 15:41:08):Thanks for pointing out! In some converters (onnx-tensorrt, onnx-mlir) they do have protobuf version requirements... To grant users flexibility for using different versions of protobuf, remove the minimum version requirement for ONNX here.
garymm(2021-04-22 21:35:09):@gramalingam PTAL
gramalingam(2021-04-22 22:13:22):On a related note: the existing description of operator versioning (as a three-tuple ```(domain, op_type, op_version)``` is a bit misleading, as well as the comment that nodes refer to them by the 3-part identifier. It would be helpful to fix it as well.
garymm(2021-04-23 16:59:38):> On a related note: the existing description of operator versioning (as a three-tuple `(domain, op_type, op_version)` is a bit misleading, as well as the comment that nodes refer to them by the 3-part identifier. It would be helpful to fix it as well.

I tried to clarify. LMK if this is what you had in mind.
garymm(2021-04-23 23:27:29):I took another look at the text in the Operator versioning section and tried to clarify it. In particular, it used to reference `op_version`, but from the protocol buffer files, it seems like op version is not a concept that currently exists in the system. I replaced with `since_version`.

However I think the text may still be incomplete as I'm confused as to how `since_version` works as expected when operators can be updated to have different semantics. In the example, `A` in opset version 1 and `A` in opset version 3 are different.
But maybe that's an implementation detail and doesn't need to be in the spec?
garymm(2021-04-26 17:19:40):Regarding your comment from slack about model versioning guidelines:
Happy to remove it altogether if you think that's better. LMK.
gramalingam(2021-04-26 17:50:57):Re. versioning: a later version of an op implicitly supercedes an earlier version. Thus, if we have an op Foo with since_version 1 and an op Foo with since_version 3, then a model that uses opset-version 2 would resolve Foo to Foo with since_version 1, while a model that uses opset-version 4 would resolve Foo to Foo with since_version 3 (assuming, of course, that there is no Foo with since_version 4). Concepually, we could say (Foo, version 1) and (Foo, version 2) are the same, while (Foo, version 3) is different.
gramalingam(2021-04-26 17:52:47):About model versioning guidelines: this is fine, since the document contains a disclaimer that this part is not normative, but just recommended practice.
gramalingam(2021-04-29 22:14:30):I think onnx.proto3 need to be generated and included in the commit.
garymm(2021-04-30 00:02:20):> I think onnx.proto3 need to be generated and included in the commit.

This command doesn't produce any diffs:
`./onnx/gen_proto.py --ml --lite`

Which diff to onnx.proto3 do you expect?
gramalingam(2021-04-30 00:59:08):Sorry, I meant onnx-operators.proto3. Same change as for the other proto files in your PR. I think that's why the CI is failing.
garymm(2021-04-30 15:19:27):Ah I see it now. Done! Time to try the auto-merge feature?
askhade(2021-04-30 21:26:47):@garymm Did you manually change this file? CIs are failing because they are seeing diffs : diff --git a/onnx/onnx-operators.proto b/onnx/onnx-operators.proto

garymm(2021-04-30 22:17:28):> @garymm Did you manually change this file? CIs are failing because they are seeing diffs : diff --git a/onnx/onnx-operators.proto b/onnx/onnx-operators.proto

Sorry, I think I managed to regenerate things properly now.
Is there a single command that will do it? I had to string together a bunch of commands, involving different `gen_proto.py` flags `git restore`.
gramalingam(2021-04-22 22:08:34):interemdiate => intermediate
gramalingam(2021-04-23 21:08:37):I find this (pre-existing) description also a bit confusing. How about something like:
```
ONNX uses operator sets to group together immutable operator specifications. An operator set represents
a specific version of a domain, indicated by a pair (domain, version). This represents the set of all operators
belonging to the specified domain with the specified version (referred to as the `opset` version). When the
inventory of a given operator set changes either by addition or removal, its opset version MUST increase.
```
garymm(2021-04-23 22:54:04):Done.
CLAassistant(2021-04-30 01:25:43):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=3459) <br/>All committers have signed the CLA.
gramalingam(2021-05-11 00:58:15):@matteosal , would be great if you could take a look.
matteosal(2021-05-11 17:43:14):This is right, the documentation was wrong. I've also double checked that other recurrent layers and the shape inference code are good.
gramalingam(2021-05-11 17:49:41):Thanks! @axmat : you will need to generate the operator documentation and add it to the commit. ```python onnx/defs/gen_doc.py```

axmat(2021-05-19 01:27:40):Thanks @gramalingam @matteosal for the review.
gramalingam(2021-08-05 17:43:35):Hi @axmat : it looks like the CI is failing. I think may be Changelog also needs to be added after generating the operator documentation. 
gramalingam(2021-10-08 16:40:10):This has now been resolved in PR https://github.com/onnx/onnx/pull/3762 .
gramalingam(2021-04-30 22:15:46):LGTM, thanks! Just a minor comment above.
gramalingam(2021-04-30 22:13:32):Thanks for the refactoring and moving it into a .cc file. May be useful to add a comment above that source_shape and target_shape are assumed to have same dim_size().
yuslepukhin(2021-04-30 22:21:16):I will remove it from the header
gramalingam(2021-05-04 04:47:29):if-then-else may need to be refactored. If axes is not specified, and input_shape.dim(i) has a symbolic value, it will fall into the else branch, which seems wrong.
gramalingam(2021-05-04 04:50:16):The (not_specify_axes) case is a bit more involved. If there is even one unknown input-dimension, we can't really set any of the other dimensions, since the output rank itself becomes unknown. I suggest creating a temporary shape, which is updated: if we see any unknown-dimension, bail out as we can't even set rank
gramalingam(2021-05-04 04:55:40):But this may not be a compile-time constant. Need to handle the case where axes_proto is null
jcwchen(2021-05-11 00:53:49):I did see there is a `axes_proto == nullptr` condition here... Could you please elaborate your case? Thank you!
gramalingam(2021-05-11 01:06:01):Hmmm. I am confused. Not sure what code I was looking at! Please ignore this comment!
jcwchen(2021-05-11 03:02:26):Good catch. Since using temporary shape still needs 2-pass for loop (copying from temporary shape needs one), I decided to create another for loop before the original for loop to avoid creating more variables. It will stop early if it encounters a symbolic value from input. Please review it again. Thank you! 
askhade(2021-05-12 21:45:27):nit: change to axes_not_specified
jcwchen(2021-05-12 22:23:07):Updated. Thanks
gramalingam(2021-05-12 22:27:23):nit: ```input_shape.dim(i).has_dim_value()``` can be dropped since it is already checked in previous loop.
jcwchen(2021-05-12 23:17:17):Good point. I have removed two of them. Thanks
gramalingam(2021-05-12 23:31:58):minor nit: add ```!axes_not_specified``` as an extra condition above. (I think behavior should be the same since axes will be an empty vector if axes_not_specified ... but just for documentation/robustness.)
jcwchen(2021-05-13 16:05:55):Added. Thanks
zhenhuaw-me(2021-05-05 12:48:29):Hi @askhade thank you for the quick review. I have fixed your concerns, please help to check.
zhenhuaw-me(2021-05-06 15:01:31):Hmmm... Cannot reproduce the error with python 3.8. When trying to repro with the steps of the pipeline `onnx-lite` build, fail to build. Is it necessary to build protobuf online for the CI test?
zhenhuaw-me(2021-05-15 12:17:24):Close since run twice for _ONNX_ and _ONNX-ML_ should be by design. Thanks for the review!
askhade(2021-05-18 17:03:09):@jackwish : I think you missunderstood the comment. Your PR is a good enhancement for the current scenario it just needs a little change. 

When ONNX_ML is set to 0 when building onnx we do not build ml ops and therefore if you run the gen doc script it will create an empty operators-ml.md file. This is what needs to be fixed. In CIs we do set ONNX_ML to 0 and after running gen_doc.py it produces as empty operators-ml.md and therefore CIs are failing.

I hope you will reopen it and make this fix. 


askhade(2021-05-04 17:54:51):for better readability change this to 
gen_doc(is_ml=False)
gen_doc(is_ml=True)
askhade(2021-05-04 17:55:36):nit: change is_ml -> is_ml_domain ?
askhade(2021-05-04 18:02:11):update type info
type: (Sequence[OpSchema], Text) -> Text

jcwchen(2021-05-15 00:39:30):Currently the CIs failed with ONNX_ML=0 because ONNX_ML won't create ONNX_ML schema and the produced Operator-ml.md and Changelog-ml.md would be basically empty. 

Therefore I would say the behaviour of gen_doc.py should be as follows:
1. If ONNX_ML=1, it will update both general and *-ml.md documents
2. if ONNX_ML=0, it will only update general document

Then, it should help resolving the current CI failures. Thank you!
gramalingam(2021-05-04 17:03:26):Is there some pending change required (just to know)?
askhade(2021-05-04 17:16:38):> Is there some pending change required (just to know)?

Wei-Sheng had a few open comments regarding adding an optional attribute for alignment to maintain backward compatibility and bumping the op version. With this change more tests also need to be added which include this attr 
gramalingam(2021-05-04 17:22:15):I got the impression they agreed to decouple that change from this change. May be they can comment.
gramalingam(2021-05-04 17:23:48):@fdwr @wschin for your feedback
fdwr(2021-05-04 18:14:21):>I got the impression they agreed to decouple that change from this change. May be they can comment.

Also, either way works for me. I can just re-cherry-pick what I already had again later.
askhade(2021-05-05 16:22:58):What is the motivation for this? We do run release CIs when PRs are pushed to master right
jcwchen(2021-05-05 17:07:01):> What is the motivation for this? We do run release CIs when PRs are pushed to master right

I think it's good for development related to release CIs. Sometimes we want to test release CIs in certain PR but it is not for merging into rel-* and developers need to change code in release CIs to manually trigger (and then change back before merge)... Adding a label to trigger release  CIs would be more convenient and safer (no need to change CI code in a PR).

Another thought: Since current release CIs are much faster from the beginning, actually I am thinking that perhaps we can move all AzurePipelines (all config combination) into GitHub Action and make it be run by every PR... Then this PR is not needed. Thanks.
jcwchen(2021-05-11 21:49:37):For instance, we can prevent issues like this: https://github.com/onnx/onnx/pull/3485. If a PR does change the release CI, the PR can be tested by manually trigger before merge.
askhade(2021-05-12 22:22:52):why add master here?
jcwchen(2021-05-12 23:12:40):Add PR for master here to let the following code (L24) be able to consider which labels that PR is using. If the PR for master has a label "run release CIs", it will run whole release CIs; If not, it won't.
calvinmccarter-at-lightmatter(2021-05-07 16:55:34):This is a fix for issue #3478
calvinmccarter-at-lightmatter(2021-05-07 17:48:10):@fumihwh - can you verify this? thanks!
gramalingam(2021-05-11 22:21:27):Also, please do verify that the generated binary data for these test-cases hasn't changed (as expected).
calvinmccarter-at-lightmatter(2021-05-12 01:51:37):> Also, please do verify that the generated binary data for these test-cases hasn't changed (as expected).

@gramalingam - besides verifying that pytest succeeds, is there anything else I should do for this?
jcwchen(2021-05-12 22:17:03):> > Also, please do verify that the generated binary data for these test-cases hasn't changed (as expected).
> 
> @gramalingam - besides verifying that pytest succeeds, is there anything else I should do for this?

I have just checked the produced model after your modification. They are basically identical so you should not need to update them. You could use `python onnx/backend/test/cmd_tools.py generate-data --op_type GlobalAveragePool` to check these updated models.

BTW, sorry that I forgot to mention one more thing last time. TestCoverage.md needs to be updated as well. You could use `python onnx/backend/test/stat_coverage.py` to get the updated one. Hopefully all CIs will pass. Thank you. @calvinmccarter-at-lightmatter !
jcwchen(2021-05-13 15:59:16):Please also regenerate Operators.md and TestCoverage.md after removing those print. Thanks!
calvinmccarter-at-lightmatter(2021-05-13 17:40:57):@jcwchen - done! 
gramalingam(2021-05-11 17:15:38):Thanks for the fix. Is it possible to use the ```keepdims``` parameter of ```np.max``` and eliminate the code below?
gramalingam(2021-05-11 17:16:43):And eliminate ```spatial_shape```? ```spatial_shape+2``` can be replaced by ```np.ndim(x)```.
gramalingam(2021-05-11 17:17:33):If we use ```np.mean```, can we use the ```keepdims``` parameter, and eliminate the code below?
calvinmccarter-at-lightmatter(2021-05-11 21:01:45):Yes, done!
calvinmccarter-at-lightmatter(2021-05-11 21:02:01):@gramalingam Yes to both - done!
zhenhuaw-me(2021-05-06 15:06:25):I found this when investigating the pipeline failure of https://github.com/onnx/onnx/pull/3467
zhenhuaw-me(2021-05-06 15:08:56):As a follow up, do we really need to build protobuf on the fly? I can see that Mac CI is installing with `brew`. So why not `apt install` for Linux? Going to try if we can merge this.
jcwchen(2021-05-10 23:44:53):> As a follow up, do we really need to build protobuf on the fly? I can see that Mac CI is installing with `brew`. So why not `apt install` for Linux? Going to try if we can merge this.

Ideally, ONNX CIs use the same version of protobuf (3.11.3). For Mac-CI, I tracked brew history and get that version of protobuf. (Currently Mac-CI uses the latest protobuf, but it will be fixed in this PR: https://github.com/onnx/onnx/pull/3448) For Linux-CI, I cannot find a proper pre-installed apt package for 3.11.3 so it needs to build from source now... (Fortunately it does not take a long time). Still, please let me know if you figure out a way to apt install 3.11.3 protobuf directly. Thank you!  
zhenhuaw-me(2021-05-11 06:30:00):Thanks for the quick check! Fixed the GitHub CI too. Please help to recheck.
CLAassistant(2021-05-06 16:51:00):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=3474) <br/>All committers have signed the CLA.
CLAassistant(2021-05-11 17:17:25):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=3484) <br/>All committers have signed the CLA.
gramalingam(2021-05-11 17:38:45):Thanks for the PR. It may be helpful to join the operator SIG call this Thursday to discuss the PR. (See https://github.com/onnx/sigs/tree/master/operators#sig-meeting-info for meeting info.)
gramalingam(2021-07-08 03:53:27):Hi @shubhambhokare1 : is this intended to be included in the upcoming release? Please be aware that the cutoff for this release is July 15th.
postrational(2021-07-08 16:19:01):Please explain the behaviour of the `accumulate` attribute for each of the operators in the specification. Does it make sense for Gather operators?

As part of this PR, or a separate one, can we disambiguate the current behaviour of these operators when encountering duplicate indices?
gramalingam(2021-08-06 04:43:50):Hi @shubhambhokare1 : there are CI failures (equality comparison in python, etc.). Also, you need to signoff your commits as mentioned in the DCO above. Thanks!
BowenBao(2021-08-27 20:21:39):PR looks good to me overall, @shubhambhokare1  could you please rebase with latest master?

@gramalingam @askhade @jcwchen please take a look.
gramalingam(2021-05-11 17:35:25):Please add a description of what this attribute means to the operator documentation above.
gramalingam(2021-08-04 21:02:16):Nit: replacing "multiply" by "mul" will make it a bit more consistent with the names used for the corresponding ONNX ops.
gramalingam(2021-08-04 21:03:06):Change "+=" tp "*="
gramalingam(2021-08-04 21:03:28):Below also.
gramalingam(2021-08-31 20:29:22):This looks fine. But I wonder if we need it. Isn't this a compatible extension? Wouldn't a "CompatibleAdapter" work? If no value is specified for the reduction attribute, the default value "none" produces the old behavior, right?
shubhambhokare1(2021-09-09 18:47:07):Added compatible adapters instead
askhade(2021-05-11 20:09:33):Is this PR reverting the one just checked in https://github.com/onnx/onnx/pull/3473 ?
jcwchen(2021-05-11 21:46:41):> Is this PR reverting the one just checked in #3473 ?

It's fixing the issue that PR brought, but it's not a full revert. In Azurepipelines, sudo is required and sudo can be used. By contrast, GitHub Action does not need sudo. ldconfig is not needed if CI uses static protobuf so I just removed them.
zhenhuaw-me(2021-05-12 00:49:17):> installing protobuf does not need sudo and ldconfig.

For the `ldconfig` part, it depends on how the protobuf is used.
* True for just building, the compiler can find where the libraries are installed where they are the well-known paths (e.g. `/usr/lib`).
* But we do need `ldconfig` for running the test because the linker uses a cache which is updated by `ldconfig`. We don't encounter the failure (something like libprotobuf not found) here because we [install more packages via `apt`](https://github.com/onnx/onnx/pull/3485/files#diff-b8ce27cfa015392b153122c9a224c2651f31ab00451a6a50571ab7e01cace6e6R62), which should involve the `ldconfig` when configuring the package. I.e. `ldconfig` was performed implicitly.
jcwchen(2021-05-12 02:06:30):> For the `ldconfig` part, it depends on how the protobuf is used.
> 
> * True for just building, the compiler can find where the libraries are installed where they are the well-known paths (e.g. `/usr/lib`).
> * But we do need `ldconfig` for running the test because the linker uses a cache which is updated by `ldconfig`. We don't encounter the failure (something like libprotobuf not found) here because we [install more packages via `apt`](https://github.com/onnx/onnx/pull/3485/files#diff-b8ce27cfa015392b153122c9a224c2651f31ab00451a6a50571ab7e01cace6e6R62), which should involve the `ldconfig` when configuring the package. I.e. `ldconfig` was performed implicitly.

Agreed. Even though it works now, to prevent future issues, I just added ldconfig back. (which is same as what protobuf's official guide mentioned). Thank you.
snnn(2021-05-12 16:28:48):Probably you should completely remove the "ldconfig" line. 

You have set BUILD_SHARED_LIBS=OFF. If there is no shared libraries installed, then you don't need to run ldconfig. 

gramalingam(2021-05-12 23:59:07):Suggest renaming "3D_shape" to a valid C identifier, eg., "ThreeD_shape" or "Shape3D".
gramalingam(2021-05-13 00:06:52):Just curious: is this really required? It seems like reshaping a 2D with target-shape {0,0,-1} should have the same effect, if I look at the reshape spec. Wonder if there is some bug/ambiguity in Reshape impl/spec.
gramalingam(2021-05-13 00:18:10):minor nit: ```ctx.getInputType(0)->tensor_type().elem_type()``` is repeated thrice. May be worth making it a variable. Changing the control-flow to ```if (ctx.getInputType(0) == nullptr) return;``` would make it doable.
ashbhandare(2021-05-13 00:39:33):you are right, this unsqueeze can be skipped
askhade(2021-05-19 19:29:41):typo
askhade(2021-05-19 19:32:03):nit: of type SparseTensor
askhade(2021-05-19 20:00:25):can you also fix similar typo on line 481
gramalingam(2021-05-19 20:06:35):I think we should move this into the if-then branch? Otherwise, we could create a rank-zero (scalar) instead of an unknown-rank tensor for the shape=None case?
askhade(2021-05-19 20:37:04):i dont think so... if shape is None as explained in the comment below this field will be omitted from the resulting protobuf... 

we take a similar approach for tensor proto as well
gramalingam(2021-05-19 21:09:07):yes, the fact that is is used for tensor suggests it works. I am still unclear what exactly happens. It is possible that a "shape" object is created (when the above statement is executed), and that when it is saved to protobuf it is omitted because it has no fields? There is a difference between the "shape" object and its field "dims" ... the comment below describes whether a dims field is created or not. But my question is about whether the "shape" field is created or not, since in C++ we do check for the existence of the "shape" field to determine if it has a rank or not.
gramalingam(2021-05-19 21:10:22):Nevertheless, wouldn't creating the shape field inside the if-then more accurately reflect what we want?
yuslepukhin(2021-05-19 21:17:06):Is `be` missing? `Needs to be`?
yuslepukhin(2021-05-19 21:18:22):This comment is not clear to me. `Data type` refers to the `elem_type` and the return type is `ValueProtoInfo`.
gramalingam(2021-05-19 21:23:56):Ok, I tried it out. It seems to work. It is a bit mysterious how exactly the protobuf implementation works.
yuslepukhin(2021-05-19 21:26:47):Yes, it creates a shape object, that is empty. It is the same as calling mutable_shape() in C++
askhade(2021-05-19 21:44:19):I meant something like this: Makes a ValueInfoProto of type Sparse Tensor based on the elem_type and shape
askhade(2021-05-19 21:45:17):this is just to improve readability... feel free to ignore if it is confusing... but there are 2 types in play here... 1. type of typeproto and the underlying data type of the tensor proto
yuslepukhin(2021-05-19 21:49:26):And the actual return type which is ValueInfoProto
gramalingam(2021-05-20 22:43:47):Ok, this seems to work. This line does not actually create a shape field. A bit mysterious, but seems ok (just python-approach to protobuf).
jcwchen(2021-07-15 17:58:07):Close since ReadMe.md has been improved by https://github.com/onnx/onnx/pull/3575.
askhade(2021-06-09 18:05:23):we should keep this in sync with the ones in setup.py


<!--StartFragment-->install_requires.extend([    'protobuf',    'numpy&gt;=1.16.6',    'six',    'typing&gt;=3.6.4; python_version &lt; "3.5"',    'typing-extensions&gt;=3.6.2.1',])<!--EndFragment-->


in setup.py there is no version mentioned for protobuf we should add it there
jcwchen(2021-06-10 18:38:24):Good catch! I did it in another PR: https://github.com/onnx/onnx/pull/3448, but I forgot to add it here... Just updated. Thanks.
neginraoof(2021-06-08 16:19:02):@askhade @gramalingam @postrational 
Please let me know if you have comments.
Thanks!
askhade(2021-06-08 16:32:04):link the OnnxTypes.md here?
askhade(2021-06-08 16:33:25):typo - Optional typs -> Optional types
askhade(2021-06-08 16:35:28):for every example can you add how the resulting onnx graph would look like with this type added... I know we dont have operator support yet but you can add a placeholder for these ops
gramalingam(2021-06-08 16:53:28):I think it would be good to clarify IR-version distinctions: that is, optional type is supported from IR-version X, etc.
gramalingam(2021-06-08 16:54:01):Even release version/IR-version etc.
gramalingam(2021-06-08 16:55:14):May need to update above line
gramalingam(2021-06-08 17:00:53):Is that supposed to be "else if"?
gramalingam(2021-06-17 16:57:51):Can we delete the above line "ONNX currently does not define a sparse tensor type." Thanks.
gramalingam(2021-06-17 16:59:48):Can we drop all the extraneous attributes like strides, requires_grad, and device? They are somewhat confusing and misleading in an ONNX context. Same in all lines below as well. Thanks!
askhade(2021-06-17 17:10:55):why is this commented?
neginraoof(2021-06-17 22:01:22):Thanks for catching this!
jcwchen(2021-06-03 19:51:35):Don't forget to remove this
jcwchen(2021-06-03 19:54:19):I see the reduction attribute is using mean by default: https://github.com/onnx/onnx/blob/04971f7d26aad7bbc71e2d92433631f985cb96c6/onnx/defs/math/defs.cc#L3143
I am curious that will it still be nullptr if reduction is not set?
gramalingam(2021-06-03 20:27:49):minor suggestion: could simplify right-hand-side to ```getAttribute(ctx, "reduction", "mean")```. See: https://github.com/onnx/onnx/blob/04971f7d26aad7bbc71e2d92433631f985cb96c6/onnx/defs/shape_inference.h#L92 
askhade(2021-06-03 20:28:41):thanks!

askhade(2021-06-03 21:13:45):the ctx here refers to "FunctionBodyBuildContext" and not "InferenceContext"

I will use this in the type and shape inference function which uses InferenceContext
gramalingam(2021-06-03 21:30:18):Sorry, missed that
askhade(2021-06-03 21:48:21):@jcwchen : This code sources the attribute from InferenceContext... if the attribute is not explicitly present it will return nullptr
gramalingam(2021-06-08 21:04:02):Can't we move line 1634 to after the first loop to fix (2)?
matteosal(2021-06-10 11:54:30):> Can't we move line 1634 to after the first loop to fix (2)?

Yes, that worked
matteosal(2021-06-14 15:07:11):@gramalingam any news on this?
gramalingam(2021-06-08 21:09:21):In principle, I think we can omit the ```|| std::find(axes.begin(), axes.end(), i) != axes.end())``` part. Thus, we assume that the axes specification is correct for shape-inference purpose. If it is incorrect, the runtime execution needs to catch and report it anyway. Does that make sense? However, this would require minor changes to the loop below in line 1657 ... we should check if input_shape_has_dim_value before accessing it.
matteosal(2021-06-10 11:46:48):And what should happen in the loop below if `has_dim_value` is false? I still see no option other than giving up the shape inference.
I understand your point about the checking that the symbolic size is actually 1 at runtime, that should happen anyway. But still shape inference should give up (i.e. restrain from specifying anything about the output shape) in this case.
gramalingam(2021-06-14 17:01:44):This is a fine point of distinction. In the loop below, if the outer condition is true (that is, axes is specified and contains i), then we should omit dim(i) from the output. The idea is that the shape-inference contract is that it indicates what the output shape will be IF the op executes successfully and produces an output. Type and shape inference does NOT guarantee that the op will execute successfully (because of the need for some runtime checks). Does that make sense?
gramalingam(2021-06-14 17:02:43):If you are not convinced, we can merge this as is, and we can do this as a separate PR later after discussion.
matteosal(2021-06-15 11:06:18):> The idea is that the shape-inference contract is that it indicates what the output shape will be IF the op executes successfully and produces an output. Type and shape inference does NOT guarantee that the op will execute successfully (because of the need for some runtime checks). Does that make sense?

Ok so you are suggesting to omit dim(i) from the output if:
* dim(i) is numerical and equal to 1 (usual case) OR
* dim(i) is symbolic

In the second case, runtimes should obviously check that dim(i) is actually 1 when the Squeeze op is evaluated. Is that correct?
matteosal(2021-06-15 12:00:47):I have added the change, please let me know if it's ok
askhade(2021-06-14 19:39:18):Is this PR ready to be reviewed?
jcwchen(2021-06-25 20:45:35):Updates:
- Eliminate unnecessary const_cast
- Improve Python test utilities
- Traverse all graphs first to add existing symbols
- Make SymbolTable as an interface

askhade(2021-06-08 16:09:22):createNew simply generates a random symbol right... how do you guarantee this symbol is unique within the graph? You need to maintain a graph level symbol list and make sure every new symbols is indeed unique for this graph
jcwchen(2021-06-09 03:03:23):Although using sscanf here can easily detect "unk_XXX", sscanf might be a dangerous function...
askhade(2021-06-09 15:53:23):why make it global? This should be in InferShapesImpl
askhade(2021-06-09 15:58:21):I dont exactly get whats happening here... Are you assuming the model will have symbols in a specific numerical range ? Why no simply maintain a list of all symbols in the graph and then when you create a new one just make sure it doesnt already exist
jcwchen(2021-06-09 19:40:26):My intention was to save some memory (additional existing shape hashset) and time... For instance, if "unk_1", "unk_2"..."unk_50"  are used, using a list to check will go through "unk_1" to "unk_50" and then finally find "unk_51" is usable. This will take more time... but as you said, to assume "unk_XXX" here, the logic will look quite confusing. I am changing the logic to maintain a shape list and check. Thanks for the review!
askhade(2021-06-09 22:10:31):well the time depends on the data structure you use. 
gramalingam(2021-06-14 19:26:37):Instead of checking that the dimension has a specific name, it is better to check that it has some name. (We can do this check for 'C', and/or 'output'.) Alternatively, we can check that 'C' and 'output' both have a name and that the name is the same.
gramalingam(2021-06-14 19:27:20):Same comment as above.
gramalingam(2021-06-14 19:28:42):Same comment as above. Can check that the new name is different from the old names. 
gramalingam(2021-06-14 19:31:02):Nit: rename 'checkExistingSymbolicShape' to 'AddExistingSymbolicDims'.
gramalingam(2021-06-14 19:33:48):It would be useful to have the symbol_prefix as an optional parameter to this function (with a default value "unk__"). This allows callers to create more meaningful names if they want.
gramalingam(2021-06-14 19:36:10):Nit: rename 'SymbolicShape' as 'SymbolTable' (a well-known term in compilers).
gramalingam(2021-06-14 19:37:59):Nit: rename 'existing_shape_set' to 'existing_symbols'.
gramalingam(2021-06-14 20:24:04):Unclear why we need the first two parameters, and what the relationship between them is. It looks like in the calling context they differ only in the "const" being cast? Can't we use a single non-const parameter?
gramalingam(2021-06-14 20:25:22):Shouldn't first condition be "! has_dim_value()"?
gramalingam(2021-06-14 20:26:38):This is odd. I would make the parameter non-const here (and make this the caller's issue).
gramalingam(2021-06-14 20:28:06):nit: may be nicer to rename 'enableSymbolicShape' to 'materializeSymbolicShape'
gramalingam(2021-06-14 20:33:36):What is this for? I think an alternative way of checking that doesn't assume what the generated names look like might be better.
jcwchen(2021-06-15 01:25:22):You are right. Besides this one, shall I add another condition: `inferredType.shape().dim(i).has_dim_param() && inferredType.shape().dim(i).dim_value() == -1` ?
jcwchen(2021-06-15 01:50:48):(Updated) Sorry that the function here looks confusing previously. ~Actually the second argument is a mutable tensor type for updating inferredType after symbolic shape_inference. The first argument is the original inferredType. So I think both of them are needed...~ To make it clearer, I have changed the variable name and updated some comments here.
jcwchen(2021-06-15 01:52:22):Makes sense. Updated. Thanks
askhade(2021-06-15 04:22:24):what is the Text for?
askhade(2021-06-15 04:31:11):SymbolTable& symbolTable
askhade(2021-06-15 04:32:57):typo: materializeSymbolicShape**Shape**
askhade(2021-06-15 04:37:44):this is very confusing... I don't understand what are you trying to do here...

All you need to do is check whether inferredType consists an unknown symbol, if so then generate a new symbol and set dim param... why do you need inferredType and existingTensorType.... + these names are misleading because both are same based on line 142
askhade(2021-06-15 04:38:01):SymbolTable& symbolTable
askhade(2021-06-15 04:38:39):this method should be simplified
askhade(2021-06-15 04:38:56):use auto*
askhade(2021-06-15 04:40:04):how are inferredType and symbolicInferredType different?
askhade(2021-06-15 04:42:11):you want to update inferredType then why not just do inplace update? why are you returning const TypeProto? + 
symbolicInferredType == inferredType  (line 140) this is very confusing...
askhade(2021-06-15 04:42:28):fail_shape_inferece?
askhade(2021-06-15 04:47:06):GraphProto& g?
if you use a pointer then you should check for nullptr... if nullptr is unacceptable then you should simply use a reference and make this a caller issue...
askhade(2021-06-15 04:48:33):const auto& since you are not modifying the proto
askhade(2021-06-15 04:49:01):why mutable when you are not modifying the proto? you are simply collecting all the symbols right
askhade(2021-06-15 04:49:37):dont you need to check for sparse tensor and sequences?
askhade(2021-06-15 04:52:08):you should add this new symbol to the list as well... symbol Table should include all the symbols applicable for the current graph
gramalingam(2021-06-15 19:55:29):+1
gramalingam(2021-06-15 20:01:31):minor nit: recomputing the symbolTable every time seems less efficient; we need to create it just once.
jcwchen(2021-06-15 20:41:20):Good idea. Instead of checking the exact dim_param name, right now it only checks whether a dim_param is produced. I use "-1" as dim_value to represent a symbolic dim. Also, it further checks whether the symbolic dim of 'C' is same as 'output'. 
jcwchen(2021-06-15 20:44:51):Now it will check the number of unique symbol. If it correctly creates a new one without using the same symbol, the number should increase.
jcwchen(2021-06-15 20:48:36):Initially it removed "dim_param: unk_XXX" for matching the inferred valueinfo and expected valueinfo. Now it will check each dim instead of matching the whole string. If it encounters symbolic shape, it will only check whether a dim_param is produced.
jcwchen(2021-06-15 20:50:30):just a nit: correct the comment here that there should be 2 bool (check_type and strict_mode). 
jcwchen(2021-06-15 21:26:50):Oh. I should also remove Text here. Thanks for catching this!
jcwchen(2021-06-16 04:34:19):Updated: Only using mutable_tensor_type() can achieve it. Thank you for the comment!
jcwchen(2021-06-16 04:38:29):Thanks for catching this @askhade and @gramalingam ! I have simplified the arguments for `generateSymbolicShape` and `materializeSymbolicShape` by only using mutable_XXX_type()
askhade(2021-06-16 16:02:12):inferredDim unused
askhade(2021-06-16 16:04:14):use typeAfterSymbolicShapeDim instead of "inferredType->shape().dim(i)"
askhade(2021-06-16 16:06:00):add sequence to the comment
jcwchen(2021-06-17 02:40:32):Actually only TypeProto_Tensor or TypeProto_SparseTensor and will use generateSymbolicShape? Sequence will merely be expanded in materializeSymbolicShape.
jcwchen(2021-06-17 02:45:30):Good point. Now single InferShapes will only have single symbolTable (among InferShapesImpl, InferShapeForFunctionNode, doInferencing). To make subgraph inference share the same symbolTable from the main graph, I introduced symbolTable in graphInferenceContext. Thanks for the suggestion!
askhade(2021-06-17 16:05:21):SymbolTable& symbolTable
jcwchen(2021-06-17 16:09:12):Good catch. I have added sparse_tensor and sequence for AddExistingSymbolicDims. Thank you!
askhade(2021-06-17 16:09:25):you are right
askhade(2021-06-17 16:10:27):nit: fix spacing SymbolTable& symbolTable
askhade(2021-06-17 16:13:16):nit: add blank line before mtd start and after mtd end for this struct
askhade(2021-06-17 16:15:22):nit: do you need addFromGraph? can you directly call AddExistingSymbolicDims from the constructor?
askhade(2021-06-17 17:35:44):I dont understand the purpose of this... what is this for?
askhade(2021-06-17 18:00:18):how you are making the symbolTable available to GraphInferenceImpl needs to be simplified

context_ is private var for this class... can you not simply add a public mtd to fetch symbol table from context? So you that can get rid of symbolTable_ from here
jcwchen(2021-06-17 18:21:48):addFromGraph has another purpose. If shape_inference bumps into a sub-graph, it will rather use addFromGraph to add symbols from sub graph.
jcwchen(2021-06-17 18:33:02):Actually we can simply use symbolTable above... Just removed it. Thanks!
askhade(2021-06-17 21:10:35):use reference... don't copy
auto& symbolTable
askhade(2021-06-22 16:54:26):I think we should remove the const qualifier... making context_ const suggests that we do not change anything but we do... we do update the symboltable ... 
askhade(2021-06-22 17:01:38):I think the case of GraphInferencer we should first merge and then cal materializeSymbolicShape on graphInput instead of inferredInput... 
jcwchen(2021-06-22 21:00:00):Good point. I have made GraphInferenceContext non-const
jcwchen(2021-06-22 21:13:24):The original expected flow is TypeandshapeInferencefunction -> generated symbols for unknown types -> attempt Partial DataPropagation -> merge.

Are you saying that moving generated symbols and partial DataPropagation after merge since they can manipulate  non-const graphInput after merge?

I have a concern: do we need `checkTensorShapesAndTypes` for checking the shape after Partial DataPropagation? Currently shape_inference does checkTensorShapesAndTypes in merge function. If we move merge before generated symbols and partial DataPropagation, it won't check the final propagated shape with existing shape.
gramalingam(2021-06-23 21:58:27):Technically, we need to decide on the scope of the dimension-symbols like M and N. The existing spec is
```
Currently, dimension variables are not scoped. A dimension variable "N" represents the same value
across the entire graph in a model. For example, if the graph has two inputs X and Y each with
shape ["N"], then at runtime the values passed in for X and Y MUST be tensors of rank 1 with the
same dimension. Nested sub-graphs currently share the same scope for dimension variables as the
main-graph. This allows a model to relate the dimensions of tensors inside the subgraph to the
dimensions of tensors in the outer graph.
```
(see under https://github.com/gramalingam/onnx/blob/master/docs/IR.md#static-tensor-shapes ). If we stick to this, we should visit all subgraphs and add their symbols right when constructing the top model-level symbol table.
gramalingam(2021-06-23 22:01:24):nit: spelling "unqiue" => "unique"
gramalingam(2021-06-23 22:10:39):may be better to return ```v.type.tensor_type.shape``` here and have a separate equality-check method for two shapes, instead of creating another alternative shape representation. Also, would be useful to document assumptions for use of this method (e.g., whether the type is expected to be a tensor_type, or should we be checking that also here).
gramalingam(2021-06-23 22:17:37):Doesn't seem to handle the case where inference did not infer a type for a name specified in ```valueinfo``` correctly. I think it may be better to treat this method as a shorthand for invoking ```get_shape_from_name``` and equality check for every entry in ```valueinfo```. 
jcwchen(2021-06-25 03:31:05):Good catch. I have considered the case if the name does not exist and reuse get_shape_from_name here.
jcwchen(2021-06-25 03:34:30):Finally I chose to return shape without other check method and directly compare whether the TensorShapeProto is the same. Or I checked sparse_tensor_proto as well. Thanks!
jcwchen(2021-06-25 03:38:37):After offline discussion, I agreed with you. Since the inferredInput here isn't inferred in this level, it has generated symbol previously and it should be OK for us to do merge first then materializeSymbolicShape. In that case, we also don't need to use const_cast to break the const... Updated. Thanks!
askhade(2021-06-25 21:03:10):add comments for every mtd
askhade(2021-06-25 21:16:57):you don't need this anymore right? since you are already traversing the whole graph including subgraphs in the beginning
askhade(2021-06-25 21:20:11):Add comment something like:
Creates a new unique symbol with the given prefix and adds it to the SymbolTable
Returns the newly created symbol
askhade(2021-06-25 21:29:11):shouldn't this be original_count + 2?

Concatenation of A[2, 'unk__0'], B[2, 3] should result in output C of shape [2, 'unk__2'] 
Concatenation of C[2, 'unk__2'] , D[2, 'D'] should result in output of shape [2, 'unk__3']
what am I missing?
askhade(2021-06-25 21:31:35):can you add a test case for unknown shape... in all these tests there is a symbol present... can we add something where 

(!has_dim_value() && !has_dim_param()) == true
jcwchen(2021-06-25 22:43:56):Good catch. I think my previous example here is not a proper one (looks confusing):
- The original symbol table is {'unk__0', 'unk__1', 'D'}
- After inference symbol table is {'unk__2', 'unk__0', 'unk__3', 'D'}

Although original output shape is 'unk__1', but the inferred output shape will be 'unk__3', which replaces original 'unk__1'. 

To resolve confusion, I will use 'unk__0' as output shape. Then the inferred count will be count + 2 as you said.
gramalingam(2021-06-25 23:31:53):nit: => "symbolic dimension must exist"
jcwchen(2021-06-17 18:12:33):Close it now since it is not correct.
gramalingam(2021-06-17 17:57:13):Doesn't sound right. Consider the simple case where there is no padding, stride is 1, and kernel-size is same as input-size. We expect the output size to be 1, right? So, the original "+1" sounds right, not "-1". Can you double-check? Thanks!
gramalingam(2021-06-17 18:07:07):I responded to the original issue, please take a look.
askhade(2021-06-14 17:07:27):is this the one used to publish weekly packages to test pypi?
jcwchen(2021-06-14 17:10:47):> is this the one used to publish weekly packages to test pypi?

No, this is the one for checking (onnx.checker) all models from ONNX Model Zoo. The one for testpypi works normally. Thanks
askhade(2021-06-14 17:12:25):Can you trigger the failing CI itself for this PR
jcwchen(2021-06-14 17:20:50):> Can you trigger the failing CI itself for this PR

Since we can only manually trigger the CIs from the repo's branch, I can only trigger it in my forked repo: https://github.com/jcwchen/onnx/actions/runs/936466270...
askhade(2021-06-16 20:33:55):Closing this PR in favor of : https://github.com/onnx/onnx/pull/3532 to resolve DCO failures

mrry(2021-06-16 20:49:01):Nit: Can two different `FunctionProto`s in this list have conflicting imports? (I'm guessing they can't, but the wording here is ambiguous.)
askhade(2021-06-16 20:50:35):will fix the typo "list of"
mrry(2021-06-16 20:50:48):Also, is it useful to be able to control the opset imports on a per-function basis, given this restriction? Would it be more ergonomic for the functions to inherit the imports from the containing ModelProto?
mrry(2021-06-16 20:51:26):Is this field meaningful for model-local functions?
mrry(2021-06-16 20:52:49):Does the set of functions in a ModelProto count as an "OperatorSet" for the purposes of this comment? (i.e. Can a model-local function refer to other model-local functions?)
mrry(2021-06-16 20:54:53):How does the model refer to these functions? If they were in an operator set, presumably there would be a domain, but I don't see a domain here.

Should this field be an OperatorSetProto (or some subset of that proto)?
mrry(2021-06-16 20:55:17):Nit: missing space before `=`.
gramalingam(2021-06-16 21:56:25):Yes, this originates from the use of FunctionProto defined outside of models (in defining standard ops as functions). We could potentially distinguish between the two usages and restrict some fields to be used only in FunctionProtos outside of models.
gramalingam(2021-06-16 21:58:29):Agree, it does not seem relevant to model-local functions. It is unclear if this is even relevant for standard-defined functions any longer.
gramalingam(2021-06-16 21:59:44):I think we should allow a model-local function to refer to other model-local functions.
askhade(2021-06-16 22:09:51):ONNX does not allow importing different versions of the same domain so there cannot be conflicting imports... 

opset imports were added to FunctionProto to allow functions to easily import opsets other than what the model imports... the restriction is only on the version of the domain being unique within the model...

askhade(2021-06-16 22:13:07):FunctionProto has a "name" field. The model compares this with the op_type field from NodeProto.

Rama and I were discussing whether we want domain qualified names, meaning should we add domain field to the functionproto as well... we can do that if we think just the name field wont be enough... 
askhade(2021-06-16 22:27:35):I dont think so but we share the same schema for both model local functions and function ops, so I did not  remove this.

Earlier ONNX standard used to support experimental ops and I believe OperatorStatus was added back then... I don't think this is relevant anymore but removing this altogether might have some implications... 
gramalingam(2021-06-16 22:30:52):Two possible options: 
(a) We add a domain field to FunctionProto. (We then need to decide whether we want to allow these in-model functions to override out-of-model functions defined in opschema registries.)
(b) Reserve a domain for in-model functions (like 'onnx.ai.local') if we want to avoid the above.
askhade(2021-06-16 23:08:00):Yes one model local function can refer to another model local function...

more and more I think about this I am leaning towards adding domain field to FunctionProto... 
mrry(2021-06-21 17:55:42):Typo: s/opserator/operator (in a few other places too)
mrry(2021-06-21 18:01:32):Typo: s/Verison/Version/ (in a few other places too)
mrry(2021-06-21 18:05:31):I'm struggling to understand why allowing different versions is useful here. I could probably contrive two functions that imported different opset versions for a domain without using ops that are different across the versions, but this seems like it would be fragile.

Would it be better to be more restrictive at first (all versions must be equal for a given domain) and then relax it in a later version?
mrry(2021-06-21 18:08:38):Typo: s/opsett/opset/
mrry(2021-06-21 18:09:56):Typo(?): s/my/by/
mrry(2021-06-21 18:10:44):Is there a sensible/backwards-compatible default value for `domain` if it is unspecified?
mrry(2021-06-21 18:13:31):Is this allowing the model to import opsets at any version less than the model imports for the same domain? (This seems different from the comment on the protobuf, but I haven't internalized all the logic yet, so I might be missing something....)
mrry(2021-06-21 18:14:36):I'm also not quite sure why this logic is here rather than in `check_opset_compatibility()`. Might be worth adding a comment to explain.
mrry(2021-06-21 18:15:19):Nit: Run `clang-format` over the PR before submitting.
mrry(2021-06-21 18:15:29):Typo: s/verison/version/
mrry(2021-06-21 18:20:43):IIUC, the check here means that every function in a domain `"foo"` must have an entry for `"foo"` in its `opset_import` with a particular version, which becomes `domain_version` here. Is that right?

This logic seems a little tortuous... would it make more sense to have a `domain_version` attribute on the `FunctionProto`? Or - perhaps better - the version could be inferred from the `opset_import` on the containing model (or inferred from the op schema, if the function is not a model-local function)?

At the very least, we need to document this requirement, because it was not obvious to me from reading the protobuf spec.

[EDIT: I see code in schema.cc that appears to add this information if it's missing. Is it still required for model-local functions?]
mrry(2021-06-21 18:23:03):Typo: s/funciton/function/
askhade(2021-06-21 18:40:13):restricting the version to be same as the version imported by model will not work for FunctionOps. Consider this example: Function Foo belongs to domain custom_domain and imports op "Add" from onnx domain in its function body... At the time of authoring this function "Foo", onnx latest released opset version was 13 so Foo imports onnx domain opset 13. Now, with this restriction on opset version, function Foo can only be used in models which import opset 13.... later opset wont work even if schema for Add remains same... Allowing different yet compatible version will allow models of higher opset to use function Foo.
askhade(2021-06-21 18:45:06):removed it in latest iteration
askhade(2021-06-21 18:45:29):added domain field for FunctionProto
askhade(2021-06-21 19:03:15):Right every function in a domain "Foo" must have an entry for "foo" in its opset_import with a particular version.

For function ops - If this info is missing then it is inferred ( from op schema ) and implicitly added during schema finalization.

For model local functions - this info is required to be present. We can infer the version for the function from model opset imports but I wanted to reduce the differences in construction or interpretation of FunctionProto for FunctionOps vs model local functions so I decided not to make an exception for model local functions in this case. 

Let me know your thoughts...


gramalingam(2021-06-21 21:37:10):Agree. It feels like this might be redundant, given check_opset_compatibility. If not, having the logic in one place seems better.
mrry(2021-06-21 22:12:46):Thanks for clarifying! I think there might be a more precise way to state this. Something like:

> The operator sets imported by a FunctionProto must be "compatible" with the ones imported by the ModelProto that invokes them. Two operator sets are compatible if (i) they have the same domain and version, or (ii) they have the same domain and different versions, and all operators from the earlier version are present, with the same schema, in the later version. Intuitively condition (ii) means that a graph using the earlier version of a domain can be transparently "upgraded" to the later version by changing the version number.

I might be mangling the distinction between operator set and domain-version pair here. I'm also not sure if compatibility between model/function imports should be symmetric - as in the text here - or asymmetric. I think the checker implementation is asymmetric, and that might make sense, since IIUC we'd only consider "upgrading" a function (by inlining it into a model), and not a model itself.

gramalingam(2021-06-21 22:24:41):"should be same" => "should be same for every node in the function body".
askhade(2021-06-21 22:37:08):can you elaborate on symmetric vs asymmetric compatibility
gramalingam(2021-06-21 22:38:55):There seem to be some corner cases where it is worth documenting what the standard requires. (a) If we have a model-local function "bar" in a domain "foo", is the model/graph required to have an import for "foo" (even though the model does not contain any calls to anything other than "foo")? (b) If so, if the FunctionProto for "bar" also includes an import for "foo", is it allowed to be different from what the graph specifies? The opset-compatibility discussion does not clarify this special-case. It doesn't seem important to allow them to be different in this case. (c) If we don't allow to be different, forcing the model to include the same information in two places seems less desirable. So, inferring it from the model's imports may be reasonable.
mrry(2021-06-21 23:00:47):Symmetric would be more permissive. e.g. It would be legal to have model-imports-ONNX-10 and function-imports-ONNX-9 **or** model-imports-ONNX-9 and function-imports-ONNX-10. 

Whereas an asymmetric policy would only allow the larger version number in one case... most likely the model. This approach might make sense for an implementation based on inlining, because you could implement this as "upgrading" the function when it is inlined.

IIUC the logic here is asymmetric:

https://github.com/onnx/onnx/pull/3532/files/8a4cdd1d04c002f8db20e2a3ec32a0258515d99c#diff-27b8667be1a3c2c7ce2a2d324700c2f7bf400916c6b0e3508c8b27f85399ac6aR748

I suppose you could "downgrade" the function to an earlier opset when it is inlined, if you wanted to support a symmetric policy. But this doesn't seem necessary for the example you gave in your comment.
gramalingam(2021-06-21 23:13:43):In general, it seems that model-local functions should not need to use opset-imports and should be able to get away with the model's own opset-imports (for the ops called from the function body). So, doing the same for the function's own domain (look it up from the model's imports) seems reasonable. It is also important to clarify which of the three options we go with (i) required, (ii) allowed but not required, or (iii) not allowed. In functions outside models, we do need to allow opset imports.
askhade(2021-06-21 23:30:43):We want asymmetric compatibility and not symmetric. This is in sync with what we do for operators as well... A model can import a version <= model opset import version but never the other way... 
mrry(2021-06-21 23:36:18):That makes sense. In that case, we'll definitely need to change the comment to make that clear: the current "the versions for the operator set may be different" implies that the function version > model version is allowed.
gramalingam(2021-06-22 01:05:43):I am unclear why we need asymmetric compatibility. Symmetric ("downgrading" the function as Derek says) would work just as well, without imposing any extra work or challenges, right? That way, when I define a function, I don't need to go to the trouble of finding the earliest opset it will work with ... I define it for the current opset, and transparent downgrading makes it as general as possible.
askhade(2021-06-22 19:52:46):When defining a function the author needs to import the domain and version which they are using for defining the function body. For example if the function body contains op 'A' from domain 'X' then the author should import {'X', 'then_latest_ version'} in the function body. This way any model which imports version equal to or greater than the one imported by the function proto should work. If the schema for op 'A' changes then the function op needs to be updated with the new schema for op 'A' and corresponding import. 

gramalingam(2021-06-22 20:01:14):The author has several choices in above setting: import {'X', 'latest version of X'} (assuming this contains op 'A') or import {'X', 'earliest version of X that contains the corresponding op 'A''}. Since the opset-compatibility test can figure all this out, why do we need the extra constraint 'model which imports version equal to or greater than the one imported by the function proto' ?
askhade(2021-06-22 21:29:28):import {'X', 'latest version of X'} (assuming this contains op 'A') => if <latest version> -1 contains op A then <latets version> will also contain op A. I dont see any motivation for the author to use anything other than latest version... By using a version earlier than latest version you restrict the usage of the function. 
Opset compatibility test only validates whether opsets are compatible... the upgrades are automatic meaning there is no schema change because the schema has not changed. 
askhade(2021-06-22 23:54:02):after our discussion I updated the checker to reflect symmetric compatibility
gramalingam(2021-06-25 00:12:49):I am not sure about this. I think we can allow func_opset_version > model_opset_version as long as both schemas and exist and we can check the equality of since_version for both as below.
gramalingam(2021-06-25 00:23:43):Extraneous " at end.
annajung(2021-06-22 15:53:46):@askhade Please review when you can. I also wasn't sure if I needed to update any doc or run any script to update anything. If needed, please let me know! thanks
gramalingam(2021-06-22 16:56:26):I think that these two lines for scale and zero-point can be moved up before line 189 (since this is valid even if we don't know the input shape).
annajung(2021-06-22 17:01:18):Sure that makes sense. Will update
annajung(2021-06-22 17:09:14):done! PTAL
askhade(2021-06-24 04:39:41):Parser does not handle multiple opset imports "opset_import: [ "" : 13, "custom_domain" : 1]" the comma between the 2 opsets does not work... it requires "opset_import: [ "" : 13 "custom_domain" : 1],". Can you please fix this.
askhade(2021-06-24 04:40:01):Can you add some documentation around usage for python apis
askhade(2021-06-24 04:40:26):Please add some tests
lgtm-com[bot](2021-06-24 17:15:33):This pull request **introduces 1 alert** when merging 78ace429004a60084b4b6bc624281a14db4014f4 into 3c857c57826cf434626cb8c99f0b9517efb62451 - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-04840f5c62930a0d103e9880a1c15434dc2fcd2f)

**new alerts:**

* 1 for Module is imported with &#39;import&#39; and &#39;import from&#39;
lgtm-com[bot](2021-06-24 21:16:20):This pull request **introduces 1 alert** when merging a6e997be26795f4c04d04542d5532efd3355128c into 3c857c57826cf434626cb8c99f0b9517efb62451 - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-3cc88bc7de397430e7411aca89aa7b192d53e450)

**new alerts:**

* 1 for Module is imported with &#39;import&#39; and &#39;import from&#39;
lgtm-com[bot](2021-06-24 21:54:05):This pull request **introduces 1 alert** when merging f3d82c137e44274e56b9edc1930dd70cf7fd251c into 3c857c57826cf434626cb8c99f0b9517efb62451 - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-7907cc8caad276fee401d23594689e38f7448491)

**new alerts:**

* 1 for Module is imported with &#39;import&#39; and &#39;import from&#39;
lgtm-com[bot](2021-06-24 23:50:25):This pull request **introduces 1 alert** when merging b039899b04cfd0d352e21fc164a855f334f7b96c into 3c857c57826cf434626cb8c99f0b9517efb62451 - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-c612cfdb43c37b8c557b5fe192f6024a0554e797)

**new alerts:**

* 1 for Module is imported with &#39;import&#39; and &#39;import from&#39;
lgtm-com[bot](2021-06-25 01:01:21):This pull request **introduces 1 alert** when merging 44d8e5607dfbc6e8e7eb200c2f2d3db545aea98a into 3c857c57826cf434626cb8c99f0b9517efb62451 - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-3b69e3d8fc20814a64eda3655faf993fbf621728)

**new alerts:**

* 1 for Module is imported with &#39;import&#39; and &#39;import from&#39;
lgtm-com[bot](2021-06-28 23:35:12):This pull request **introduces 1 alert** when merging 6416d9ce2160a462a9fd649fce6f1d32017d75b4 into c79ab4c78599eb7f09cee8bf2d53af8250ed7dc9 - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-920d4c6fa06f746df9b448bf3f11cd0bba4194cf)

**new alerts:**

* 1 for Module is imported with &#39;import&#39; and &#39;import from&#39;
askhade(2021-06-29 17:04:24):@gramalingam 

1. Can you fix the lgtm-bot error
2. FunctionVerification.VerifyModelLocalFunctions is failing in CIs, this is because this test imports opsets without "," as a separator this needs to be fixed along with your fix for parser to allow "," as a separator between multiple opset imports

lgtm-com[bot](2021-06-29 19:11:08):This pull request **introduces 1 alert** when merging 7fc25dbde0650994adea28e66ee2ef50474bdacd into c79ab4c78599eb7f09cee8bf2d53af8250ed7dc9 - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-f4eb2c098d2079be8ff4f0b8718d945f828b31e4)

**new alerts:**

* 1 for Module is imported with &#39;import&#39; and &#39;import from&#39;
lgtm-com[bot](2021-06-29 19:56:21):This pull request **introduces 1 alert** when merging 162082894e0029689d7a2ebd1cf8ef99dabbe811 into c79ab4c78599eb7f09cee8bf2d53af8250ed7dc9 - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-ee15cd4667f3e786e578c2e431b62c120d92a95b)

**new alerts:**

* 1 for Module is imported with &#39;import&#39; and &#39;import from&#39;
gramalingam(2021-06-30 18:16:17):@postrational @wschin @ebarsoum : any feedback on this PR? Thanks!
askhade(2021-06-24 16:10:37):@take-cheeze : Can you elaborate on what do you mean by "Skipping test is not enough to check new operator support"
In what situations are you expecting to use this behavior. Thanks.
take-cheeze(2021-06-25 01:34:53):On my local env I'm adding these lines to backend tests to check I'm not adding support to new operator unexpectedly:
```python
NODE_FAILLIST = [
    # Failed tests:
    'test_resize_downsample_scales_linear',
    'test_resize_downsample_scales_linear_align_corners',
    # Failures from 1.9
    '_batchwise_',
]
NODE_FAIL_PATTERN = re.compile('|'.join(NODE_FAILLIST))

test_cases = backend_test.enable_report().test_cases
for cat, tcs in test_cases.items():
    for n, tc in tcs.__dict__.items():
        if NODE_FAIL_PATTERN.search(n):
            mark = pytest.mark.xfail(strict=True)
            setattr(tcs, n, mark(tc))
```
Commenting out these line would do what I expected but this check should run on CI.
I just want more maintained failed list instead of skipping with `exclude`.

After this PR I found this doc: https://github.com/onnx/backend-scoreboard/blob/master/ADD-BACKEND.md
And maybe using this feature it but I don't know how it's generated so I'll investigate later.
askhade(2021-07-13 22:38:51):@take-cheeze : Can you fix the DCO issue
take-cheeze(2021-07-14 06:48:02):@askhade Rebased and signed: https://github.com/onnx/onnx/pull/3542/commits/80653ecfbaf469a855933b9772877aa5e8e9d636
CLAassistant(2021-06-25 07:02:04):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=3545) <br/>All committers have signed the CLA.
mindest(2021-06-25 07:35:50):@SherlockNoMad Please help review this PR, thanks!
mindest(2021-06-28 08:05:04):@gramalingam @askhade Could you please help review when you have the time?
gramalingam(2021-06-28 15:51:27):Two questions:
(a) For the remaining type-variables that are shared (e.g., scale and bias; similarly for T2), are we sure that constraining them to be the same is okay? Or, is there any chance we will need to allow them to be of different types in the future?
(b) Can you please add a line of documentation to clarify the semantics when different precision is used? Eg., is the implementation required to cast everything to higher-precision upfront and perform the operation, and then downcast results as required? Or, is it more complex than that?
mindest(2021-06-30 07:18:34):@gramalingam 

> (a) For the remaining type-variables that are shared (e.g., scale and bias; similarly for T2), are we sure that constraining them to be the same is okay? Or, is there any chance we will need to allow them to be of different types in the future?

In common practice the scale/bias (which is like weight/bias in `torch.nn.Linear()`) and mean/var are of the same type, respectively. Loosening more of the type binding should also work, but its necessity is not clear from our practice and PyTorch/CuDNN usage of BatchNorm.

> (b) Can you please add a line of documentation to clarify the semantics when different precision is used? Eg., is the implementation required to cast everything to higher-precision upfront and perform the operation, and then downcast results as required? Or, is it more complex than that?

In training mode, if `T2` is float16, we require that mean/var be cast to float type because calculation of mean/variance can cause overflow in float16 due to summation. I've added this piece in the documentation, could you please check if it is okay?
askhade(2021-06-30 16:55:51):this change will need a version bump as well
gramalingam(2021-07-02 16:25:30):As Ashwini mentioned, the opset version needs to be increased. The old definition is copied to the nn/old.cc file. The opset number is increased to 15 for the new definition and defs/operator_sets.h must be updated. See https://github.com/onnx/onnx/pull/3412 for an example PR.
askhade(2021-06-30 16:38:54):did you manually edit this?
This change should be added to the defs.cc file, this way when you generate docs it will automatically pick it up
askhade(2021-06-30 16:41:07):CIs are failing for the same reason... 
gramalingam(2021-06-30 17:17:42):Yes, please update the defs.cc and regenerate the documents. I would change the comment to something like
```
The computation of ReduceMean and ReduceVar uses float to avoid overflow for float16 inputs.
```
Does this mean that it is not required for bfloat16 (only for float16)? Just trying to understand, even if it is an implementation-dependent feature.
mindest(2021-07-01 06:51:13):Regenerated the docs, sorry for the mistake.
@gramalingam for bfloat16 case, since bfloat16 has the same range as float32 (not as precise because its fraction bits are truncated), it does not cause overflow. There could be precision issue with bfloat16, but I think this is not unique to BatchNormalization.
sstamenova(2021-07-01 21:46:43):LGTM. Thanks!
askhade(2021-07-02 00:01:16):@sstamenova: addressed all the comments. Please take a look again.
sstamenova(2021-07-02 01:45:56):There's still one typo to fix in the readme, but looks good otherwise. Thanks!
jcwchen(2021-07-01 20:07:16):Do we need to fix numpy version for conda?
jcwchen(2021-07-01 20:10:31):Is the docker updated? I saw there are new commits this year [here](https://github.com/onnx/onnx-docker), but the docker was updated a year ago
jcwchen(2021-07-01 20:29:21):Please also remove DONNX_USE_PROTOBUF_SHARED_LIBS=XXX in https://github.com/onnx/onnx/blob/master/.github/workflows/win_no_exception_ci.yml and https://github.com/onnx/onnx/blob/master/.github/workflows/release_win.yml
sstamenova(2021-07-01 21:49:28):I can confirm that version 3.12.3 works as well (though we build it rather than installing from conda)
sstamenova(2021-07-01 21:52:02):I see that protobuf_MSVC_STATIC_RUNTIME changed from OFF to ON in the instructions. Does that actually work? I thought it had to be set to OFF and in our environment that's how we build it.
sstamenova(2021-07-01 21:55:40):Incidentally, if you are building with Visual Studio, the CMAKE_BUILD_TYPE variable should not be needed.
sstamenova(2021-07-01 21:59:06):protobuf_USE_STATIC_LIBS is not valid for the command for building protobuf. The correct setting is: ```protobuf_BUILD_SHARED_LIBS=OFF```
askhade(2021-07-01 22:04:02):I changed the instructions to match the current onnx release package config... Linking statically to runtime works as long as we are not building shared lib
sstamenova(2021-07-01 22:04:48):The comment for the option should also be updated to reflect that the flags for using static libs will be impaced:

```
option(ONNX_USE_PROTOBUF_SHARED_LIBS "Build ONNX using protobuf shared library. Sets PROTOBUF_USE_DLLS CMAKE Flag " OFF)
```
sstamenova(2021-07-01 22:05:01):protobuf_USE_STATIC_LIBS should be removed from those two yamls as well
sstamenova(2021-07-01 22:10:16):And from the build lines of protobuf in the yaml as well, since it does nothing. The flags that control how protobuf is build are ```BUILD_SHARED_LIBS``` (which controls the default of ```protobuf_BUILD_SHARED_LIBS``` and ```protobuf_BUILD_SHARED_LIBS``` itself.
askhade(2021-07-01 22:10:39):using 3.11.3 as we use this version in release and CI pipelines 
sstamenova(2021-07-01 22:11:35):protobuf_BUILD_SHARED_LIBS is a better option than BUILD_SHARED_LIBS since BUILD_SHARED_LIBS can be overwritten by passing a different value for protobuf_BUILD_SHARED_LIBS. It also makes it consistent with Windows.
sstamenova(2021-07-01 22:11:46):Same here
sstamenova(2021-07-01 22:14:16):Can you expand this section to include the interaction between the ONNX_USE_PROTOBUF_SHARED_LIBS setting and the two (three if you count the one about MSVC STATIC RUNTIME) relevant options from protobuf?
askhade(2021-07-01 22:36:20):changed to use "protobuf_BUILD_SHARED_LIBS" to control the behavior. This is set to ON by default but nice to explicitly state this
sstamenova(2021-07-02 01:43:56):When set to *ON* - onnx will dynamically...
lgtm-com[bot](2021-07-14 21:06:03):This pull request **introduces 1 alert** when merging 0fdec05b22174009366ffc33eb5619e474a4b146 into 2e5caf717817953f312e46e697557a51468715d7 - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-5217ddc0d5ced523f09fbefcd2334391e53100d0)

**new alerts:**

* 1 for Returning stack\-allocated memory
postrational(2021-08-24 16:00:53):Unfortunately code from this change breaks building on older compilers (break occurs on CentOS 7.6)
jcwchen(2021-08-24 16:05:18):> Unfortunately code from this change breaks building on older compilers (break occurs on CentOS 7.6)

@postrational Do you have detailed errors that I can look into? Thanks!
askhade(2021-07-06 17:47:12):what is the purpose of this function?
askhade(2021-07-06 17:50:12):after reading the rest of the PR now I understand this is the data propagator for shape op... Please update the function name to make it more clear and add some comments
askhade(2021-07-06 17:52:49):every op will not have a data propagator defined... add a check something like
schema->has_data_propagation_function() before creating dataPropagationCtx and calling schema->GetDataPropagationFunction()(dataPropagationCtx);
jcwchen(2021-07-07 01:58:49):Good catch. Added in both Python and C++.
gramalingam(2021-07-07 05:45:50):minor nit: perhaps we can just call it ```DataPropagationFunction``` ... shorter, and consistent with the getter function below.
gramalingam(2021-07-07 05:46:45):What about ```getAttribute``` ? Logically, it could be relevant for some data-propagation.
gramalingam(2021-07-07 05:50:56):Why do we need something name-based? I think index-based should be sufficient, and is more efficient.
gramalingam(2021-07-07 05:51:40):Positional access is what we use in the other Contexts.
jcwchen(2021-07-07 10:31:08):Right now it is solely used by test (verifying the generated propagation data by  given output name). Current getPropagatedData can only get data from input index. Perhaps I should to add another get function to get from output index instead of string name.
gramalingam(2021-07-07 16:16:12):Yes, I think so. Further, I would suggest names like ```getInput``` or ```getInputData```.  It is also worth documenting that what is propagated are actual (partial) values, encoded using a TensorShapeProto, as opposed to shapes of inputs and outputs. E.g., for first input, we get whatever we know about the first input value (not just its shape). 
gramalingam(2021-07-07 16:25:01):The "by-name" ability is not required for the node-level propagation-method, which doesn't even know the name of the actual-variable passed in as input or output. The tester is more like the caller, which can use the actual implementation class (which exposes more methods than the Context interface).
gramalingam(2021-07-09 15:46:50):nit: I still think it is better to replace ```getInputGeneratedShapeData``` by ```getInputData```. It is shorter, and more accurate. I am afraid the ```Shape``` part will likely confuse some developers. It is *not* related to the shape of the input ... it is actually information about the value of the input, (which may be used subsequently to infer the shape of some other tensor).
gramalingam(2021-07-09 15:48:07):Similarly, better to change ```getOutputGeneratedShapeData``` to ```getOutputData``` and ```addGeneratedShapeData``` to ```setOutputData```.
gramalingam(2021-07-09 15:51:44):Couldn't we move this constructor into the then-branch below?
askhade(2021-07-09 16:34:35):agree with all the 3 recommendations
askhade(2021-07-09 16:35:10):when do we need output name? and what about input name? do we need that too?
gramalingam(2021-07-09 17:28:21):Agree. I don't think it needs to be in this interface. (Not needed for the node-level propagation function, even though the *implementation* may use the names.)
gramalingam(2021-07-09 17:32:49):Another suggestion is to add a ```typedef TensorShapeProto PartialInputData``` or ```typedef TensorShapeProto SymbolicInputData``` and use those new type names in this interface. We are reusing TensorShapeProto to avoid defining a new data-structure to store either constant-values or unknown-values. E.g., we could imagine a more sophisticated representation in the future.
askhade(2021-07-09 18:58:17):why is getOutputData required? When will this be used? You only setoutput data ... the next node calls getInputdata to fetch this data right?
jcwchen(2021-07-09 21:35:56):Good catch. Removed.
jcwchen(2021-07-09 22:09:49):Just simplified the function name. GetOutputData is just for testing and won't be used in the propagation.

Yes it's still doable for testing without getOutputData... I have removed that function and add one more node behind the tested node to get the input data to verify.
gramalingam(2021-07-09 22:42:04):Just a note for the future: using the parser will make writing such test-cases much simpler. E.g., see the examples here: https://github.com/onnx/onnx/blob/56065a0decdafee066a448d72dd8d4fd17bf131d/onnx/test/cpp/parser_test.cc#L213 ... no need to change this now since you have already written it.
gramalingam(2021-07-09 22:48:24):This is not correct. If all dimensions of the input are known values, we should return the actual size, otherwise an unknown value.
jcwchen(2021-07-12 13:56:05):Thanks for the info. I will utilize it with future tests
jcwchen(2021-07-12 13:56:41):Thanks for catching this. Just removed size in this PR
jcwchen(2021-07-12 13:57:34):Not sure whether this comment is needed or it should be improved
jcwchen(2021-07-12 23:38:15):Originally InferenceContext has `getInputData` function to get "TensorProto". I think DataPropagationContext will need this function for future propagation functions as well... To sync the naming convention, I will make `getInputData` to get "TensorProto", `getInputShapeData"`to get the produced "TensorShapeProto", and `addOutputShapeData` to propagate the "TensorShapeProto". This PR is ready for review again @gramalingam @askhade. Thank you!

(*Updated)
Revert it due to https://github.com/onnx/onnx/pull/3551#discussion_r668420817. Still only using addOutputData and getInputData
askhade(2021-07-13 04:38:31):Instead of having 
virtual const TensorProto* getInputData(size_t index) const = 0;
 virtual const TensorShapeProto* getInputShapeData(size_t index) const = 0;

can you simply keep one like this:
virtual const TensorShapeProto* getInputData(size_t index) const = 0;

The implementation of DataPropagationContext can convert the TensorProto to TensorShapeProto... otherwise in every data propagation function you will need to call 1 and if it returns nullptr then call the other... 

askhade(2021-07-13 04:46:58):use inputIndexToNameMap_ instead of allInputData_? The check is good as both these containers will be of same size but using inputIndexToNameMap_  is cleaner
askhade(2021-07-13 04:49:29):whether to enable data propagation or not needs to be controlled by an option... we can talk about it offline
gramalingam(2021-07-13 17:15:01):Attaching a more complete comment below. But I think a better place for this would be where
DataPropagationContext is defined.
```
We use data propagation to perform partial evaluation of the model, to compute statically
known information about tensor values. It is intended to improve the precision of shape
inference. We reuse TensorShapeProto to represent the statically known values. One
limitation of this is that TensorShapeProto can represent only integer values.
As an example, data-propagation is intended to handle code-fragments like below:
   shape = Shape(X)
   batchsize = Slice(shape, [0], [1])
   newshape = Concat (batchsize, [1024, 1024])
   Z = Reshape(Y, newshape)
If the shape of X is statically known, then data-propagation should be able to determine
the value of newshape, as well as the shape of Z.
```
jcwchen(2021-07-13 18:24:53):I added it as "data_prop" (any other idea for the name?). Since the function now is not complete, setting it as an option (default is False) makes more sense to me. Also onnx.shape_inference should not run it if a user just wants a general shape_inference without partial data propagation... Please let me know if you have other concern. Thanks
jcwchen(2021-07-13 18:25:24):Updated. @gramalingam Thank you for the detailed explanation!
jcwchen(2021-07-13 18:29:07):Merging it as single `virtual const TensorShapeProto* getInputData(size_t index) const = 0;` sounds good. The updated getInputData will first check whether `allInputTypes_` exists a valid Shape inside TypeProto. If not, it will further check TensorShapeProto from `generatedShapeData_`. Please review it again
jcwchen(2021-07-13 18:31:46):I decided to use `allInputTypes_` now because it will access allInputTypes_[index] in the next lines anyway so using it here will look more simpler...
askhade(2021-07-13 20:09:43):it would be good to create an options struct
struct ShapeInferenceOptions {
bool check_types;
enum error_mode; or bool strict_error_mode;
bool enable_data_propagation;
}

askhade(2021-07-13 20:11:16):making this bool makes more sense... something like bool enable_data_prop
reason for error_mode being int is tomorrow we can introduce more modes easily but data propagation is binary yes or no
askhade(2021-07-13 20:24:50):this is incorrect.... 
here you are returning the shape of the input and not data... you need to convert TensorProto into TensorShapeProto
For shape computations TensorProto ->TensorShapeProto is possible... 
jcwchen(2021-07-14 02:49:05):Good idea. Added
jcwchen(2021-07-14 02:53:30):Thanks for catching this. I used ParseData to convert allInputData_ in getInputData. Please review it again
gramalingam(2021-07-14 04:00:03):This is a different variable from the outer-scope variable. This won't work since it won't be used in the loop below in line 342. I think you may have to duplicate the loop lines 342-344 separately for the then branch and else branch.
jcwchen(2021-07-14 04:08:33):Updated. Thanks for the catch
askhade(2021-07-14 16:22:04):I suggest change this to error_mode and let the value determine whether it is strict or something else
per current behavior : when the mode is strict aka 1 shape inf will throw any node level shape inf errors whereas when it is 0 it does not.... other errors like merging existing shape with inferred etc are thrown regardless...
askhade(2021-07-14 16:31:11):please use reference: const ShapeInferenceOptions& options
askhade(2021-07-14 16:37:35):const ShapeInferenceOptions& options
askhade(2021-07-14 16:48:22):tsp is set to nullptr and tsp->mutable_dim() is being called before mem allocation. There is a deeper problem here... even if you initialize who will free it? the ownership for this is not clear.

1 suggestion- create TensorShapeProto and add it to generatedShapeData ... this way generatedShapeData  will own it and if the same initializer is being used else where you can save some time in reconverting TensorProto to TensorShapeProto. If you do this then I you will have to move the check for generatedShapeData above InputData
askhade(2021-07-14 16:53:13):also check for shape of input... allow only 1D or scalars
gramalingam(2021-07-14 17:27:34):Is that strictly necessary (for correctness)? We treat the returned value as representing the underlying sequence of data-values in the tensor, without saying anything about its shape. (The shape info, if any, is captured in the type.) I understand that in common use-cases this may not arise.
gramalingam(2021-07-14 20:21:55):Restricting this to 1D or scalars is okay (it should work for the common-case), but I still think it is not necessary. Allowing other cases enables a more general implementation without much of a cost. (But, given the deadline, I am okay with this restriction for now.)
gramalingam(2021-07-14 20:52:48):Why is the ```const``` missing in ```getOutputType``` and ```getInputData``` ?
askhade(2021-07-14 21:01:58):I suggest going with the restriction ... once we come across the models which require a more generic implementation we can update this... 
askhade(2021-07-14 21:05:30):nit: you can combine the if on line 350 and 352
jcwchen(2021-07-14 21:07:35):Current getInputData will add initializer into generatedShapeData_.
getOutputType returns the reference of &allOutputTypes_.

So both of them will potentially modify its member.

askhade(2021-07-14 21:16:19):nit: move all the member variables below the methods?
askhade(2021-07-14 21:17:27):this wont work... you moved tsp on line 364. you should return &generatedShapeData_[inputIndexToNameMap_.at(index)]
gramalingam(2021-07-14 21:39:59):Can ```getOutputType``` return a ```const TypeProto*```? If so, it can be marked ```const```, right?
jcwchen(2021-07-14 21:50:19):For now I still added the restriction since it's the most common case. Still, I added some comments here as TODO for future reference. Thanks for pointing out.
jcwchen(2021-07-14 22:10:07):Yes for `DataPropagationContext`'s `getOutputType`, it is doable. I have updated it as a const function. Thanks. 

Just FYI: `getOutputType` from `InferenceContext` is not a const function either and there are quite many places directly using getOutputType and then modify it (like Loop op or shape_inference.h)...
askhade(2021-07-15 03:55:27):nit: you can take in TensorShapeProto& tsp as an input param... this way you dont have to create and copy this obj
askhade(2021-07-15 03:56:14):like I mentioned above you can change this to 

vectorToTensorShapeProto(ParseData<int64_t>(input_data), tsp);
askhade(2021-07-15 03:57:26):will we even need GraphInferenceContext in DataPropagation? 
jcwchen(2021-07-15 15:03:44):For now no, I think only control ops like `Loop` or `If` will need it and they are not included in the supported list for data propagation. I have removed it. Thanks.
askhade(2021-07-15 15:53:19):since you removed graphinferencecontext - I dont think you need this anymore
gramalingam(2021-07-16 19:28:43):I think "Cast" must specify the target of Cast, like "Cast<to = ...>(y)"
jcwchen(2021-07-16 19:35:18):Adding required input "axes" for Unsqueeze
gramalingam(2021-07-16 20:02:51):I think we can make y a constant as ```y = Constant <...>()``` to test this out.
gramalingam(2021-07-16 20:04:19):Of course, that would require a data-propagation for Constant. May be that is the issue (until we allow initializers in the parser).
jcwchen(2021-07-16 20:49:59):Added. Since the data propagation of Unsqueeze won't use axes, not sure whether Constant works as expected in this case.

> Of course, that would require a data-propagation for Constant. May be that is the issue (until we allow initializers in the parser).

Actually I think we can just skip Constant for data propagation? Constant data can still go into inputDataByName for other op's data propagation function to use, right?
gramalingam(2021-07-16 20:53:32):You are right, my mistake, sorry. Please ignore my comment.
gramalingam(2021-07-16 20:54:13):I mean it is not required for Unsqueeze
askhade(2021-07-19 04:03:30):this check does not seem right... 
if A->dim(i).has_dim_value() is false then you should not fetch dim value... also why is there no check for dim_param
from the function name it seems like you are simply comparing the shapes but you are actually just checking the dim values
askhade(2021-07-19 04:13:26):this graph is invalid...
Shape of x is [7, 4, 1] this is correct however the shape of xs and y will not be [7, 4, 1] it will be [3] because shaoe node will output [7, 4,3] and its shape will be [3]... similarly cast will simply cast the data without changing the shape so shape of y will be [3]

Your test passes because you are not calling TypeAndShapeInferenceFunction for the node before calling DataPropagationFunction

Also check on line 105 is not right... that function should simply return the computed shape data... every individual node test should run comparison, for nodes like shape you will need to hard code the expected output
askhade(2021-07-19 04:14:06):same comment as above
askhade(2021-07-19 04:14:30):same comment as above .... the shape changes after Shape node... 
jcwchen(2021-07-19 14:38:23):Do we need to further compare the data_params here?
gramalingam(2021-08-05 18:23:01):@hwangdeyu : can  you please look at this comment https://github.com/onnx/sigs/blob/master/operators/meetings/023-20210708.md#issue-treatment-of-reflect-attribute-in-pad-op : it is about the treatment of reflection when padding. It would be helpful to clarify the treatment in this op as well.
hwangdeyu(2021-08-09 16:11:10):> @hwangdeyu : can you please look at this comment https://github.com/onnx/sigs/blob/master/operators/meetings/023-20210708.md#issue-treatment-of-reflect-attribute-in-pad-op : it is about the treatment of reflection when padding. It would be helpful to clarify the treatment in this op as well.

Sure, will add the description of the `reflect` attribute in the `Pad` op. Thanks for reminding.
hwangdeyu(2021-08-31 00:43:31):@gramalingam @askhade 
https://github.com/microsoft/onnxruntime/pull/8551
The `GridSample` operator kernal inplementation is merged into onnxruntime repo as an contrib op.
Any further advise or comment for this ONNX spec PR?
hwangdeyu(2021-09-07 03:11:20):@askhade Could we merge this PR？
msollami-sf(2021-09-07 23:44:15):Someone please merge this
askhade(2021-09-08 04:46:14):@hwangdeyu : This is a new operator and before we merge this we should make sure the spec is finalized. I recommend to wait till the evaluation of the model with this op is complete. I have marked this PR with 1.11 milestone. Next ONNX release will be in ~Nov  so we have some time. Can you work with 1P team to make sure there is no more change required to the spec?
hwangdeyu(2021-09-08 08:16:27):> @hwangdeyu Deyu Huang FTE : This is a new operator and before we merge this we should make sure the spec is finalized. I recommend to wait till the evaluation of the model with this op is complete. I have marked this PR with 1.11 milestone. Next ONNX release will be in ~Nov so we have some time. Can you work with 1P team to make sure there is no more change required to the spec?

Get it. Sure, will connect with them when the evaluation of the model almost finished.
avostryakov(2021-10-28 07:34:30):Hi, @hwangdeyu, @askhade! Just worry about it. Will it be included in onnx 1.11 because we have a model with grid_sampler operation inside? Now we have a workaround with mmcv library to export from pytorch to onnx and start a converted model in onnxruntime but we can't quantize this model for example. Anyway, it's better to have this functionality right in onnx/onnxruntime.
hwangdeyu(2021-10-29 07:07:35):> Hi, @hwangdeyu Deyu Huang FTE, @askhade Ashwini Khade FTE! Just worry about it. Will it be included in onnx 1.11 because we have a model with grid_sampler operation inside? Now we have a workaround with mmcv library to export from pytorch to onnx and start a converted model in onnxruntime but we can't quantize this model for example. Anyway, it's better to have this functionality right in onnx/onnxruntime.

Yes, this op is marked in 1.11 milestones. You can also use it in ORT as an contrib op now. https://github.com/microsoft/onnxruntime/blob/85874bb3150b9d9a7cce0ed3fa6500b49e385c2b/docs/ContribOperators.md#com.microsoft.GridSample
hwangdeyu(2021-11-02 15:24:14):> @hwangdeyu : This is a new operator and before we merge this we should make sure the spec is finalized. I recommend to wait till the evaluation of the model with this op is complete. I have marked this PR with 1.11 milestone. Next ONNX release will be in ~Nov so we have some time. Can you work with 1P team to make sure there is no more change required to the spec?

@askhade I have made a comfirm with 1p team, there is no more change required to the spec.
hwangdeyu(2021-11-15 15:16:25):@askhade  @gramalingam  Is it a good time to merge this PR？
askhade(2021-11-15 17:08:20):> @askhade @gramalingam Is it a good time to merge this PR？

@hwangdeyu:  Thanks for your contribution! The PR looks good now. I have set it to auto merge. Once all the CIs are successful it will go through. 
laisimiao(2021-12-23 08:50:24):> @hwangdeyu : This is a new operator and before we merge this we should make sure the spec is finalized. I recommend to wait till the evaluation of the model with this op is complete. I have marked this PR with 1.11 milestone. Next ONNX release will be in ~Nov so we have some time. Can you work with 1P team to make sure there is no more change required to the spec?

It's Dec. When will onnx 1.11 release? 
jcwchen(2022-01-03 19:29:08):Hi @laisimiao,
Because ONNX has released 3 times last year and at that time there weren't enough and significant updates, eventually ONNX decided to postpone 1.11 release. It will probably happen this month or next month. Sorry for the confusion and inconvenience. If you are urgent to use new features after ONNX 1.10, you can try onnx-weekly [package](https://test.pypi.org/project/onnx-weekly/) from TestPyPI. Thank you!
askhade(2021-07-06 16:08:59):add explaination for different padding modes - 
askhade(2021-07-06 16:12:43):the align_corners option selected here should be in sync\same as with the option selected for resizing the image (if necessary)  before grid sampling right? 
askhade(2021-07-06 16:18:06):Can you add details around possible values\range for values in grid, and how should the out of range values be treated
askhade(2021-07-06 16:20:37):need to add tests before finalizing the spec
askhade(2021-07-06 16:21:14):you should add this to opset 15 as this is the latest version
askhade(2021-07-06 16:21:41):nit: GridSampler_ver15_doc
askhade(2021-07-06 16:27:44):propagateShapeAndTypeFromFirstInput will copy the shape from input to output... for this op this is not correct... you need to construct the shape from input and grid shape
hwangdeyu(2021-07-06 16:38:02):Now exporter only support opset 14 after this PR merged. https://github.com/pytorch/pytorch/pull/59486.
So I'm not sure if it's better to support exporter to community or 1-p customer in opset14, or it's needed to use the latest version for ONNX？ 
hwangdeyu(2021-07-06 16:38:57):Thanks, that's the part I am doing..
askhade(2021-07-06 16:40:31):opset 14 is already released and you cannot add a new op to a released opset. Work for opset15 is in progress now and any op update or new op should be added to opset 15. Once onnx releases (next one is scheduled for July end) exporter can add support for this new opset.
gramalingam(2021-07-06 16:57:09):Please take a look at the example here: https://github.com/onnx/onnx/blob/ea8e57a71e01d1e70b188465c028eb3c6c6cc118/onnx/defs/object_detection/defs.cc#L114 ... ```unifyInputDim``` should help simplify this.
hwangdeyu(2021-07-06 17:01:11):Consider if we also need to support volumetric (5-D) input support.
gramalingam(2021-07-06 17:01:58):It is unclear to me what this function is supposed to compute. Can we expand the description to be more complete, or add a link to some existing function in other frameworks, or both? Thanks.
hwangdeyu(2021-07-06 17:02:44):Add more shape inference test for different shape and param.
gramalingam(2021-07-06 17:03:44):Is the "offset and output" a typo? Don't understand it.
askhade(2021-07-06 17:09:38):I prefer both as opposed to just adding the link to some existing function in other framework...
gramalingam(2021-07-06 17:11:03):IIUC, this is used as an index into the input coordinates. Not sure we need to use the same type "T" as the other "T" ... would it make sense to have this be a "T2"?
jcwchen(2021-07-06 18:02:14):nit: Existing op like RoiAlign uses `(N, C, H, W)` to represent the 4d input. Sync the same naming convention might be better.
jcwchen(2021-07-06 18:07:41):nit: where outH and outW `are` the height and width of offset and output
hwangdeyu(2021-07-07 16:30:10):Thanks, added.
hwangdeyu(2021-07-07 16:32:07):> It is unclear to me what this function is supposed to compute. Can we expand the description to be more complete, or add a link to some existing function in other frameworks, or both? Thanks.

I only find this op in PyTorch, have posed the link in description. And this op is from Spatial Transformer Networks, I also added the paper link too.
hwangdeyu(2021-07-07 16:32:46):> Please take a look at the example here:
> 
> https://github.com/onnx/onnx/blob/ea8e57a71e01d1e70b188465c028eb3c6c6cc118/onnx/defs/object_detection/defs.cc#L114
> 
> ... `unifyInputDim` should help simplify this.

Thanks, that's very helpful.
hwangdeyu(2021-07-07 16:33:41):> Is the "offset and output" a typo? Don't understand it.

Added more description..
hwangdeyu(2021-07-07 16:34:21):Thanks. Good point. 
hwangdeyu(2021-07-07 16:39:17):Actually, I don't find any implementation in numpy for grid_sampler to add the backend tests yet.. Do you have any suggestions to do it?
hwangdeyu(2021-07-09 06:40:31):Yes, I think that means a grid generated by affine_grid() should be passed to grid_sample() with the same setting for this option.
refer link: https://pytorch.org/docs/master/generated/torch.nn.functional.affine_grid.html?highlight=align_corners.

askhade(2021-07-09 18:55:17):There are couple of issues opened for this op. We can ask there if people need 5-D support if so we can add it now otherwise it is OK to add it later as need arises.
hwangdeyu(2021-07-12 11:39:18):According to the example of several github issues that are provided input, all the inputs are 4-D shape. I think we can support 4-D it firstly.
https://github.com/pytorch/pytorch/issues/41957
https://github.com/pytorch/pytorch/issues/27212
https://github.com/AliaksandrSiarohin/first-order-model/issues/92
https://github.com/PaddlePaddle/Paddle2ONNX/issues/273
hwangdeyu(2021-07-14 15:45:12):Consider using script to generate the test outputs.
hwangdeyu(2021-07-15 15:34:48):added the torch test and hardcoding outputs. It looks like CI has no module named 'torch'.

gramalingam(2021-08-13 19:09:47):I don't understand this. If x' = 1.5 is reflected in border 1, doesn't that give us x'' = 0.5? Why is is -0.5?
hwangdeyu(2021-08-16 03:53:26):You are right. It also looks like an error in [grid_sample PyTorch documentation](https://pytorch.org/docs/master/generated/torch.nn.functional.grid_sample.html#torch-nn-functional-grid-sample). Will fix this fault.

hwangdeyu(2021-08-29 03:51:21):Fixed, and also post an issue to PyTorch repo. https://github.com/pytorch/pytorch/issues/63559
askhade(2021-11-08 18:50:34):Please change to operator version 16 as this is the current working version. Make sure you regenerate the tests after you update the version.
hwangdeyu(2021-11-09 11:15:21):Done~
hwangdeyu(2021-11-12 03:17:57):> Please change to operator version 16 as this is the current working version. Make sure you regenerate the tests after you update the version.

Anything else required to merge this PR?
askhade(2021-07-08 16:42:15):use OpSchema::all_tensor_types_with_bfloat() instead? for better readability
askhade(2021-07-08 16:42:27):same as above use : OpSchema::all_tensor_types_with_bfloat()
gramalingam(2021-07-08 17:39:35):That includes the complex types, but cast doesn't support complex. I have copied the same types as in cast. 
askhade(2021-07-09 05:52:59):not sure I understand why would these 2 be None?
gramalingam(2021-07-09 16:18:21):I copied it from Cast. My guess is that minor adjustments were needed for types like bfloat16 because of the numpy doesn't support this type, so the numpy values have a different type from the intended type (so, the call to ```expect``` is different here, and the intended-type is explicitly mentioned).
jcwchen(2021-07-08 17:08:54):Just out of curiosity: is there any example for tuple output?
gramalingam(2021-07-08 17:32:46):I don't understand how this is supposed to work. In particular, I wonder if this assumes some restrictions on the types of the values stored in the list. Will it work for a sequence of sequence of tensors, for example? I think it may be better to recursively call assert_similar_outputs for every pair of corresponding elements.
chudegao(2021-07-09 00:39:38):Acutally I did not hit tuple output so far. Just add tuple in case for some special case. Let me know I should keep it or remove it. 
chudegao(2021-07-09 00:58:15):> I don't understand how this is supposed to work. In particular, I wonder if this assumes some restrictions on the types of the values stored in the list. Will it work for a sequence of sequence of tensors, for example? I think it may be better to recursively call assert_similar_outputs for every pair of corresponding elements.

I checked onnx operator output, it only support seq(tensor) . But it's good idea to to recursively call assert_similar_outputs(). I will update. Thanks. 
chudegao(2021-07-12 06:14:45):@gramalingam I updated to call assert_similar_outputs() recursively, please review.
gramalingam(2021-07-12 16:38:57):It would be nice to have a comment to explain what case is handled by the then branch. (I understand that this is a problem with the pre-existing code, but it would help readers/reviewers/maintainers in the future.)
gramalingam(2021-07-12 16:39:19):In case you know.
askhade(2021-07-09 05:49:46):why are you installing protobuf here?
entrypoint.sh does this right?
https://github.com/onnx/onnx/blob/master/.github/workflows/manylinux/entrypoint.sh#L18


jcwchen(2021-07-09 07:04:18):entrypoint.sh installs protobuf inside the docker for building the wheel. The protobuf installation here is in the GitHub Action environment for testing the wheel.
askhade(2021-07-09 16:28:19):can you add this comment to the yml to make this more clear. Thanks!
askhade(2021-07-09 16:29:13):also since this is relevant for testing can we move this below just before testing starts?
jcwchen(2021-07-09 19:46:16):Updated. Thanks for the suggestions!
askhade(2021-07-09 16:13:24):@postrational can you help review this. We need to get this in onnx 1.10

These ops were reviewed and validated in onnxruntime as contrib ops: https://github.com/microsoft/onnxruntime/pull/7946
askhade(2021-07-09 16:21:10):nit: rename to export_get_sequence
gramalingam(2021-07-09 17:02:06):Does this map to a one-dimensional tensor in ONNX? I think we want a scalar (zero-dimensional tensor), right?
gramalingam(2021-07-09 17:08:23):Add ```&& (attr_proto == nullptr)``` to the condition.
gramalingam(2021-07-09 17:09:58):change ```optional type``` to ```optional type value```
gramalingam(2021-07-09 17:10:37):change ```optional type``` to ```non-empty value```
gramalingam(2021-07-09 17:16:05):General comment: We are forced to enumerate the set of allowed types here by ONNX's current design. This makes these ops more restrictive than necessary. We should generalize the implementation to allow unconstrained types as discussed in https://github.com/onnx/onnx/issues/2324. However, I doubt if that can be done by the deadline for current release, so I feel it is okay to have this in this PR for the short term.
gramalingam(2021-07-09 17:35:46):I don't understand why None is allowed here. The python type signature above is inconsistent with this. I think there might be a better way to handle this at the caller, instead of changing this function. Where is it being called with None?
gramalingam(2021-07-09 17:41:55):May be we should ```assert inferred_vi.HasField('optional_type')``` here.  (Similarly above for other cases.)
neginraoof(2021-07-09 18:15:55):This is a zero dimension numpy scalar. The ONNX model output value info does not have shape (only boolean dtype).
neginraoof(2021-07-09 18:20:01):Makes sense. Thanks.
askhade(2021-07-12 05:19:16):please add this comment back :

 TODO: Account for recursive sequence case. Right now, this function supports
 Sequences of Tensors.

we are still only supporting Sequence of tensors

askhade(2021-07-12 05:24:06):In the case where type_proto is None there is no else block to set type_proto... meaning if control does not enter 
if isinstance(input, list) or elif isinstance(input, np.ndarray) or np.isscalar(input) then type_proto will be None when the control reaches line 118.


askhade(2021-07-12 05:34:11):how will this work in the case when optional proto holds None value? type_proto will be None?
gramalingam(2021-07-12 16:24:15):Is this an accidental change? Moving the line around doesn't seem to change anything.
gramalingam(2021-07-12 16:27:56):I don't understand why this is marked as resolved. What happens if both an input is supplied and the type attribute is specified? I think it is safer to flag it as an error.
gramalingam(2021-07-12 16:32:16):Agree. Better to have an assertion/check if the value is None.
neginraoof(2021-07-12 17:57:53):@askhade I don't think we need the comment back. The function now does support sequence of sequence or other recursive types if the type_proto for such type is provided. So users can create this kind of type_proto and then use _extract_value_info to create the value info.
neginraoof(2021-07-12 18:03:46):@askhade In the case where optional_proto holds None, we rely on the user to provide a corresponding type_proto. For a None tensor, the type_proto has elem_type (depending on the None tensor data type).
I'll add a check to fail if both input and type_proto are None.
neginraoof(2021-07-12 18:11:36):Yes, thanks. This is not intended.
gramalingam(2021-07-14 03:36:53):shape should be None as before. We can't assume all elements in the sequence have the same shape (unless we check and verify this).
hariharans29(2021-07-14 19:06:13):very minor nit: is the single quote and the superfluous double quotes intentional ?
gramalingam(2021-07-09 16:36:16):The checker can be called by ONNX's own whole-model-checker but also by others (like ORT). I wonder if this is the behavior we want when it is called by ORT? We could consider the following options: (a) check if domain == standard domain directly, (b) replace the "||" by "&&", or (c) Have the caller specify via an option whether they want a strict check here or not. I think (b) may be a reasonable compromise for now. (If one is available and the other is not, treat it as error?)
gramalingam(2021-07-09 16:37:49):A combination of (a) and (b) would be to add an extra-check that the domain is not the onnx standard-domain.
askhade(2021-07-09 16:43:55):combination of a and b makes sense for now... we can take up option c later and apply it for check_node function as well
askhade(2021-07-09 16:51:24):actually just b should suffice... right after this check_node is called and it does check if domain is one of the standard domains... adding (a) would make the check duplicate
garymm(2021-11-11 23:27:12):@askhade in the future could you please fill out the `Why is this change required? What problem does it solve?` section of the PR description?

This change actually caused us some trouble integrating ONNX 1.10 into PyTorch and it'd be nice to understand the context.
jcwchen(2021-07-09 19:48:39):add -c conda-forge should work
gramalingam(2021-07-20 15:31:01):Closing this in favor of #3580 .
jcwchen(2021-07-14 14:24:13):nit: perhaps axis_negative_1? negative is more commonly used as test name under onnx/backend/test/data/node/
askhade(2021-07-14 14:50:32):nit: use axis instead of Index?
askhade(2021-07-14 14:52:38):nit: elaborate this sentence similar to:  Dimension of the given input tensor's shape at the specified axis. 
askhade(2021-07-14 14:57:56):the spec does not mention a default value... should this be changed to 1. first verify attr has a value and then validate the value?
gramalingam(2021-07-14 16:49:24):There is no default-value, it is a required attribute. The default-checker would already have validated that the attribute is specified. I used ```getAttribute(...)``` as a short-cut (that does the checks anyway), but the default-value won't be used. I could replace this code by ```ctx.getAttribute("axis")->i()``` which is equivalent, if that seems preferable. I am inclined to avoid redundant checks here.
gramalingam(2021-07-14 17:08:22):Sure. Perhaps something like "The slice of the input tensor's shape at the specified axis"? This started me thinking about the name of the op. The word "dimension" is itself overloaded, since it is also used, like "axis", to indicate a specific axis. But I can't think of a better name for the operator itself. MLIR does use the same opname "Dim", for what it is worth. (We could overload "Shape", making axis an optional parameter ... we can't define it as a function then, but this could be an option.)
askhade(2021-07-14 17:11:01):if checker is run on the model then yes this should be caught... my point was current code makes you think default value is 0 which is not right... maybe add a comment 

My preference is to use getAttribute, validate not nullptr (fail if nullptr) and fetch value like we do for other required attrs
rajeevsrao(2021-07-16 22:41:30):ONNX-TensorRT can support either this or the original (`Dim` op) proposal, with a slight preference for using this approach (using shape op and return slice).
askhade(2021-07-19 22:17:17):I think this approach is better over introducing Dim (PR #3574 )
askhade(2021-07-19 22:13:06):is this if required? if (!hasNInputShapes(ctx, 1)) is checking the same thing right?
askhade(2021-07-19 22:16:04):validate the end-start before setting dim_value?
make sure end -start is positive > 0
hariharans29(2021-07-21 01:13:51):@gramalingam @askhade : Is it worth explicitly calling out the behavior in the spec that by setting start > end (and start == end because of the exclusive nature of end) the resultant shape slice will be empty (over it being an error) ? Just by reading the spec, this portion was unclear to me until I saw the shape inference. (Sorry if I overlooked something obvious in the spec)
neginraoof(2021-07-14 23:54:39):@askhade  I am addressing the failures on the CI. Will update as soon as this is resolved.
neginraoof(2021-07-15 00:15:03):@askhade The CI looks green now. Can you take a look? Thanks a lot.
yufenglee(2021-07-15 18:21:28):nit: [v_1, v_2, ..., v_K]
gramalingam(2021-07-15 19:07:27):Typo: change to "to per column quantization"
gramalingam(2021-07-15 19:07:51):typo: change to "to per row quantization"
gramalingam(2021-07-15 19:11:23):But can it be [D1, D2, 1]? Or, do we allow only the two extremes of a zero-point for the entire tensor or a zero-point for each row?
gramalingam(2021-07-15 19:13:34):Or [D1, D2, 1, 1], I mean.
gramalingam(2021-07-15 19:15:19):type "tesnor"
askhade(2021-07-15 19:17:10):right now we only allow per tensor or per row\col  zero points
jcwchen(2021-07-16 17:39:32):Maybe something like 1.9.2 for VERSION_NUMBER? Typically we use 1.10.0 for the final verified package
rajeevsrao(2021-07-20 21:16:49):@jcwchen should I use the 1.9.x version only on release branch (after this is merged to `master`) and prior to generating the test wheels as the workflow document suggested or do that even for `master`?
rajeevsrao(2021-07-20 21:34:26):Updated as suggested - downgraded release version to 1.9.100 for test package.
jcwchen(2021-07-21 02:56:57):@rajeevsrao Formerly ONNX <= 1.8 did bump the version to the target version directly in the main branch. However, ONNX has an auto-generated weekly package from main branch now so if the release is not completed, there might be a 1.10 onnx-weekly package first before the real 1.10 release... which is quite confusing.

@askhade Do you have any preference for it (bumping to which VERSION_NUMBER)?
rajeevsrao(2021-07-21 15:44:48):I changed it to v1.9.100 to be safe.
askhade(2021-07-21 16:39:29):I suggest changing this to 1.10.0 in master.... In release branch create a new PR to just change the version in this file to 1.9.100

The idea is any new PR going into master post this PR is merged will be towards the next release.
askhade(2021-07-21 16:40:31):this should be 1.10.0

rajeevsrao(2021-07-21 16:42:51):Switched it back to 1.10 based on comments.
rajeevsrao(2021-07-21 16:43:14):Thanks - will use 1.9.100 only on the release branch.
jcwchen(2022-06-23 23:52:57):tiny nit: it would be helpful if we can have a comment about why we exclude it for future reference.
askhade(2021-07-19 22:05:59):will update once #3551 is checked in
askhade(2021-07-19 22:06:13):will update once #3551 is checked in 
jcwchen(2021-07-20 14:30:58):nit: "unk__" is the default value for createNew already?
jcwchen(2021-07-20 14:41:40):Should be `EXPECT_TRUE(
        (inferredShape.dim(i).has_dim_value() && expectedShape.dim(i).has_dim_value()) ||
            (inferredShape.dim(i).has_dim_param() && expectedShape.dim(i).has_dim_param())) <<
        "Inferred and expected dim values are different.";` ?
jcwchen(2021-07-20 14:57:30):`getShapeInput` here in InferenceContext is basically same as `TensorShapeProto* getInputData` in DataPropagationContext, right?

Actually I am thinking that whether to change `getInputData` into `getShapeInput` in DataPropagationContext to prevent confusion. Otherwise, `getInputData` functions in InferenceContext and DataPropagationContext are doing different things...
jcwchen(2021-07-20 15:10:24):Do we need to consider int32_t for ParseData?
jcwchen(2021-07-20 15:14:23):Perhaps else if for (shapeInput) ?

I thought targetShapeInitializer and shapeInput cannot be valid at the same time.
askhade(2021-07-20 15:25:27):It is for SymblTableImpl but not for for SymbolTable Interface... 
We should be using SymbolTable and not SymbolTableImpl in all function interfaces otherwise we are defeating the purpose of having SymbolTable and SymbolTableImpl.... 
askhade(2021-07-20 15:25:55):yes updated

jcwchen(2021-07-20 15:31:14):nit: Redefinition for dim_value
gramalingam(2021-07-20 15:34:40):I would recommend ```getSymbolicInput``` to avoid confusion.
gramalingam(2021-07-20 16:15:18):Logically, this subsumes ```getInputData```. Eg., in the long term, we could conceivably replace ```getInputData``` by this function (just like in DataPropagationContext), but that would require updating all current uses. If possible, I would like to avoid a potential confusion between ```Get input data encoded as a TensorShapeProto``` (which is what this is) and ```Get shape of input data``` (which this is not).
gramalingam(2021-07-20 16:39:42):A more general solution would be to treat this as an unknown dimension, instead of failing. I think that is preferable.
gramalingam(2021-07-20 16:41:11):There is a tricky problem here, in case the actual value at runtime happens to be either 0 or -1. In that case, propagating the dimension in the output-shape is problematic.
askhade(2021-07-20 17:12:57):Nope only int64 is allowed 
for shape input

From spec:
shape (non-differentiable) : tensor(int64)
Specified shape for output.
gramalingam(2021-07-20 21:12:34):Also ```outputProductValid = false``` here?
askhade(2021-07-20 16:06:23):are you supporting broadcasting? I suggest for simplicity we only support dim_size == 1... the dim size of shape will always be 1 .i.e is 1D tensor
askhade(2021-07-20 16:11:06):this is not correct ... size returns total number of elements... you should iterator over the dims and calculate the number of elements

askhade(2021-07-20 16:11:57):Rama updated this... merge from master?
askhade(2021-07-20 16:14:38):need to handle for negative axis... you can refer to Rama's implementation of data propagator for Shape-15
jcwchen(2021-07-20 21:16:04):After offline discussion, the minor broadcasting here should be valid. I have further considered failed broadcasting if the dim sizes are incompatible
jcwchen(2021-07-20 21:16:58):After offline discussion, the logic here should be good. I have updated Size unit test to prevent confusion.
jcwchen(2021-07-20 21:19:02):Thanks for catching this! I have reused the same logic from `TypeAndShapeInferenceFunction` as a `processSliceInputs` function to handle negative start or end. Also, I added a unit test for testing negative start and end.
jcwchen(2021-07-20 21:29:19):Trying to refactor L32-L72... Too much duplicate
gramalingam(2021-07-20 21:48:39):I think this could potentially confuse the situation where the input is specified (but data-propagation has no value for it) and the case where the input is not specified (and has a default value). May be better to use ```const bool axes_specified = ctx.getNumInputs() >= 4``` instead of  ```axes == nullptr``` down below.
gramalingam(2021-07-20 21:50:20):nit: using ```step > 0``` here may be slightly better and preferable. (Similarly in shape inference method above.)
gramalingam(2021-07-20 21:59:17):Actually, I wonder if we should restrict ourselves to the case where input(0) is 1-dimensional tensor? If it is multidimensional, then it is more complicated.
askhade(2021-07-20 21:59:51):-3 is not a valid value... This is an invalid model... can you change the input shapes so that that values are valid... 

invalid model test case will make sense if we have some logic to catch\handle such errors and we want to test that... 
gramalingam(2021-07-20 22:03:52):I think that in the else case we should return (instead of creating an incorrect OutputData). (While we could produce an error/exception, it may be simpler to not bother about that for now.
gramalingam(2021-07-20 22:04:34):The index value is allowed to be negative to count backwards.
gramalingam(2021-07-20 22:26:23):I think we can handle dim_params slightly better (e.g., if some are known and some are unknown). We should also handle the case where neither dim_param nor dim_value is set. I suggest something like below (also avoiding code duplication):
```cpp
inline void MathOpDataPropagator(DataPropagationContext& ctx, std::string op_type) {
  const auto input_0 = ctx.getInputData(0);
  const auto input_1 = ctx.getInputData(1);
  if (input_0 == nullptr || input_1 == nullptr) {
      return;
  }

  auto size0 = input_0->dim_size();
  auto size1 = input_1->dim_size();

  if ((size0 != size1) && (size0 != 1) && (size1 != 1) {
        fail_shape_inference("Invalid rank for ", op_type, " broadcasting: (",
            size0, ") vs (", size1, ").");
  }

  TensorShapeProto tsp;
  for (int i = 0; i < std::max(size0, size1); ++i) {
    auto& val0 = input_0->dim(size0 == 1 ? 0 : i);
    auto& val1 = input_1->dim(size1 == 1 ? 0 : i);
    auto* result = tsp.mutable_dim()->Add();
    if (val0.has_dim_value() && val1.has_dim_value())
       result->set_dim_value(MathOpTwoIntegers(op_type, val0.dim_value(), val1.dim_value()));
  }

  ctx.addOutputData(0, std::move(tsp));
}
```
jcwchen(2021-07-20 22:30:02):Thank your for the suggestion! I just finished something like this. I will further improve it based on yours.
askhade(2021-07-20 23:18:12):this can be converted in 1 line:
return (val < low) ? low : (val > high) ? high : val;

also I suggest change low/high to min/max for better readability

another suggestion is you can simply use this 1 line instead of creating a inline function
askhade(2021-07-20 23:28:57):agree
askhade(2021-07-20 23:31:39):suggest adding a test with axis -ve
jcwchen(2021-07-20 23:37:25):Done.
jcwchen(2021-07-20 23:38:18):Good catch! Updated. Also, I have applied the same idea to steps.
jcwchen(2021-07-20 23:40:06):Did you mean negative axis for indices? If so, just added. Thanks
jcwchen(2021-07-20 23:40:54):Thanks for catching these 2 issues. Both updated
jcwchen(2021-07-20 23:52:09):We do have this restriction now for every data propagation, right? (only 0D or 1D tensor can be getInputData)
For other N-D tensors (which N > 1), their getInputData() will be nullptr and it will stop data propagation.


gramalingam(2021-07-21 04:11:15):It is okay with me. But if we try to add [1, 2, M] and [3, 4, 6], this will fail to compute anything, whereas we can actually compute [4, 6, ?] (where ? means neither dim_param nor dim_value set) easily enough (as indicated in my code sketch). 
gramalingam(2021-07-21 16:05:32):Hi: I suggest adding a dim in all cases, and setting its value only if both have values, like below:
```cpp
auto* new_dim = tsp.mutable_dim()->Add();
if (input_dim_0.has_dim_value() && input_dim_1.has_dim_value()) {
   new_dim->set_dim_value(
          MathOpTwoIntegers(op_type, input_dim_0.dim_value(), input_dim_1.dim_value()));
}
```
jcwchen(2021-07-21 16:39:36):Update here to test with "?" shape (which means no value and no param because it is not computable) -- if the dim does not have value or parameter, it is still a valid dim.
jcwchen(2021-07-21 20:47:56):Why don't we keep symbolTable* as default nullptr for this function?
snnn(2021-07-26 16:15:03):I'll update this PR shortly and address the comments. 
askhade(2021-07-23 17:36:46):why ON for Linux? 
askhade(2021-07-23 17:37:11):what about Mac?
askhade(2021-07-23 17:38:07):when -DONNX_USE_PROTOBUF_SHARED_LIBS=ON, -Dprotobuf_BUILD_SHARED_LIBS should also be ON
askhade(2021-07-23 17:39:41):when -DONNX_USE_PROTOBUF_SHARED_LIBS=ON, -Dprotobuf_BUILD_SHARED_LIBS should also be ON
snnn(2021-07-23 17:56:22):Because most shared libraries are built without "-fPIC". And this sentence is for people who have both static and shared libraries. In that case, pretty much the static one won't work. 
snnn(2021-07-23 17:57:48):See: https://github.com/onnx/onnx/blob/master/CMakeLists.txt#L51 

Now Protobuf_USE_STATIC_LIBS  is fully controlled by ONNX_USE_PROTOBUF_SHARED_LIBS. Users can't set it.
snnn(2021-07-23 17:58:55):I don't know much about Mac. 
snnn(2021-07-23 18:05:55):Maybe I should change the wording to:
```
What if you have both:
Window: we recommend  you using the static one.
Linux: if your static library was built without "-fPIC"(the most common case), then it can't be used by onnx. If you don't know how it was built, let's assume it doesn't have the flag.  Otherwise we recommend you using the static one.
```
snnn(2021-07-23 18:08:40):Or just remove the line:
~If you have both, then we recommend you setting it to OFF for Windows and ON for Linux.~

jcwchen(2021-07-23 18:12:28):nit typo: also ~**let**~ you instead of also **let's** you
jcwchen(2021-07-23 18:40:03):Shall we add comments to reminder users that `libprotobuf-dev` and `protobuf-compiler` from apt-get would be quite old protobuf versions since we do recommend to use protobuf 3.16.0?
snnn(2021-07-23 19:00:56):They are old but they should work without problems. I think we don't need to mention 3.16.0 specifically. Would you mind help me testing the command on Ubuntu 18.04? Supposedly it should work without problems.  You may need to add "cmake  git " to the apt-get install command line.
jcwchen(2021-07-23 20:39:24):I tested this updated document (`apt-get install python3-pip python3-dev libprotobuf-dev protobuf-compiler`) with Ubuntu 18.04 and the installation went well. Thanks for the update!
askhade(2021-07-23 21:01:24):lets*
askhade(2021-07-23 21:02:32):what I meant is
On line 137 we are asking users to export CMAKE_ARGS="-DONNX_USE_PROTOBUF_SHARED_LIBS=ON"

Then the following instructions should match this setting. One line 148 -Dprotobuf_BUILD_SHARED_LIBS should be ON
askhade(2021-07-23 21:13:51):if the shared lib is built without -fPIC meaning it is NOT position independent then this is a problem right... and in this case using static lib should be the right way
snnn(2021-07-23 21:18:47):Shared lib won't be.
askhade(2021-07-26 04:54:50):Meaning? Are you saying we can assume shared lib will be build with -fPIC becasue this is the default these days? then yes I agree. 
Now I understand the instructions better... earlier I did not quite understand what was the intention.

May be we can change the instructions to: 

What if you have both:
Window: we recommend  you using the static one.
Linux: When building from source we recommend using static library following the instructions in this doc. If using prebuilt libs, we recommend using shared lib.


askhade(2021-08-02 13:47:33):nit: ending* with (there are 2 occurrences)
snnn(2021-08-02 17:56:43):Thank you for point it out. 
jcwchen(2021-07-23 18:55:06):Shall we add some warning message to let users know it is a deprecated function? Especially it only works for TensorProto
jcwchen(2021-07-23 18:44:46):nit: need 2 lines as spaces between two functions to fix flake8 issue. You can also check it (run flake8) in your local
jcwchen(2021-07-23 21:30:10):```suggestion
def make_sequence_value_info(
        name,  # type: Text
        elem_type,  # type: int
        shape,  # type: Optional[Sequence[Union[Text, int, None]]]
        doc_string="",  # type: Text
        elem_shape_denotation=None,  # type: Optional[List[Text]]
):  # type: (...) -> ValueInfoProto
    """Makes a Sequence[Tensors] ValueInfoProto based on the data type and shape."""
    print("Warning: make_sequence_value_info will be deprecated in ONNX 1.11.0.",
        "Please now use make_tensor_sequence_value_info for TensorProto sequence. ")
    make_tensor_sequence_value_info(name, elem_type, shape, doc_string, elem_shape_denotation)
```
TomWildenhain-Microsoft(2021-07-23 21:40:31):Python warnings might be better: https://docs.python.org/dev/library/warnings.html#module-warnings
TomWildenhain-Microsoft(2021-07-23 21:41:08):Deprecation module will do it all automatically: https://deprecation.readthedocs.io/en/latest/#api-documentation
rajeevsrao(2021-07-23 21:52:56):@TomWildenhain-Microsoft would prefer not to introduce another package dependency (deprecation) and will use the python warnings instead.
@jcwchen I'll update the PR - should we mark this for removal in 1.11.0 or honor the semver guarantee and hold on to it until 2.0.0?
jcwchen(2021-07-23 22:39:50):I personally vote for 1.11.0 since I even don't know when ONNX 2.0.0 will be and we don't want users to rely on old deprecated functions for a longer unexpected time.
rajeevsrao(2021-07-23 22:51:59):Given the lack of consensus and clarity on the ONNX API deprecation schedule/policy I updated the warning to: 
"Deprecated in ONNX v1.10.0; `onnx.helper.make_sequence_value_info` alias will be removed in an upcoming release."
CLAassistant(2021-07-23 23:52:20):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=3604) <br/>All committers have signed the CLA.
lgtm-com[bot](2021-07-24 00:20:23):This pull request **introduces 1 alert** when merging 70aefc2c5c3e9964fe93177da53384bc435aa78c into a57bc99daa6ddeef2ad535f8f78d1847f57216f0 - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-9183af51001efadf098cdc75c3f306de7ab789d4)

**new alerts:**

* 1 for Implicit string concatenation in a list
lgtm-com[bot](2021-07-26 22:08:20):This pull request **fixes 1 alert** when merging 12a826b22144666f4b89642f13550b08433dea24 into a57bc99daa6ddeef2ad535f8f78d1847f57216f0 - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-a2460ca62919ab8a3c42c94ebafbe4f22dfc8bc0)

**fixed alerts:**

* 1 for Unused import
lgtm-com[bot](2021-07-26 23:58:24):This pull request **fixes 1 alert** when merging b36c23d9949896d04bc830e5cb6fd1f5ea9866b4 into a57bc99daa6ddeef2ad535f8f78d1847f57216f0 - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-141b41e60bfe360af1d779038f4f7daa223671d3)

**fixed alerts:**

* 1 for Unused import
lgtm-com[bot](2021-07-27 01:43:00):This pull request **fixes 1 alert** when merging 45b499369266b17fe3e08ac30d6e605c83e79b56 into a57bc99daa6ddeef2ad535f8f78d1847f57216f0 - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-95a497b5551d504dde6bdfa7ab12f28b9fcdc59f)

**fixed alerts:**

* 1 for Unused import
lgtm-com[bot](2021-07-27 02:05:15):This pull request **fixes 1 alert** when merging 394c4cb8561f5b36cb90b81bfb0aef9d7df57360 into a57bc99daa6ddeef2ad535f8f78d1847f57216f0 - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-c3674f67703e7d902451af2187e8ed07e3fe915e)

**fixed alerts:**

* 1 for Unused import
lgtm-com[bot](2021-07-27 18:03:57):This pull request **fixes 1 alert** when merging 833d4f548f496cb6b6f5b900012b24dcee035c9f into a57bc99daa6ddeef2ad535f8f78d1847f57216f0 - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-e3b8044c551992962a991a224d1ec69b750f4cde)

**fixed alerts:**

* 1 for Unused import
lgtm-com[bot](2021-08-18 20:10:04):This pull request **fixes 1 alert** when merging 00b846e378bd1b62c8ef9360675d9863faf9176a into c24cd429c4ab47d2b057da8174788c39c60f8760 - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-d8eb56be0f13c79cab0e6040cc4085275b43e867)

**fixed alerts:**

* 1 for Unused import
lgtm-com[bot](2021-09-01 21:54:18):This pull request **fixes 1 alert** when merging fad6234b3500701196bac7db5a350e12490465f9 into 5a06641c84939a330ed15475f162e42b4a3d717a - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-8353f1693356b620165f4771d74e2ef9011317b4)

**fixed alerts:**

* 1 for Unused import
stillmatic(2021-09-01 22:21:21):@jcwchen Thanks for taking a look (and reminding me!) 

I rebased this PR off of the main branch and it looks like tests are now passing. Please LMK what else would be helpful to do! 
lgtm-com[bot](2021-09-02 16:54:52):This pull request **fixes 1 alert** when merging c118a1e6d30ac794f28adedc982eb1398bbf8a66 into 2f67aeac9ce46e0200623f9bf8ad2dd0e952d3ed - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-a9b81685c7f76154809eec8c6b7705eefaee2d0a)

**fixed alerts:**

* 1 for Unused import
lgtm-com[bot](2021-09-02 18:24:04):This pull request **fixes 1 alert** when merging 3104e90c0012987647cf18b2436f0c918d63aa4c into 2f67aeac9ce46e0200623f9bf8ad2dd0e952d3ed - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-700dc9a5803fe09a12a14cd99371d310ffc7b50d)

**fixed alerts:**

* 1 for Unused import
stillmatic(2021-09-02 18:55:14):Thanks for the catch - removed all references to six
lgtm-com[bot](2021-09-02 20:19:31):This pull request **fixes 1 alert** when merging 7539d6c4dc47b9d4ae24870e4bb4fd5eef7512b8 into 2f67aeac9ce46e0200623f9bf8ad2dd0e952d3ed - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-7ff58104722bb3f62578ba300cd69a49fa911717)

**fixed alerts:**

* 1 for Unused import
lgtm-com[bot](2021-09-07 19:40:40):This pull request **fixes 1 alert** when merging d575c26754a0c3d0ac6651dbb91b8472c8cdad46 into 9f978e1ffbd4ba29263f9fdae592e2256b80ec2a - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-1b4f0ebb634746ea32310ab89bde1576aafbf9e1)

**fixed alerts:**

* 1 for Unused import
lgtm-com[bot](2021-09-16 16:53:21):This pull request **fixes 1 alert** when merging 45efa9f07e4e7eac5529df26ed2d9d0408042a79 into a5e7ee51176bf78a60c118758174e13d85a87b46 - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-2ac4a35734ac6375007d916857cbc114b56b2210)

**fixed alerts:**

* 1 for Unused import
stillmatic(2021-10-25 21:02:02):@jcwchen looks like I need to rebase, but anything else I can do to help put this over the line?
jcwchen(2021-10-26 04:09:48):@stillmatic I just helped to resolve the conflict. Thanks for the reminder!
lgtm-com[bot](2021-10-26 04:18:56):This pull request **fixes 1 alert** when merging 4d31c0e179797a48d0bf8329cd885387b5411b16 into d0151d78c2ba53c3c71d05608b1d892ada20507d - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-4a0742e214cd4f87201875e3f338acb52092b969)

**fixed alerts:**

* 1 for Unused import
lgtm-com[bot](2021-12-06 17:35:47):This pull request **fixes 1 alert** when merging 0a7d1e51b7db6a6e6a7e2e11359846eef1ac3652 into fa6f8cfdce3d86346e8a7494f3062b98416c85fb - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-69ec065dd01785196aacc9953d0802734620b23e)

**fixed alerts:**

* 1 for Unused import
lgtm-com[bot](2022-01-08 05:27:20):This pull request **fixes 1 alert** when merging 33db05aa6584354d4312e4611e8ed05da52179f3 into 60531231618431a480f2b7b18ee94763829b2c3e - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-b480b57c07c214b06fc07710d6fefccb5a559e82)

**fixed alerts:**

* 1 for Unused import
lgtm-com[bot](2022-01-08 06:15:25):This pull request **fixes 1 alert** when merging 8d5eb62d5299f6dcb6ac787f0ea8e6cf5b8331a7 into 60531231618431a480f2b7b18ee94763829b2c3e - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-07777744167034bcbe461a723507be023b340776)

**fixed alerts:**

* 1 for Unused import
jcwchen(2022-01-20 04:52:21):Superseded by https://github.com/onnx/onnx/pull/3939 and https://github.com/onnx/onnx/pull/3926. ~Close this one.~
jcwchen(2022-01-20 04:58:11):Now ONNX has removed six https://github.com/onnx/onnx/pull/3926 and used a newer mypy version (0.760) https://github.com/onnx/onnx/pull/3926. Reopen it since actually this one is for upgrading to the latest mypy. (Or feel free to close this one and submit a new clean PR for it). Thank you for all the contribution!
stillmatic(2022-01-20 06:21:14):thanks @jcwchen  -- let's close this and track followups separately. most of the valuable pieces from this PR have been broken out now :) 
garymm(2022-01-22 01:16:23):Is there an issue or open PR for updating mypy?

Using the version that's currently in setup.py (0.760) I see this error:
```
➜  python setup.py typecheck
running typecheck
onnx/__init__.py:30: error: syntax error in type comment
```

I think this may be a mypy bug that's fixed in a later version.

jcwchen(2022-01-22 01:40:37):> Is there an issue or open PR for updating mypy?

There isn't yet. @stillmatic Do you still have bandwidth to propose a new PR for it? Thank you!

> I think this may be a mypy bug that's fixed in a later version.

@garymm Are you talking about the error in this PR https://github.com/onnx/onnx/pull/3962 ? If so, yes I think latest mypy should support Python type annotation according to the document: https://mypy.readthedocs.io/en/stable/type_inference_and_annotations.html.
stillmatic(2022-01-22 02:04:08):I will open an issue to track the work but likely won't have much more bandwidth for implementation: https://github.com/onnx/onnx/issues/3964

Hmm - is the type check path not part of the CI tests? I also get a different error when running that command.

```
~/Development/onnx  main ✔                                                                     1h6m  ⍉
▶ ~/Development/onnx  main ✔                                                                      1h6m
▶ mypy --version
mypy 0.760
(onnx)
~/Development/onnx  main ✔                                                                      1h6m
▶ python setup.py typecheck
/Users/hua/.pyenv/versions/onnx/lib/python3.8/site-packages/setuptools/installer.py:27: SetuptoolsDeprecationWarning: setuptools.installer is deprecated. Requirements should be satisfied by a PEP 517 installer.
  warnings.warn(
running typecheck
tools/protoc-gen-mypy.py:122: error: Missing type parameters for generic type "Generator"
Found 1 error in 1 file (checked 245 source files)
(onnx)
```

From googling a bit, this may be related to Python version. I am on 3.8.12 -- are you on 3.9?
garymm(2022-01-22 02:10:54):@jcwchen 
>  Are you talking about the error in this PR #3962 ?

No, this is on tip of main with no changes.

@stillmatic I'm using python 3.9.

I guess `python setup.py typecheck` fails in multiple ways on tip of main, depending on python version used. Not sure why this isn't caught by CI checks.

@jcwchen should I open another issue to track the current failures?
jcwchen(2022-01-24 04:32:40):> @jcwchen should I open another issue to track the current failures?

Yes, that would be great. Thanks! Actually current CIs are only using Python 3.6 and 3.7 (no 3.8 and 3.9) for typecheck. I tried to run 3.8 and 3.9 in CIs and bumped into similar errors to yours. We should add Python 3.8 and 3.9 in AzurePipeline as well to cover this. I will pick it up. Thank you both for catching this!
jcwchen(2021-12-06 19:34:51):nit: sync all `# type: ignore` with the same space (there are 4 places)
```suggestion
        for items_map in sorted(self._filtered_test_items.values()):  # type: ignore
```
Thank you for the update! mypy upgrade seems OK for ONNXRUntime as well. I hope we can forward this PR soon.

One question: Do you know why the same function here need to be ignored for the updated mypy?
CLAassistant(2021-07-25 00:35:24):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=3605) <br/>All committers have signed the CLA.
askhade(2021-07-27 20:22:17):have you tested whether shape inference and checker works for these ops? A simple test can be done by using the existing test framework to generate a test model with optional outputs and run shape inference and checker on this model
neginraoof(2021-08-24 23:14:13):> have you tested whether shape inference and checker works for these ops? A simple test can be done by using the existing test framework to generate a test model with optional outputs and run shape inference and checker on this model

Yes, we have an exported model passing checker using these ops. Shared the example T5 with you and Hari.
neginraoof(2021-08-26 18:24:02):@gramalingam @askhade Thanks for your review.
@postrational Please provide your feedback, thanks.
hariharans29(2021-07-27 18:13:53):For the target models to be exported, can the constraint that scan output needs to be tensors still hold true ? Or should it possibly support optional(tensors) as well ? If it can support optional (tensors), the spec probably needs some update as to how 'None' is dealt with while concatenating the scan outputs (ignored, I assume).
hariharans29(2021-07-27 18:19:21):Should there be an 'else' that says propagating non-tensors and non-sequence optionals is unsupported currently ? Otherwise the elem_type of the output is undefined/dangling isn't it ? (This call is currently only invoked from code paths where-in we are not likely to see non-tensor/non-sequence optionals, but this seems like a generic helper...) 
gramalingam(2021-07-27 20:06:57):It may be helpful to extract lines 285 to 301 or so as a utility function ```ClearShape(TypeProto&)```. If the function is made recursive, it can handle arbitrary types a bit more easily. (Will help in the future if we expand the types even more).
gramalingam(2021-07-27 20:17:35):I think this is more specialized/complex than it needs to be. I think it is better if the utility function works for arbitrary optional types. Further, the extra checks on the output-type is unnecessary. (I realize that this comes from the template used for other types, but it would be useful to start cleaning this up for the new code at least.) It should basically be something like "*outputType = inputType" (or as required by the protobuf generated classes to copy/clone).
gramalingam(2021-07-27 20:19:04):I think this include file is becoming too large. It would be better to have just the interface/signature in the shape_inference.h file and move implementation to shape_inference.cc
askhade(2021-07-27 20:29:56):+1

askhade(2021-07-27 20:31:00):this code only allows Seq[Tensors] is Seq[Seq[Tensor]] not valid?
neginraoof(2021-08-19 21:36:00):Scan outputs are very special, and they can be handled with sequence type inputs instead. I don't think extending the scan output type is necessary.
neginraoof(2021-08-19 22:54:37):I refactored and deleted some part of the code. I think the code looks simpler now.
hariharans29(2021-08-20 04:01:21):Agreed. I see that even when sequence type support was added for Loop, scan outputs were still just limited to tensors only. I think we can follow the same pattern for now.
neginraoof(2021-08-23 17:59:47):@askhade  I updated the optional type propagation for optional and sequence types. This should work for nested types as well now.
gramalingam(2021-08-24 18:25:14):Is this supposed to be ver16?
neginraoof(2021-08-24 20:39:01):Thanks, yes this was missed in the updated.
askhade(2021-07-28 20:25:39):Current PR does not explicitly select the MSVC option MT\MD and based on the documentation (https://cmake.org/cmake/help/latest/prop_tgt/MSVC_RUNTIME_LIBRARY.html) default is MD whereas for ONNX we need MT  as default. 
snnn(2021-07-28 20:38:27):Both MD/MT work well in most of cases. If you want to make the default to MD, I think we can put such things in setup.py: detect if current os is windows. If it is true, then add "-DCMAKE_MSVC_RUNTIME_LIBRARY=xxx" to the build flags.  
askhade(2021-07-29 20:24:12):Yes please add CMAKE_MSVC_RUNTIME_LIBRARY build flag to make sure the default settings dont change
lgtm-com[bot](2021-07-30 01:05:25):This pull request **fixes 1 alert** when merging 19e15347fa2857435455e3927f02ed25a08a43bf into 04bb946bc2dcc490266e36b810b00dce16b1edd4 - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-80ac4af2b5598f63200058f654fe9b81958ae598)

**fixed alerts:**

* 1 for Inconsistent definition of copy constructor and assignment \(&#39;Rule of Two&#39;\)
jcwchen(2021-07-30 01:17:09):Thanks for the fix! Let me reopen it to trigger release CIs.
jcwchen(2021-07-30 02:47:32):I saw it's targeting the main branch. Will it be included in ONNX 1.10.0?
rajeevsrao(2021-07-30 02:50:43):> I saw it's targeting the main branch. Will it be included in ONNX 1.10.0?

I can cherry-pick when updating the version on rel-1.10.0 branch.
askhade(2021-07-30 05:41:00):@jcwchen we should find out why this was not caught in the CIs... We do build with ONNX_WERROR=ON
lgtm-com[bot](2021-07-30 18:03:54):This pull request **fixes 1 alert** when merging e4eb7f0d05f8e31e6923bd80a9358061600b99b9 into 5d9ba9b6db8f35a15654b169928e6dacd95d83be - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-dd2c15233734bd5c70eaeb6d38c2b5c322b25b81)

**fixed alerts:**

* 1 for Inconsistent definition of copy constructor and assignment \(&#39;Rule of Two&#39;\)
askhade(2021-08-02 16:24:04):A few more files need to be updated:
Versioning.md and helper.py they include the version number too.

jiafatom(2021-08-03 01:34:17):@askhade @wschin @gramalingam Could you please take a look? Thanks
cc @fdwr
fdwr(2021-08-03 02:08:20):@jiafatom: This is great - thanks.

Regarding the coordinate transformation mode, we had [discussed](https://github.com/onnx/onnx/pull/3429#issuecomment-825076299) using the same attribute name as `Resize` for consistency `coordinate_transformation_mode`, but only supporting two modes (proper `half_pixel` and the old output-only `output_half_pixel`). We should not just copy attributes and names from other API's without holistically considering existing ONNX precedent.

Additionally, for anyone opting into the newer opset, they would get the correct behavior *by default*, just like `Resize` defaulted to the correct `half_pixel` behavior in later opsets (convertors could still convert to the old mode using the old enum value). I'd like to see that here - let's not default to broken behavior.
jiafatom(2021-08-03 03:50:49):> @jiafatom: This is great - thanks.
> 
> Regarding the coordinate transformation mode, we had [discussed](https://github.com/onnx/onnx/pull/3429#issuecomment-825076299) using the same attribute name as `Resize` for consistency `coordinate_transformation_mode`, but only supporting two modes (proper `half_pixel` and the old output-only `output_half_pixel`). We should not just copy attributes and names from other API's without holistically considering existing ONNX precedent.
> 
> Additionally, for anyone opting into the newer opset, they would get the correct behavior _by default_, just like `Resize` defaulted to the correct `half_pixel` behavior in later opsets (convertors could still convert to the old mode using the old enum value). I'd like to see that here.

Thanks for your reply. 
~~I feel that this current PR is enough for `RoiAlign`.
I am from converter perspective. The onnx `RoiAlign` is in sync with torchvision `roi_align`, and only pytorch has this `roi_align`. So the only use case is that people convert a pytorch model to onnx model. And people usually want to see the correspondence between pytorch `RoiAlign` operator and the onnx one. There is no `coordinate_transformation_mode` in torch `roi_align`, they just have `aligned` there, and we can have a direct correspondence there. So I prefer to use `aligned`.
`Resize` (with `CropAndResize`) are different. Although `tf.image.crop_and_resize` (onnx `Resize`) does something similar as `RoiAlign`, but the algorithm is completely different, so it is better not to unify them.
About the default behavior. pytorch default is the legacy implementation, and we just use same default value for consistency. There is no use case to construct an onnx `RoiAlign` directly.~~

I just change the attribute name to `coordinate_transformation_mode`, supporting two modes (proper `half_pixel` and the old output-only `output_half_pixel`), and default behavior is the aligned one `half_pixel`. Thanks.

Just synced offline with @fdwr, this issues is addressed.
jiafatom(2021-08-03 22:45:50):@postrational Could you please review this PR as well? Thanks!
askhade(2021-08-04 20:42:30):LGTM
jcwchen(2021-08-05 17:34:44):nit suggestion: Since RoiAlign-16 default behavior has changed from RoiAlign-10, it would be helpful to avoid confusion if there is also a note about the behavior difference in the old document for the old op (RoiAlign-10).
jiafatom(2021-08-05 18:24:50):> nit suggestion: Since RoiAlign-16 default behavior has changed from RoiAlign-10, it would be helpful to avoid confusion if there is also a note about the behavior difference in the old document for the old op (RoiAlign-10).

synced offline, issue addressed.
jiafatom(2021-08-05 23:07:55):Did the version conversion test locally. Just have a model (opset 15) with RoiAlign. When I run the model conversion (target opset 16), it outputs the model with attribute `coordinate_transfomation_mode="output_half_pixel"`. So it works.
gramalingam(2021-08-03 20:31:53):From what I understand in the discussion, this is not a compatible extension. Don't we have to use a non-default value for the new attribute to get the old behavior? If so, the adapter should add that attribute to the node.
jcwchen(2021-08-03 20:47:52):Sorry that I don't have the context here... So previously RoiAlign-10's behaviour is same as using output_half_pixel instead of half_pixel (default for RoiAlign-16). Then yes please add a new adapter for it. Thanks @gramalingam for catching this!
gramalingam(2021-08-03 20:50:44):How about rewording it as below:
```
The coordinate transformation mode. Allowed values are 'half_pixel' and 'output_half_pixel'.
Use the value 'half_pixel' to pixel shift the input coordinates by -0.5 (the recommended behavior).
Use the value 'output_half_pixel' to omit the pixel shift for the input (use this for a
backward-compatible behavior).
```

jiafatom(2021-08-03 21:06:03):Added an adapter `RoiAlign_15_16`.
gramalingam(2021-08-03 21:09:02):"Upsample" looks like a copy-paste error. Is it supposed to be "RoiAlign"?
jiafatom(2021-08-03 21:10:29):done
askhade(2021-08-04 20:38:54):nit: ver10
CLAassistant(2021-08-03 03:30:18):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=3626) <br/>All committers have signed the CLA.
askhade(2021-08-03 05:00:26):Looks like you did update the Upsample adapters. Remove the comment "The code here are not updated, because it is not common to convert Opset version on an arm32 device, and so far the crash is only in int64_t but not float" from the description ?
guoyu-wang(2021-08-03 05:43:01):> Looks like you did update the Upsample adapters. Remove the comment "The code here are not updated, because it is not common to convert Opset version on an arm32 device, and so far the crash is only in int64_t but not float" from the description ?

These was pushed accidentally, but changed to use ParseData, so these are changed as well
askhade(2021-08-03 04:35:39):can you use ParseData here?
askhade(2021-08-03 04:55:57):same as above if you use ParseData it will replace the entire if else here
skottmckay(2021-08-03 06:35:35):Would be good to capture why we do this in a comment so it isn't accidentally removed in the future. 
snnn(2021-08-14 02:52:38):@gwang-msft
The culprit is reinterpret_cast, which is a warn sign. 
Your change is good, but the reinterpret_cast is unnecessary.
guoyu-wang(2021-08-14 03:00:20):Yes, the reinterpret_cast here is redundant, can remove in a future change
lgtm-com[bot](2021-08-17 18:46:21):This pull request **introduces 2 alerts** when merging f88927ab90e1c6a17cdbab6c039e589aecc4414e into c24cd429c4ab47d2b057da8174788c39c60f8760 - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-b9d7aca5c1b7d31091f7238c558a7fb02c07edbe)

**new alerts:**

* 2 for Comparison result is always the same
askhade(2021-09-09 18:47:26):I think we should convert "set_external_data" to private method
jcwchen(2021-09-13 21:27:46):> I think we should convert "set_external_data" to private method

According to our offline discussion, there might be existing users using it... (e.g., ONNXRuntime) ONNX should keep it as public function at this moment.
jcwchen(2021-08-11 04:15:30):Remove this test for good because in some environments loading model > 2GB will crash and it cannot be tested.
askhade(2021-08-25 15:57:28):do we really need this? 
askhade(2021-09-08 16:03:49):not sure why we need this warning? This is confusing... we should never have both
askhade(2021-09-08 16:05:21):Why do we land in this situation? Maybe we are not clearing the raw_data field after serializing the raw data to external data... we should fix that
askhade(2021-09-09 17:45:48):why is this change part of this PR? bad merge?
askhade(2021-09-09 18:03:28):I suggest you check in the following order
If (ExternalData) { raise error}
Else if (RawData) {return raw data}
Else {return typed data}

External data should be checked based on data type != undefined and data location == external
Raw data should be checked using data type != undefined and has_raw_data() == true


askhade(2021-09-09 18:10:00):some error checks here would be good... you can calculate expected size based on dims and then check it matches the <type>_data_size
askhade(2021-09-09 18:12:41):May be change to something like: Ideally TensorProto should not hold any raw data but if it does it will be ignored.
askhade(2021-09-09 18:53:38):add some comments explaining why you are checking for uses_external_data. It is not clear
askhade(2021-09-09 18:55:48):where will the second save, save the data? In a different file or same as original?

jcwchen(2021-09-10 01:48:57):Good point. Added. IIUC, it's possible that the tensor's dims is empty, right? If so, the check will simply skip here: `tensor_proto->dims_size() != 0`
jcwchen(2021-09-10 01:49:56):I think it's my typo. I corrected it to use model for second save. Thanks
jcwchen(2021-09-10 01:53:15):The enhanced size check caught this error written by me
jcwchen(2021-09-10 01:54:05):After offline discussion, remove this warning
jcwchen(2021-09-10 01:57:58):After offline discussion, remove this warning because the override behavior is obvious when using load_external_data_for_tensor. Added a comment in the function instead
jcwchen(2021-09-10 01:58:20):Yes.. reverted
jcwchen(2021-09-10 02:01:34):I put checking defined data type in the first condition to simplify the cases here... (I assumed all valid tensors should have defined data type) Please take a look at these new if conditions. Thank you!
askhade(2021-09-13 15:58:53):more elaborate would be nice - something like
Writing to external data happens in 2 passes. In first pass tensors with raw data which pass the necessary conditions (size threshold etc) are marked for serialization. In the second pass, the raw data in these tensors is serialized to a file. This is the second step. Serialize if tensor has raw data and it was marked for serialization. 
askhade(2021-09-13 16:22:05):do you have a test where both saves will save some tensors as external data? this will validate whether the write to external file during second save is done properly (meaning offsets are honored etc)

During the second save you can turn on attribute serialization to external data to make the test easy
jcwchen(2021-09-13 21:24:06):Updated. Thanks
jcwchen(2021-09-13 21:25:47):Just added a test `test_save_model_with_external_data_and_threshould` to test saving a model with typical tensor and external tensor.
askhade(2021-09-14 19:23:06):rephrase error message as "Data size mismatch. Tensor: <name> expected size <expected_size> does not match the actual size <data.size()>"
askhade(2021-09-14 19:24:33):no need for this comment now that you added detailed comments on line 259
askhade(2021-09-14 19:25:47):nit: rename test_save_model_w/o_loading_external_data
askhade(2021-09-14 19:27:32):nit: larger
askhade(2021-09-14 19:29:37):size_threshold is still 1024 -> 0?
askhade(2021-09-14 19:30:02):looks like copy paste error - here you are testing w_small is serialized right?
askhade(2021-09-14 19:31:25):nit: rename : test_save_model_with_external_data_multiple_times (dont think threshold is necessary here) the test checks whether multiple saves can work
askhade(2021-09-14 19:32:46):suggest testing that external data can be loaded for these tensors... this will verify the file writes did not mess anything...
askhade(2021-09-14 19:35:37):see if you can move the model creation code to a utility function and call it from all these tests... atleast a couple of tests can share the same model 
jcwchen(2021-09-14 20:25:35):to_array has already loaded the external tensors here. I thought it should be covered?
jcwchen(2021-09-14 20:28:09):Yes. I should use `w_small_tensor = model_without_loading_externall.graph.initializer[0]` here. Updated
askhade(2021-09-14 20:32:06):aah my bad... I missed it... looks good then
jcwchen(2021-09-14 23:08:23):I have made these 3 tests (test_reshape_inference_with_external_data_fail, test_to_array_with_external_data, test_save_model_with_external_data_multiple_times) using the same model, which simplifies the code. Thanks for pointing out.
askhade(2021-08-25 19:48:34):Please update Syntax.md as well to include syntax for adding in model functions
gramalingam(2021-08-25 21:09:54):> Please update Syntax.md as well to include syntax for adding in model functions

Done, thanks.
askhade(2021-08-25 19:45:57):Can you also add a test for nested functions
gramalingam(2021-08-25 21:13:24):A call to a function looks no different from a call to an op (from the syntax/parser perspective). The difference appears only when the checker is invoked, which ensures that these calls can be properly resolved, etc. So, I feel that a test for this is not relevant for the parser. It is more appropriate to add it to a test of the checker functionality.
gramalingam(2021-08-26 15:22:03):Extended test-case to incorporate multiple functions.
snnn(2021-08-11 20:31:36):@BowenBao could you please help validate this package? 
askhade(2021-08-11 20:51:31):@jcwchen : You mention for ONNX <= 1.6 this was not a issue because onnx was not linking to msvc runtime libs statically but actually it was see: https://github.com/onnx/onnx/blob/rel-1.6.0/setup.py#L176

IMO this is a good change because we have run into other issues as well because of static linking (like when onnx throws exceptions we see crashes in some situations) plus other benefits as well.

I don't know the exact rationale behind choosing static linking but it could be that if we link statically then it is contained within onnx binaries and users don't have to worry about installing\no versioning issue etc... 
askhade(2021-08-11 20:52:45):Please update other files like README.md and setup.py
jcwchen(2021-08-11 21:25:45):(Updated comment)

> @jcwchen : You mention for ONNX <= 1.6 this was not a issue because onnx was not linking to msvc runtime libs statically but actually it was see: https://github.com/onnx/onnx/blob/rel-1.6.0/setup.py#L176

Thank you for catching this! I wasn't aware that setup.py has been updated about USE_MSVC_STATIC_RUNTIME after ONNX 1.6.

> Please update other files like README.md and setup.py

Both updated. Thanks!
jcwchen(2021-08-11 21:42:15):One question: Is it OK to let ONNX change the default value in setup.py? Some users might already build ONNX in their project and always use the default value for USE_MSVC_STATIC_RUNTIME. They will possibly bump into issue if they use the updated ONNX due to this change (Although it will be an easy fix to them).

If not, definitely we can just set it as 0 in release CIs and keep it as 1 by default.
BowenBao(2021-08-27 21:33:58):@snnn @jcwchen validated locally with wheel from this PR that the crash issue is resolved. Thanks!
snnn(2021-10-04 17:44:06):Let me add a summary:  the static one would work if no C++ exception came across DLL boundaries. Who threw an exception, who must process it. You can't throw an C++ exception from ONNX to pytorch. If ONNX needs to tell pytorch that something went wrong,  it should use error code, returned values or python exceptions instead of C++ exceptions. 

jcwchen(2021-08-11 21:21:21):It's not related to the main issue in this PR, but I think it would be better if we can sync how to set config variable to prevent confusion. I thought DEBUG=0 should build a release build yet it did not..
guoyu-wang(2021-08-12 02:39:12):Cannot fix the DCO failure, closed this one and opened a new one #3647 
askhade(2021-08-12 04:21:33):add gen_onnx_proto? If not why?
guoyu-wang(2021-08-12 05:16:09):`gen_onnx_proto` is already the dependency of `gen_onnx_operators_proto` and `gen_onnx_data_proto`, so it is an implicit dependency of `onnx_proto`
lgtm-com[bot](2021-08-17 22:09:40):This pull request **introduces 1 alert** when merging dae42a173fb53c56161081d6fdfc27de14b61e83 into c24cd429c4ab47d2b057da8174788c39c60f8760 - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-3102595534bb005a9db393d002a073dc967f8588)

**new alerts:**

* 1 for Module is imported with &#39;import&#39; and &#39;import from&#39;
lgtm-com[bot](2021-08-18 00:49:27):This pull request **introduces 1 alert** when merging d545a050387bcfcc69346ff93948bb7f5e42cf17 into c24cd429c4ab47d2b057da8174788c39c60f8760 - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-f691a693df5059fc833ea184286550f60fa72620)

**new alerts:**

* 1 for Unused local variable
lgtm-com[bot](2021-08-18 17:41:07):This pull request **introduces 1 alert** when merging ef6420d66620e745a27ee1caebd9b2e13a96daaa into c24cd429c4ab47d2b057da8174788c39c60f8760 - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-7572408694303573a36f4a5b5515f71337441a3f)

**new alerts:**

* 1 for Unused import
mhamilton723(2021-08-18 18:28:06):> Good idea. However, I don't think it's a good idea to add this kind of tool into ONNX repo. Instead, it may be added into onnx/model or onnx/tools (if it's there).

Thanks for the feedback @linkerzhang, some of the other folks in the community suggested we put this here because onnxmltools is just for converters. Do you have an email that I could loop into this discussion thread I just started? I can see motive for both and just want to ensure the community is aligned on what we should do.
linkerzhang(2021-08-19 05:25:20):> > Good idea. However, I don't think it's a good idea to add this kind of tool into ONNX repo. Instead, it may be added into onnx/model or onnx/tools (if it's there).
> 
> Thanks for the feedback @linkerzhang, some of the other folks in the community suggested we put this here because onnxmltools is just for converters. Do you have an email that I could loop into this discussion thread I just started? I can see motive for both and just want to ensure the community is aligned on what we should do.

my email is, linkerzhang@yeah.net.

How about onnx/model then? 
askhade(2021-08-19 15:44:49):The advantage of adding this to onnx main repo is it can be part of the onnx release package and then users can simply import onnx.hub and get models... There is no release package for onnx\models ... 
mhamilton723(2021-08-20 17:10:27):> > > Good idea. However, I don't think it's a good idea to add this kind of tool into ONNX repo. Instead, it may be added into onnx/model or onnx/tools (if it's there).
> > 
> > 
> > Thanks for the feedback @linkerzhang, some of the other folks in the community suggested we put this here because onnxmltools is just for converters. Do you have an email that I could loop into this discussion thread I just started? I can see motive for both and just want to ensure the community is aligned on what we should do.
> 
> my email is, [linkerzhang@yeah.net](mailto:linkerzhang@yeah.net).
> 
> How about onnx/model then?

We brought up this discussion in the steering committee and they mentioned onnx/onnx was the way to go to make it simple for folks to get started. onnx/model doesn't have any python package so it would be a big infra change and also less desireable from a easy user experience
linkerzhang(2021-08-23 01:27:56):> > > > Good idea. However, I don't think it's a good idea to add this kind of tool into ONNX repo. Instead, it may be added into onnx/model or onnx/tools (if it's there).
> > > 
> > > 
> > > Thanks for the feedback @linkerzhang, some of the other folks in the community suggested we put this here because onnxmltools is just for converters. Do you have an email that I could loop into this discussion thread I just started? I can see motive for both and just want to ensure the community is aligned on what we should do.
> > 
> > 
> > my email is, [linkerzhang@yeah.net](mailto:linkerzhang@yeah.net).
> > How about onnx/model then?
> 
> We brought up this discussion in the steering committee and they mentioned onnx/onnx was the way to go to make it simple for folks to get started. onnx/model doesn't have any python package so it would be a big infra change and also less desireable from a easy user experience

OK. Thanks for clarification!
memoryz(2021-08-23 02:29:49):> Interesting... 0 file changed?

Sorry my mistake - I just fixed the problem. Please check again.
mhamilton723(2021-08-23 17:47:46):@askhade and @linkerzhang Ready for your review, thanks for your help! 
mhamilton723(2021-08-25 15:22:52):@linkerzhang Would it be possible to approve/review? We cant merge with the open request for changes. Thanks so much!
mhamilton723(2021-08-25 15:23:08):> Thank you for proposing this! Although the API looks quite easy-to-use, shall we add a brief document in ONNX about how to use it?

Yes I will add this in a follow up PR!
askhade(2021-08-26 20:40:26):CIs are failing because of flake8 errors:
onnx/hub.py:233:19: F523 '...'.format(...) has unused arguments at position(s): 0, 1
onnx/hub.py:239:1: W391 blank line at end of file
onnx/test/hub_test.py:79:1: W391 blank line at end of file
flake8 returned failures
memoryz(2021-08-26 20:56:32):> CIs are failing because of flake8 errors:
> onnx/hub.py:233:19: F523 '...'.format(...) has unused arguments at position(s): 0, 1
> onnx/hub.py:239:1: W391 blank line at end of file
> onnx/test/hub_test.py:79:1: W391 blank line at end of file
> flake8 returned failures

Looking...
memoryz(2021-08-26 21:41:42):> CIs are failing because of flake8 errors:
> onnx/hub.py:233:19: F523 '...'.format(...) has unused arguments at position(s): 0, 1
> onnx/hub.py:239:1: W391 blank line at end of file
> onnx/test/hub_test.py:79:1: W391 blank line at end of file
> flake8 returned failures

Fixed.
memoryz(2021-08-18 09:09:47):Case insensitive search. It would be better to support fuzzy matching based on Levenshtein distance, but case insensitive matching should be good for now.
```suggestion
        return [
            m
            for m in manifest
            if len(set(map(str.lower, m["metadata"]["tags"])).intersection(set(map(str.lower, tags)))) > 0
        ]
```
memoryz(2021-08-18 09:28:25):For lfs, the download URL should be media.githubusercontent.com/media. See "download_url" field in [link](https://api.github.com/repos/onnx/models/contents/vision/classification/resnet/model/resnet101-v1-7.onnx) for example. And "raw=true" is no longer needed for this URL.
```suggestion
        return "https://media.githubusercontent.com/media/{}/{}/{}/".format(repo_owner, repo_name, repo_branch)
```
mhamilton723(2021-08-18 19:07:30):Tried this and unfortunately this 404s
mhamilton723(2021-08-18 19:07:37):Done
memoryz(2021-08-18 19:29:32):That's weird. I tried "https://media.githubusercontent.com/media/mhamilton723/models/onnx-hub/vision/classification/resnet/model/resnet101-v1-7.onnx" and it's giving me the payload. Let me play with the code a bit more.
mhamilton723(2021-08-18 19:38:26):use ?raw=true
jcwchen(2021-08-20 18:25:31):For others' reference: https://github.com/python/mypy/issues/10994#issuecomment-901621815 Remove python 2 typecheck since it cannot use dynamic dictionary... It should be OK since ONNX does not support Python 2 for a while
askhade(2021-08-20 23:11:32):use %APPDATA% if platform windows?
memoryz(2021-08-21 05:28:27):`os.path.expanduser('~')` on Windows already expands to `%userprofile%`. Is there any special reason to use `%APPDATA%` instead?
askhade(2021-08-23 18:17:48):good to know "os.path.expanduser('~') on Windows already expands to %userprofile%" In that case we dont need changes here.
kevinch-nv(2021-08-26 18:02:14):```suggestion
    Get the list of model info consistent with a given name and tags
```
kevinch-nv(2021-08-26 18:10:11):Maybe add an optional parameter here (`download_latest = False`) that the user can override if they want to just download the latest version of a model
mhamilton723(2021-08-26 19:23:37):We are updating this function so that it provides the model info for the same model returned by "load" as opposed to providing a list of model info. The semantics for this and load will return the latest model if the offset is None, and users can choose earlier models by choosing earlier offsets. I think this will align with your requested behavior.

The functionality for querying model info by name will be folded into list_models
pranavm-nvidia(2021-08-26 19:34:33):```suggestion
    if not matching_models:
        raise AssertionError("No models found with name {}".format(model))
```
`assert` will be optimized out when using `python -O`:
```
-O     : remove assert and __debug__-dependent statements; add .opt-1 before
         .pyc extension; also PYTHONOPTIMIZE=x
```
And it seems like this is meant to be an error message for the user rather than an assertion
pranavm-nvidia(2021-08-26 20:04:14):Probably would want to add a docstring for this class
memoryz(2021-08-26 20:13:08):Done.
memoryz(2021-08-26 20:13:16):Done.
pranavm-nvidia(2021-08-26 20:16:33):Thanks, but I think the fields need to be documented too. The reason being that it appears to be a user-visible class (e.g. returned by `list_models`) and it's not immediately clear what kind of information it actually includes.
pranavm-nvidia(2021-08-26 20:28:46):Looks great, thanks!
memoryz(2021-08-26 20:28:54):Thanks for the feedback. I added more details to the docstring to document the attributes of the class and parameters to the `__init__` function.
lgtm-com[bot](2021-08-18 16:00:12):This pull request **introduces 1 alert** when merging 76dc8fb11804b96b6422e7ede9adbf4a6a33d2f3 into c24cd429c4ab47d2b057da8174788c39c60f8760 - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-fbb201b5997c35a3ea7aaa35975b7d583453c047)

**new alerts:**

* 1 for Constant return type on member
gramalingam(2021-08-24 22:52:19):I suggest updating the docs/IR.md file as well to document this. See the section https://github.com/onnx/onnx/blob/master/docs/IR.md#nodes, the line "In nested subgraphs used as attribute values, users MUST NOT use the same name etc.". We should add "In models with IR version >= 4" before this line, and add "In models with IR version <= 3, users MAY use the same name as both a subgraph initializer and subgraph input.". I think a bit more is required, but I will bring it up in the original issue discussion to check with the converter usage.
gramalingam(2021-08-24 22:58:22):Ok, I suggest adding the following lines to IR.md after the lines mentioned above: "Note that in models with IR version <= 3, the actual inputs expected by the control-flow operator and supplied by the node containing the invocation of the control-flow operator must appear as the initial inputs of the subgraph, with extraneous initializers following subsequently, so that the actual inputs and formal inputs can be matched positionally."
jcwchen(2021-08-25 04:47:40):Have updated the docs. Thanks @gramalingam for the sentences
gramalingam(2021-08-25 15:11:34):LGTM, thanks! @TomWildenhain-Microsoft , can you please take a look? Thanks!
gramalingam(2021-08-24 22:38:12):I suggest that if (numInputs < g_input_size()), then we check to ensure that the missing inputs all have an entry in the initializers? That is, if the graph has inputs [x, y, z] while numInputs is 2, then we must check to make sure that z is in the initializer list.
jcwchen(2021-08-25 04:45:25):Good idea. I have added this case and written a test for it. Thanks!
gramalingam(2021-08-25 15:09:44):Minor nit: we can move the declaration and initialization of this set into the "else-else" branch. It is not required otherwise.
jcwchen(2021-08-25 16:23:43):Good catch! Just updated
gramalingam(2021-08-26 15:15:21):ONNX requires all names to be valid C identifiers (even though it is not currently enforced strictly because of convertors that violate this, like here). I suggest renaming these here to be consistent with the ONNX spec: e.g., change this name to "while_maximum_iterations_0" and similarly elsewhere.
jcwchen(2021-08-26 22:43:13):Good point. Updated
TomWildenhain-Microsoft(2021-08-30 21:20:05):Does this properly enforce that none of these inputs have initializers?
TomWildenhain-Microsoft(2021-08-30 21:35:10):I like the clarification, but for IR version <= 3 this is not 100% clear as to whether inputs with initializers (optional inputs) can be used as initial, actual inputs (provided they line up correctly).  If you are disallowing it, you should maybe change
`the actual inputs expected by the control-flow operator and supplied by the node containing the invocation of the control-flow operator must appear as the initial inputs of the subgraph, with extraneous initializers following subsequently, so that the actual inputs and formal inputs can be matched positionally.`
to
`the actual inputs expected by the control-flow operator and supplied by the node containing the invocation of the control-flow operator cannot have initializers and must appear as the initial inputs of the subgraph, so that the actual inputs and formal inputs can be matched positionally. Extraneous initializers may follow subsequently.` 

TomWildenhain-Microsoft(2021-08-30 21:36:29):You also need to check that the initial inputs do not have initializers, if you are disallowing them. (see first comment)
gramalingam(2021-08-30 22:03:56):How about changing the two sentences to
```
In models with IR version <= 3, users MAY use the same name as both a subgraph initializer and subgraph input,
but this is restricted to support constants via initializers that are not intended to correspond to any
actual inputs expected by the control-flow operator and supplied by the node containing the invocation
of the control-flow operator. In particular, the control-flow operator semantics determines the set of inputs
supplied to the execution of the subgraph, and these input names MUST NOT appear as subgraph initializers.
Subgraph initializer names must appear in the graph input list _after_ the actual inputs. This allows the actual
inputs and formal inputs to be matched positionally.
```

TomWildenhain-Microsoft(2021-08-30 22:14:50):Yes that's good.  nit: I'd revise `not intended to correspond to any
actual inputs expected by the control-flow operator and supplied by the node containing the invocation of the control-flow operator.` 
to 
`not intended to correspond to any actual inputs passed from the node into the subgraph.`
jcwchen(2021-08-31 04:54:30):Good point. I added the check (initial inputs do not have initializers) here and there (your first comment). Thanks!
jcwchen(2021-08-31 04:56:10):I took both Rama's new sentences and Tom's revision since it looks clearer to me
jcwchen(2021-09-02 18:26:57):No it does not and I have added a check for it. Thanks
gramalingam(2021-09-02 19:02:31):I think this may also require changing the for-loop lower bound to ```i = 0``` ?
gramalingam(2021-09-02 19:03:05):Or, split it into 2 separate loops.
askhade(2021-08-25 15:48:11):this is testing the weekly package right? install onnx-weekly?
jcwchen(2021-08-25 15:57:07):Oh it should be onnx-weekly. Updated. Thank you for catching this
geofft(2021-08-19 01:22:23):Is this required? https://github.com/onnx/onnx/blob/master/CONTRIBUTING.md says that you're moving away from the CLA, and signing a CLA requires me to do a bunch of paperwork with my employer, so I'd prefer to avoid it if possible. (DCO is of course fine :) )
prasanthpul(2021-08-23 18:33:31):CLA is not required. just DCO. Will remove the CLA bot.
CLAassistant(2021-09-12 22:40:07):[![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/onnx/onnx?pullRequest=3667) <br/>Thank you for your submission! We really appreciate it. Like many open source projects, we ask that you all sign our [Contributor License Agreement](https://cla-assistant.io/onnx/onnx?pullRequest=3667) before we can accept your contribution.<br/>**1** out of **2** committers have signed the CLA.<br/><br/>:white_check_mark: jcwchen<br/>:x: geofft<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/onnx/onnx?pullRequest=3667) it.</sub>
geofft(2021-09-16 14:08:44):The CLA bot is still asking for a CLA. Should I start the process on my side to do CLA paperwork?

Also, can someone take a look at the actual change? It's very tiny :)
geofft(2021-11-18 00:19:38):Hello, can someone please take a look at this PR and also why the CLA bot is asking for a CLA?
jcwchen(2021-09-16 16:44:27):Another PR does a better job with this (upgrade to higher version): https://github.com/onnx/onnx/pull/3604. Close this now.
jcwchen(2021-09-16 18:50:35):Since it can help to resolve the IR issue (input from initializer is not found) for version converter, the main contributor of current version converter @matteosal could you please help to review this if you have bandwidth? Thank you for your contribution!

Friendly ping ONNX IR experts @linkerzhang @daquexian @fumihwh Feel free to review this new PR for fixing the old IR issue.  Thanks!


gramalingam(2021-10-02 01:19:09):Sorry, I don't understand the context for this change. I have a feeling this is not doing the right thing. In particular, why should we change the conversion of GraphProto to IR? What's the issue with this conversion? It simply maps the inputs/initializers of the graph-proto to the IR, and, hence, should do the right thing whatever the IR version is. The likely issues are:
(a) Checking whether a particular node-input X has a constant-value: I think some of the existing transformers may actually be wrong if they pickup the constant-value from initializers, but don't check whether X is an input. (But I don't think this is the original issue people had. But this is a different soundness concern.)
(b) If the transformer creates an initializer, without a corresponding input, does the conversion of the IR to GraphProto complain? Where exactly is the original conversion failure happening?
jcwchen(2021-10-02 03:45:24):Thank you for the review! Let me give you more context from my understanding:
> why should we change the conversion of GraphProto to IR? What's the issue with this conversion? 

Previously the conversion only considers graph input for node input and does not consider initializer. And while loading node input, it won't consider initializer and complain input is undefined. To add initializer into node input, initializer needs to be converted to `Value` (I also stored them into `value_by_name_of` to simplify code). Original conversion cannot achieve that because the old IR only stores initializer as a vector of Tensor. That's why I additionally created a node list of initializer (just like what input did) for storing `Value` from initializer.

To prevent confusion, actually I was considering to remove `initializers_` because the new node list of initializer seems to cover all information and `initializers_` is not used internally from ONNX. (but others might have already relied on it and it needs more investigation)

> (a) Checking whether a particular node-input X has a constant-value: I think some of the existing transformers may actually be wrong if they pickup the constant-value from initializers, but don't check whether X is an input. (But I don't think this is the original issue people had. But this is a different soundness concern.)

Actually current IR node just does not consider initializer at all because old IR (< 4) always assumes initializer exist input.

> (b) If the transformer creates an initializer, without a corresponding input, does the conversion of the IR to GraphProto complain? Where exactly is the original conversion failure happening?

Yes it will complain [here](https://github.com/onnx/onnx/blob/a9c17dae3cd7a7948d9a813c76039487411aab20/onnx/common/ir_pb_converter.cc#L308) because the node input cannot find the corresponding "Value" from `value_by_name_of`. Original `value_by_name_of` only considers graph input. 

The discussion here leads me to think about another thing: I should further test it with certain operator's adapter for version_converter which involves initializer to verify this change.

matteosal(2021-10-07 13:31:54):Sorry for the delay in looking at this. I can't really comment on the code changes since my familiarity everything outside the version converter is quite low, but I have tested this change in a few different cases (including [yolov4](https://github.com/onnx/models/tree/master/vision/object_detection_segmentation/yolov4) which was affected by the problem) and confirmed it solves the issue.
matteosal(2021-12-17 13:48:53):@jcwchen are there any news for this?
jcwchen(2021-12-17 20:07:31):> @jcwchen are there any news for this?

Before merging it, I was doing more verification because it's a fundamental change. Thank you for letting know yolov4 was the model which cannot be converted. I have verified that model can be converted correctly by this PR and also made sure the output by ONNXRuntime remains the same. Besides, I tested this PR with all models (IR <= 3) from ONNX model Zoo and all models seems converted successfully as before (same failure number due to other reasons).

Please let me know if there are other models which have this issue as well. We hope we can verify this PR as much as we could. If everything goes well, we will forward this PR before the next ONNX release. Thank you!


matteosal(2022-01-03 15:55:24):@jcwchen we have some new unittests which are very suitable to test this fix but I've realized that this PR currently only contains a change in test_model_zoo.py and nothing about the IR. Am I missing something? Where is the fix?
jcwchen(2022-01-04 01:26:00):> @jcwchen we have some new unittests which are very suitable to test this fix 

@matteosal Good to know! Thank you for testing.

> but I've realized that this PR currently only contains a change in test_model_zoo.py and nothing about the IR. Am I missing something? Where is the fix?

Sorry that last time I somehow messed up the commit log. I just recovered it. Thank you for the reminder.


matteosal(2022-01-04 15:22:46):@jcwchen 
I have ran our tests and found no problem at all. They consist in the following steps for 700+ operators:
* Export an op from our framework to old ONNX opset versions (without including initializers in the input list)
* Use the version converter to upgrade the ONNX model to opset 15
* Make a roundtrip by importing the operator back and run consistency tests between the original and the roundtripped op

Everything went smooth by using this branch, so I would consider it to be ready
jcwchen(2021-10-01 16:09:36):Need to erase created init node here... Doing
askhade(2021-10-01 19:24:44):did you check addInitializerAndInput line 1122. It does something similar
askhade(2021-10-01 19:25:26):Check eraseInitializerAndInput
askhade(2021-10-01 19:29:27):did you test this?
This is what happens in addInitializer: 
```
initializers_.push_back(std::move(initializer));
initializer_names_.push_back(std::move(name));
```
you are using initializer and name after move.
askhade(2021-10-01 19:34:01):please use naming convention initializer_node_
askhade(2021-10-01 19:34:47):also suggest renaming this to something like input_as_initializer_
gramalingam(2021-10-02 00:55:42):I think it should be okay since addInitializer's parameters are passed by-value, so no impact on the values here?
gramalingam(2021-10-02 06:10:29):@jcwchen : thanks for the explanation (in the other thread). In ir_version >= 4, initializer may or may not be in the input list. I think you need to check here whether initializer X already has a value (because it is in the input list) also. We should addInitializerNode only if X is not an input.
gramalingam(2021-10-02 06:16:57):Eg., change the condition to
```cpp
   if ( (ir_version >= 4) && (value_by_name_of.count(init.name()) == 0)) {
```
jcwchen(2021-10-12 20:04:21):Done. I created a map `initializer_to_offset_map_` to store offset for erasing initializer node
jcwchen(2021-10-12 20:08:35):Yes I added some comments to distinguish them:
1. addInitializerAndInput: create a initializer whose value will be stored in graph's input (input_)
2. addInitializerAndCreateValue (added in this PR): create a initializer whose value will be stored in graph's initializer (initializer_node_)
jcwchen(2021-10-12 20:11:33):Two things:
1. I made addInitializer to decide whether to create a new unique name or not (only create when name is missing)
2. To prioritize input value over initializer value if both exist, yes that condition is needed. Added. Thanks!
jcwchen(2021-10-12 20:14:15):Thanks for reminding the naming convention. I still used initializer_node_ here because input_as_initializer_ looks confusing about the case of `addInitializerAndInput`... It should store those initializers which do not exist in graph's input
askhade(2021-10-28 18:24:41):nit: initializer_to_offset_map_
askhade(2021-10-28 19:20:27):this is incorrect. you have already moved initializerCopy
askhade(2021-10-28 19:22:26):  initializers_.push_back(initializer); works the same way... in the current implementation you are making a copy and then moving
askhade(2021-10-28 19:25:38):can the initializer name be empty? I dont think so
askhade(2021-10-28 19:26:07):nit: auto
jcwchen(2021-10-30 02:48:47):For initializer comes from real world models, no it cannot be empty.
 
However, the reason why I made the condition here is it can be useful for creating a new initializer during version_converter or testing. In that case users don't need to worry about how to create a unique name. Also this is how existing addInitializerAndInput is doing.
jcwchen(2021-11-06 03:53:17):This is for testing version_converter with models from ONNX model zoo. Will remove this update before merge.
askhade(2021-08-30 21:33:21):This seems to have been identified as a bug in C++11 and was later corrected.
http://www.open-std.org/jtc1/sc22/wg21/docs/cwg_defects.html#1288 (Search for 1095. List-initialization of references) and gcc 4.9 onwards the compiler supports it too.

We can make this change to support older compilers but given gcc 4.8 is pretty old wondering whether we should request users to update gcc .

How urgent is this change? Can we discuss this in this week's arch-infra op-sig meeting?
gramalingam(2021-09-01 17:31:09):The change itself makes sense to me, since the fix/benefit comes at no cost/downside. We can decouple the question of whether we need another release from this PR, I think.
jcwchen(2021-08-26 18:49:33):[WIP] Use 3.11.3 first to see whether CI can catch this error
askhade(2021-08-31 18:17:58):make this true by default so that existing scenarios are unaffected.
askhade(2021-08-31 18:19:12):Typo: Update comment. "Whether to run model checker on the extracted model"
zhenhuaw-me(2021-09-01 10:33:53):Fixed. Thanks!
zhenhuaw-me(2021-09-01 10:34:00):Fixed. Thanks!
askhade(2021-09-02 15:52:20):what does this fix? Is this required because the test model was changed to MNIST?
mhamilton723(2021-09-02 15:59:48):Yes. I had this in my local branch but forgot to add it in last PR. My bad!
jcwchen(2021-09-02 16:45:29):FYI: https://github.com/onnx/models/tree/master/vision/classification/mnist Update it as 8 since there is no 12 for MNIST.
askhade(2021-09-21 04:28:27):you should simplify this check
You can change the macro to take the enum type field something like this:

DEFINE_PARSE_DATA(int32_t, int32_data, TensorProto_DataType_INT32)

and then directly compare tensor_proto->data_type() == enum_type


jcwchen(2021-09-21 15:34:22):Good idea. Updated.
askhade(2021-09-21 15:59:39):do you need the if check above? Can you combine if and else if into 1 if loop?
You can update the error message here to say something like 
```
fail_shape_inference("ParseData type mismatch for tensor: ", tensor_proto->name(), "Expected", <>, "Actual", <>);
```

jcwchen(2021-09-21 17:52:17):I think these two if are required since TensorProto_DataType_UNDEFINED is not in the existing type-string map? Two different error messages look clear to me for two different kinds of errors. Let me know if you still think they should be merged since we can still extend the map with TensorProto_DataType_UNDEFINED. ("undefined" to TensorProto_DataType_UNDEFINED)
jcwchen(2021-09-21 17:53:21):I moved this utility function to public function to get type string from tensor_data_type for ParseData
askhade(2021-09-21 22:00:16):No need to extend the map... as ToDataTypeString will throw an error if type is not in its mapping... Maybe it is OK to keep these 2 checks... Can you make sure we have enough tests for ParseData
jcwchen(2021-09-23 02:38:10):Added one test to test invalid type for ParseData: strict shape inference should catch it and normal shape inference should not produce unexpected shape
jcwchen(2021-09-23 14:32:57):Added another test to test undefined Tensor type
jcwchen(2021-09-09 20:55:21):cc @TomWildenhain-Microsoft 
TomWildenhain-Microsoft(2021-09-09 21:36:02):I like the line `If both are scalars, axis is ignored.`.  The line `If either y_zero_point or y_scale is a 1D tensor, then an axis must be specified.` seems to conflict with axis having a default value (and technically being 1 if it is unspecified).
gramalingam(2021-09-09 22:48:49):Good catch. My fault, I didn't notice the default value and suggested a more complete description to Jacky.
TomWildenhain-Microsoft(2021-09-09 22:52:30):I actually wish the default value didn't exist so we could do it your way but unfortunately it's too late to change I think.
jcwchen(2021-09-10 01:33:29):Just removed `If either y_zero_point or y_scale is a 1D tensor, then an axis must be specified.` to prevent confusion. No worries @gramalingam I didn't notice it in the first place and thank @TomWildenhain-Microsoft for catching it!
gramalingam(2021-09-10 20:01:39):Minor nit: we should change ```both``` to ```both y_zero_point and y_scale``` since we deleted the preceding line. Otherwise, it is confusing what ```both``` refers to.
jcwchen(2021-09-10 20:41:45):Updated. Thanks!
jcwchen(2021-09-08 16:56:18):Thanks for proposing the fix. Reopen this PR to trigger Release CIs.
neginraoof(2021-09-09 05:09:49):Thanks @jcwchen for review. Please let me know if we can merge this PR.
+ @askhade 

Thanks
askhade(2021-09-09 03:21:24):typo the an
askhade(2021-09-13 16:48:12):typo Futhermore -> Furthermode
askhade(2021-09-13 16:50:28):add a best practice note to set force_reload=False to avoid unnecessary bandwidth cost. 
jcwchen(2021-09-15 18:42:41):nit:
```suggestion
These important fields are:
```
Perhaps we can mention that ONNX encourages users to use cache to prevent downloading frequently.
askhade(2021-09-13 17:36:07):what does this change imply?
jcwchen(2021-09-13 17:56:32):Previously --no-binary :all: means all binary packages (including dependencies) are disabled, but actually ONNX should only disable "onnx" binary here (only build ONNX from source distribution). 
jcwchen(2021-09-13 18:00:06):https://github.com/pypa/pip/issues/7831 The source distribution of setuptools is missing in TestPyPI. To solve it, use old `--no-use-pep517` to not install setuptools from source and use `--use-deprecated=legacy-resolver` to resolve the package name mismatch warning (onnx-weekly vs onnx).
askhade(2021-09-14 20:20:14):what is the current behavior for ndarrays?
jcwchen(2021-09-14 20:23:40):Current make_tensor only accepts 1D ndarray... Any numpy array whose rank>1 cannot be used by make_tensor and users need to flatten their N-D array on their own. The code here enables make_tensor to use N-D numpy array
askhade(2021-09-17 16:58:50):nit: structure this a little different 
```
        # float16/bfloat16 is stored as int32 (uint16 type); Need view to get the original value
        if (tensor_dtype == TensorProto.FLOAT16
                or tensor_dtype == TensorProto.BFLOAT16):
            return (
                np.asarray(
                    tensor.int32_data,
                    dtype=np.uint16)
                .reshape(dims)
                .view(np.float16))
        data = getattr(tensor, storage_field),  # type: Sequence[np.complex64]
        if (tensor_dtype == TensorProto.COMPLEX64 
          or tensor_dtype == TensorProto.COMPLEX128):
            data = combine_pairs_to_complex(data)
   
        return (
            np.asarray(
                data,
                dtype=storage_np_dtype)
            .astype(np_dtype)
            .reshape(dims)
        )
```
askhade(2021-09-21 04:41:47):add a comment explaining why you are mapping bfloat16 to np float16 something like (numpy does not support bfloat16...)
askhade(2021-09-21 04:44:31):this comment "# type: Sequence[np.complex64]" does not make sense... 
jcwchen(2021-09-21 17:05:12):Yes it is incorrect... Just removed it. Thanks
askhade(2021-09-21 17:08:11):can you add a test with raw=True in make_tensor call for both FLOAT16 and BFLOAT16
jcwchen(2021-09-21 18:26:14):Added. To make it work, I made split_complex_to_pairs and numpy_array.view go into the else condition (raw=False). IIUC, the vals for raw=True should be the final binary data after conversion so no more conversion is needed inside this helper function (make_tensor).
askhade(2021-09-14 21:40:50):check status.IsOK before returning?
askhade(2021-09-14 21:48:05):I think we have a limitation in current code which requires function attr name to match node attr name.... can we get rid of this limitation now? If we change the attr name for function from axes to something different will it work?
gramalingam(2021-09-14 22:12:32):I don't think there was a limitation in proto. The builder utility earlier had a limitation (that is, it had only one name parameter that it used for both). Even that was subsequently removed. See https://github.com/onnx/onnx/blob/a5e7ee51176bf78a60c118758174e13d85a87b46/onnx/defs/attr_proto_util.h#L45 and https://github.com/onnx/onnx/blob/a5e7ee51176bf78a60c118758174e13d85a87b46/onnx/defs/attr_proto_util.h#L54 ... unless I misunderstood your question.
gramalingam(2021-09-14 22:17:17):On a different note: I don't think the "type" field is required in an attribute-reference ... however, all the examples I saw set the type, so I did the same to not change the behavior. But it would be useful to check and clarify whether this field is required or not. I don't think it is used. It can lead to inconsistent models (where the actual attribute is of type int and the reference says float etc., which we don't check for, I think.) It is simpler to pick up the type from the attribute passed into the function.
askhade(2021-09-15 22:25:21):I don't remember what was the exact issue this was at least 2 years back and since then we have made quite a bit of improvements in this area.

I agree adding a type here can lead to inconsistent models. Can we move to picking the type from attr instead?
gramalingam(2021-09-15 23:16:55):Done.
gramalingam(2021-09-15 23:22:21):Not sure I understood "pick the type from attr instead". I think we are doing that already. I think we just need to clarify in the spec that setting the type for this case in not needed (and that this type is ignored). And decide what to do in the checker. And update the current uses to drop this. But the issue is compatibility, if other users have custom functions that do this. In any case, I think this is a separate discussion, and we should consider it separately from the current PR.
gramalingam(2021-09-16 04:11:41):I tried removing the " : ints". However, the checker is currently checking this type, so this causes a failure. I am rolling back that change, and adding the ": ints" back. 
lgtm-com[bot](2021-09-15 20:08:32):This pull request **introduces 1 alert** when merging 6a376fc465938c6ee156063cd65dee2a89c439ba into a5e7ee51176bf78a60c118758174e13d85a87b46 - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-0ca3b7e0a1a6296ff62a0559d23ddee8c8bf60ac)

**new alerts:**

* 1 for Unsigned comparison to zero
askhade(2021-09-15 22:30:54):Closing this in favor of #3722. I mistakenly created this branch in onnx instead of using a fork. 
jcwchen(2021-09-15 21:02:32):So ONNX finally decides not to print such warnings anymore (due to https://github.com/onnx/onnx/issues/3696 and https://github.com/onnx/onnx/issues/2239)?

I am afraid without those warnings, users will complain the shape inference is incomplete and there is no any warning/error... Could ONNX at least throw a warning about unsupported error when enabling strict_mode?
jcwchen(2021-09-15 21:07:31):Or perhaps ONNX can add a new "mode" for shape_inference to detect such warning/error (unsupported schema)
askhade(2021-09-21 04:38:11):should this be node->hasAttribute(kmode) && || node->s(kmode) == "constant"
Can you elaborate a little more about this check. Thanks!
liuyu21(2021-09-22 09:06:30):the default mode is 'constant' and then **node->hasAttribute(kmode)** is false
so, if **node->hasAttribute(kmode) == false**, it is a DEFAULT 'constant' mode
the code run in either **default constant mode** or **constant mode**
gramalingam(2021-09-27 23:13:47):I wonder if the IR automatically takes care of returning the default-value? If not, then shouldn't we also check if the "value" attribute is present down below, before using?
jcwchen(2021-09-16 16:19:41):Thanks for the effort! Two more things:
- Since ONNX has improved its shape inference, shall we further mention this document in the original shape inference document (limitation section): https://github.com/onnx/onnx/blob/master/docs/ShapeInference.md#limitations?
- Shall we add a brief demo about how to enable [data propagation](https://github.com/onnx/onnx/blob/a5e7ee51176bf78a60c118758174e13d85a87b46/onnx/shape_inference.py#L39)? We can mention it either in this new data propagation doc or [PythonAPIOverview.md](https://github.com/onnx/onnx/blob/master/docs/PythonAPIOverview.md)
askhade(2021-09-16 22:37:40):> Shall we add a brief demo about how to enable data propagation? We can mention it either in this new data propagation doc or PythonAPIOverview.md

Yes lets add it to backlog items. 
jcwchen(2021-09-16 15:36:39):nit: capital case 
- L23: Shape computations
- L25: Symbolic dimensions
jcwchen(2021-09-16 15:39:14):Shall we enumerate (1, 2, 3) the items here (L13-L17) since L23 and L25 mention limitation "1" and "2"?
jcwchen(2021-09-16 16:05:28):```suggestion
It is called “partial” data computation and propagation because this will only be done for shape computations. It is not meant to be a full-fledged kernel for the operator. For the same reasons data computations will be implemented for a limited set of operators. While we will increase the coverage in the future iterations it is important to note that for some operators like LSTM, convolution ops, pooling ops etc. These will never be added because such ops are not used in shape computations. 
```
jcwchen(2021-09-16 16:10:55):```suggestion
When the shape contains symbolic dimensions, we try and propagate them downstream, however in cases where some arithmetic operations are performed on these symbolic dims we create new symbols and propagate them instead.   
```
askhade(2021-09-16 19:51:34):this refers to data propagation function. Updated to use "data propagation function"
gramalingam(2021-12-07 19:51:44):Wondering about the status of this PR. I guess not much change is required to merge this PR, is there? Thanks!
askhade(2021-12-08 18:01:18):> Wondering about the status of this PR. I guess not much change is required to merge this PR, is there? Thanks!

No there isn't much work remaining. I will pick it up early next week and have it ready. Thanks!
askhade(2022-01-04 18:47:39):Linux CI failures are due to an unrelated issue. #3916 should fix it.
askhade(2022-01-05 16:48:20):"LinuxRelease_i686 / build (3.8, x64) (pull_request) " pipeline is failing due to an unrelated reason. #3918 should fix it. Merging this PR now.
gramalingam(2021-09-20 20:52:28):Do we really need this (function to combine the domain and opname into one string) as a parameter? I understand having a single place where we can change it easily, if we want to, but unclear if we need to expose it as a parameter (that others start taking a dependence on).
gramalingam(2021-09-20 20:58:58):Just noting for future extension (not in this PR): Ideally, we should be saving the results of shape-inference applied to functions, and right now we don't have a place to save this. (That is, the analogue of "value_info" field in a graph.)
jcwchen(2021-12-08 02:11:56):tiny nit: sync space
```suggestion
        ir_version{ir_version_in},
```
jcwchen(2021-12-08 02:13:20):Ditto
```suggestion
        function_identifier{get_func_id} {}
```
jcwchen(2021-12-08 02:38:20):+1 We should carefully review it since it's an important change that involves many functions and people will start to rely on it. It looks useful to me though. It will be more convincing if there are some real use cases for it.
askhade(2021-09-25 17:34:49):this seems hacky... can you simply remove bfloat16 from the map?
You can take care of bfloat16 type in the calling code... 
askhade(2021-09-25 17:37:07):This is where you need bfloat16 support from this map right:
expected_size = 1 if (not raw) else (mapping.TENSOR_TYPE_TO_NP_TYPE[data_type].itemsize)

You can handle bfloat16 separately in this code... 
jcwchen(2021-09-25 18:29:37):I thought about that as well, but actually helper make_tensor is not the only function uses mapping. If we decide to add bfloat16 condition whenever using mapping, there will be quite a few places need this update. (`helper.py`, `numpy_helper.py`, `test/case/code/node/castlike.py`, `test/case/code/node/cast.py`, `test/case/node/__init__.py`) It will be hard to maintain in the future (when new functions need to know the mapping relationship for bfloat16 or bfloat16 is eventually supported by numpy) and also seem hacky...

Merely update mapping.py at least people will clearly know the relationship that
- TensorProto.FLOAT16 -> numpy.float16
- TensorProto.BFLOAT16 -> numpy.float16
- numpy.float16 -> TensorProto.FLOAT16 (not TensorProto.BFLOAT16)

Please let me know if you still think handle bfloat16 separately in other codes is better. Thanks!
askhade(2021-09-28 03:14:38):You add "int(TensorProto.BFLOAT16): np.dtype('float16')," to the map only in your last PR right?
gramalingam(2021-09-28 06:28:28):Regardless of the number of places where it is used, the important thing is to understand *why* it is used in all those places to make sure the right thing is happening in all those places. And see if we can have different methods that are less likely to be wrongly used in the future (e.g., a "sizeof" method would be a better way to encapsulate its use to determine size).
jcwchen(2021-09-29 02:37:16):After discussion, we decided to add a if condition to exclude TensorProto.BFLOAT16 while constructing NP_TYPE_TO_TENSOR_TYPE. It looks clearer for future reference and less hacky.
gramalingam(2021-09-29 21:05:46):minor nit/suggestion: I think you can just add the if condition to the original construct as below:
```py
   NP_TYPE_TO_TENSOR_TYPE = {v: k for k, v in TENSOR_TYPE_TO_NP_TYPE.items() if k != TensorProto.BFLOAT16}
```
jcwchen(2021-09-29 21:20:50):Looks good... I will update in another PR: https://github.com/onnx/onnx/pull/3674 Thanks!
gramalingam(2021-09-22 16:52:29):The previous implementation does seem incomplete/broken, thanks for catching it. In particular, it does not handle two cases properly, the case where elem_type is UNDEFINED and the case where elem_type is OPTIONAL. The representation does not seem well-documented. In particular, what is elem_type supposed to be for a None value? I think elem_type should be UNDEFINED (rather than OPTIONAL). My guess is that there are two issues, requiring two different fixes. First is to add
```
   if elem_type == OptionalProto.UNDEFIND:
      return None
```
_before_ line 291. The second is to add
```
   elif elem_type == OptionalProto.OPTIONAL
      return to_optional(value)
```
before line 300/302. The proposed fix makes me wonder if we are choosing to represent a None value by setting the elem_type to OPTIONAL and leaving the optional field empty, which seems a bit problematic.
chudegao(2021-09-23 03:39:37):Thanks for comments. I added the two fixes in comments and still keep the original fix as there's case that elem_type is tensor/seq/map but no value filed.

```
>>> proto_filename='onnx/backend/test/data/node/test_optional_has_element_empty/test_data_set_0/input_0.pb'
>>> f=open(proto_filename, 'rb')
>>> protobuf_content = f.read()
>>> optional = onnx.OptionalProto()
>>> optional.ParseFromString(protobuf_content)
18
>>> optional
name: "optional_input"
elem_type: 1

>>> 
```
gramalingam(2021-09-23 23:49:34):Thanks for the test-case where it fails. I think the problem is in the creation of the test-case data (in the function ```from_optional``` below). I think we should add the following before line 335/341 (old and new line numbers):
```
   elif (opt is None)
      elem_type = OptionalProto.UNDEFINED
```
I think this should also be cleaned up more (to omit the parameter dtype), but it can be done separately, assuming that the other fixes work and do the right thing.
chudegao(2021-09-24 04:33:17):Yes, it's caused from_optional not set elem_type as ptionalProto.UNDEFINED. I added it as your comment and update the checker to skip checking when elem_type is OptionalProto.UNDEFINED.
snnn(2022-02-22 02:54:09):BTW, I just found the latest protobuf release doesn't work.  It needs this fix: https://github.com/protocolbuffers/protobuf/pull/9437 

It doesn't impact your this change.  Just FYI.
liudonghua123(2022-03-10 00:53:28):Recently I installed onnx in my windows python 3.10 x64, I have to build protobuf before build onnx locally which it takes a lot of time. I would like to install via a prebuild whl file. 😄 
jcwchen(2022-03-11 01:11:12):@liudonghua123 FYI you will see Python 3.10 wheel [here](https://test.pypi.org/project/onnx-weekly/) from onnx-weekly package after this weekend. Have fun!
askhade(2021-09-28 03:12:53):3.10.0-rc.2? why not 3.10?
jcwchen(2021-09-28 14:43:24):Python 3.10 support is still experimental in GitHub Action so 3.10 cannot be found for now. Need to specify the exact version name here to find the right python-version.
jcwchen(2022-02-28 20:35:13):https://github.com/numpy/numpy/issues/20277 NumPy didn't provide prebuilt Python 3.10 i686 wheel. Since now ONNX uses NumPy 1.21.5 in general, it needs to build NumPy from source for Python 3.10, which is probably not a good idea. Besides, this built Python 3.10 i686 ONNX wheel seems problematic: it hangs a long time while generating data. It took me quite a while to investigate what the root cause is, but I still don't know why.

And in newer NumPy (>1.22), they even don't provide i686 wheel for all Python versions anymore. I think we should reconsider to deprecate manylinux i686 packages since people are retiring them and the release CI for i686 will be unstable. If we decide to do so, the announce needs to be done early before next release to let users be aware.
jcwchen(2022-03-02 03:22:21):Created an issue https://github.com/onnx/onnx/issues/4049 for tracking. I tend to skip Python 3.10+i686 for now in this PR and then we can at least let CIs cover Python 3.10 in other environments.
gramalingam(2022-03-10 21:01:00):What's the FIXME? Is there a reason 3.10 cannot be tested here? Thanks!
jcwchen(2022-03-10 21:44:05):Sorry for the confusion. The reason is ort-nightly has not supported Python 3.10 yet: https://test.pypi.org/project/ort-nightly/#files. I think I should use TODO instead, which is more proper. Just updated. Please review it again. Thank you!
askhade(2021-09-28 21:20:42):Can you create an issue to track this follow up work... 
askhade(2021-09-25 17:32:24):Is BFLoat16 support required for this OP? Meaning is any runtime blocked currently or is this simply to close the gap?
gramalingam(2021-09-27 22:57:02):>> Is BFLoat16 support required for this OP? Meaning is any runtime blocked currently or is this simply to close the gap?

The function definition of DropoutGrad, for example, does not work for bfloat16, because the Where op does not support bfloat16. (See:https://github.com/microsoft/onnxruntime/blob/7d28b596f482f70052cc9e58374455bd5af7c92d/orttraining/orttraining/core/graph/training_op_defs.cc#L1702 ).
gramalingam(2021-09-29 18:27:54):It would be helpful to add the motivation for these ops (eg., if these are commonly required for audio processing).
CLAassistant(2021-10-05 21:11:00):[![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/onnx/onnx?pullRequest=3741) <br/>Thank you for your submission! We really appreciate it. Like many open source projects, we ask that you sign our [Contributor License Agreement](https://cla-assistant.io/onnx/onnx?pullRequest=3741) before we can accept your contribution.<br/><hr/>**Sheil Kumar** seems not to be a GitHub user. You need a GitHub account to be able to sign the CLA. If you have already a GitHub account, please [add the email address used for this commit to your account](https://help.github.com/articles/why-are-my-commits-linked-to-the-wrong-user/#commits-are-not-linked-to-any-user).<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/onnx/onnx?pullRequest=3741) it.</sub>
napohou(2021-11-10 08:06:41):any update?
wuerflts(2021-11-16 10:13:52):Besides audio processing the FFT is an integral component of MRI reconstruction. See this open challenge by NYU / Meta: https://fastmri.org/
One important caveat is that the N-dimensional FFT is needed for those applications which can be implemented by successive 1D FFTs. However, it would make for a cleaner interface in ONNX to simply define only one Operator which is N-dimensional by default since tensorflows and pytorchs special 1D, 2D, 3D and ND cases can be represented by that. This would provide a more concise ONNX graph.

napohou(2021-11-18 06:16:14):Hi @smk2007 
Did you implement fft and ifft in your code of https://github.com/smk2007/onnx? If so, which branch should we use, master or user/sheilk/signal_ops?
garymm(2022-02-23 19:24:20):@xadupre do you think any of these can be expressed as functions (similar to what you did in https://github.com/onnx/tensorflow-onnx/pull/1371)?
xadupre(2022-02-24 09:10:49):@garymm they can be expressed as functions. One question before that is about complex. Right now, ONNX operators do not fully support complex types (Add, Sub, ...). It is possible to do without, that's what we did in tf2onnx but the implementation would be more simple with than without.
garymm(2022-02-25 01:29:32):Let's discuss with @gramalingam on Monday. Perhaps we should wait until ONNX Script is ready and then we can use it to define these functions.
garymm(2022-03-31 17:02:46):Can you please update the PR description to include `Fixes #3573`?
ternaus(2022-04-21 19:15:13):@smk2007 Any updates? This PR should unlock a set of very important models to be converted to ONNX.
postrational(2022-04-28 14:37:59):Shouldn't the window functions have the `periodic` attribute to match the PyTorch and Tensorflow implementations?
postrational(2022-04-28 14:51:33):I believe the `ISTFT` is currently missing from the Pull Request.
smk2007(2022-05-03 21:41:07):working on an update this week.

---
In reply to: [964875867](https://github.com/onnx/onnx/pull/3741#issuecomment-964875867) [](http://example.com/codeflow?ancestors=964875867,964875867)
smk2007(2022-05-03 21:48:12):Hi. To accomodate the n-dimensional FFT I have added the axis attribute to allow chaining 1d FFTs.
While the graph may be cleaner with a single nFFT, I think that this can be easily handled by converters as most do not author onnx graphs from scratch.
Unless there is a pressing performance reason to rationalize nFFT as a single operator, I am postpone nDFT to a later time when needed.

---
In reply to: [970120447](https://github.com/onnx/onnx/pull/3741#issuecomment-970120447) [](http://example.com/codeflow?ancestors=970120447,970120447)
averkij(2022-05-24 11:49:29):Thank you for that.

I have a question though, — is it possible to use this version now for exporting pytorch model somehow?
jcwchen(2022-05-24 13:40:45): > I have a question though, — is it possible to use this version now for exporting pytorch model somehow?

I don't think so -- since ONNX just introduces Singal operators, it will take some time to let converters consume this update and be able to convert signal-related operators.
urinieto(2022-06-15 19:25:37):Is there a way to use something like [Custom Operators](https://pytorch.org/docs/stable/onnx.html#custom-operators) to be able to export pytorch models that use some of these audio operators?

(I know the default pytorch converter does not consume this update yet, but I am trying to figure out possible workarounds)
gramalingam(2022-06-15 19:46:46):@urinieto : possibly. You might get a more complete answer if you ask in the pytorch-converter repo.
justinchuby(2022-07-22 18:01:16):@urinieto Yes. That is possible. Happy to discuss more in the pytorch repo. Contributions are always welcome and appreciated!
jcwchen(2021-09-28 02:11:19):unused is_read causes Linux-CI failing.
jcwchen(2021-09-28 02:14:52):The latest opset version is 16 so please replace all 13 with 16.

You need to run `tools/update_doc.sh` or `tools/update_doc.bat` to get updated documents (Changelog.md, Operators.md, TestCoverage.md) and please include them in this PR. Thanks!
gramalingam(2021-09-29 18:11:39):Adding a single line to indicate the order of elements for [n_fft] may be helpful, just like in line 3695 above.
gramalingam(2021-09-29 18:13:44):Best to make all the code from here conditional on ```hasInputShape(ctx,0)```, and check that the shape has the expected dimensions.
gramalingam(2021-09-29 18:16:43):Is the goal to support multidimensional? If so, my recommendation would be an attribute to indicate if the input is complex or real (and then the number of dimensions is automatically determined by the input rank).
gramalingam(2021-09-29 18:19:06):This does not seem consistent with the shape-inference function below, which may add an extra dimension for complex output.
gramalingam(2021-09-29 18:22:33):Is there a reason for not including bfloat16? Suggest also replacing this with one of the predefined sets, like ```all_numeric_types_with_bfloat()``` : https://github.com/onnx/onnx/blob/5282dd787b2c913110c26f34de7b624cf7ab9c01/onnx/defs/schema.h#L633 
gramalingam(2021-09-29 18:32:10):Can we add any link to help explain what a Hann window is?
gramalingam(2021-09-29 18:33:18):If all the window ops are mostly identical, I would suggest factoring it out to avoid duplication. This will help making updates/changes while keeping them in sync. (E.g., see https://github.com/onnx/onnx/blob/5282dd787b2c913110c26f34de7b624cf7ab9c01/onnx/defs/math/defs.cc#L53 )
gramalingam(2021-09-29 18:38:21):Suggest swapping the order of inputs to make the optional ones come in the end
smk2007(2021-10-15 15:05:18):I added this to indicate that I think this op could be extended to multidimensional in the future. Kind of like a note that this operator is future proofed for multi-dim, but i didnt add the attribute pre-emptively at the moment. Can the attribute be omitted for 16, and added in the future?
smk2007(2021-10-15 15:07:18):Fixed locally, will push soon.
smk2007(2021-10-15 15:11:32):Updated description locally: 

"The inverse 1-dimensional discrete Fourier transform of the input. "
"The output has the shape: [batch_size][n_fft][2]"
"For all types of input, the last dimension of the output represents the components of a complex number.",
smk2007(2021-10-15 15:13:49):Done locally.
smk2007(2021-10-15 15:19:42):Done locally.
smk2007(2021-10-15 15:19:49):bfloat16 added.
smk2007(2021-10-15 15:37:02):Added a link to all windows to the following paper: https://ieeexplore.ieee.org/document/1455106
smk2007(2021-10-15 20:09:48):Updated locally with generator.
jcwchen(2021-10-18 20:52:12):Thank you @smk2007 for the update!

Regarding to Update Docs generating many unrelated changes -- you can decouple `update_doc.bat` into these commands (using --op_type to specify updated operator to get rid of global update):
```
python onnx\defs\gen_doc.py
python onnx\backend\test\stat_coverage.py
python onnx\backend\test\cmd_tools.py generate-data --op_type DFT
python onnx\backend\test\cmd_tools.py generate-data --op_type IDFT
…
```
Since the test scripts under test/case/xxx.py are not ready yet, `python onnx\backend\test\cmd_tools.py generate-data --op_type DFT` won't generate anything. (You can simply run `gen_doc.py` and `stat_coverage.py` to only update documents and remove other unrelated changes for now) Please note that once you have provided those test scripts and node models, you will need to specify every op_type for generate-data. Thanks!
wuerflts(2021-11-16 10:01:52):Please make a choice now whether to extend it to multi-dimensional now before adding it to ONNX or to offer another operation like FFTn similar to what Pytorch does. Otherwise image processing applications like magnetic resonance image reconstruction (https://fastmri.org/) will have to resort to weird workarounds which will break in upates. The implementation is not much different since FT is separable. 
askhade(2021-11-16 18:29:12):what are these changes for?
xadupre(2022-03-08 14:31:10):size is expected to be a float here. I assume size is the only input defined as an integer.
fdwr(2022-03-11 06:54:26):(nit) Space between words
"(RFFT).When"
"signal.The"
"signals.If".
fdwr(2022-04-21 20:51:19):https://github.com/smk2007/onnx/pull/1
gramalingam(2022-04-25 18:24:03):Ideally, it should allow negative values, just like other ops.
gramalingam(2022-04-25 18:24:03):Ideally, it should allow negative values, just like other ops.
gramalingam(2022-04-25 20:27:06):Hi, the shape inference code has issues that must be addressed, since we cannot assume that the rank/dimensions of an input are known during shape-inference time. Please see this PR who updated documentation on shape-inference to help clarify a few things. Please do let me know if you have any questions/suggestions. Thanks!
gramalingam(2022-04-25 20:27:41):Forgot the PR link: https://github.com/onnx/onnx/pull/4163

xadupre(2022-04-26 12:32:45):I would choose to support multidimensional. We need to choose how to handle complex. Most of these operators return complex. Three options for the return: complex tensor, real tensor + 1 dimension, 2 tensors (real + imaginary parts). Using complex seems the best choice but we need to add new operators (real part, imaginary part, conj). Otherwise we may keep real for now and extend it later to complex.
gramalingam(2022-04-27 16:57:56):@wuerflts : do you see a need for 2D or 3D, or generic N-D? Do you favor separate ops, or a single op for all dimensions?
xadupre(2022-04-27 17:22:01):Where is `n_fft` defined?
gramalingam(2022-05-03 23:40:55):Here, the specific `dim_value` may not be known.
gramalingam(2022-05-03 23:42:06):Here too: must check `hasInputShape` first.
gramalingam(2022-05-03 23:45:14):Check `hasInputShape`
gramalingam(2022-05-03 23:46:00):`dim_value` may be unknown here
gramalingam(2022-05-03 23:48:27):Here too `dim_value` may be unknown
smk2007(2022-05-05 01:25:37):Updated to include hasInputShape checks.
smk2007(2022-05-05 01:26:55):I think multidim fft can be added if need is there in a separate PR. I am okay either way.
smk2007(2022-05-05 01:28:57):cast to float
smk2007(2022-05-05 01:29:23):Update in the code, this will be generated at the end and reflected in this file.
smk2007(2022-05-05 01:29:56):Updated to allow negative values. The axis is defaulted to 1.
smk2007(2022-05-05 01:30:05):done
smk2007(2022-05-05 01:30:26):added check.
smk2007(2022-05-05 01:30:34):added check.
smk2007(2022-05-05 01:30:43):added check.
smk2007(2022-05-05 01:30:52):Added check.
smk2007(2022-05-05 01:31:07):added check.
gramalingam(2022-05-06 21:15:45):This is a bit different from the usual handling (eg.: see: https://github.com/onnx/onnx/blob/dad79aca26a276a7b8175b9add0614b5ebaa6f89/onnx/defs/controlflow/old.cc#L156 ). Also, doesn't look like modulo will work as needed here for C++? (https://stackoverflow.com/questions/11630321/why-does-c-output-negative-numbers-when-using-modulo )
gramalingam(2022-05-06 21:25:48):Why this special case handling? Is there some compatibility or convenience reason? Whether the input is real or complex is typically independent of the rank of the input tensor. We could have 2D or 3D real-values, for example. Using different conventions for 2D reals and 3D reals is not a good idea.
gramalingam(2022-05-06 21:27:01):Uniform convention would be always use one extra dimension (with a value of 1 for real-inputs and 2 for complex-inputs), regardless of whether rank is 2 or greater.
smk2007(2022-05-06 21:28:35):This should have been
    auto axis_idx = (input_shape.dim_size() + axis % input_shape.dim_size()) % input_shape.dim_size();
this will allow for arbitrary index with wrapping.

Is this behavior expected?
gramalingam(2022-05-06 21:29:03):Please explicitly specify the order. Eg., if real comes first, then I suggest adding the words `in that order`
smk2007(2022-05-06 21:31:33):This is here because the existing contirib op in onnxruntime allows the special case of 1d reals that do not have a complex dimension. The general rule however is to always have a complex dimension.
gramalingam(2022-05-06 21:43:06):This will likely cause confusion. I think we should try to make the spec simple. I suggest that we choose between the two simple options we have:

Option 1: Always use 1 extra-dimension as discussed above (no special cases).
Option 2: Always use a (new) attribute to indicate if the input is real or complex. If the input is real, no extra-dimension is used, and an extra-dimension is used only for complex.

Will neither of these options work?
gramalingam(2022-05-06 21:46:02):No. All other ops require axis to be in the range [-rank, rank-1] and reject an axis greater than rank. So, it is preferable that this op do the same. Thanks.
smk2007(2022-05-06 21:50:20):Option1 works. I only documented the behavior with the extra dimension.
I can remove the shape inference special case - which is a little hidden easter egg right now.
smk2007(2022-05-06 22:00:15):Im assuming that this is a numpy convention?
Seems kind of arbitrary to restrict the range.
gramalingam(2022-05-06 22:56:15):The if-condition should also be dropped, right?
gramalingam(2022-05-14 00:40:08):I don't think we need this condition and can drop it?
gramalingam(2022-05-14 00:44:00):I think that 
(a) this if-statement can be moved into the then-branch above, after line 3531.
(b) we should add an else-branch to this condition, clearing value and param (like lines 3561/62)
(c) we can drop the else-branch above in line 3532 (because in that case, we use the input-dim as-is).
Do the changes make sense?
gramalingam(2022-05-14 00:46:29):IIUC, the `periodic` attribute affects the size by 1?
smk2007(2022-05-16 16:48:13):I believe that it merely affects the computation. Per the PyTorch documentation:

"torch.hann_window(L, periodic=True) equal to torch.hann_window(L + 1, periodic=False)[:-1])"

which suggests that the size of the returned window is always the user specified window length, but the values of a N-length periodic window will be the same as the first N elements of a N+1 symmetric window.


smk2007(2022-05-16 17:24:21):dft_length is optional, it may not be supplied.

So i figured there were 3 cases.
1) Not specified -> will wont have InputShape
2) Specified, but not constant -> getInputData will fail
3) Specified, and constant -> getInputData will succeed

Is that not right?
gramalingam(2022-05-16 17:59:08):You are right, we must distinguish between the three cases. But I was confused by the use of `hasInputShape`. I think that is not equivalent to checking if a particular input is present or not (an input might be specified, but its shape may not be available; however, a type is always available for a specified input in a valid program). I suggest using the condition `(ctx.getNumInputs() >= 2) && (ctx.getInputType(1) != nullptr)` as a way to check if this input is specified. @jcwchen : Does this sound reasonable? Can we add a method `hasInput(i)` to `InferenceContext` that is defined to be equivalent to a check like above? 
gramalingam(2022-05-16 23:44:55):Since these suggestions correlate to the previous comment, maybe it is better if I describe the issues I think could be addressed (regardless of how the logic is structured). 
(a) The `return` in like 3535 could be made more precise. We cannot infer the dimension for the fft-axis, but the other dimensions can be propagated as is, correct?

xadupre(2022-05-18 13:03:16):Should we expose fft_length as well?
smk2007(2022-05-20 18:08:41):Updated!
smk2007(2022-05-20 18:09:32):I updated the previous check.
I left the return in there because it seemed weird to set a partially specified tensor shape.
smk2007(2022-05-20 18:11:10):it is an input
jcwchen(2022-05-20 23:26:22):```suggestion
        step = np.array((8)).astype(np.int64)
```

jcwchen(2022-05-20 23:28:26):```suggestion
        length = np.array((16)).astype(np.int64)
```
NumPy will use numpy.int32 by default on Windows and use numpy.int64 by default on Mac. Specifying the variable type should resolve this issue.
jcwchen(2022-05-20 23:28:35):ditto
askhade(2021-11-17 22:27:14):Let's revive this work?

garymm(2022-03-09 19:18:33):Don't need this because we have https://github.com/onnx/onnx/pull/3734
TomWildenhain-Microsoft(2021-10-05 00:40:12):I didn't make this change. Not sure why it is here.
jcwchen(2021-10-06 15:40:29):Hmmm it seems like an interesting bug. I cannot repro it from my end (Neither can the CIs).  I guess with certain Python version and platform `inspect` package behaves differently... (whether parse the comments in the end or not) Which Python version are you using?

Since I cannot repro it, could you please try the following commands from your end?
```
import inspect
def test_parse_comment():
   # Begin comment
   print('this is a test')
   # End comment
inspect.getsource(test_parse_comment)
```
My output is:
```
"def test_parse_comment():\n   # Begin comment\n   print('this is a test')\n"
```

I guess yours will include the last comment `# End comment\n`

A quick workaround will be moving these comments from the end of the files to the line before `expect()` function for `gather.py`, `onehot.py`, just like what other comments do

TomWildenhain-Microsoft(2021-10-06 18:35:34):Yes, this is what is happening.
gramalingam(2021-10-08 19:03:41):I don't understand this. Why doesn't this allow a scalar or 1D, just like Quantize? 
gramalingam(2021-10-08 19:05:23):I see there is no axis attribute. So, I guess 1D won't work. But how would one dequantize back if they used per-row or per-column quantization? Is this some missing functionality?
TomWildenhain-Microsoft(2021-10-08 20:19:35):This is the old opset 10 version. There was no per-row, only per-tensor at the time. 
neginraoof(2021-10-06 05:37:50):cc @askhade @gramalingam @jcwchen 
This PR would fix the optional issues submitted by @hariharans29 
I would appreciate your review, thanks!
jcwchen(2021-10-06 14:38:52):Reopen the issue to run release CIs
neginraoof(2021-10-06 18:30:46):> Reopen the issue to run release CIs

Thanks @jcwchen 
neginraoof(2021-10-11 18:39:04):Hi @jcwchen @askhade 
I'm seeing CI jobs cancelled with the error below:

[error]This is a scheduled Ubuntu 16.04 brownout. Ubuntu 16.04 LTS environment will be removed on October 18, 2021. For more details, see https://github.com/actions/virtual-environments/issues/3287
[error]The remote provider was unable to process the request.

Do you know if the CI environment needs updates?




jcwchen(2021-10-11 18:48:51):> Hi @jcwchen @askhade I'm seeing CI jobs cancelled with the error below:
> 
> [error]This is a scheduled Ubuntu 16.04 brownout. Ubuntu 16.04 LTS environment will be removed on October 18, 2021. For more details, see [actions/virtual-environments#3287](https://github.com/actions/virtual-environments/issues/3287) [error]The remote provider was unable to process the request.
> 
> Do you know if the CI environment needs updates?

Thanks for catching this. Will fix it by https://github.com/onnx/onnx/pull/3767.
neginraoof(2021-10-20 18:48:31):@jcwchen Thanks, would you be able to merge the PR?
gramalingam(2021-10-11 23:11:31):change name to ```input_opt_type```
gramalingam(2021-10-11 23:28:31):Do we really need these methods in the include file? I think it would be better if we try to keep the include-file minimal. Ideally, we should have only ```void propagateElemTypeWithValidation(const TypeProto* input_type, TypeProto* output_type)``` signature in the include-file, and move these other functions and their implementation into shape_inference.cc
matteosal(2021-10-08 14:47:06):Nice, this removes quite a lot of boilerplate code! Some comments/ideas:
* At this point, `remove_consumed_inputs.h` could also be removed because it's only used in one place, `reshape_4_5.h`. 
* Thinking about further factoring inspired by this, another common operation performed in the version converter is to turn static attributes into runtime inputs. There's already some factoring happening for that (e.g. `AxesAttributeToInput`), but it would be nice to add `NodeTransformerFunction` helpers to do this.
* Going even further, a system to combine `NodeTransformerFunction`s would make us able to express adapters like `reshape_4_5.h` as a combination of the existing `RemoveAttribute` + the helper I mentioned in the previous point
gramalingam(2021-10-08 21:39:07):@matteosal : thanks for the suggestions. Yes, I am planning on doing the extensions you suggest, probably as a separate PR.
askhade(2021-10-12 22:29:35):Thanks for this PR. I especially like 
>(We can even eliminate the dependence on the IR if desired in the future.)

Do we have enough test coverage for version converter specially to test this change?
matteosal(2021-10-08 14:26:05):There's one thing that our importer tests have caught when using this, which in the end looks like not being a problem with ths PR but it's worth double checking. This model upgrades just fine on master, but fails in this branch: https://github.com/onnx/models/tree/master/vision/body_analysis/arcface
Error is 
```
RuntimeError: /home/matteo/Git/onnx/onnx/version_converter/adapters/transformers.h:30: operator(): Assertion `node->i(attr) == value` failed: Attribute spatial must have value 1
```
and it's coming from this line. Now all of this looks correct, because `spatial = 1` is indeed not supported in opset >9 and the problem is in the old adapter because this looks like a malformed assertion that isn't actually throwing anything: https://github.com/onnx/onnx/blob/master/onnx/version_converter/adapters/batch_normalization_8_9.h#L26

However, upgrading this model on master actually produces a legal model, because even though `spatial` is 0 the shapes are set as if it was 1 in all batchnorm nodes. For example:
```
>>> original_model.graph.node[4]
input: "conv0"
input: "bn0_gamma"
input: "bn0_beta"
input: "bn0_moving_mean"
input: "bn0_moving_var"
output: "bn0"
name: "bn0"
op_type: "BatchNormalization"
attribute {
  name: "epsilon"
  f: 2e-05
  type: FLOAT
}
attribute {
  name: "momentum"
  f: 0.9
  type: FLOAT
}
attribute {
  name: "spatial"
  i: 0
  type: INT
}
>>> original_model.graph.input[4]
name: "bn0_gamma"
type {
  tensor_type {
    elem_type: 1
    shape {
      dim {
        dim_value: 64
      }
    }
  }
}
```
The above batchnorm has `spatial = 0` but the its `bn0_gamma` input is a channel-wise vector, which is what's expected from `spatial = 1`. In addition, the checker doesn't spot this problem.

So it seems that this didn't uncover a problem with the current change, but actually 3 old problems:
1) This model is invalid because batchnorms have `spatial = 0` but are set with inputs for `spatial = 1`
2) The checker doesn't catch that the model is invalid
3) That assertion in the adapter on master is malformed
gramalingam(2021-10-08 21:37:37):@matteosal : thanks for the detailed analysis. There is one special-case where "spatial" may not matter, and that is if the input is 2-dimensionsal with shape [N,C]. I checked in the above example, but the input "conv0" is 4-dimensional with shape [1, 64, 112, 112]. So,  it does look like the input model is erroneous. I wonder what is the fix? Perhaps we should update the model? @wenbingl ?
gramalingam(2021-10-12 18:00:37):@matteosal : would making this change cause any downstream problem? or, is it ok to merge this? Thanks!
matteosal(2021-10-13 16:13:20):No, that's completely fine for me thanks. It would be nice to update the Arcface model though. It is sufficient to flip all the `spatial` setting of batchnorms from 0 to 1 and leaving everything else unchanged (I can provide such a file). 
Also, in principle the checker or the shape inference should be improved so that they can catch this inconsistency, but since we are talking about opset 8 we probably don't care much anymore :)
jcwchen(2021-10-14 00:49:49):Original dropout_6_7 adapter is ==1, is it same as !=0 here?
matteosal(2021-10-19 12:41:41):@gramalingam I have fixed the model by flipping the `spatial` flags: https://drive.google.com/file/d/10zVLKh6HUhbm7HLnuQXz2cKzLxo9L4-3/view?usp=sharing
Now it upgrades correctly via the version converter. Whoever is in charge of it can update it now.
gramalingam(2021-10-07 16:53:45):Thanks for the fix ... I guess it is a redo of the outdated PR: https://github.com/onnx/onnx/pull/3459 ... we can close the other one, once this is merged in.
matteosal(2021-10-07 13:44:43):Not sure where this is coming from. Is there something wrong with the `defs/gen_doc.py` script?
gramalingam(2021-10-07 16:46:26):There is a similar discussion in https://github.com/onnx/onnx/pull/3754 ... can you try/check that (fix) out?
matteosal(2021-10-08 09:56:51):That didn't work, I have just manually removed the change
gramalingam(2021-10-07 19:52:05):Thanks for this PR. The documentation file is auto-generated from the opschemas. The documentation change should be made to the opschema definition (here: https://github.com/onnx/onnx/blob/767f752829f83dbc9bd0a364d6138890f667fc38/onnx/defs/nn/defs.cc#L788 ) and the documentation generated (after installation; eg.: see: https://github.com/onnx/onnx/blob/767f752829f83dbc9bd0a364d6138890f667fc38/tools/update_doc.sh#L32 )
gramalingam(2021-10-08 00:35:13):I am unclear why the changes in the generated documentation includes other things unrelated to the real change made to the opschema.
AlexandreEichenberger(2021-10-08 00:36:20):@gramalingam Its my first contribution to this repo, and I am a bit surprised by the number of failures associated with a doc change. Is there something that appears like an obvious error on my part?
gramalingam(2021-10-08 00:41:35):I think the CI failures are likely because of the same question I asked above: the generated documents should differ only in the text you changed. But it includes many other things. I am not sure what is causing this. May be it is a build-problem? I have seen problems that disappeared if I did a clean build from scratch.
gramalingam(2021-10-08 00:44:14):@askhade or @jcwchen, any other suggestions?
gramalingam(2021-10-08 00:49:32):The generated documentation suggests that the generator is accessing older and out-of-date opschemas for other ops ... either because of build issues or because the source is not the latest.
jcwchen(2021-10-08 03:16:05):I saw a lot of old changes -- like removing Castlike, moving bool too np.bool. It seems that it was produced by an old ONNX because the update from `defs/nn/defs.cc` was not included either.

@AlexandreEichenberger could you please make sure you have uninstalled all pre-installed ONNX, `python setup.py install` and `python onnx/defs/gen_doc.py`? Then `gen_doc.py` should take your update from the right ONNX you just built. Thanks!
askhade(2021-10-08 04:09:49):@AlexandreEichenberger I suggest running ```pip install -e . ```from onnx source dir. 

This is mostly because you have an old version of onnx installed.
AlexandreEichenberger(2021-10-08 13:56:26):Thanks for all the suggestions. Indeed, we are using an older onnx version for onnx-mlir. I need to "disconnect" it
AlexandreEichenberger(2021-10-08 14:47:48):@gramalingam @askhade or @jcwchen I now see that a lot of the process was explained in the contributing page... My bad, thanks for the help. Looks like its passing now :-)
gramalingam(2021-10-07 19:57:33):minor nit: may be we can change "is a multiple" to "must be a multiple" or "is expected to be a multiple"? Just to distinguish input-invariants (expected by the op implementation) from output-invariants (guaranteed by the op implementation).
AlexandreEichenberger(2021-10-07 20:32:35):Sorry for the not identify the right place to make the changes. Looked at past PRs but must have not identified it correctly. Will make your requested change too, it like it better than the original text.

FYI: changed the "must be" to "should" as the prior constraint used the "should". If "must" is preferred, will change both.
AlexandreEichenberger(2021-10-07 20:50:49):@gramalingam Note that M % G == 0 is a requirement on the inputs, since W is an input, and G is an attribute. They eventually get reflected in the output sizes too.
askhade(2021-10-21 20:28:27):"propagateElemTypeWithValidation" will need change too
Can you merge with master, there are some changes from Negin's PR which need to be pulled in.
askhade(2021-10-21 20:47:33):"AddExistingSymbolicDims" also needs to be updated for it to pick up existing symbols from map type proto
gramalingam(2021-10-27 20:39:04):This looks fine, thanks! I do have a general question/comment though: there are two reasons why all of this code cannot be replaced by a simple copy-constructor assignment like:
```cpp
   *targetTypeProto = *sourceTypeProto
```
One valid reason is that we want to propagate the type, but not the embedded shape (if any). A second reason is the "validation" part. I am not quite sure if we need it here (in the context of op/node level shape-inference ... we do need it in graph-level shape-inference which happens elsewhere). This relates to how `InferenceContext.getOutputType()` works ... does it (a) Return the original-type-and-shape (known from the caller's context), or (b) Return an empty placeholder to hold the output result? I think (a) requires the node-level inference to do "validation". But (b) enables the caller to do this validation, and it is not required in the node-level inference. It would be useful to check if ORT and the ONNX graph-level inference code do the same thing in this respect or not.
This is a suggestion for future cleanup, not for this PR.
gramalingam(2021-10-27 20:48:35):Specifically, the future cleanup might just be clarifying this part of the contract in the documentation of `InferenceContext` if we decide to retain the validation, or dropping the validation if that is preferable.
askhade(2021-10-22 21:05:23):add this to shape_inference.cc?
jcwchen(2021-10-22 23:38:14):Updated. Thanks
askhade(2021-10-13 15:34:47):@postrational can you list the changes we are picking for 1.10.2 in this wiki - https://github.com/onnx/onnx/wiki/Logistics-for-ONNX-Release-1.10

@jcwchen : Can you make sure we are picking the recent shape inference fix for shape op
gramalingam(2021-10-14 00:52:53):I agree that this is better for a scalar. However, I wonder if we should worry about backward compatibility here (in the op spec). Is it possible that there are models using a rank-1 tensor with one element in this situation? @postrational , any thoughts on this? Thanks!
jcwchen(2021-10-14 14:58:52):That's a good point. According to this PR: https://github.com/onnx/onnx/pull/2224, I think in all times for OneHot-11 and OneHot-9 do allow rank-1 tensor. (Even in the beginning OneHot-9 only allows rank-1 tensor) Therefore it's "possible" that there are some existing models using rank-1 tensor?

Ideally ONNX should fix it while bumping OneHot-9 to OneHot-11, but somehow it didn't... Three options here:
1) Fix it and opset bump (OneHot-16) this time
2) Fix it in the future when there is another opset bump for other reason
3) Directly fix it without a opset bump

I am good with 1) and 2), and 3) doesn't sound like a good idea.

askhade(2021-10-20 22:20:12):The spec states depth should be a scalar. The shape inference however initially only allowed 1D tensor of length 1. We made the fix a while back to allow scalars, but we kept the support for 1D tensors of length 1 for maintaining backward compatibility. So today while the spec states depth should be a scalar, shape inference allows both.

This PR only updates the tests to use scalar so that they are in sync with the spec, this is not a bc breaking change.

Having said that we should bring up the topic of how to treat scalars and 1D tensors of length 1 in the next op sig meeting. This is an open item for a while now... 


CLAassistant(2021-10-19 14:09:53):[![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/onnx/onnx?pullRequest=3778) <br/>Thank you for your submission! We really appreciate it. Like many open source projects, we ask that you all sign our [Contributor License Agreement](https://cla-assistant.io/onnx/onnx?pullRequest=3778) before we can accept your contribution.<br/>**2** out of **3** committers have signed the CLA.<br/><br/>:white_check_mark: Liuyu198701<br/>:white_check_mark: gramalingam<br/>:x: Yu Liu<br/><hr/>**Yu Liu** seems not to be a GitHub user. You need a GitHub account to be able to sign the CLA. If you have already a GitHub account, please [add the email address used for this commit to your account](https://help.github.com/articles/why-are-my-commits-linked-to-the-wrong-user/#commits-are-not-linked-to-any-user).<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/onnx/onnx?pullRequest=3778) it.</sub>
askhade(2021-10-20 21:13:25):Linux CI
1. Since linux CI includes comprehensive testing also add python 3.8 and 3.9 versions
2. For linux CI - there are 2 configurations for each version of python - I suggest flipping onnx-lite flag between 0 and 1 for each. Per my understanding there are partners who are interested in proto full version and there are other who are interested in lite. 

askhade(2021-10-20 21:24:11):Windows CI:
add python versions : 3.6, 3.7, 3.8, 3.9
For this CI I dont think we need 2 configurations for each verison. We should just test with onnx_ml = 1 (which is the default). Just make sure verify proto3 is set for atleast 1 config, alternatively you can add 2 configs for say python 2.8 and set verify proto to 1 for one of those.
askhade(2021-10-20 21:09:26):4 month cadence... we plan to do 3 releases per year
askhade(2021-10-20 21:11:46):why are you removing this?
askhade(2021-10-20 21:28:00):Naming suggestions
When -> When it runs
Purposes -> break this column into 3 Description, Config and Tests
Description can include Linux CI or Release pipeline (if you think Description is not needed feel free to omit it) Config can include the configs enabled and Tests can include all the tests which are run as part of this CI
askhade(2021-10-20 21:37:39):Any PR -> Every PR
askhade(2021-10-20 21:38:12):What does branch/weekly mean?
askhade(2021-10-20 21:38:54):test will run instead of will be run
askhade(2021-10-20 21:39:06):run -> trigger
askhade(2021-10-20 21:39:30):cautious -> caution

askhade(2021-10-20 21:40:37):what does Test Conda mean?
askhade(2021-10-20 21:41:28):ONNX C++ tests -> Only ONNX C++ tests 
because c++ tests are being tested in every CI, this one only tests c++ tests and nothing else like pytest 
askhade(2021-10-20 21:42:26):What does uploaded node models exactly mean?
askhade(2021-10-20 21:43:58):Verify ONNX with the latest NumPy
Verify ONNX with the latest Protobuf
Verify ONNX with minimum supported Protobuf

These can be reworded as Verify with different dependency versions - latest numpy version, latest and min supported protobuf version
askhade(2021-10-20 21:46:15):can delete Kind column
askhade(2021-10-20 21:46:53):if you change Any PR to every PR this sentence is not required
jcwchen(2021-10-27 20:05:14):IIUC, this flag is not used. Please let me know if it is needed.
jcwchen(2021-10-27 20:55:49):I changed it to "Test building ONNX in conda environment".
jcwchen(2021-10-28 02:11:43):- Change branch/weekly to "main branch, release branch, weekly"
- Change uploaded node models to "Verify backend node data"
jcwchen(2021-10-28 03:10:10):Good point. I removed Kind and add Config/Tests
liqunfu(2021-11-18 00:44:27):master branch? 
jcwchen(2021-11-18 03:57:59):Good question. I remembered @askhade proposed ONNX should use main branch instead of master branch eventually. For some reasons, it has not been done now. I think this rename thing should be put on the table again. Thus, for now, I prefer to use "main branch" in new document/code.
askhade(2021-12-08 18:06:18):Please note that ort-nightly does *not* support
askhade(2021-12-08 18:07:05):add a note that this incurs quota usage that is the reason this test is restricted to only 1 pipeline.
askhade(2021-12-08 18:08:38):I don't understand what is being conveyed under (1)
Specially these 3 sentences:
After a PR merges into main/rel-* branch, the CI will be run.
    * These release CIs will be run weekly (Sunday midnight) and release Python wheel to [onnx-weekly](https://test.pypi.org/project/onnx-weekly/) package in TestPyPI.
    * The PR to merge into rel-* branch will be run because they are supposed to be released soon.


jcwchen(2021-12-08 20:23:27):Sorry it looked confusing. I have added a subtitle "When the release CIs will run" in (1) and also polish those 3 sentences. Hope it looks better and clearer now. Please review it again. Thanks!
xuzijian629(2022-01-14 00:15:23):> Basically LGTM. Thank you for the enhancement! Only a nit comment: I am thinking perhaps we can put all data propagation related tests to a new python file under onnx/test since current shape_inference_test.py focuses on pure (without data propagation) shape inference and it's quite huge.

Thank you for the reviews!!
I agree with your opinion to separate tests with data propagation. Since the modification of the test file could be large and unrelated to ConstantOfShape, I'll create another PR (todo: https://github.com/onnx/onnx/issues/3944).
gramalingam(2022-01-12 22:44:30):I think the above 2 lines can be dropped.
xuzijian629(2022-04-29 01:48:33):@gramalingam Could you review this PR, please?
xuzijian629(2022-04-29 03:09:13):I was not considering the case when inputs contain zeros.

Let me do some fix and refactoring..
xuzijian629(2022-04-29 03:37:48):Since the change would be large, I'll remake a PR when completed. Thanks.
askhade(2021-10-20 16:21:20):Have you tested this with ORT?

jcwchen(2021-10-20 18:44:40):> Have you tested this with ORT?

Yes they were verified by `ort_test_dir_utils.run_test_dir`
jcwchen(2021-10-21 03:39:35):cc @hariharans29. Thank you for catching this issue!
askhade(2021-10-22 19:52:13):Can we close this PR since now it is clear this is a wrong fix? 
jcwchen(2021-10-22 21:27:47):After offline discussion with @askhade and @hariharans29, actually the [enhanced check](https://github.com/onnx/onnx/pull/3702) for ParseData in ONNX is totally fine. Instead, [AttentionFusion](https://github.com/microsoft/onnxruntime/blob/0510688411db8cff48722fa9a8a3bd0d62dc0f45/onnxruntime/core/optimizer/attention_fusion_helper.h#L325) from ONNXRuntime did not parse uint8 in the right way. Please note that current ONNX does not support ParseData for uint8 tensor. I will propose another PR to check string if tensor has raw_data. Close this PR now because it is not an ONNX issue. Thanks @askhade for pointing out! 
hariharans29(2021-10-21 04:47:48):nit: `supported_tensorproto_datatypes` to convey intent clearly ?
askhade(2021-10-21 16:34:01):I think it is better move this check inside the else if block on line 57. This is because this check is applicable when we are
parsing data from a typed field. 
When data is stored as raw data then you should add another check to validate that the data type is not string :
> // When this field is present, the data_type field MUST NOT be STRING or UNDEFINED
  bytes raw_data = 9;
jcwchen(2021-10-21 21:18:52):Sounds good. Updated. Thanks!
jcwchen(2021-10-21 21:37:53):> I think it is better move this check inside the else if block on line 57. This is because this check is applicable when we are

Did you mean moving `if (tensorproto_datatypes.count(tensor_proto->data_type()) == 0)` into L57 that only checking it when raw_data does not exist?
Actually I think this data_type check can benefit no matter the tensor has raw_data so we should keep it in the same place. For instance, it can check parsing a int32 tensor which uses raw_data to store data and however it should be parsed by int64. Please correct me if I am wrong. Thanks!

> When data is stored as raw data then you should add another check to validate that the data type is not string :

Good idea. I added the check to disallow STRING data_type if raw_data exists. Also thanks for the reminder of String because previously I missed handing STRING...  Just added one more case as below:
`DEFINE_PARSE_DATA(std::string, string_data, string_support_types)`
askhade(2021-10-21 22:41:23):this is not right... it is true that multiple types can be stored in say int32_data field but the datattype of the tensor will be the intended data type... the check in initial code is right
askhade(2021-10-27 16:34:57):#3807 also adds rank inference as fallback. It is much simpler can you please check. Thanks!
xuzijian629(2022-01-11 02:24:17):@askhade I followed #3807 and simplified the implementation. Sorry for sooo late response..
gramalingam(2022-01-25 21:21:09):LGTM, thanks! A minor comment: I think we would want to use the same logic for any input that denotes a shape (in any op). So, extracting this as a generic utility may be useful. Eg., a function like:
```
   TensorShapeProto getShapeInput(InferenceContext& ctx, size_t input_index);
```
xuzijian629(2022-01-26 01:34:33):> LGTM, thanks! A minor comment: I think we would want to use the same logic for any input that denotes a shape (in any op). So, extracting this as a generic utility may be useful. Eg., a function like:
> 
> ```
>    TensorShapeProto getShapeInput(InferenceContext& ctx, size_t input_index);
> ```

Thank you for the suggestion. I have a question about the implementation.
How do you handle the case when the shape input has neither initializer nor symbolic data? When the shape input does not even have shape, we cannot do anything, i.e., we cannot return a `TensorShapeProto`.
Maybe it's better to prepare a function like `setShapeInputData(InferenceContext& ctx, size_t input_index, TensorShapeProto* target_shape)` to avoid such situation?
gramalingam(2022-01-26 01:58:35):Good question! Your suggestion is fine, but I think we will need to return a boolean value to indicate whether a value was found or not.

I think this case is not being handled correctly by the existing code, so it probably needs to be fixed anyway. I think the existing code uses the default value assigned to ```TensorShapeProto second_shape;``` which has zero dimensions, which is incorrect. If no information is available, we should skip setting the output shape.
xuzijian629(2022-02-02 07:04:59):@gramalingam Sorry for late response! I implemented `getShapeInput` and used it in the shape inference of Expand :+1:
gramalingam(2022-03-17 23:35:44):Hi @xuzijian629 : thanks for the update! Would it be possible to fix the conflicts, so we can merge this in? Thanks!
xuzijian629(2022-03-22 02:43:23):Hi @gramalingam,
I resolved the conflicts and merged main.

memo: https://github.com/onnx/onnx/pull/4019/commits/37ae3126ce5e480931d5e52089230cec80ab5b5d returns early if symbolic input does not have dim_value. In such case, `GetShapeInput` returns with `found == false`, so already handled.
gramalingam(2022-03-23 17:31:55):Great, thanks very much @xuzijian629 
jcwchen(2022-03-23 17:44:35):@gramalingam Could you please sign-off approval for this PR again and let's merge this one? Thanks!
jcwchen(2022-01-15 18:15:06):What is this else for? (Future note?)
xuzijian629(2022-01-22 02:32:55):Sorry, this was just a mistake when I manually merged main
askhade(2021-10-22 19:38:19):LGTM thanks! 

@gramalingam, @postrational , @kevinch-nv , @AlexandreEichenberger  comments?
gramalingam(2021-10-22 22:32:11):Thanks for the clarification @TomWildenhain-Microsoft ... this is really helpful!
kevinch-nv(2021-10-22 20:23:44):```suggestion
Mathematically, a tensor can be defined as a pair of sequences/lists (V, S) where S is the shape of the tensor (a list of non-negative integers) and V is a list of values with length equal to the product of the dimensions in S. Two tensors (V, S) and (V', S') are equal if and only if V = V' and S = S'. The length of S is referred to as the rank.
```
gramalingam(2021-10-22 22:28:20):Can we split the second sentence into a separate point? And may be make it a more general claim, by saying:
```
- A shape is always represented by a tensor of rank 1 (that is, a vector)
- The shape of an empty vector is [0]
```
gramalingam(2021-10-22 22:31:22):This is an interesting trivia / tidbit, but is it really relevant? Just wondering if there is some underlying confusion/concern this is trying to address.
TomWildenhain-Microsoft(2021-10-25 18:27:01):Removed the last point. It was from a post explaining this. Just a fun fact.
The "shape of the shape of as scalar" thing is to point out a specific case where a tensor may have a dimension of 0.
askhade(2021-10-22 18:30:47):Can you share an example model so that I understand the use case better. 

In your description you mention sometimes output of scatter is "shape" input to Reshape. 
What is the order of ops? some op -> Scatter -> Reshape or Shape->Scatter-> Reshape? If the order matches 1 then your changes wont work. The data propagation function is meant for propagating the shape data and not the actual outputs... If the output of scatter is treated as shape input then 
a.) if the input to scatter is constant (initializer) then you can run constant folding
b.) if input to scatter is not known then you cant do anything, the shape inference will fall back to rank inference

askhade(2021-10-22 18:32:22):Please add tests here: https://github.com/onnx/onnx/blob/master/onnx/test/cpp/data_propagation_test.cc
xuzijian629(2021-10-25 07:54:09):Hi askhade, thank you for your comments. Sorry that I was confusing data propagation with shape inference when I submit my first change😔
But I still need data propagation for ScatterND, and I explain why it's necessary in this comment.

The personal motivation for adding this data propagation is to support generating backward graph for Gather.

Consider a simple example `Gather<axis = 0>(data, indices)` where `data.shape = [3,4,5]` and `indices = [0,1]`. Then, the output shape is `[2,4,5]`. When the output grad `grad@out` of shape `[2,4,5]` is obtained, the input grad `grad@data` is computed as `IndexAdd<axis = 0>(zeros_like(data), indices, grad@out)`. Since we don't have the `IndexAdd` op in ONNX, please consider it as a pseudo operator (cf: [pytorch definition](https://pytorch.org/docs/stable/generated/torch.Tensor.index_add_.html#torch.Tensor.index_add_)).

Things become a little more complicated when indices has multiple dimensions. Consider when `indices = [[0,1],[2,1]]` for example, and the output shape will be `[2,2,4,5]`. **When the indices input in IndexAdd can only be 1D tensor** (as in pytorch), we cannot compute `grad@data` in the same way as above. However, even in such cases, by temporarily reshaping indices to `[0,1,2,1]` (then the output will be reshaped to shape `[4,4,5]` correspondingly), we can use the same approach as the first case. More formally,

```
indices_flatten = Reshape(indices, Constant([-1]))  // shape: [4]
shape = Shape(data)
new_output_shape = ScatterND(shape, [axis], Size(indices))  // [4,4,5]. new_output_shape is shape except for new_output_shape[axis] = Size(indices)
output_grad_reshaped = Reshape(output_grad, new_shape)
IndexAdd<axis = 0>(zeros_like(data), indices_flatten, output_grad_reshaped)
```

We need data propagation in the above ScatterND, whose output is consumed by the second input of Reshape.
xuzijian629(2022-04-29 01:54:13):I worked around this issue by a better gradient generation and this fix is no longer needed.

If someone needs more discussion, please reopen this. Thanks.
jcwchen(2021-10-23 00:03:35):license/cla is pending... Close this PR for a while and will reopen it
gramalingam(2021-11-03 22:21:53):Why is this "void *" ? Why not "onnxExtensionFunctionPointer", which is the declared type of *function?
gongsu832(2021-11-04 03:39:22):Because that would produce the same warning as the implicit cast before due to the different function signature.
```
[ 23%] Building C object third_party/onnx/CMakeFiles/onnxifi_dummy.dir/onnx/onnxifi_dummy.c.o
/root/Downloads/onnx-mlir/third_party/onnx/onnx/onnxifi_dummy.c: In function ‘onnxGetExtensionFunctionAddress’:
/root/Downloads/onnx-mlir/third_party/onnx/onnx/onnxifi_dummy.c:177:23: warning: cast between incompatible function types from ‘onnxStatus (*)(void *, const char *, onnxStatus (**)(void))’ {aka ‘int (*)(void *, const char *, int (**)(void))’} to ‘onnxStatus (*)(void)’ {aka ‘int (*)(void)’} [-Wcast-function-type]
  177 |           *function = (onnxExtensionFunctionPointer)&onnxGetExtensionFunctionAddress;
      |                       ^
/root/Downloads/onnx-mlir/third_party/onnx/onnx/onnxifi_dummy.c:180:23: warning: cast between incompatible function types from ‘onnxStatus (*)(void *, uint32_t,  const onnxTensorDescriptorV1 *, uint32_t,  const onnxTensorDescriptorV1 *, onnxMemoryFenceV1 *)’ {aka ‘int (*)(void *, unsigned int,  const onnxTensorDescriptorV1 *, unsigned int,  const onnxTensorDescriptorV1 *, onnxMemoryFenceV1 *)’} to ‘onnxStatus (*)(void)’ {aka ‘int (*)(void)’} [-Wcast-function-type]
  180 |           *function = (onnxExtensionFunctionPointer)&onnxSetIOAndRunGraph;
      |                       ^
```
askhade(2021-10-25 16:31:55):@neginraoof FYI
chudegao(2021-11-02 03:43:40):@neginraoof can you help review?
hariharans29(2021-11-19 22:21:34):Sorry, I am not following what this change is achieving.

Previously the type attribute contains a TypeProto which is basically `sequence(tensor(float))` and there are no inputs to the `Optional` node. Looking at the spec - https://github.com/onnx/onnx/blob/master/docs/Operators.md#Optional, this is valid and this operator will produce an empty `optional(sequence(tensor(float)))` as is right ? Why do we need an empty sequence input flowing into the Optional node and also change the type to reflect this (I would have thought the type attribute is moot after the input comes into the picture). It seems to me like even as is, the Optional node will produce an empty optional sequence of float tensor and after this change it will still do the same.

And also not following the part about making the then and else output types match - I would have thought the types of `then_out` and `else_out` are the same even now - `optional(sequence(tensor(float)))`

Sorry if I am overlooking something very obvious. 
chudegao(2021-11-22 07:57:08):```
Previously the type attribute contains a TypeProto which is basically sequence(tensor(float)) and there are no inputs to the Optional node. Looking at the spec - https://github.com/onnx/onnx/blob/master/docs/Operators.md#Optional, this is valid and this operator will produce an empty optional(sequence(tensor(float))) as is right ? 
```
I don't think it's right. The output is still seqence as the node definition.
```
>>> ten_in_tp = onnx.helper.make_tensor_type_proto(onnx.TensorProto.FLOAT, shape=[5])
>>> seq_in_tp = onnx.helper.make_sequence_type_proto(ten_in_tp)
>>> opt_empty_in = onnx.helper.make_node(
            'Optional',
            inputs=[],
            outputs=['optional_empty'],
            type=seq_in_tp
        )
>>> 
>>> opt_empty_in
output: "optional_empty"
op_type: "Optional"
attribute {
  name: "type"
  tp {
    sequence_type {
      elem_type {
        tensor_type {
          elem_type: 1
          shape {
            dim {
              dim_value: 5
            }
          }
        }
      }
    }
  }
  type: TYPE_PROTO
}

>>> 
```

After the update, the output is optional(sequence):
```
>>> ten_in_tp = onnx.helper.make_tensor_type_proto(onnx.TensorProto.FLOAT, shape=[5])
>>> seq_in_tp = onnx.helper.make_sequence_type_proto(ten_in_tp)
>>> opt_in_tp = onnx.helper.make_optional_type_proto(seq_in_tp)
>>> seq_empty_node = onnx.helper.make_node(
            'SequenceEmpty',
            inputs=[],
            outputs=['seq_empty_out']
        )
>>> opt_empty_in = onnx.helper.make_node(
            'Optional',
            inputs=['seq_empty_out'],
            outputs=['optional_empty'],
            type=opt_in_tp
        )
>>> opt_empty_in
input: "seq_empty_out"
output: "optional_empty"
op_type: "Optional"
attribute {
  name: "type"
  tp {
    optional_type {
      elem_type {
        sequence_type {
          elem_type {
            tensor_type {
              elem_type: 1
              shape {
                dim {
                  dim_value: 5
                }
              }
            }
          }
        }
      }
    }
  }
  type: TYPE_PROTO
}


```

hariharans29(2021-11-22 19:17:34):Not quite following. The "type" attribute in the Node contains a sequence but the output type of the Node will still be an optional right ? 

See how the output type is stamped for the Optional node:
https://github.com/onnx/onnx/blob/34dbc3e3a8f4709d96e7ceba3ac38510f4ed2ff3/onnx/defs/optional/defs.cc#L53
chudegao(2021-11-24 03:55:10):@hariharans29 You are right. I misunderstand the usage of attr type. I revert the change. Please review. Thanks
hariharans29(2021-11-24 18:22:40):The change to feed an empty sequence into Optional is redundant because the behavior is still the same with or without it. Without it, an "empty" optional is created from the type attribute
chudegao(2021-11-25 01:59:00):Ok. I will update the backend implement to use the type to generate the right output. I will close this PR.
hariharans29(2021-11-25 02:47:18):Makes sense to me, thanks. It isn't that there is something wrong with your test case, it is just that nothing seems to be "broken" in the existing test that needs a "fix". AFAIK  per the operator spec, this test is valid. I implemented this op in onnxruntime and I was able to run the test fine.
askhade(2021-11-02 17:25:15):The change looks good... please add tests
jcwchen(2021-11-04 01:03:09):> The change looks good... please add tests

Good point. I added 2 tests for checking repeated perm for Transpose and empty string for QuantizeLinear. 

Regarding to test for ParseData with raw_data and string type, actually ParseData [does not support parse string type now](https://github.com/onnx/onnx/blob/1faae95520649c93ae8d0b403816938a190f4fa7/onnx/defs/tensor_proto_util.cc#L116) so the condition cannot be tested... If that is the case, do we still need to add the check here?
askhade(2021-10-26 18:22:41):nit: Attribute perm for Transpose has repeated entry for value fromIndex? or something similar?
askhade(2021-10-26 18:31:57):nit: tensor_proto->name() data type is string. string content is required to be stored in repeated bytes string_data field. raw_data type cannot be string. 
askhade(2021-10-26 18:34:56):this looks good. Wondering how can we avoid this issue in case of all optional inputs. Right now I think only QuantizeLinear does type propagation from optional input to output... but if tomorrow there is some other op which does the same then the type inference will have to take care of this condition. May not be worth it right now... @gramalingam thoughts?
jcwchen(2021-10-27 19:45:51):Yes I was thinking of it too... This situation can happen again in the future. Perhaps we can extend existing getNumInputs with more parameter to represent whether it disallows optional input for certain index (or a set of indexes)
jcwchen(2021-10-27 19:46:34):Both updated for improving error messages. Thanks
askhade(2021-10-27 16:33:50):Please add a shape inference test for this
askhade(2021-10-27 16:27:22):nullptr check for shape_initializer needs to happen before this line "shape_initializer->data_type()"
askhade(2021-10-27 16:29:14):Infact you can remove this check ... ParseData checks this: https://github.com/onnx/onnx/blob/master/onnx/defs/tensor_proto_util.cc#L42
TomWildenhain-Microsoft(2021-10-27 19:38:34):Why is dtype validation needed at all in shape inference? Does it not happen before shape inference automatically?
askhade(2021-10-27 19:53:47):> Does it not happen before shape inference automatically?

What do you mean by "before shape inference" and "automatically"?

If by before shape inference you mean during model load then no... this check does not happen. I need to check whether onnx checker covers this but one can run shape inference without running the checker. 

As to whether this check is really needed? consider this as sanity check and now since it is added to ParseData none of the node level shape inference functions are required to do this. 


TomWildenhain-Microsoft(2021-10-27 21:58:32):Cool. Added a test and removed the check.
jcwchen(2021-12-08 19:34:30):Thanks for the review. I have updated comments, used any_of and moved the negative axes condition into one place to simplify the code.
askhade(2021-11-17 22:25:54):Instead of throwing an error can we either work with the negative value (when possible) or fallbacl to rank inference? This is because some runtimes (example ORT) did allow negative values even in cases where the standard did not explicitly state it... I am afraid throwing an error here will break existing scenarios... 
jcwchen(2021-11-18 03:19:41):Good point. After offline discussion, to prevent breaking existing old models, ONNX should at least perform rank inference instead of throwing an error. Just updated. Also it can resolve the shape inference error from #2281. Please review it again. Thank you!
askhade(2021-11-18 05:34:49):You said: Also it can resolve the shape inference error from #2281. Please review it again.
Which error?
askhade(2021-11-18 05:38:35):Is this change complete? I only see you switched from throwing error for attr axis and not for start and end
jcwchen(2021-11-18 20:41:25):Oh sorry. I meant this one: https://github.com/onnx/onnx/issues/3565 Also just updated start and end. Thank you!
askhade(2021-12-08 18:17:43):adding this check as else if does not seem right... both the if condition above and this else if condition regarding negative axes check can be true at once right? 
gramalingam(2021-12-08 18:51:37):In light of the discussion in the thread, can we expand this comment and say
```
   Negative axes were not explicitly discussed in the spec before opset-10. Hence, they are
   officially not part of the spec, but some models/runtimes may use them. Hence, we 
   perform simple rank inference in this case.
```
gramalingam(2021-12-08 18:59:15):It seems to work ok (since we fail in the previous case).
gramalingam(2021-12-08 19:00:17):Replacing `! none_of` by `any_of` may make it slightly easier to read.
askhade(2021-12-08 19:03:12):aah right. I think only changing !none_of to any_of (for better readability) should be enough
askhade(2021-11-17 22:18:53):Why not simply skip propagating the empty output shapes downstream in graph level shape inference? Wouldn't that be a much simpler approach?
jcwchen(2021-11-18 21:24:35):> Why not simply skip propagating the empty output shapes downstream in graph level shape inference? Wouldn't that be a much simpler approach?

After offline discussion, for now we decide not to extend InferenceContext for detecting empty output during node-level shape inference. To keep it simple without changing interface, shape inference will detect it in graph-level and won't infer a shape for empty output.
askhade(2021-11-18 21:28:42):// Shape inference will happen even in case of empty optional outputs, 
// graph-level shape inference should not propagate the shape downstream for empty optional outputs. 
askhade(2021-11-18 21:32:24):suggest making this a separate condition and adding it above line 428. We should not even fetch the inferredType when output is empty.


jcwchen(2021-11-18 22:10:35):> suggest making this a separate condition and adding it above line 428. We should not even fetch the inferredType with output is empty.

Sounds good and I just moved the condition 

> On a separate note why is there no check for inferredType != nullptr? Can we safely assume ctx.getOutputType(i) never returns a nullptr?

For now I think it should be fine because the vector of output type will be initialized here: https://github.com/onnx/onnx/blob/master/onnx/shape_inference/implementation.h#L184
askhade(2021-11-20 03:21:39):change to : // skip type and shape propagation for missing optional outputs.
jcwchen(2021-11-02 00:18:12):The CI is good now. Close the PR here
lgtm-com[bot](2021-11-16 18:51:40):This pull request **introduces 1 alert** when merging 0fa7ac6aa554624acb51f435860393c9e6c19030 into bc4c4c48f664d4d08cabd30a57063588d7537408 - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-f83ccc1661dee1a607e763b5ac945c9892eb8de4)

**new alerts:**

* 1 for Unused import
lgtm-com[bot](2021-11-16 19:18:40):This pull request **introduces 1 alert** when merging 4368ce8d71b10fe130070094ed1f2da4a943893b into bc4c4c48f664d4d08cabd30a57063588d7537408 - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-85ca196c432d87f85d58c56772e817839d401820)

**new alerts:**

* 1 for Unused import
askhade(2021-11-18 00:09:58):Please add usage here: https://github.com/onnx/onnx/blob/master/docs/PythonAPIOverview.md#utility-functions
askhade(2021-11-20 03:20:19):> Please add usage here: https://github.com/onnx/onnx/blob/master/docs/PythonAPIOverview.md#utility-functions

Once this change goes in we can check in this PR. Thanks!
jantonguirao(2021-12-02 16:03:02):@askhade @gramalingam @jcwchen Thank you very much for your review.
I've now addressed all your comments. Could you please take one more look?
askhade(2021-11-10 19:56:28):add merge at model level as well? ir version, opset imports, model local functions all of these are part of the model proto and will need to me picked up in the resultant model or need some kind of merge or error checking.

For example in case of model local function, you can simply merge both the lists however in case of ir version or opset imports, some error checking should be done to make sure both the models\graphs are compatible for merging. 
jantonguirao(2021-11-11 16:55:20):I've added now merge_models (and renamed this to merge_graphs). Could you please check if the validation on OpSet and IR versions is enough?
askhade(2021-11-12 23:47:02):should populate  
1. producer_name and producer_version - Something like "onnx model merge utility", version 1?
2. domain and model_version - These can be user inputs

Merge m1 and m2 fields
1. metadata_props and functions

I dont think "training_info" is being used, we can drop this.


askhade(2021-11-12 23:49:27):is this enough? even when only rename_inputs is set to true you need to rename the corresponding node inputs right?
askhade(2021-11-12 23:49:59):same comment as above corresponding node inputs also need to be renamed
askhade(2021-11-12 23:51:33):when rename_edges is true : corresponding graph inputs and outputs also need to be renamed
askhade(2021-11-12 23:54:23):this is good but do check onnx parser... this utility makes it very easy to create onnx models for test purposes. Example: https://github.com/onnx/onnx/blob/master/onnx/test/parser_test.py#L27
askhade(2021-11-13 00:04:46):I am contemplating whether we should make the ir version requirement strict meaning both models should have same IR version. 
we may see issues if one of the models (belonging to older ir version) includes a deleted field. 
jantonguirao(2021-11-16 18:37:18):I think I covered that. Please check I didn't miss anything
jantonguirao(2021-11-16 18:38:08):Much easier with onnx.parser. Thanks for the suggestion. I updated the test code.
jantonguirao(2021-11-16 18:38:44):Updated. Now both models should have the same IR version
jantonguirao(2021-11-16 18:39:58):You are right. I've fixed it (and added tests)
askhade(2021-11-17 21:56:06):nit: type Inserts an Unsqueeze node
gramalingam(2021-11-30 22:38:38):What about name clashes between the two graphs? Eg., if they both use some temporary with the same name? Conceptually, it should be straight-forward to make the names distinct by adding some prefix/suffix in this case. Right now, it looks like this is not handled?
gramalingam(2021-11-30 22:42:29):This looks like a reasonable heuristic. However, I wonder if it would be useful to allow users to override this by explicitly indicating which outputs should be included as outputs of the merged-graph?
gramalingam(2021-11-30 22:50:39):minor nit: if we use the reverse of io_map, it may be more efficient to eliminate the outermost loop (line 58) and use a lookup here ```if name in reverse_io_map```. 
gramalingam(2021-11-30 22:51:33):There could be functions in the two models that use the same name, which would lead to a conflict here.
gramalingam(2021-11-30 22:54:04):I see a separate utility is added later for this. May be we could have some higher-level function that calls both (the renamer and merge_graphs)?
gramalingam(2021-11-30 22:55:27):Some initializers and value_infos may share the same name as graph inputs. If the graph-input is renamed, then these need to be handled too?
jcwchen(2021-12-01 20:43:35):```suggestion
        raise ValueError("model argument is not an ONNX model")
```
jcwchen(2021-12-01 20:48:27):minor nit for L48 & L50: `"Input {g2_in_name} is not present in g2"`
jantonguirao(2021-12-02 15:51:19):I've addressed that. Now both merge_graphs or merge_models accept optional prefix1 and prefix2 arguments, that if supplied will be used to rename all relevant names in each model/graph
jantonguirao(2021-12-02 15:51:39):I think I covered it now. Can you please check again?
jantonguirao(2021-12-02 15:52:21):I've added a name collision check. Is there anything else that can be done? Should we rename local functions, or just leave it as a limitation?
jantonguirao(2021-12-02 16:01:06):Done
jantonguirao(2021-12-02 16:02:04):Done. Added optional `inputs` and `outputs` argument, with the help extract_model utility
jantonguirao(2021-12-02 16:02:11):Done
jantonguirao(2021-12-02 16:02:20):Done
jantonguirao(2021-12-02 16:02:24):Done
gramalingam(2021-12-06 21:59:19):Thanks for all the other changes, which look good. Not sure I understand this one though (but I didn't explain the issue clearly either). Suppose g2 has an input X which also appears in the initializer list of g2.  Further, assume X is included in the io-map. Then, X will be omitted from the merged graph's inputs. So, it should also be omitted from the merged graph's initializer, right? So, may be line 208 should be more like line 197?
gramalingam(2021-12-06 22:01:55):This is helpful. Renaming local functions could be a helpful extension, but if not done now, it can be added later too.
jantonguirao(2021-12-07 09:54:53):I've now added renaming of local functions, including tests
jantonguirao(2021-12-07 09:56:16):I understand now. I've fixed handling of initializers, sparse initializers and value_infos.
gramalingam(2021-12-07 16:23:33):Great, thanks very much!
jcwchen(2021-11-08 20:59:02):Thank you for proposing this PR. Basically looks good to me. Shall we apply this doc rewording to Where-9 as well?
askhade(2021-11-08 21:40:12): it is bit of challenge to easily figure out what changed in a particular op version... while this is not an ideal solution the history section makes it easier for folks to understand what changed 
garymm(2021-11-10 00:58:08):Agreed. Filed https://github.com/onnx/onnx/issues/3834 to track a systemic solution.
For now I kept the History text here.
garymm(2021-11-10 00:58:15):Done.
gramalingam(2021-11-10 01:06:24):Thanks!
askhade(2021-11-08 17:15:00):Curious - Are you using ONNX's graph level shape inference? meaning are you using onnx shape inference apis? When using onnx graph level shape inference I don't know how you could land in this situation this because - either the input data is an initializer in which case all dimensions are dim values or the input data is dynamic in which case graph level shape inference will create a symbol for every unknown dim... (https://github.com/onnx/onnx/blob/master/onnx/shape_inference/implementation.cc#L141)

However, not all runtimes implement symbol generation and therefore it may be best to include your fix.
askhade(2021-11-10 19:35:38):@xuzijian629 : The PR looks good to me. I added a minor comment. As soon as you address it we can merge this. Thanks for your contribution.
askhade(2021-11-17 22:37:27):@xuzijian629: Any updates?
askhade(2022-01-05 23:28:14):Any updates?
xuzijian629(2022-01-06 23:46:43):Sorry, I missed the notification. I will soon make an update :bow:
xuzijian629(2022-01-09 13:02:18):> https://github.com/onnx/onnx/pull/3828#issuecomment-963380020

Yes, I encountered this problem when I was using ONNX's graph level shape inference.

I got the bug in the following part of a model.

![](https://github.com/xuzijian629/tekito/blob/main/%E3%82%B9%E3%82%AF%E3%83%AA%E3%83%BC%E3%83%B3%E3%82%B7%E3%83%A7%E3%83%83%E3%83%88%202022-01-09%2021.52.57.png?raw=true)

Since I cropped the graph, the inputs of `Gather` and `Sub` are hidden. Note that the inputs to `Unsqueeze`s are all 0-dimensional (i.e., scalar).
xuzijian629(2022-01-11 09:15:49):I summarized the above explanation into code.

Note that before running this, you may need to merge https://github.com/onnx/onnx/pull/3784 because the following code uses data propagation for ConstantOfShape.

```python
import onnx
from onnx import parser, helper, shape_inference

input = '''
   agraph (float[3, n] x) => ()
   {
        shape = Shape(x)
        one = Constant<value = int64 {1}>()
        scalar = Gather(shape, one)
        subbed = Sub(scalar, one)
        unsqueeze_dim = Constant<value = int64[1] {0}>()
        unsqueezed = Unsqueeze(subbed, unsqueeze_dim)
        concat_out = Concat<axis = 0>(shape, unsqueezed)
        out = ConstantOfShape(concat_out)
   }
'''
graph = parser.parse_graph(input)
original_model = helper.make_model(graph, producer_name='onnx-examples')
onnx.checker.check_model(original_model)

# Apply shape inference on the model
inferred_model = shape_inference.infer_shapes(original_model, data_prop=True)
onnx.checker.check_model(inferred_model)
print(inferred_model.graph.value_info)
```

With this PR, the output shape of `out` is `[3, n, unk__0]`, where as the existing ONNX infers as `[3, n]`, which has incorrect rank.
xuzijian629(2022-01-11 09:16:10):Please take a look! @askhade :bow:
askhade(2021-11-08 21:41:00):can you simply add a dim?  tsp.mutable_dim()->Add() instead of Copy
gramalingam(2022-01-12 22:32:47):Thanks for fixing this! This looks correct. But I think the whole function can be simplified to
```
   * tsp.add_dim() = dim;
```
xuzijian629(2022-01-13 00:10:43):Thank you!
yihonglyu(2021-11-19 00:07:29):Updated docs/Operators.md and docs/TestCoverage.md
jcwchen(2021-11-19 00:29:31):> Updated docs/Operators.md and docs/TestCoverage.md

Awesome! Now they pass CIs. Last thing, please sign-off the latest commit as well. Every commit in ONNX needs to be sign-off. Thank you!
yihonglyu(2021-11-19 21:46:37):I rebase -i the commits and there is only one sign-off commit now.
jcwchen(2022-01-29 01:32:44):This PR will forward after 1.11 release since it involves too many files and makes a fundamental change about data generation.

Here is the summary that data generation behavior will be changed as follows:


**Previous**

1. `python onnx/backend/test/cmd_tools.py generate-data` always produces node models with the "latest" opset version. There is no way to generate a model with older opset version by newer ONNX.
2. Because of 1., newer ONNX always updates many unrelated models by bumping their opset version to the latest one (although they are not updated)
3. Because of 1., newer ONNX cannot created models with older opset version. Users need to get older models by older ONNX package.
4. Many old models were not updated when its opset version has bumped. 

**New**

1. By default, `python onnx/backend/test/cmd_tools.py generate-data` will now produce the node models with "since_version".
2. Users can specify a desired opset version for generate-data. It will find the latest version ("since_version") before desired opset version.
3. Due to 2., now generate-data is able to create a whole set of node models with an older opset version.
4. Developers can specify older opset_version by giving `opset_imports` while writing a node test. It will use since_version based on specified opset_version.
5. Node models are all updated by their opset's "since_version".
6. Due to 5., now CI should be able to check the consistency of all generated data.



jcwchen(2022-03-31 20:48:40):I have verified these updated node models with the latest ONNXRuntime. ~They all passed~. I hope we can forward this PR soon to leave more time before next release. @askhade Previously you requested some changes and I have updated the PR. Do you still have any concern about it? Thanks.
jcwchen(2022-04-14 17:37:55):Sorry I was wrong. Actually there are three test failures from ORT due to this update. They are
- onehot_negative_indices
- onehot_with_axis
- onehot_with_negative_axis

The error messages are all the same, which is: `[ONNXRuntimeError] : 2 : INVALID_ARGUMENT : Invalid rank for input: depth Got: 0 Expected: 1 Please fix either the inputs or the model.` I will take a closer look at this issue.
jcwchen(2022-04-18 23:25:50):> This is because of this ONNX PR: https://github.com/onnx/onnx/pull/3774. The depth should be scalar. Perhaps the issue is in > ORT that it expects the depth for OneHot is 1D. I will dig deeper into it.

I found these failures only happened when new input+old ONNX node model so it is a false alarm. With new input and new ONNX node model, all tests for node model from ONNX backend passed by ONNXRuntime (I tested them locally by `onnx_backend_test_series.py` in ORT). This PR is ready for review.
jcwchen(2022-05-10 23:44:59):Update: I am trying to test the consistency of test generation in CIs. It seems that only Windows CI can have exactly the same node tests as this PR (I updated these node test models on Windows). I suspect there is an issue like this one: https://github.com/onnx/onnx/pull/3120. Still investigating.
jcwchen(2022-05-18 03:20:29):> Update: I am trying to test the consistency of test generation in CIs. It seems that only Windows CI can have exactly the same node tests as this PR (I updated these node test models on Windows). I suspect there is an issue like this one: #3120. Still investigating.

This issue exists because NumPy functions might behave differently on different platforms. Therefore skip all output.pb and some input.pb for now. I have added test generation in Windows CI, Linux CI and Mac CI in this PR. At least all ONNX models produced by test generation are consistent.

jcwchen(2022-05-19 00:38:18):One last update: I tried to bump the latest opset version and see whether any test has updated. Then I figured out simple tests still used make_model without specifying opset version so it will keep changing if the latest opset version bumps. To solve this, I further use make_mode_gen_version and specify opset version to make this test generation consistent.
askhade(2022-05-20 17:30:59):overall the PR looks good to me. I suggest using this branch in ort CIs and make sure all pipelines are green.
jcwchen(2022-05-20 17:52:26):Thanks for the review! I have verified these updated models once in ORT CIs and there isn't any related failure: https://github.com/microsoft/onnxruntime/pull/11214
askhade(2021-12-08 18:52:54):use max_inclusive_version for fetching schema to make sure we fetch the right schema for tests IF they target an older opset. Today none of the tests do but it is best if we add this check. 
askhade(2021-12-08 18:55:42):Instead of a random target opset version the option should be to use the max inclusive version for the test... If we use a random opset version most likely we won't be able to generate test data for all tests this is because if users choose an older opset version for which the tests are not applicable we have to skip those tests. 


askhade(2021-12-08 18:56:13):Also add comments on line 129 regarding this change
jcwchen(2021-12-08 23:25:43):Good point. I have updated get_schema with max_inclusive_version and make target opset_version as use_max_opset_version, which is more accurate. It takes some time for to inspect whether there is any updated model. Will update them later.
askhade(2021-12-08 23:27:14):> It takes some time for to inspect whether there is any updated model. 

If you run your script and do a git diff that should tell you right?
jcwchen(2021-12-09 00:07:30):There might be other changes due to system difference... To be careful, I will go through all of them (perhaps write a simple script to detect it).
jcwchen(2021-12-09 03:34:20):Eventually I wrote a script for only updating models/output.pb if their opset_version have been changed by this PR. However, there are 566 models which need to be updated (864 models in total). This PR will impact projects which are relying on backend test data.

One of the reasons why so many models are affected is -- typically an opset bump only involves some conditions and mostly only they were updated. Then the other models of other conditions would still use an old opset_version. 
askhade(2021-12-09 17:28:48):in that case we can change the test to include max_inclusive_version parameter to restrict the model to older opset

askhade(2021-12-09 17:29:54):going forward this should not be a problem... we should ask the authors to update all the models for the changing op... infact it will be gated by the git diff check in the PR.
So basically the author will have 2 options - update the test to include max_inclusive_version or simply update all the test data.
jcwchen(2022-01-26 00:01:47):Based on the specified opset version here: https://github.com/onnx/onnx/blob/7f12395a87cc85d9c6138a8b99530571014d3307/onnx/backend/test/case/model/__init__.py#L40, updating opset_10 here should be correct. I was surprise that these haven't been updated for a while. Since now we do test all models from ONNX model Zoo weekly, perhaps we can consider to remove/refactor these old models.
jcwchen(2022-04-16 07:21:33):By default helper.make_model will produce a model with the latest IR_VERSION, which means giving an older opset_version make_model will create a model with older opset_version but with the latest ir_version. This looks problematic to me. Therefore, I use the version map here to get the proper ir_version according to the opset_version.

However, it even brings more updates than before. The reason is typically when a contributor introduced a new node test data, at that moment the opset_version has been already bumped, but the ir_version has not been bumped before the IR bump PR got merged. Thus, it's possible that some node test data were using opset_version with (corresponding ir_version -1). This PR will make them use corresponding ir_version instead, which makes data generation always consistent.
jcwchen(2022-04-26 05:59:35):Some training testing models originally used ai.onnx.preview.training and didn't have IR_VERSION. In this PR, these models are updated with specified IR_VERSION.
gramalingam(2022-05-09 20:22:48):Do we really need to do this? If the user explicitly specifies opset version in the test-case, why not just use that? Automatically adjusting it can be problematic in some edge cases (eg., what if the user really wants a specific opset-version? Even though the test-case is for a single node, that node could be a control-flow node with a subgraph of operators which may require specific opset version, for example). On the flip side, does adjusting it give us any benefit or advantage?
gramalingam(2022-05-09 20:29:06):Who uses this? Is this meant to be a replacement for an existing function, or a new function? I can't find where this is called. Most importantly, I am trying to understand the meaning of the `opset_version` parameter, and why we need this  `_TargetOpsetVersion` behavior.
jcwchen(2022-05-10 15:56:16):Good question. It's a new function because previously the test generation can only create the latest opset version of models-- the intention here is enabling test generation to create a whole set (or a single operator) of certain old opset version. Then, potentially we can test these node tests with older opset version to enhance test coverage without downloading older ONNX packages.
jcwchen(2022-05-10 16:01:31):Thank you @gramalingam for catching it. I agree with your point -- If specifying opset version here, we should simply use it instead of using max_inclusive_version. It provides more flexibility for users if they really want specific opset version, especially for some control-flow cases. I will update here soon to use the opset version directly.
gramalingam(2022-05-10 21:09:54):Is there a pressing need for this? Or, is this something that can be addressed separately later? May be we can talk offline in the meeting tomorrow. Just trying to understand what the spec should be for this.
jcwchen(2022-05-10 21:28:08):There is no urgent need for it. I am fine to decouple this part to another new PR since this PR is already a huge one. I will do it later. Thanks!
jcwchen(2022-05-18 03:18:38):Updated. Please review it again. Thank you!
askhade(2021-12-08 23:20:37):since you are listing all these add Compose.py and hub.py as well.
askhade(2021-12-08 23:21:22):Nit: To build ONNX from source please follow the instructions listed [here]()
jcwchen(2021-12-09 00:04:01):Both updated. Thanks!
askhade(2021-12-09 00:05:32):delete the lines 5 through 11... All of this should be covered in the build instructions. 
askhade(2021-12-09 00:07:57):on line 18 you can add - Assuming build succeed in the initial step, simply running 
```
pip install -e .
```
from onnx root dir should work.
askhade(2021-12-09 00:09:15):utility to merge onnx models 
askhade(2021-12-09 00:09:42):utility for downloading models from <>
jcwchen(2021-12-09 00:12:32):All updated. Thanks!
jcwchen(2021-11-30 05:24:27):Not sure whether the version conversion here for Softmax can be done without knowing the input shape due to dynamic situation... At least version converter should catch this failure and not produce an invalid model. (has a Reshape node with an empty Shape)
gramalingam(2021-11-30 23:04:12):Why can't we allow an empty shape? That's what we need for a scalar. So, if we want to convert a tensor with shape [1] into a scalar with shape [], I think that should be permitted?
jcwchen(2021-12-01 03:49:54):Thank you for catching it! I wasn't sure for the case of empty shape because the spec is not very clear. But, it makes sense to me that using empty shape for reshaping 1D to scalar. Will correct it and focus on the fix in version_converter.
gramalingam(2021-12-03 00:19:57):Thanks for clarifying the documentation. May be it will help to also say that "The input tensor's shape and the output tensor's shape are required to have the same number of elements."
gramalingam(2022-01-04 19:10:50):True. This problem also appears elsewhere, since the version-converter seems to assume that the shapes are known in many places. How does the underlying IR represent the cases where the rank is unknown or a dimension is unknown?
jcwchen(2022-01-05 01:12:01):IIUC, it will create a node with empty sizes() (empty vector of Dimension).
jcwchen(2022-02-05 02:09:50):Update: add more context in the PR description about why I think ONNX should add this API.
jcwchen(2022-03-14 18:07:01):Update: to apply data propagation in more scenarios, applying it to older opsets is required. It can be done either in this PR or another PR. As an experiment, let me put the support for old opsets in this PR first. I can further decouple these changes in another PR if needed. For now, I added data propagation support for older opsets as below:
- Shape-1
- Gather-11
- Gather-1
jcwchen(2022-05-03 15:58:40):I removed the original `appendDimToTensorShapeProto` since now it only has one line code, which seems unnecessary to me.
gramalingam(2022-05-03 21:48:05):I wonder why some of the code below doesn't happen in the constructor of ShapeInferenceImpleBase? May be with a separate, different, constructor from the existing one?
gramalingam(2022-05-03 21:50:39):nit: seems like this PR is doing two different things? Just wondering why it also has updates to specific data. prop. logic?
gramalingam(2022-05-03 21:51:28):So, is this used because the caller iteratively uses results from previous calls to do inference for other subgraphs later?
gramalingam(2022-05-03 21:54:56):Seems okay for now. But this may end up creating unnecessary copies of a large data-structure. It may be useful to consider a design that uses a `std::move` or some other approach.
jcwchen(2022-05-03 23:17:52):Good point. Actually I was considering to modify `generated_shape_data_by_name` in place without returning it by `InferShapesAndDataPropagation` since another input parameter `ModelProto` is also called by reference and can be modified in place. So here are two options:
1. Make `generated_shape_data_by_name` call by reference as `InferShapesAndDataPropagation`'s input.
2. Return `generated_shape_data_by_name` additionally by `InferShapesAndDataPropagation`. (What this PR currently does and it needs some improvement for moving it as you mentioned)

@gramalingam do you have a preference about it? Thanks.
jcwchen(2022-05-04 04:51:37):Correct. PyTorch-ONNX exporter uses shape inference with subgraphs so this function can help the data propagation propagate from previous subgraph to next one. 
jcwchen(2022-05-04 04:54:40):Good question. For the first step, I enabled Shape and Gather in PyTorch-ONNX exporter and the tests there cover older opset versions as well. Therefore, I further applied Shape and Gather in older opset versions in this PR.

BTW, the Gather data  propagation was also improved in this PR by catching some invalid cases.
jcwchen(2022-05-04 05:09:07):I think the code below are only common in 2 places: one is in ` InferShapes` and another one is `InferShapesAndDataPropagation`, because only these 2 take ModelProto as input. Although we cannot put them into ShapeInferenceImpleBase (it's for GraphProto), perhaps at least we can make them as a common function to reduce duplicate code.

Still, it seems that `TraverseGraphsToAddExistingSymbol` can be included in the ShapeInferenceImpleBase constructor. I will give it a try.
gramalingam(2022-05-04 16:30:49):There are two interfaces where copying can happen: when the top-level `Infer...` API is called, and second when the Base class constructor is called. So, both will need to be addressed. 

I think that if option 1 works, that should be good.
gramalingam(2022-05-04 16:32:37):Specifically, the question is: can we also change the base-class constructor to take a non-const reference, and avoid copying there?
jcwchen(2022-05-05 20:42:09):I introduced two functions, `GetOpsetImportsFromProto` ~and `GetModelLocalFunctionsMapFromModel`~, to reduce duplicate code between `InferShapes` ~and `InferShapesAndDataPropagation`~. In addition, I moved TraverseGraphsToAddExistingSymbol into process(GraphProto& graph) since it's a required step while processing a graph.

BTW, I might find a potential bug that it does not consider symbols in a FunctionProto. I created an issue to track that: https://github.com/onnx/onnx/issues/4191.
jcwchen(2022-05-05 20:44:39):Thanks for your opinion. I made base-class constructor take a non-const reference and `InferShapesAndDataPropagation` use `generated_shape_data_by_name` by reference, which can address some potential copy issues. Please review it again.
gramalingam(2022-05-09 18:00:05):A few things are unclear here:
* Difference between the `InferShapes(modelProto, registry, options)` and this method.
* I believe that the `ShapeInferenceOptions.enable_data_propagation` is the one that still determines whether data-propagation happens or not.
My understanding is that this extension, over the previous `InferShapes(modelProto, registry, options)` function, is only to enable caller to specify initial value of `generated_shape_data_by_name` and get back the final value. Is that right?
gramalingam(2022-05-09 18:01:41):Would it make sense to then just extend the previous method with a 
```cpp
   std::unordered_map<std::string, TensorShapeProto>* generated_shape_data_by_name = nullptr
```
?
jcwchen(2022-05-09 21:24:09):FYI: I changed the order here for `ModelLocalFunctionsMap& model_local_functions` to make it consistent to other APIs.
jcwchen(2022-05-09 22:06:04):Thank you for the great idea! Yes, I think we can simply extend existing `InferShapes` instead of introducing more similar API. I have just removed new `InferShapesAndDataPropagation` and use generated_shape_data_by_name as pointer globally. Please review it again.
gramalingam(2022-05-10 20:28:17):What is the specification/contract for this API? I think that to keep this backwards-compatible, the implementation below should handle the case where `(generated_shape_data_by_name == nullptr) && (options.enable_data_propagation )` by creating a local `map` and passing its address to the constructor. I realize that one of the other functions expects the user to do this, but here we are constrained by backwards-compatibility.
gramalingam(2022-05-10 20:31:19):Okay, never mind, I see that this is happening in `InferShapesImpl` !
jcwchen(2021-12-08 21:36:54):Close here since I finished the experiment.
askhade(2022-01-06 17:19:53):Is this function needed? Can the above one not handle a case with node + attr
For example Cast<to=1>(input) is supported right?
askhade(2022-01-06 17:21:30):if we remove this then we can make the function body construction code more uniform... for cases where attr value is static all nodes can be constructed with 1 Add call others can use "Add(const char* node_txt, const std::string& attr_name, T attr_value)"
gramalingam(2022-01-06 17:45:29):For the first question, (as your 2nd message implies), there are cases where the value is not static and cannot be embedded in the string. For the second comment, the ```Add(const char* node_txt, const std::string& attr_name, T attr_value)``` variant is syntactic shorthand for invoking this and is defined in terms of this. So, not sure what we achieve by eliminating this. Since there are other utility functions to create AttributeProto (other than just name/value), having the two variants doesn't hurt and could be useful.
askhade(2022-01-06 17:56:13):I was just suggesting that we can keep 2 methods public since they cover all the scenarios: 
Add(const char* node_txt)
Add(const char* node_txt, const std::string& attr_name, T attr_value)

Add(const char* node_txt, const AttributeProto& attr) can be made private.

The reason I am suggesting this is - too many options can confuse the author and usually these utilities are used by a large number of folks (who are contributing to onnx functions) and everyone does not have a thorough understanding of this.  
gramalingam(2022-03-31 16:23:28):@AlexandreEichenberger @postrational : any further comments on this PR? If there are no further comments by end of next week (April 8th), I assume it is okay to go ahead and merge this PR. Thanks!
gramalingam(2022-01-04 19:47:31):I don't understand this. The name of the actual parameter in the call-site node is irrelevant here. Lines 57-58 seem to do the correct thing here.
gramalingam(2022-01-04 19:47:43):Same comment as above.
gramalingam(2022-01-04 20:12:07):Not sure why we need this?
gramalingam(2022-01-04 20:18:52):Is the idea to support a form of broadcast (if one of the inputs is not a sequence)? 
gramalingam(2022-01-04 21:27:38):I suspect that instead of using the actual input-parameter names, it would be necessary to introduce (formal) inputs to the FunctionProto, and use those names. These names can be created (eg., "input1", "input2", etc.) as long as we avoid name collisions.
jantonguirao(2022-01-05 16:45:52):Yes, that was my idea
jantonguirao(2022-01-05 16:46:40):I guess it is a leftover from trying to troubleshoot the function expansion. I removed it now
jantonguirao(2022-01-05 16:46:44):Removed
jantonguirao(2022-01-05 16:47:32):I removed this, and switched to using formal inputs/outputs, as suggested below.
jantonguirao(2022-01-05 16:47:45):Good point. I switched to creating formal inputs/outputs. Can you check again?
gramalingam(2022-02-08 18:42:36):You are right, the existing code doesn't seem to handle subgraphs in nodes in the function body. The right fix would be to recursively traverse the subgraph and rename references to function input names (just like it is done for the top-level). The fix below will likely fail if we have two calls to the function which are both expanded, since the names will clash.
gramalingam(2022-02-08 18:53:25):I assume that all sequences are expected to have the same length? It would be useful to explain that constraint here.
gramalingam(2022-02-08 19:14:39):Maybe it would be useful to return an error-message for the else-cases here and down below?
gramalingam(2022-02-08 19:18:00):Change `tensor inputs/outputs` to plain `inputs/outputs`? Makes it somewhat future proof, since conceptually there is no reason why it should be tensors (even though the type signature below constrains it currently).
gramalingam(2022-02-08 19:19:01):These lines are a bit confusing since they are talking about the inputs to SequenceMap node, and not the inputs to the body graph, and this documentation is for the body graph.
gramalingam(2022-02-08 19:27:20):I think we don't yet have good conventions for handling error conditions when building a function body. Other examples so far simply return false. If we do want to throw exceptions, I think we should be using the conventions here https://github.com/onnx/onnx/blob/main/onnx/common/common.h to support no-exception builds
jantonguirao(2022-02-10 08:07:15):I've now implemented the subgraph traverse algorithm you describe. Can you please take a look?
jantonguirao(2022-02-10 08:07:19):Done
jantonguirao(2022-02-10 08:07:31):Goop point. Done.
jantonguirao(2022-02-10 08:07:44):That's true, those lines don't belong here. Removed
jantonguirao(2022-02-10 08:10:43):I've changed those to ONNX_THROW_EX. I believe there's value in throwing actual errors instead of silently returning false. The motivation is that as a user I would prefer an error message saying that I passed an invalid "body" argument rather than reading "SequenceMap not available".
gramalingam(2022-02-11 21:48:15):Is this supposed to be `new_graph.node.extend(new_nodes)` ?
gramalingam(2022-02-11 21:50:40):May be simpler to say
```py
   new_nodes = [ _rename_edges_helper(node_desc, rename_helper, attribute_map) for node_desc in new_graph.node]
```
gramalingam(2022-02-11 22:03:47):I think there is a tricky issue here. The current checker seems to allow a nested subgraph to have an input name same as a name from outer scope. (See here: https://github.com/onnx/onnx/blob/fb586d09aed80fa8529a87fdf01a6a98cd3619d5/onnx/checker.cc#L599 ). The comment there seems to question whether this should be allowed or not. But since it is currently allowed, the renaming needs to handle this case correctly. 
jantonguirao(2022-02-14 18:06:19):True, I'll adjust
jantonguirao(2022-02-14 18:06:27):Absolutely
jantonguirao(2022-02-14 18:23:49):The comments, and the code, suggest that shadowing is not allowed:
```
  // Inherit values available in outer scope
  // Note that we do not allow shadowing, so the presence of an already-defined
  // name is always an error.
```
and
```
if (lex_ctx.this_graph_has(value_info.name())) {
      fail_check(...)
```

So, if I understood correctly, if the subgraph shares an input name with the parent graph, an error will be thrown.
Did I miss anything?    
gramalingam(2022-02-14 18:31:30):There is an inconsistency between the comments and the code. The code seems to allow subgraph inputs to shadow names from outer scope (the lines 597, 598, 599 etc.). While it is a good point that this should probably be cleaned up, changing it could potentially break existing models. Meanwhile, it seems better to use the code as the official spec, in case of conflicts like this.
AlexandreEichenberger(2022-02-24 17:34:40):Please specify that an output at position X in the sequence is computed from applying the the subgraph from the input at the same position X in the input sequence.

I would add the following. Users cannot expect any specific ordering in which each subgraph is computed.
jantonguirao(2022-02-24 18:51:29):@AlexandreEichenberger That's a great point, thank you. I've updated the text. Could you please check that it makes sense?
jantonguirao(2022-02-24 18:52:12):I think the issue with shadowed names should be addressed now
gramalingam(2022-03-07 19:20:17):The change looks good, thanks! A minor suggestion: I think it would be better to omit `new_graph.value_info` above. The reason is to distinguish between "name binding occurrences" which can introduce a new name (for which updating `sg_rename` makes sense) from "references to names defined/bound elsewhere".
gramalingam(2022-03-07 19:31:57):nit: drop the word `appeared`
jantonguirao(2022-03-08 08:15:45):Done
jantonguirao(2022-03-08 08:16:34):Done
gramalingam(2022-04-09 18:46:13):I think the above line must be moved into the branches of the if-else statement, like in the original version. Specifically, I think in the else-case of the if-statement in line 38, we should not be inserting new_attr. 
matteosal(2022-01-04 15:57:03):@askhade there is a problem with the CI which looks unrelated to my changes. The output cells of a demo jupyter notebook are throwing some error. That code is about roundtripping arrays from ONNX to numpy and don't use any of my changes. I have also ran the notebook locally and didn't see any error. Is there a way to restart the CI?
askhade(2022-01-04 17:16:17):> @askhade there is a problem with the CI which looks unrelated to my changes. The output cells of a demo jupyter notebook are throwing some error. That code is about roundtripping arrays from ONNX to numpy and don't use any of my changes. I have also ran the notebook locally and didn't see any error. Is there a way to restart the CI?

Usually just closing and reopening the PR is easiest way to restart all CIs
gramalingam(2022-01-04 18:39:47):This looks good, thanks!
gramalingam(2022-01-04 18:40:28):Wondering what happens with the counterexample you give and onnxruntime? Thanks!
gramalingam(2022-01-04 18:54:32):FWIW: it looks like this is calculated here: https://github.com/microsoft/onnxruntime/blob/97659495d982982a06eee6a4e3560923b988ff7e/onnxruntime/core/providers/cpu/nn/conv_transpose_attributes.h#L174 
matteosal(2022-01-05 15:03:27):The example I gave in the PR description is fixed by this. What I did here was just to bring [this change](https://github.com/onnx/onnx/pull/3188/files#diff-b9e98b88715e8a7e56c0dfa317f21a39582f919c87185f827c4a011ad9010639L1228) to the shape inference of the old ConvTranspose with opset < 11. There is no functional difference between opset < 11 and opset >=11 in ConvTranspose, as opset 11 only introduced clarifications in the documentation. So they should behave the same way, which is how the "new" operator behaves.
gramalingam(2022-01-05 19:11:14):Thanks, sounds good to me.
gramalingam(2021-12-15 18:52:58):Maybe it would be better to change line 100 above and replace 'M' by None? And leave the current check? 
gramalingam(2021-12-15 18:57:17):Should we change line 79, to specify output shape as [2, None] or [None, None], and leave the current check as-is?
askhade(2021-12-15 19:35:31):condition
```
(!existingDim->has_dim_value() && !existingDim->has_dim_param()
```
can be folded in for loop on line 179. This way when existing shape is empty we dont have to run through 2 for loops (1 on line 179 and 1 on line 184)
shinh(2021-12-15 20:43:31):Ah, nice. Done!
shinh(2021-12-15 20:43:36):Ditto. Done!
shinh(2021-12-15 20:57:30):Is this what you meant? https://github.com/onnx/onnx/pull/3896/commits/be56060267a7ade6f420152e11c742f8d15a00ab

Note I think we still need `(!existingDim->has_dim_value() && !existingDim->has_dim_param()` here for the case where `existingType` has a known rank (i.e., `existingType->has_shape()` is true) with unknown dimensions.
askhade(2021-12-15 21:12:31):yes thanks for adding line 175. And yes, I agree we still need this check here for the case where existingType has a known rank.
gramalingam(2021-12-15 17:36:34):Just wondering: should we permit both floats as well as tensor attributes? (That is, support both the old style and new style.) That will be backwards-compatible, making it easier for exporters and the version-converter. This would mean introducing the tensor attributes with a new attribute name, and requiring that the model specify either the original or new attribute.
gramalingam(2021-12-15 17:41:18):Need to also update the version-converter (and version-conversion tests). This depends on how the above comment is addressed
xadupre(2021-12-15 17:47:00):I looked into that but it does not seem to support other domains than the main one: https://github.com/onnx/onnx/blob/master/onnx/version_converter/convert.h.
xadupre(2021-12-15 17:52:48):> Just wondering: should we permit both floats as well as tensor attributes? (That is, support both the old style and new style.) That will be backwards-compatible, making it easier for exporters and the version-converter. This would mean introducing the tensor attributes with a new attribute name, and requiring that the model specify either the original or new attribute.

In that case, which name do you suggest, something like `class_weights_tensor`?
gramalingam(2021-12-15 17:57:44):> > Just wondering: should we permit both floats as well as tensor attributes? (That is, support both the old style and new style.) That will be backwards-compatible, making it easier for exporters and the version-converter. This would mean introducing the tensor attributes with a new attribute name, and requiring that the model specify either the original or new attribute.
> 
> In that case, which name do you suggest, something like `class_weights_tensor`?

Yes, for every FLOATS attribute named X, we could add an attribute named X_tensor or X_as_tensor
xadupre(2021-12-15 18:01:48):Ok, let's do that.
gramalingam(2022-01-04 19:42:41):Hi: I think extending the version-converter infrastructure to support ONNXML would be useful. It requires extending the structure to include "domain" as an additional field/parameter (with a default value of "", so that none of the existing ONNX converters need to be changed). Would it make sense to do that as part of this PR? Thanks!
xadupre(2022-01-05 09:40:51):I would prefer to do it in another PR. It seems a different topic to me.
gramalingam(2022-01-05 19:20:24):minor nit: could improve the error message to say "Only one of the attributes ... and ... should be specified."
gramalingam(2022-01-06 16:47:11):second "nodes_hitrates" => "nodes_hitrates_as_tensor"
gramalingam(2022-01-06 16:48:12):Same as above: second "nodes_hitrates" => "nodes_hitrates_as_tensor"
xadupre(2022-01-12 12:17:01):done
gramalingam(2022-01-18 16:53:28):Hi, looks like the other one was fixed, but this still needs to be fixed. Thanks!
xadupre(2022-01-18 16:59:50):Just fixed. Sorry for that. I commit before it was saved.
askhade(2021-12-17 18:38:41):Although the PR is merged it would be nice if you created an issue and linked this PR. It will be easier for tracking. We should ideally have issues for all such tasks - changing names to more neutral names, changing master -> main and may be more.
AlexandreEichenberger(2021-12-17 21:54:57):@askhade I don't understand your request: you prefer that there is one issue for each PR (happy to add one), or that there is one issue to represent all of the changes for neutral names PRs? Thanks for clarifying
askhade(2021-12-20 17:32:40):Either way works - I just want a way to track these work items, so that we can easily gather all the work that was done towards making code\branch names etc neutral. 

CLAassistant(2022-01-19 20:15:59):[![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/onnx/onnx?pullRequest=3902) <br/>Thank you for your submission! We really appreciate it. Like many open source projects, we ask that you all sign our [Contributor License Agreement](https://cla-assistant.io/onnx/onnx?pullRequest=3902) before we can accept your contribution.<br/>**9** out of **11** committers have signed the CLA.<br/><br/>:white_check_mark: AlexandreEichenberger<br/>:white_check_mark: shinh<br/>:white_check_mark: liqunfu<br/>:white_check_mark: askhade<br/>:white_check_mark: jcwchen<br/>:white_check_mark: xadupre<br/>:white_check_mark: gramalingam<br/>:white_check_mark: xuzijian629<br/>:white_check_mark: matteosal<br/>:x: lewtun<br/>:x: namasikanam<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/onnx/onnx?pullRequest=3902) it.</sub>
CLAassistant(2022-01-19 20:15:48):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=3908) <br/>All committers have signed the CLA.
gramalingam(2022-03-16 00:27:24):Thanks Jacky for the update!
jcwchen(2022-03-25 17:13:57):@diyessi Do you have bandwidth to finish this PR? If not, please allow me to send a duplicate PR of yours and go forward.  Thank you for your contribution.
diyessi(2022-03-25 17:39:42):> 

I might today; I need to re-figure out how to run the ONNX stuff (the README needs some updates). If I don't, feel free to jump in.

Do I just need to merge from main, run `update_doc.sh` and push back?
jcwchen(2022-03-25 17:49:34):> Do I just need to merge from main, run update_doc.sh and push back?

Thank you for the quick reply. Other than those, you will also need to fix the DCO issue because some of your commits were not sign-offed. A faster way to solve it is to squash all your old commits into one (for instance, git reset softly to the base and make/sign-off a single commit with your whole change).
diyessi(2022-03-25 21:33:16):@jwchen I think it's ready now.
jcwchen(2022-03-25 21:50:41):> @jwchen I think it's ready now.

Thank you for the quick update! Sorry that I might not be clear: actually using update_doc.sh will update `Operators.md` and `Changelog.md`. The CI failed because `Changelog.md` was not updated. Please use update_doc.sh to update `Changelog.md` as well and the PR should be ready to go.
diyessi(2022-03-25 22:17:04):> > @jwchen I think it's ready now.
> 
> Thank you for the quick update! Sorry that I might not be clear: actually using update_doc.sh will update `Operators.md` and `Changelog.md`. The CI failed because `Changelog.md` was not updated. Please use update_doc.sh to update `Changelog.md` as well and the PR should be ready to go.

It actually updates a lot of things when I run it. Hopefully this time it will all be good.
gramalingam(2022-01-04 19:25:12):I wonder if https://numpy.org/doc/stable/user/basics.indexing.html?highlight=slice#slicing-and-striding is a better link?
gramalingam(2022-01-04 19:27:31):Should that be ```dims[axes[i]]-1``` ?
gramalingam(2022-01-04 19:35:29):Thanks for the clarification. The spec is somewhat ambiguous. One interpretation would be that the sliced indices are all the values "start + j*step" that lie within the valid range obtained by clamping. The other interpretation would be to clamp start first, and generate values "clamped(start) + j*step". Do we know which is intended? Thanks!
diyessi(2022-01-06 15:55:03):A link to NumPy would make more sense for something preceded by "Similar to numpy".
diyessi(2022-01-06 16:00:10):It's `dim[axes[i]]` since you want `-1` to go to the last element.

The problem with negative steps is that there is really no clean way to stop at 0 since `end` is exclusive and negative values get shifted by the dimension size; you need to pick a really big negative end that is still below 0 when shifted by the size.

There is some code just below the doc that does some of this stuff; it looked similar to what I had implemented in our fork of glow based on experiments with numpy.
diyessi(2022-01-10 16:50:39):I see what you meant now. It gets a little confusing with [] being closed intervals, start being inclusive and end being exclusive. I also made the code and the text consistent.
gramalingam(2022-01-27 00:21:18):Thanks. Everything seems fine, except one point I am confused about: what happens if `starts[i] = dims[axes[i]]` and steps is negative ? Your new version says it will get clamped into the value `dims[axes[i]] - 1`. I tried it is onnxruntime, and this is what happens. But, then, this line in the shape-inference code doesn't seem to be consistent with that: https://github.com/onnx/onnx/blob/50a1981dc3d50af13075cec33b08b4c87fb0e41f/onnx/defs/tensor/defs.cc#L824

diyessi(2022-02-01 19:22:02):In the PR I had changed that to `input_rank - 1`.

BTW, when looking for that I noticed a one-word typo/omitto which I just fixed.
gramalingam(2022-02-01 19:59:50):Sorry, my fault. I saw that, but it didn't register, I thought the changes were only in the documentation, not the code. LGTM, thanks!
jcwchen(2022-03-25 22:34:39):Thanks for the update. The CIs now all passed! 

Is this `1s` here (L897) intended or a typo?
diyessi(2022-03-25 22:36:36):A slice can have more than one thing in it, so "1s" as in `(1, 1, ...)`
hariharans29(2022-04-05 23:37:38):Should this read `dims[axes[i]]-1` or am I mis-understanding something ? 
diyessi(2022-04-06 16:42:03):Starts are inclusive, ends are exclusive, so the slice covers indices `{i | start <= i < end}`.
If it still seems wrong or you see a way to make it clearer, let me know.

hariharans29(2022-04-06 20:38:56):Sorry for not being clear enough. I think the logic is right. Looking at it, it seems inconsistent with the way it is `worded`. 

"...so for positive stepping `ends[axes[i]]` is clamped to `[0, dims[axes[i]]]`..."

Looking at the above line, I would have thought, the last set of words should be `.... is clamped to [-1, dims[axes[i]] - 1]` instead of the `ends` you are referencing. Does this make sense or am I on the wrong track ? :) 

diyessi(2022-04-06 21:12:18):Let's say we have a tensor X[5]. Then a `slice(X, start=0, end=0)` is empty, `slice(X, start=0, end=5)` is all of X, and `slice(X, start=0, end=101086)` is the same as `slice(X, start=0, start=5)`.
In ranges, `[` and `]` are inclusive, `(` and `)` are exclusive.
hariharans29(2022-04-06 21:17:32):I agree with everything above. 

I was just pointing out the clamping wording while negative stepping:
`[-1, ends[i]-1]`   ->   `[-1, dims[axes[i]] - 1]` ?
gramalingam(2022-04-06 21:29:10):@hariharans29 : I agree with your comment about the wording, which seems like a typo. More importantly, I was wondering about the issue you created https://github.com/onnx/onnx/issues/4115 ... do you still have a concern about this? It seemed like an important point, but I haven't checked it out yet.
hariharans29(2022-04-06 21:44:40):Thanks Rama, yes, it seems like a typo.

Re: #4115 - To have the link to Numpy's slicing in the op's description and prescribe a different clamping for `start` is a bit misleading. ORT implements the Numpy clamping (which does the clamping based on the slicing direction) for `start` since the time `steps` input was introduced in Slice-10. This is because the spec initially didn't have the clamping logic called out explicitly and I used Numpy's clamping as the de-facto (Numpy's Slice was also linked to in the op's spec in V10).  FWIW- The "real-world" impact of this should be minimal. It is just that ORT will need to make changes if it needs to "honor" this newly called out clamping logic. As much as this discussion is more pedantic than a burning issue, it would be nice to clarify this.
diyessi(2022-04-06 21:52:08):I hadn't seen #4115 and thought you were referring to the positive stepping case. Your change looks correct and agrees with what I implemented in our glow fork (although I used the equivalent adjusted start instead of `dim[axes[i]]-1`, so that's two implementations.
gramalingam(2022-04-06 22:42:26):The original textual description was unclear. However, how about the original shape-inference code?
(a) Is the original shape-inference code consistent with the ORT/glow implementation or not?
(b) Does the change in shape-inference code in this PR change the behavior or does it not? This line of change seems relevant to the kind of counterexample that Hari discusses in the above issue, but I feel there may be edge-cases where the shape-inference code may not be consistent, and it would be helpful to check this.
hariharans29(2022-04-06 22:54:36):This was the original shape inference code:
 
 if (step < 0)
    start = clamp(start, 0, input_rank - 1);
  else
    start = clamp(start, 0, input_rank);

This is how ORT implements it: https://github.com/microsoft/onnxruntime/blob/26fceca90f35fe94f39864ebaec0f2d51e367f10/onnxruntime/core/providers/cpu/tensor/slice_helper.h#L132

So, the old shape inference code matches with ORT's implementation. 


gramalingam(2022-04-06 22:59:04):So, I think we should revert to the old shape-inference code. Is there any case where the old shape-inference code is incorrect?
gramalingam(2022-04-06 23:00:50):And change the textual description accordingly. (Since we are not bumping the opset version, this PR should be only a clarification, not a change in behavior.)
gramalingam(2022-04-06 23:02:03):Does that sound reasonable? Thanks!
diyessi(2022-04-06 23:02:31):I'm not seeing how the step > 0 case above is correct; start is inclusive, which is [0, input_rank), not [0, input_rank].
diyessi(2022-04-06 23:08:04):NumPy does behave like the original.
hariharans29(2022-04-06 23:16:47):@gramalingam : I think reverting to the old shape inference will be a good idea. It atleast maintains status quo and as you rightly say changing any behavior without an opset bump up is not a good idea.

@diyessi : Try this example on Numpy (this is for step > 0),
x = np.array([1, 2, 3])
x[5:6:1]
array([], dtype=int32)

If `start` had been clamped to an inclusive value as you are suggesting, with the `ends` clamping the way we all agree it to be, the result would actually be non-empty


askhade(2022-01-05 16:52:49):@liqunfu : DCO check is failing for this PR because the commits are not signed. including "-s" option in every git commit cmd enables sign off. 
```
git commit -s -m "my commit"
```

You can check your local logs to make sure the commit is signed. If you push more than 1 unsigned commits there is no way to go back and sign the only option I know is to resend a new PR. Unfortunately, this is the current limitation. 
askhade(2022-01-05 16:55:00):Can you add details around what exactly this test covers.
liqunfu(2022-03-09 00:47:51):close this pr.
Local function is better tested be runtime (e.g. https://github.com/microsoft/onnxruntime/blob/master/onnxruntime/test/ir/onnx_model_test.cc).
askhade(2022-01-05 16:54:07):I suggest using onnx parser api for test model creation. It is more readable and easier to use. 
askhade(2022-01-06 17:25:06):can you add some simple instruction explaining how to delete or is it very straightforward?
jcwchen(2022-01-06 20:07:17):Just added simple steps in the doc. Thanks!
jcwchen(2022-01-07 23:04:18):~Please note that current CIs are failing in this PR because ONNX hasn't migrated the branch yet~... Once we have finished the migration, we will go back to this PR and the CIs should be green.
jcwchen(2022-01-09 02:22:28):Renaming was just completed. Reopen this PR to re-trigger CIs for verification.
askhade(2022-01-07 22:52:19):Changes to this file dont look relevant
jcwchen(2022-01-07 23:02:18):Actually I only change this line `https://github.com/onnx/onnx/blob/master/onnx/onnx.proto` into `https://github.com/onnx/onnx/blob/main/onnx/onnx.proto`. My vscode somehow changed the key order here and make the change look irrelevant... Will try to make it look more reasonable.
prasanthpul(2022-01-07 23:12:38):Consider: "NOTICE" instead of warning
AlexandreEichenberger(2022-01-07 23:18:57):I would recommend to use relative links, which work well IMHO. Path starts in directory of the file. And you then don't need to mention the branch either.
AlexandreEichenberger(2022-01-07 23:20:37):Are you sure that the link still work? Did protobuff folks also switch to using main instead of master?
AlexandreEichenberger(2022-01-07 23:20:53):relative link please
AlexandreEichenberger(2022-01-07 23:21:19):relative link please.
jcwchen(2022-01-07 23:52:40):Thank you so much for the good catch! Protobuf repo hasn't moved the branch yet. Just corrected.
jcwchen(2022-01-08 00:01:03):Thank you for the suggestion! Yes it can reduce the maintenance cost in the future and make the document cleaner. Actually I was considering the same thing, but I think for README.md we might need to keep it as absolute path -- the absolute path can make the [links](https://pypi.org/project/onnx/) in PyPI workable. We changed it into absolute path since ONNX 1.9 by this PR: https://github.com/onnx/onnx/pull/3386. Except README.md, I went thru all documents and changed the absolute path into relative path.
jcwchen(2022-01-08 00:13:09):Please note that I removed #L943 here since defs.cc keeps changing. People should be able to figure out where TopK is from their own.
lgtm-com[bot](2022-01-08 06:26:03):This pull request **fixes 1 alert** when merging b2a153bd2f0fcf617196d59927441973627d8a89 into 60531231618431a480f2b7b18ee94763829b2c3e - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-ab0d7a4b56c65fd26b45b22c89457967d289f0ec)

**fixed alerts:**

* 1 for Unused import
lgtm-com[bot](2022-01-10 15:31:24):This pull request **fixes 1 alert** when merging f3c4ddf01b164df2f761f92f0e86450c72ed59a1 into 96516aecd4c110b0ac57eba08ac236ebf7205728 - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-342c52eccedd215b386b1103ed8dc309e101c91b)

**fixed alerts:**

* 1 for Unused import
lgtm-com[bot](2022-01-12 23:16:52):This pull request **fixes 1 alert** when merging 15341f9076f0cfdf8a84ef04ab1e8f99f9f5d4c1 into 10da6f2e919c8515877e227a41cd44e86ae0bb2d - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-b7716d8560c4c2cbd36fc41ef6b9ec739829eb7c)

**fixed alerts:**

* 1 for Unused import
lgtm-com[bot](2022-01-13 00:20:19):This pull request **fixes 1 alert** when merging 5157ecc6b7421b8afaceaf96a818180410d78814 into 10da6f2e919c8515877e227a41cd44e86ae0bb2d - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-db1bfd7e8d960e44207dac0a43e5b11a04c39d2d)

**fixed alerts:**

* 1 for Unused import
lgtm-com[bot](2022-01-13 00:51:37):This pull request **introduces 1 alert** and **fixes 1** when merging dc853b0cfe95c18cb4aa587a6d3c1f9abe8a4249 into 10da6f2e919c8515877e227a41cd44e86ae0bb2d - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-5ed9f582283aa800a8a75641d5ce7cafb0d1016e)

**new alerts:**

* 1 for Unused import

**fixed alerts:**

* 1 for Unused import
lgtm-com[bot](2022-01-13 05:36:58):This pull request **introduces 1 alert** and **fixes 1** when merging 9c5618e78f49b1a34b0e05c20bae61efc0080563 into a161a0db3464710e6acf7f34989f005d806f576d - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-00e8a5c81bc34ebd7c630ae4d166d03d7e4724f8)

**new alerts:**

* 1 for Unused import

**fixed alerts:**

* 1 for Unused import
stillmatic(2022-01-13 07:33:54):Thanks! I think the path forward is to close #3604 in favor of this and other smaller PRs eg https://github.com/onnx/onnx/pull/3939, there is actually quite a lot of work to update as each mypy version brings in new requirements, and I think the original PR accidentally masks lots of them. 
lgtm-com[bot](2022-01-13 23:50:45):This pull request **introduces 1 alert** and **fixes 1** when merging 2da2026ce115bed1271e20a0b7797fa53f03f494 into 67ca4ba9c7f82be5d119aac202516fa7161fbb52 - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-c3bd5e54f405afd2ec4b33154d2c5061dad4c500)

**new alerts:**

* 1 for Unused import

**fixed alerts:**

* 1 for Unused import
askhade(2022-01-18 20:54:45):@stillmatic : Please fix the unused import alert. Then we can merge this PR.

![image](https://user-images.githubusercontent.com/6475296/150017103-98d9717f-626e-4944-86ed-f748839f264a.png)

lgtm-com[bot](2022-01-19 00:08:14):This pull request **fixes 1 alert** when merging 28880d95ab2d78aa32f7ccfe3fb8d58b7a498211 into 793ddeda9e2de8b71bc2d1e6d957c4e6d1274e5c - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-5b58fc7aa360f742c953535cd1fbfbc17cacb820)

**fixed alerts:**

* 1 for Unused import
lgtm-com[bot](2022-01-19 00:40:14):This pull request **fixes 1 alert** when merging 70e4cf85072f0a79c6a2a69f82bb42fc0b10612f into 793ddeda9e2de8b71bc2d1e6d957c4e6d1274e5c - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-b294d3bedd41dfb59a677db11248643c98a4b68c)

**fixed alerts:**

* 1 for Unused import
CLAassistant(2022-01-19 20:15:34):[![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/onnx/onnx?pullRequest=3928) <br/>Thank you for your submission! We really appreciate it. Like many open source projects, we ask that you sign our [Contributor License Agreement](https://cla-assistant.io/onnx/onnx?pullRequest=3928) before we can accept your contribution.<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/onnx/onnx?pullRequest=3928) it.</sub>
dependabot[bot](2022-02-18 23:17:45):Looks like numpy is up-to-date now, so this is no longer needed.
xuzijian629(2022-01-11 00:22:52):Thanks for your PR!
By the way, I remembered I submitted the same PR three months ago. https://github.com/onnx/onnx/pull/3784

I also submitted several other PRs regarding data propagation.
Could you give some feedback? https://github.com/onnx/onnx/pulls/xuzijian629
jcwchen(2022-01-13 17:46:42):Close it since it's a duplicate of #3784 
xuzijian629(2022-01-11 00:21:12):When the dim has `dim_param`, it's better to also propagate it.
askhade(2022-01-19 19:55:51):LGTM! Please resolve conflicts then we can merge this.
askhade(2022-01-19 19:50:10):can remove this if since onnx does not support python 2 anymore
stillmatic(2022-01-19 23:21:19):yep - was removed in the other commit, let me fix the merge conflict
askhade(2022-01-13 19:33:23):Wondering how we can catch this sooner next time... meaning before the pipeline fails. I would expect to see some warning in the CI pipelines to let the users know a certain image is going to be deprecated. 
jcwchen(2022-01-13 19:40:13):> Wondering how we can catch this sooner next time... meaning before the pipeline fails. I would expect to see some warning in the CI pipelines to let the users know a certain image is going to be deprecated.

Actually it did show a warning in previous CI log: `##[warning]The macOS-10.14 environment is deprecated and will be removed on December 10, 2021. For more details see https://devblogs.microsoft.com/devops/hosted-pipelines-image-deprecation/`

Mac-OS upgrading in CI was in my TO-DO list, but I was OOF at that time and then just forgot... We should take an action for upgrade in time next time if we notice the warning. Thanks for the reminder this time!
thiagocrepaldi(2022-01-18 16:28:52):@askhade Could you check whether I am going towards the right direction with this PR?
askhade(2022-01-18 19:06:55):DCO is not signed for this PR. If you only have a single commit then it is easy to fix it... Just follow the directions here: 
https://github.com/onnx/onnx/pull/3948/checks?check_run_id=4856175809

In future adding -s option to git commit adds sign off for that commit. example:
git commit -s -m "some comment"
askhade(2022-01-18 19:21:25):Few more comments:
1. Run flake 8 in local repo. CIs are reporting errors from flake8
```
python -m pip install -q flake8
flake8 onnx
```

2. Generate test data and add it to "https://github.com/onnx/onnx/tree/main/onnx/backend/test/data/node". This step picks your node unit test and creates onnx models + test data.
```
python onnx/backend/test/cmd_tools.py generate-data --op_type Col2Im --output <optional. provide output directory>
``` 
The script can be found here: https://github.com/onnx/onnx/blob/main/onnx/backend/test/cmd_tools.py#L90

3. Make sure you are not manually editing the Changlelog.md, Operators.md and TestCoverage.md files. You can use this script to auto generate them:
```
python onnx/backend/test/stat_coverage.py
python onnx/defs/gen_doc.py
ONNX_ML=0 python onnx/defs/gen_doc.py
```
askhade(2022-01-19 18:47:05):fix DCO
CLAassistant(2022-01-19 20:15:21):[![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/onnx/onnx?pullRequest=3948) <br/>Thank you for your submission! We really appreciate it. Like many open source projects, we ask that you sign our [Contributor License Agreement](https://cla-assistant.io/onnx/onnx?pullRequest=3948) before we can accept your contribution.<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/onnx/onnx?pullRequest=3948) it.</sub>
thiagocrepaldi(2022-02-03 15:56:35):@askhade @gramalingam I have addressed all comments and rebased to the latest version. Ready for a new round of review :)
garymm(2022-03-31 19:44:57):Could you please add `Fixes #4106` to the PR description?
garymm(2022-04-04 17:04:32):Rama told me in person that:
* He wants this PR to also include redefining the ConvTranspose op as a function using the new Col2Im op.
* He wants Col2Im added to ORT as a contrib op before merging this PR (so we're sure the spec can be implemented in a backend)

lgtm-com[bot](2022-04-05 19:17:17):This pull request **introduces 4 alerts** when merging 62c98da7f8bd9ad823ac4b9574f622080f90a848 into 356108796c72f151fb4980e8c377b766265e76e4 - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-9878f902467ce11e49687c172a4c1871feee2b01)

**new alerts:**

* 4 for Comparison result is always the same
thiagocrepaldi(2022-04-06 16:41:22):> Rama told me in person that:
> 
> * He wants this PR to also include redefining the ConvTranspose op as a function using the new Col2Im op.
> * He wants Col2Im added to ORT as a contrib op before merging this PR (so we're sure the spec can be implemented in a backend)

Given [`ConvTranspose`'s inputs](https://github.com/onnx/onnx/blob/main/docs/Operators.md#inputs-2---3-1) can be more than 4-dimensional (e.g. `(N, C, k1, k2, k3, ..., kn)`) and Col2Im' s is at most 4 dimensional (e.g. either `(N, C, H, W)` or `(C, H, W)`), is it possible to really rewrite `ConvTranspose` as a function of `Col2Im`? 

ONNX Runtime uses Col2Im to implement ConvTranspose for 4 dimensions, and Im2Col for 5+ dimensions. As ONNX does not have Im2Col yet, we need to find another way to support ConvTranspose3D and higher. Maybe rewriting it as transpose of Convolution as opposed to GEMM + Col2Im?
garymm(2022-04-06 18:57:30):@thiagocrepaldi looks like ORT uses Col2imND for ConvTranspose:
https://cs.github.com/microsoft/onnxruntime/blob/2dfd81b9bb097c90388010e5b7d298498274f8d9/onnxruntime/core/providers/cpu/nn/conv_transpose.cc?q=repo%3Amicrosoft%2Fonnxruntime+Col2Im#L192

So if they already have an N-dimensional Col2im implementation, the question is can we generalize the ONNX op spec to support Col2imND such that it's easy to re-use that implementation?
If so, I suggest you do that, and then you can use that in the function definition of ConvTranspose.
thiagocrepaldi(2022-04-06 19:35:54):> @thiagocrepaldi looks like ORT uses Col2imND for ConvTranspose: https://cs.github.com/microsoft/onnxruntime/blob/2dfd81b9bb097c90388010e5b7d298498274f8d9/onnxruntime/core/providers/cpu/nn/conv_transpose.cc?q=repo%3Amicrosoft%2Fonnxruntime+Col2Im#L192
> 
> So if they already have an N-dimensional Col2im implementation, the question is can we generalize the ONNX op spec to support Col2imND such that it's easy to re-use that implementation? If so, I suggest you do that, and then you can use that in the function definition of ConvTranspose.

`Col2ImNd` is just a wrapper around a `Im2Col` call.

```
template <>
void Col2imNd<float, CPUMathUtil, StorageOrder::NCHW>(const float* data_col, const int64_t* img_shape,
                                                      const int64_t* output_shape, int64_t channels_col, int64_t img_size,
                                                      const int64_t* kernel_shape, const int64_t* stride,
                                                      const int64_t* dilation, const int64_t* pad, ptrdiff_t N,
                                                      float* data_img, CPUMathUtil* context) {
  Set<float, CPUMathUtil>(gsl::narrow<ptrdiff_t>(img_size), 0, data_img, context);
  Im2col<float, StorageOrder::NCHW>()(
      data_col,
      img_shape,
      output_shape,
      channels_col,
      kernel_shape,
      stride,
      dilation,
      pad,
      N,
      data_img,
      true);
}
```

We could have `Im2Col` as another ONNX op or as implementation detail for Col2Im's implementors. However, `Col2Im` users might find ONNX's N-dimensional spec complicated if compared to all other frameworks/libraries
garymm(2022-04-06 21:28:40):@thiagocrepaldi I think my original question is still the important one: can we generalize the ONNX op spec to support Col2imND such that it's easy to re-use the existing implementation in ORT? I think it doesn't matter if the existing implementation uses Im2col internally.
thiagocrepaldi(2022-04-07 17:44:07):> @thiagocrepaldi I think my original question is still the important one: can we generalize the ONNX op spec to support Col2imND such that it's easy to re-use the existing implementation in ORT? I think it doesn't matter if the existing implementation uses Im2col internally.

I think so and pushed a new revision
gramalingam(2022-04-11 21:09:11):I agree with @garymm 's points above. One clarification though: it doesn't matter if the existing implementation uses Im2col internally ... but it would be useful to verify that the existing implementation does actually work correctly for higher dimensions. The reason I ask this is that it seems surprising to me that Col2ImND is expressed more-or-less directly as an instance of im2col. Seems like magic to me. If it works, great!
garymm(2022-04-13 18:52:10):@thiagocrepaldi please write a test case for N-dimensional col2im in ORT to make sure their current implementation is correct. If so then we have confidence we can implement this in ORT without much work. Otherwise we should just stick with 4D for this op for now, even if it means we can't redefine ConvTranspose as a function.
thiagocrepaldi(2022-07-16 05:01:53):> I agree with @garymm 's points above. One clarification though: it doesn't matter if the existing implementation uses Im2col internally ... but it would be useful to verify that the existing implementation does actually work correctly for higher dimensions. The reason I ask this is that it seems surprising to me that Col2ImND is expressed more-or-less directly as an instance of im2col. Seems like magic to me. If it works, great!

The trick is that a Col2ImND cannot be implemented by a Im2Col. Instead, Col2ImND can be implemented by ORT's tweaked version of Im2Col. ONNX Runtime has added an extra flag to Im2Col to change its behavior in order to implement the intended behavior.
thiagocrepaldi(2022-07-16 05:03:44):> @thiagocrepaldi please write a test case for N-dimensional col2im in ORT to make sure their current implementation is correct. If so then we have confidence we can implement this in ORT without much work. Otherwise we should just stick with 4D for this op for now, even if it means we can't redefine ConvTranspose as a function.

I have added a unit test that produces a 5D image (batch, channel, dim_0, dim_1, dim_2) from a column block 
thiagocrepaldi(2022-08-12 00:30:39):https://github.com/microsoft/onnxruntime/pull/12311 is now ready and implements Col2Im as contrib op on ORT
thiagocrepaldi(2022-08-18 23:37:00):@askhade Could you dismiss your review? All comments were addressed back in the day, but I have no permission to dismiss nor request another review. Feel free to give it another go on the review too
thiagocrepaldi(2022-08-25 18:24:39):> @askhade Could you dismiss your review? All comments were addressed back in the day, but I have no permission to dismiss nor request another review. Feel free to give it another go on the review too

gentle ping
askhade(2022-01-18 19:10:25):this is not right... you are setting the shape of output to be same as shape of input.
askhade(2022-01-18 19:13:55):rename to output_shape to make it clearer?
askhade(2022-01-19 18:25:23):how is this change relevant to this PR?
thiagocrepaldi(2022-01-19 18:28:15):It is not. Probably one of the generator scripts created it
askhade(2022-01-19 18:29:09):Please add type and shape inference function.
askhade(2022-01-19 18:32:42):the test generator script can update other tests (example their opset). Please only include the test changes relevant to your PR.
askhade(2022-01-19 18:33:40):I think nn/defs.cc is a better location for this op
askhade(2022-01-19 18:40:00):output_size in pytorch contains output [H, W] and not the entire output shape... is this the same for the output_shape in onnx? Add more clarification. 
askhade(2022-01-19 18:40:33):faults-> defaults
askhade(2022-01-19 18:44:57):will this op support input >4D? 
askhade(2022-01-19 18:45:33):fix formatting for this spec
thiagocrepaldi(2022-01-19 18:53:35):fixed
thiagocrepaldi(2022-01-19 19:01:18):done
thiagocrepaldi(2022-01-19 19:07:46):My understanding is that it doesnt
askhade(2022-01-19 19:10:30):pytorch clearly states it does not... so if we want to keep the behavior same then let's clarify it and add a check around this in type and shape inference function
thiagocrepaldi(2022-01-19 20:36:13):I didn't understand this part. Could you elaborate?
askhade(2022-01-19 20:42:22):From: https://pytorch.org/docs/stable/generated/torch.nn.Fold.html
```
output tensor of shape (N,C,output_size[0],output_size[1],…)
.
.
.
output_size (int or tuple) – the shape of the spatial dimensions of the output (i.e., output.sizes()[2:])
```

where N and C come from:
```
Consider a batched input tensor containing sliding local blocks, e.g., patches of images, of shape (N,C×∏(kernel_size),L), 
```


thiagocrepaldi(2022-01-19 21:04:44):Added "Note that only 4-D output tensors (batched image-like tensors) are supported."
askhade(2022-01-21 19:21:47):if the input_shape is not valid then throw an error instead of simply returning. See fail_shape_inference macro
askhade(2022-01-21 19:22:31):why is this check in the else loop? Shouldn't you validate the shape regardless of there the data for output_shape is available or not?
askhade(2022-01-21 19:23:14):same comment as above... if this is invalid then throw an error instead of silent return
askhade(2022-01-21 19:29:32):same comment as line 2519
askhade(2022-01-21 19:34:14):lines 2579 to 2584 are common between if and else conditions... you can extract it out
askhade(2022-01-21 19:38:09):if stride attr is not required in shape calculation then is this required? Simply checking the validity on line 2564 if present is enough right?

thiagocrepaldi(2022-01-21 22:13:48):they are similar, not identical
askhade(2022-01-21 23:30:42):this is not right... 

some conditions are unmet because the data is not available at the time of inference and you should not throw in those cases... 

What I meant in my earlier comment is - if something is invalid then you should throw.

hasNInputShapes(ctx, 3) is allowed to be false... it is possible that output_shape is not initializer or constant and therefore you dont know the shape. In this case you should simply return.
askhade(2022-01-21 23:38:36):the error message can be more clear. May be something like : "output_shape tensor contains partial shape for output. It must contain either 2 or 3 elements"
askhade(2022-01-21 23:40:29):this check is not right... if your output_shape tensor contains data {5, 5} the shape is (2) and therefore in this case dim_size() will return 1. 
askhade(2022-01-21 23:41:24):same comment as above on line 2522
askhade(2022-01-22 00:13:21):add similar checks for the other 2 inputs "output_shape" and "kernal_shape"?  to validate the dim_size()==1
askhade(2022-01-22 00:14:01):since the shape validation for this and other attributes is not dependent on shapes of the 2 inputs you can move these checks above line 2493
askhade(2022-01-22 00:21:32):why not output_shape = ParseData<int64_t>(temp_output_shape);
askhade(2022-01-22 00:23:47):either the check or the error message is invalid.
askhade(2022-01-22 00:24:30):same as above. 
askhade(2022-01-22 00:26:24):In the else condition why can't you set dim 0 to 1 same as here and dim 1 to input_shape.dim(0) same as on line 2574?
It is OK if you can infer some values as long as the rank is correct.
askhade(2022-01-22 00:29:27):kernel shape is not being used in final_output_shape calculation so you can change this to 2 since only 2 inputs are required
thiagocrepaldi(2022-01-24 22:49:31):the check is done on line 2511 by checking `output_shape.size()`
gramalingam(2022-01-25 21:28:07):Is it the same padding for every axis? Or, does the user specify begin/end for each axis (which would not be a 2-tuple)?
gramalingam(2022-01-25 21:30:57):Don't understand this. May be one issue is a typo "input vs. output". But even if we take this to be output_shape, it is not clear to me.
gramalingam(2022-01-25 21:34:04):Suggest changing the message to "Input tensor has wrong rank. It must have rank 2 or 3."
gramalingam(2022-01-25 21:37:03):Suggested message "The output_shape tensor is expected to have 2 elements, but it does not." We could also add actual number of elements to make message more useful.
gramalingam(2022-01-25 21:41:22):As above. Mainly, I don't understand what "partial shape" means here, would prefer to make the message clearer.
askhade(2022-01-25 21:51:32):You are assuming the shapes are known... this will fail if exact value is not known instead a symbol is present in dim_param

This is what I would do:
```
// validating 'output_shape' input
if (ctx.getInputType(1)->tensor_type().shape().dim_size() != 1) {
fail_shape_inference("output_shape should be 1D");
}

std::vector<int64_t> output_shape;
const TensorProto* temp_output_shape = ctx.getInputData(1);
if (temp_output_shape) {
  output_shape = ParseData<int64_t>(temp_output_shape);
  if (output_shape.size() != 2) {
      fail_shape_inference("output_shape tensor contains partial shape for output. It must contain 2 elements");
  }
}
```
thiagocrepaldi(2022-01-26 15:34:30):you are right
thiagocrepaldi(2022-01-26 16:07:11):This is really output_shape. Although it looks redundant, it is needed when dilation > 1. Will add a note justifying this
thiagocrepaldi(2022-01-26 16:50:30):The input is either [H, W] or [N, H, W]. Padding is only meant for the [H, W] dimensions of input.

`padding[0]` means zero padding on both sizes of `input_height` dimension by `padding[0]` pixels
`padding[1]` means zero padding on both sizes of `input_width` dimension by `padding[1]` pixels

so `padding=[0,1]` means 0 pixel on both sides of input height dimension and 1 pixel in both sides of input height dimension

Is that clear?
thiagocrepaldi(2022-01-26 16:50:45):I will add that to the help message

gramalingam(2022-01-26 18:05:53):To clarify, I was making two points. First point is the phrase "The shape of the input before" doesn't match the name "output_shape". May be "The shape of the output before" would be better. Second point is to explain it better, since it is unclear how the *final* output shape is computed.
gramalingam(2022-01-26 18:09:00):I think it would be useful to make attributes like padding uniform for all ops. 
thiagocrepaldi(2022-01-26 18:27:12):I agree this part is tricky. Let me try to explain the idea and you might suggest a better writing for the PR

`im2col`'s `input` returns an `output` which is then used as `input` for `col2im` (this op).
Here,  `col2im`'s `output_shape` must match `im2col`'s `input`.

That is why I mentioned "The shape of the input before applying im2col transformation"
thiagocrepaldi(2022-01-26 18:28:01):The same for dilation and stride too?
gramalingam(2022-02-24 19:19:04):+1. Unclear why this is marked as resolved.
gramalingam(2022-02-24 19:23:12):The check ```hasNInputShapes(ctx, 2)``` from below must be moved up before we access this shape.
gramalingam(2022-02-24 19:24:31):Do you mean `rank 1`?
gramalingam(2022-02-24 19:26:21):Must check
```
   ctx.getInputType(2)->tensor_type().shape().dim(0).has_dim_value() &&
   ctx.getInputType(2)->tensor_type().shape().dim(0).dim_value() != 2
```
gramalingam(2022-02-24 19:29:36):Same comments as above (for input 2).
gramalingam(2022-02-24 19:44:20):Indentation looks off.
gramalingam(2022-02-24 19:53:22):Don't understand this. Why is this 1? And at the beginning?
gramalingam(2022-02-24 19:54:06):Is there some pseudo-code description of what the op does? That would help understand and verify this shape-inference.
gramalingam(2022-02-24 20:07:29):I think this is supposed to be batch-size, if input is unbatched? Not sure it is a good idea. If the input is unbatched (2D), then it seems better for output to be unbatched (3D).
gramalingam(2022-03-09 18:37:28):I suggest don't use the same H, W for inputs 0, 1, and 2 unless they are meant to have the same size. Use different symbols to clarify.
thiagocrepaldi(2022-03-18 17:37:10):> Is there some pseudo-code description of what the op does? That would help understand and verify this shape-inference.

Im2col transforms several patches in one big matrix
![image](https://user-images.githubusercontent.com/5469809/159054523-1330c8df-eb03-42fb-bf98-98ff5edca406.png)

while col2im transforms the big matrix big in the original shape
![image](https://user-images.githubusercontent.com/5469809/159054662-a0d375bc-0050-4ad9-976f-74c05520271e.png)

gramalingam(2022-03-18 18:20:14):Suggest changing this as below. I am using OH/OW for output-height and output-width:
``
The shape of the image after rearranging the column blocks.
This is a 1-dimensional tensor of size 2, containing the value (OH, OW).
``
gramalingam(2022-03-18 18:22:04):Suggest changing this as below. I am using KH/KW for kernel-height and kernel-width; not sure what is commonly used "block" or "kernel"?
``
The shape of the block/kernel.
This is a 1-dimensional tensor of size 2, containing the value (KH, KW).
``
thiagocrepaldi(2022-03-29 18:49:25):fixed. a tab was amongst the spaces.
thiagocrepaldi(2022-03-29 19:32:38):It is correct as it is.
`dim_size()` vs `dim_value()` has caused some confusion in previous reviews, but `ctx.getInputType(2)->tensor_type().shape().dim_size()` should return 1 *dimension* and `ctx.getInputType(2)->tensor_type().shape().dim(0).dim_value()` should return the *value held by first dim*, which should be 2
thiagocrepaldi(2022-03-29 20:11:06):great catch. The first output dimension is the input's batch dimension, which defaults to 1 for unbatched input. Fixed.
gramalingam(2022-03-29 22:01:03):Sorry, the if-condition is checking for 1 (which is correct), but the error message says `rank 2` (which is incorrect, IIUC). 
gramalingam(2022-03-29 23:41:25):This should be just `return`. Input shapes may not be statically known, and that is not a failure condition.
gramalingam(2022-04-04 16:54:11):The implementation looks correct. I think it would help understandability for future maintenance if the code is rewritten as below (giving explicit names for N, C, H, and W) and it also makes it a bit more general. Does this make sense?
```cc
   if ((static_cast<size_t>(input_shape.dim_size()) == 3) {
      // Input and Output have extra batch-dimension N:
      Dim N, C, H, W;
      N = input_shape.dim(0);
      if (block_shape_size > 0)
         C = input_shape.dim(1) / block_shape_size; // Otherwise, C is unknown.
      if (size_of_output > 0) {
         H.set_dim_value(image_shape[0]);
         W.set_dim_value(image_shape[1]);
      }
      updateOutputShape(ctx, 0, {N, C, H, W});
   } else {
      // Input and Output have no batch-dimension N:
      Dim C, H, W;
      if (block_shape_size > 0)
         C = input_shape.dim(0) / block_shape_size; // Otherwise, C is unknown.
      if (size_of_output > 0) {
         H.set_dim_value(image_shape[0]);
         W.set_dim_value(image_shape[1]);
      }
      updateOutputShape(ctx, 0, {C, H, W});
   }
```
gramalingam(2022-04-05 20:23:48):I think the above condition (and its else branch) are not required. The conditions in line 2575/2577 etc. below already handle both-cases. Does that sound right? Thanks!
gramalingam(2022-07-20 18:38:25):Why is a zero-size allowed here? Same question for other attributes below. The preceding if-condition already allows the attribute to be omitted. 
thiagocrepaldi(2022-08-12 00:55:20):What this condition is validating is "if pads were specified by user (thus pads.size() >0), they should have a pair length (each dimension need padding for the beginning and end)"
gramalingam(2022-08-15 22:00:34):Suggest adding
```
This is the block-shape before dilation is applied to it.
```
gramalingam(2022-08-15 22:06:01):For the middle-dimension, I believe that the order is block-major, correct? Also, is it correct to say that `n-ary-product(blockshape)` is the `dilated-block-size`? If so, I suggest replacing it with `dilated-block-size`. 
gramalingam(2022-08-15 22:12:26):It will also help to specify the order of the blocks. Can we say something like
```
The blocks are enumerated in increasing lexicographic-order of their indices.
For example, with an image-size 10x20 and block-size 9x18, we have 2x3 blocks,
enumerated in the order block(0,0), block(0,1), block(0,2), block(1,0), block(1,1), block(1,2).
```
gramalingam(2022-08-15 22:15:44):Needs to be moved to opset 18.
gramalingam(2022-08-15 22:18:52):I thought you mentioned you had a python implementation of the op? If so, it would be helpful to include that here.
thiagocrepaldi(2022-08-16 15:48:33):> For the middle-dimension, I believe that the order is block-major, correct? Also, is it correct to say that `n-ary-product(blockshape)` is the `dilated-block-size`? If so, I suggest replacing it with `dilated-block-size`.

I think this was an implementation detail for reusing ORT's math::Col2ImNd that relied on ORT's tweaked math::Im2Col(accumulate=true). In that case, the original image size and block size had to be adjusted before calling math::Col2ImNd, which in turn, called im2Col. However, from the user point of view, they can specify only the original image shape and block shape and the implementor would be able to derive the dilated versions, if their implementation requires them.
thiagocrepaldi(2022-08-16 15:53:31):It is a Github project https://github.com/f-dangel/unfoldNd/, which I will refer here
gramalingam(2022-08-16 17:09:45):Validating "if pads is specified" is already done by `if (getRepeatedAttribute(ctx. "pads", pads)`. I suggest omitting the condition part `(pads.size() != 0) &&`.
gramalingam(2022-08-16 17:24:54):The dim could be symbolic or unknown here. I suggest replacing this line with
```cc
Dim n_input_dims ;
unifyInputDim (ctx, 1, 0, n_input_dims);
```
and update the uses as commented below.
gramalingam(2022-08-16 17:28:55):Suggest replacing these 3 lines with
```cc
unifyDim (n_input_dims, image_shape.size());
```
to handle the case that n_input_dims is unknown or symbolic correctly.
gramalingam(2022-08-16 17:34:20):Suggest replacing these 3 lines with
```cc
if (pad.size() % 2)
   fail_shape_inference("Attribute pads must have an even size");
unifyDim (n_input_dims, pads.size() / 2);
```
gramalingam(2022-08-16 17:35:31):Replace these 3 lines with
```cc
   unifyDim (n_input_dims, dilations.size());
```
gramalingam(2022-08-16 17:35:57):As above
gramalingam(2022-08-16 17:37:08):As above
gramalingam(2022-08-16 17:38:45):Replace this condition/message with:
```cc
   unifyInputDim(ctx, 2, 0, n_input_dims);
```
thiagocrepaldi(2022-08-16 21:01:04):done
thiagocrepaldi(2022-08-16 21:03:38):done
thiagocrepaldi(2022-08-16 21:54:03):done
gramalingam(2022-08-16 23:31:30):I think we can also add the following check (since the next input (3rd input) should also have a matching dim):
```cc
unifyInputDim(ctx, 2, 0, n_input_dims);
```
gramalingam(2022-08-16 23:33:36):We should also ad the following checks to ensure that the ranks of both inputs are 1:
```cc
checkInputRank(ctx, 1, 1);
checkInputRank(ctx, 2, 1);
```
gramalingam(2022-08-16 23:36:49):Add:
```cc
// If we haven't inferred the number of image dimensions, we can't set inferred shape.
if (!n_input_dims.has_dim_value()) return;
```
gramalingam(2022-08-17 00:22:55):I believe this should account for dilation, if dilation is specified?
gramalingam(2022-08-17 00:27:11):Not sure how you get this shape for the given dilation?
thiagocrepaldi(2022-08-17 17:43:25):```python
import torch

kernel_size = (2, 2)
dilation = (1, 5)
output_size = (6, 6)

image = torch.arange(1, 37).reshape(1, 1, 6, 6).to(torch.float32)
print(f'User Image:\n{image}\n\t with shape {image.shape}')

# Im2Col used to generate input for target Col2Im
im2col = torch.nn.Unfold(kernel_size=kernel_size,
                         dilation=dilation)

col = im2col(image)
print(f'Im2Col output:\n{col}\n\t with shape {col.shape}')

# Col2Im results
col2im = torch.nn.Fold(kernel_size=kernel_size,
                       dilation=dilation,
                       output_size=output_size)
im = col2im(col)
print(f'Col2Im output:\n{im}\n\t with shape {im.shape}')
```

which returns

```python
User Image:
tensor([[[[ 1.,  2.,  3.,  4.,  5.,  6.],
          [ 7.,  8.,  9., 10., 11., 12.],
          [13., 14., 15., 16., 17., 18.],
          [19., 20., 21., 22., 23., 24.],
          [25., 26., 27., 28., 29., 30.],
          [31., 32., 33., 34., 35., 36.]]]])
         with shape torch.Size([1, 1, 6, 6])
Im2Col output:
tensor([[[ 1.,  7., 13., 19., 25.],
         [ 6., 12., 18., 24., 30.],
         [ 7., 13., 19., 25., 31.],
         [12., 18., 24., 30., 36.]]])
         with shape torch.Size([1, 4, 5])
Col2Im output:
tensor([[[[ 1.,  0.,  0.,  0.,  0.,  6.],
          [14.,  0.,  0.,  0.,  0., 24.],
          [26.,  0.,  0.,  0.,  0., 36.],
          [38.,  0.,  0.,  0.,  0., 48.],
          [50.,  0.,  0.,  0.,  0., 60.],
          [31.,  0.,  0.,  0.,  0., 36.]]]])
         with shape torch.Size([1, 1, 6, 6])
```
thiagocrepaldi(2022-08-17 18:02:27):AFAIK, for computing `C`, we don't need to account for dilation. As an example, on ORT's Col2Im, a `dilated_kernel_shape` was computed for calculating the new `image_size` when calling Im2Col, but `C` for Col2Im is still computed using regular `kernel_shape_size`

![image](https://user-images.githubusercontent.com/5469809/185209938-55911d2f-adc5-413d-9690-0fc3f0e619ec.png)


gramalingam(2022-08-17 22:43:35):Ok, I misunderstood dilations. I guess the kernel-block-size remains the same with or without dilations. So, this is fine, thanks!
gramalingam(2022-08-17 22:47:13):Ok, thanks for the explanation! LGTM!
askhade(2022-01-19 20:10:37):remove this only for 3.6
gramalingam(2022-01-20 21:56:27):I think we may need to check both (domain, op_type) to match calling-node with called-function?
gramalingam(2022-01-20 21:58:04):I think we may need a loop to accumulate functions called from other functions. Main-graph may call local function foo1, and function foo1 may contain a call to local function foo2, and so on. We should collect foo2 also.
liqunfu(2022-01-22 02:17:13):good point. coded for this case
gramalingam(2022-01-25 18:27:37):Suggest adding other fields in FunctionProto, with default-values? Default empty list may make sense for opset_imports also.
```python
   attributes = [],
   doc_string = ''
```
jcwchen(2022-01-25 06:42:34):I like providing a history section, which makes the change clearer. Just out of curiosity: Why only LeakyRelu and PRelu add history section?
gramalingam(2022-01-25 18:15:36):Good question! For the control-flow ops, especially, I was thinking about another issue: they have very long documentation, and we end up copy-pasting it when we update the op even though the documentation doesn't change much, which also presumably increases binary size etc. I was thinking a slightly better solution might be needed, if we wanted to avoid this kind of increase. Something to think about. Thanks!
fdwr(2022-01-26 20:19:09):Oh my goodness, a history section in the actual op would be so useful. I spend so much time digging through dozens of PR's each ONNX update and comparing Changelog.md entries to see what actually changed between ops for a given version.
askhade(2022-01-26 22:03:22):@fdwr : I agree it is cumbersome to find out op updates. In short term you can refer to the release wiki pages that we create to track op validation. This one is from last release :  https://github.com/onnx/onnx/wiki/Logistics-for-ONNX-Release-1.10#changelog
We will create a similar one for 1.11 soon. This table has a short description of what changed and link to onnx PR which made the update.

fdwr(2022-01-27 01:59:51):@askhade : 👍 That wiki looks nice. I read the generic notes on the Release page, but the wiki has more notes beyond just the PR title and a link.
lgtm-com[bot](2022-01-22 00:35:24):This pull request **introduces 2 alerts** and **fixes 1** when merging fccde7ad5f1904b020317ae24d832fd8e8e0ad98 into 48d2e7c14b222e6866f68e5cf067624675099e59 - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-40a63e3963bf1db03d28ba297ad3b7d5cd5a3d38)

**new alerts:**

* 2 for Unreachable code

**fixed alerts:**

* 1 for Unused local variable
lgtm-com[bot](2022-01-27 00:47:54):This pull request **introduces 1 alert** and **fixes 1** when merging f077bbf6baceed8374632d290d426a866ba84da7 into 50a1981dc3d50af13075cec33b08b4c87fb0e41f - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-74a3e6abcb066d58503e2402209cb74fea89f66b)

**new alerts:**

* 1 for Unreachable code

**fixed alerts:**

* 1 for Unused local variable
lgtm-com[bot](2022-01-27 01:05:49):This pull request **fixes 1 alert** when merging c291858de7987251f6888bdb28b3014ecf186478 into 50a1981dc3d50af13075cec33b08b4c87fb0e41f - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-baaa0f31a71adfcb9eeec9b2dfa4d89bcc2746fe)

**fixed alerts:**

* 1 for Unused local variable
lgtm-com[bot](2022-01-27 02:00:25):This pull request **fixes 1 alert** when merging 41e9adaf497e90b08a113317efa95584ed84eaec into 50a1981dc3d50af13075cec33b08b4c87fb0e41f - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-89f1e41ecb4f68588d6005b6edb502bef626edbc)

**fixed alerts:**

* 1 for Unused local variable
lgtm-com[bot](2022-01-27 22:32:41):This pull request **fixes 1 alert** when merging fe662e2201fb4737182f0fc8f656723ced6a9040 into 50a1981dc3d50af13075cec33b08b4c87fb0e41f - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-195237e5db900e5e076abafe655f7f4515752904)

**fixed alerts:**

* 1 for Unused local variable
lgtm-com[bot](2022-01-27 22:47:51):This pull request **fixes 1 alert** when merging f6aca09ac98484356691e0b30f212f58e87fbe98 into 50a1981dc3d50af13075cec33b08b4c87fb0e41f - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-5179e63efa3969a03adf5ba05bb4cec3d7773f54)

**fixed alerts:**

* 1 for Unused local variable
lgtm-com[bot](2022-01-27 23:00:33):This pull request **fixes 1 alert** when merging 05f9bbcd5256fb80127f3418019ae102e7a5c765 into 50a1981dc3d50af13075cec33b08b4c87fb0e41f - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-3a5c8dc4787b2460c1adc2e8506bdb5d7ab6d498)

**fixed alerts:**

* 1 for Unused local variable
lgtm-com[bot](2022-01-27 23:20:54):This pull request **fixes 1 alert** when merging da6efe54e00415fb47295ce0ead940763aa6438f into 50a1981dc3d50af13075cec33b08b4c87fb0e41f - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-f5c6d92ec3248b02ee084ee189cf1a77b044e533)

**fixed alerts:**

* 1 for Unused local variable
garymm(2022-01-27 23:30:18):@liqunfu could you please review?
garymm(2022-01-27 23:31:06):@gramalingam could you please approve?
jcwchen(2022-01-25 01:31:23):L868 and L800 both bumped into the following error: `error: Unpacking a string is disallowed` so I used single variable to catch it.
jcwchen(2022-01-25 01:32:20):Fix the error: `Missing type parameters for generic type "Generator"`
jcwchen(2022-01-25 01:37:09):The error is `Argument 1 to "list" has incompatible type "Optional[Sequence[str]]"; expected "List[str]"`.
I tried to make `reader.fieldnames` as `List Optional[Sequence[str]]` or others, but it still showed error anyway. Please let me know if there is a better way to solve it. Thanks!
garymm(2022-01-27 00:30:28):The problem is that `reader.fieldnames` may be None. We can assert that it's not to avoid the type error:

```suggestion
                assert reader.fieldnames
                frameworks = list(reader.fieldnames)
```
jcwchen(2022-01-27 01:37:32):Nice trick. I have updated and it worked. Thank you for the suggestion!
liqunfu(2022-01-31 22:39:12):@onnx/sig-archinfra-approvers @onnx/sig-operators-approvers Kindly remind that this PR is needed for the release cut planned for Jan 31. 
jcwchen(2022-01-27 00:13:40):nit: perhaps we can add a test for ml-opset like
```
test([("ai.onnx.ml", 3)], 8)
```
jcwchen(2022-01-27 16:11:07):Not relevant to this PR: although last release version of AI_ONNX_ML_DOMAIN has been already updated a few days ago, isn't it better to upgrade `last_release_version_map_[AI_ONNX_ML_DOMAIN] = 3` now right before the release?
liqunfu(2022-01-27 16:56:36):Similar thing happened with IR version change before 1.10.0 release. IR version was increased to 8 in a PR adding sparse tensor support. Optional data type was added after but before 1.10.0 release. Neither the optional data pr nor the release version bump pr did update the IR version because it was set correct with the first PR. I think it makes sense. A user may use any build from the main branch. Updating the version with first PR will inform the user a possible breaking change in the build. 
jcwchen(2022-01-27 19:12:57):I agreed that bumping version (like `map_[AI_ONNX_ML_DOMAIN]` above) in the first related PR is good as a notification to users, but bumping the version in `last_release_version_map_` might be quite different. Based on the naming, it seems to me it's better to bump it right before the release. @gramalingam since you are the contributor of it, do you have any suggestion about when to bump the version in `last_release_version_map_`? Thanks!
liqunfu(2022-01-28 18:32:12):for example, an op is updated. It needs to be validated in ort with build from onnx main. without increase the op domain last rel version, the validation will fail. 
lgtm-com[bot](2022-01-26 21:46:29):This pull request **introduces 1 alert** and **fixes 1** when merging e82f5dbd52b8986f607205de64103153333d93d2 into 50a1981dc3d50af13075cec33b08b4c87fb0e41f - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-2aac9721be52763c3d783b69766a8920e7e534d6)

**new alerts:**

* 1 for Unreachable code

**fixed alerts:**

* 1 for Unused local variable
stillmatic(2022-01-26 21:52:26):```
▶ python onnx/backend/test/cmd_tools.py
Traceback (most recent call last):
  File "onnx/backend/test/cmd_tools.py", line 13, in <module>
    import onnx.backend.test.case.node as node_test
  File "/Users/hua/Development/onnx/onnx/backend/test/__init__.py", line 9, in <module>
    from .runner import Runner as BackendTest # noqa
  File "/Users/hua/Development/onnx/onnx/backend/test/runner/__init__.py", line 23, in <module>
    from onnx import helper, numpy_helper, NodeProto, ModelProto, TypeProto
  File "/Users/hua/Development/onnx/onnx/numpy_helper.py", line 21, in <module>
    def to_array(tensor: TensorProto, base_dir: Text = "") -> np.ndarray[Any]:
TypeError: Type subscription requires python >= 3.9
```

looks related to Type Hinting Generics (https://www.python.org/dev/peps/pep-0585/) so unfortunately this is likely not scalably convertable in the short term 

lgtm-com[bot](2022-01-31 21:44:53):This pull request **introduces 1 alert** and **fixes 1** when merging e82f5dbd52b8986f607205de64103153333d93d2 into 50a1981dc3d50af13075cec33b08b4c87fb0e41f - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-2aac9721be52763c3d783b69766a8920e7e534d6)

**new alerts:**

* 1 for Unreachable code

**fixed alerts:**

* 1 for Unused local variable
garymm(2022-02-19 00:51:09):FYI did this in https://github.com/onnx/onnx/pull/3962
askhade(2022-01-28 17:13:10):So looks like we don't have a test for this deprecated attr in resize.py, we only have the model. This is most likely why we haven't seen failures in ORT because in ORT we generate the test data.

Another reason why we need to validate generated test data in CIs
gramalingam(2022-01-29 02:35:17):A quick comment: I suspect that issue 3969 will need a specific check in Loop operator's shape-inference method. I doubt if a generic check will be sufficient. Eg., note that a loop might use 5 loop-carried dependences ... then, it should be checking it has 5 outputs to handle them all. In fact, we should already have logic that updates the output-types for all these outputs ... that logic should ensure that these outputs do exist.
jcwchen(2022-01-29 04:53:04):>  note that a loop might use 5 loop-carried dependences ... then, it should be checking it has 5 outputs to handle them all. In fact, we should already have logic that updates the output-types for all these outputs ... that logic should ensure that these outputs do exist.

Are you saying that the input number of Loop's state variable (except M and condition) should equal to the output number? If so, yes I think current Loop's shape inference should cover that by type propagation.

A question: does _OpSchema::Variadic_ allow any optional output (or input)? If not, current checker does not check that and that's why I added a generic check (checking OpSchema::Variadic cannot have any optional input/output). IIUC, this generic check should cover cases like issue 3969. Please correct me if I am wrong. Thank you!
gramalingam(2022-02-01 21:43:39):Ok, I think there are two separate problems. For your question about Variadic: we allow the op-spec to specify a "min arity" which is the minimum number of expected parameters. This is used to calculate min_input_ and max_input_ (and similarly for outputs). This is checked here: https://github.com/onnx/onnx/blob/e4e17979171b0318205978fbb77d4316d692e771/onnx/defs/schema.cc#L151 ... but you are right, it is not being checked whether the inputs < min_input_ are an empty string. So, a check like you are adding is missing. But I think this check should be limited to inputs up to min_input_ and outputs up to min_output_.
gramalingam(2022-02-01 21:44:42):But I think the loop problem is a separate one, because the loop does not specify a minimum number of loop-carried dependences. That check is specific to that op, and hence must be in that op's shape-inference method.
jcwchen(2022-02-04 19:14:26):>  But I think this check should be limited to inputs up to min_input_ and outputs up to min_output_.

To confirm: So it is OK that for Variadic parameter, the input (or output) whose  index > min_input_ (or min_output_) has any optional variable? I wasn't sure about this case and it seems a weird use case to me. If yes, I will update my current check for only considering index smaller than min_input_ (or min_output_).

> But I think the loop problem is a separate one, because the loop does not specify a minimum number of loop-carried dependences. That check is specific to that op, and hence must be in that op's shape-inference method.

Thank you for the clarification. So another missing check only for Loop op is to make sure the exact output number needs to be exactly the same as the input number? If so, yes I can include that check into this PR as well. 

gramalingam(2022-02-04 19:52:01):> To confirm: So it is OK that for Variadic parameter, the input (or output) whose index > min_input_ (or min_output_) has any optional variable? I wasn't sure about this case and it seems a weird use case to me. If yes, I will update my current check for only considering index smaller than min_input_ (or min_output_).

You are right, these are all weird cases, and are not expected to happen in practice. I think the check you have is fine too, and let us keep it the way you have. I think the main case we want to catch is, for example, if users call the Max (or Mean, or Min, etc.) operator https://github.com/onnx/onnx/blob/main/docs/Operators.md#Max with zero inputs, because we expect 1 or more inputs. This is already being caught if the user omits the parameters completely, but if they use empty string it won't be caught. But you are right that if they call Min(x, "", y), it is a bit weird too ... should we complain, or just treat it as Min (x, y)? It may be safer to complain, it helps simplify the backends/runtimes from handling these weird cases.
gramalingam(2022-02-04 21:28:16):> So another missing check only for Loop op is to make sure the exact output number needs to be exactly the same as the input number?

Yes. But I look at the existing code. This requires some non-trivial extension. The existing InferenceContext does not give us a direct API to check if an input/output exists. The right solution might be to extend it to add `hasInput` and `hasOutput` methods. (We could use a hack of returning nullptr for `getInputType` and `getOutputType` for this purpose, but even this would require some change, eg., in onnxruntime, to do this.)
jcwchen(2022-02-07 18:26:24):> It may be safer to complain, it helps simplify the backends/runtimes from handling these weird cases.

Agreed. I will keep it then.

> Yes. But I look at the existing code. This requires some non-trivial extension. The existing InferenceContext does not give us a direct API to check if an input/output exists.

I might miss something here -- Can't we simply just use `getNumInputs` and `getNumOutputs` from current `InferenceContext`? It seems to me they respectively represent the number of `n.input` and `n.output` and we can simply compare the number of them? (If we have the OpSchema::Variadic check which this PR brings, we can make sure all inputs or outputs except M and cond do exist)

gramalingam(2022-02-07 23:43:42):> Can't we simply just use `getNumInputs` and `getNumOutputs` from current `InferenceContext`?

Yes, you are right. Combined with the separate variadic check, this should work. I just realized that there is one disdvantage with the variadic check itself. It works for current ops, but if we add an op in the future where the variadic parameters have some specific interpretation, users might want to pass in inputs like `[X, "", Y]` for a variadic parameter. But I guess we can handle it if and when we need such an op. 
garymm(2022-06-16 20:49:03):Attached a model that is now declared illegal. This model is produced by ORT training tests:
[ORT-training-model.zip](https://github.com/onnx/onnx/files/8922250/ORT-training-model.zip)

It seems like the code here creates a node where some inputs may be empty, followed by non-empty inputs:
https://github.com/microsoft/onnxruntime/blob/084165c748bcd6fbb3fca4d60b1ece911c04c043/orttraining/orttraining/core/graph/gradient_builder.cc#L1766-L1774

It seems it was intentionally added in this PR:
https://github.com/microsoft/onnxruntime/pull/8105

I'm not sure why they can't just include all inputs in that for loop. I think it might be a performance optimization.
@jcwchen @gramalingam do you think we should make the checker more lenient here, or ask the ORT team to try to change their code to be compliant? (Maybe they could use some constant value rather than empty string for example)

gramalingam(2022-06-16 20:59:00):I think we should make the checker more lenient here. (As discussed above.)  It does mean that checking that all inputs are specified for a loop op may be harder (as discussed above). But given the time-constraint, we could postpone the stronger-check for loops to after current release (as it might be non-trivial)?
jcwchen(2022-06-16 22:10:47):The final decision is reverting this PR for now: https://github.com/onnx/onnx/pull/4283. Thanks for catching this.
jcwchen(2022-02-09 17:10:19):IIUC, we cannot get the exact number of loop stare variable in outputs. (don't know the number of scan_outputs K either) So I only made a check to make sure that at least the number of output cannot be smaller than the number of loop stare variable in input. @gramalingam please review it. If it looks good to you, I will add it in older Loop inference function as well.

Another question: do we need to do similar check for Scan op?
gramalingam(2022-02-23 18:41:08):We can figure out the number of scan-outputs from the number of outputs of the loop-body subgraph.
gramalingam(2022-02-23 18:47:53):I think we already have the necessary check in line 342? So, may be the check you added for variadic part is sufficient? 
jcwchen(2022-03-03 15:57:29):Good catch. I just reverted it. Thank you @gramalingam for the review. I think this PR is almost ready to go. Please review it again.
askhade(2022-02-01 18:30:04):since this is a new file, please add comments why this file is need and why we are building protobuf from source.


askhade(2022-02-01 18:31:11):when we update protobuf to a next version we will need to update all these places... wondering whether there is an easier way to fetch the version from a common place... not implying that this needs to be done in this PR.
jcwchen(2022-02-01 18:44:02):Sounds good. Just added reasons on the top of the file. Please review it. Thanks!
jcwchen(2022-02-01 18:49:26):Yes it is a burden when every time ONNX bumps protobuf version... Also I was considering to have a common script for building protobuf from source since we have quite many different CIs using the same commands.  I will add this investigation in my TODO list.
lgtm-com[bot](2022-01-31 23:36:13):This pull request **fixes 6 alerts** when merging 4670c961f2707f7679930c59db0d099b0fc64919 into b5de5867f0e5ecb471baed763db7fc8836ccb201 - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-c75751d13662dd1813b9439c0065ad5dd56466bf)

**fixed alerts:**

* 4 for Unused import
* 1 for Unused local variable
* 1 for Module is imported with &#39;import&#39; and &#39;import from&#39;
lgtm-com[bot](2022-02-04 02:09:01):This pull request **fixes 6 alerts** when merging d3da2173ae919b43082436c70872cff47013423b into 6a2e9c12e13075ba169d17dcc19073d1a30b1ee7 - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-4753576a0f558955423f17089547d2edc30e95f6)

**fixed alerts:**

* 4 for Unused import
* 1 for Unused local variable
* 1 for Module is imported with &#39;import&#39; and &#39;import from&#39;
lgtm-com[bot](2022-02-25 22:03:30):This pull request **fixes 6 alerts** when merging 01dfa0ed7cde06eeb92be77e9a40d0cc5099e546 into c940fa3fea84948e46603cab2f86467291443beb - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-6d5b60ad20a7781c372d8ef27b73231d327b9d3e)

**fixed alerts:**

* 4 for Unused import
* 1 for Unused local variable
* 1 for Module is imported with &#39;import&#39; and &#39;import from&#39;
garymm(2022-02-25 22:43:00):@postrational @linkerzhang would you mind approving?
lgtm-com[bot](2022-02-25 22:59:43):This pull request **fixes 6 alerts** when merging fa31e87169c8d7180c945a86874731617a83b43e into c940fa3fea84948e46603cab2f86467291443beb - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-514aa3224c7c5923d83c827767ff101af81fc698)

**fixed alerts:**

* 4 for Unused import
* 1 for Unused local variable
* 1 for Module is imported with &#39;import&#39; and &#39;import from&#39;
lgtm-com[bot](2022-02-26 00:10:35):This pull request **fixes 6 alerts** when merging 5a16237f4b20032756b12acdebe696ab7fb4714e into c940fa3fea84948e46603cab2f86467291443beb - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-92bc9fd7cd8464fadd980df2af187e6e1a10f436)

**fixed alerts:**

* 4 for Unused import
* 1 for Unused local variable
* 1 for Module is imported with &#39;import&#39; and &#39;import from&#39;
lgtm-com[bot](2022-02-26 01:35:54):This pull request **fixes 6 alerts** when merging a15a9ed69c761360e444bd196f0c18d490503f49 into c940fa3fea84948e46603cab2f86467291443beb - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-5e2aa7fb3f359cee74e4fab139a31a0d9a2fb9d2)

**fixed alerts:**

* 4 for Unused import
* 1 for Unused local variable
* 1 for Module is imported with &#39;import&#39; and &#39;import from&#39;
garymm(2022-02-28 22:08:38):@gramalingam could you please approve?
lgtm-com[bot](2022-02-28 23:08:27):This pull request **fixes 6 alerts** when merging ef8b2fb02730f460811d4c11c5b8e4d0475cf046 into 5a6628799ee4fa31edc4451886363f6d47095caa - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-4e278518372d02d543f90c7e1b02b8261c33e010)

**fixed alerts:**

* 4 for Unused import
* 1 for Unused local variable
* 1 for Module is imported with &#39;import&#39; and &#39;import from&#39;
garymm(2022-02-03 01:30:00):@jcwchen can you please merge this for me?
snnn(2022-02-21 03:28:15):Fetch the tarball would be simpler, which requires less network traffic than "git clone". When you run "git clone", you download all the versions.
xuzijian629(2022-02-07 07:07:40):Thank you for your review!

I'm still not sure what is the best practice, but my current idea is as follows.
* Call `format.sh` in CIs.
* Keep `python setup.py typecheck` command
* Do not introduce `python setup.py lint` or `python setup.py format`.

The reasons are
* `python setup.py typecheck` command is explained several times in docs and some users may already get used to it.
* It's difficult to prepare two scripts, `lint` and `format`, because flake8 is a lint tool (i.e., do not format), and clang-format is a format tool. So I put all checks into single file.
* `python setup.py lint` would be something that calls `format.sh` within subprocess. I think it's redundant and just calling `format.sh` would be nicer.
xuzijian629(2022-02-07 07:10:24):Since clang-format is still not applied, CI checks are failing.

Applying clang-format will cause a large amount of changes, so I'll do it later when other code is ready.
xuzijian629(2022-03-15 07:32:49):Hi @jcwchen !
Thank you for your response!
I removed clang-format calls from CI. Now, I think CIs will pass.
jcwchen(2022-02-04 18:19:48):As you mentioned in the PR description, I think adding a new command like "python setup.py lint" and removing existing typecheck should be a good idea.
xuzijian629(2022-02-07 07:08:20):Thanks for the comments!
For `python setup.py lint` command, I summarized my opinion in https://github.com/onnx/onnx/pull/3988#issuecomment-1031139310
jcwchen(2022-03-15 15:33:53):Shall we use `flake8` instead of `flake8 onnx` to cover more files under ONNX repo? For instance, the files [here](https://github.com/onnx/onnx/tree/main/workflow_scripts).
garymm(2022-03-15 17:18:41):Please use one consistent word: either "style", "format", or "lint" for all comments, file names, error messages, etc.
I'd go with "lint" or "style" since it is definitely broader than formatting.
garymm(2022-03-15 17:19:25):`#!/usr/bin/env bash`

https://stackoverflow.com/questions/16365130/what-is-the-difference-between-usr-bin-env-bash-and-usr-bin-bash
garymm(2022-03-15 17:20:29):use long option names for readability:
```suggestion
set -o errexit
set -o nounset
```
garymm(2022-03-15 17:22:14):add newline at end of file
https://stackoverflow.com/questions/729692/why-should-text-files-end-with-a-newline
xuzijian629(2022-03-16 08:02:54):Since just using `flake8` also affects files in CI environment, I changed this to `flake8 onnx tools workflow_scripts` (also, it will also prevent checking files in `.venv/` in your local environment).

https://dev.azure.com/onnx-pipelines/onnx/_build/results?buildId=21975&view=logs&jobId=51991ee2-49eb-5ba6-75d8-d9fa44c75e77&j=51991ee2-49eb-5ba6-75d8-d9fa44c75e77&t=134bfcd8-a9e0-5ce3-345d-c211e8a60d77
kevinch-nv(2022-02-04 20:13:51):@onnx/sig-archinfra are we testing the reporting functionality in the backend tests in CI? The previous MR that added type annotations introduced a bug that didn't seem to be caught by CI.

A minimal repro:
```
import pytest

def test(inp: pytest.nodes.Item):
    print(inp)

test(0)
```
Running this script gives: 
```
python3 test.py
Traceback (most recent call last):
  File "test.py", line 3, in <module>
    def test(inp: pytest.nodes.Item):
  File "/usr/local/lib/python3.8/dist-packages/pytest/__init__.py", line 171, in __getattr__
    raise AttributeError(f"module {__name__} has no attribute {name}")
AttributeError: module pytest has no attribute nodes
```
In this MR I've updated the type annotation to use the correct one from `_pytest`, but the CI build is failing. Given that `_pytest` holds the internal classes of `pytest`, should these functions even be type annotated?

jcwchen(2022-02-04 22:14:37):```suggestion
import _pytest  # type: ignore
```
jcwchen(2022-02-04 22:15:06):Ignore it for now. In this way it should pass mypy check in CIs
jcwchen(2022-02-07 16:47:56):FYI: I made a commit to ignore _pytest because mypy doesn't have a stub for it. Sorry about directly modifying it because I think your fix is good to be included in the upcoming 1.11 release (so I want to forward this PR ASAP). Thank you for the contribution! 
jcwchen(2022-02-09 14:53:54):For others' reference: This function was moved to .cc by my PR: https://github.com/onnx/onnx/pull/3772. However, ORT training relies on it: https://github.com/microsoft/onnxruntime/blob/6a7d3deb22bd78c2c243e1c8143000663127be5d/orttraining/orttraining/core/graph/training_op_defs.cc#L869.
askhade(2022-02-18 18:54:17):LGTM. 
You created a doc to list details of all CI and release pipelines, can you update it to reflect these changes as well and include it in this PR.
jcwchen(2022-02-18 22:06:11):> LGTM. You created a doc to list details of all CI and release pipelines, can you update it to reflect these changes as well and include it in this PR.

Thanks for the reminder! I have updated.
askhade(2022-02-18 17:45:45):add python 3.9 we should have atleast 1 variant
jcwchen(2022-02-18 18:06:21):Good point. I have updated both Linux-CI and MacOS-CI to make them cover Python 3.7-3.9
mhamilton723(2022-02-11 18:02:59):Thank you @jcwchen!
jcwchen(2022-02-11 02:26:22):Remove it temporally for testing _download_file in all CIs. Will add it back before merging this PR.
jcwchen(2022-02-11 04:14:36):It passes all CIs and I am adding it back.
xadupre(2022-02-11 13:47:36):`:param`
jcwchen(2022-02-11 15:02:21):Good point. `:param` might be more prevalent than `@param`. Have just updated it globally in the same file. Thanks!
jcwchen(2022-02-16 20:04:06):Draft this because now release CIs run normally. Won't need this for now and probably after the release we will just upgrade to manylinux2014
jcwchen(2022-02-18 23:24:53):Close it since ONNX now uses manylinux2014 instead of manylinux2010 by https://github.com/onnx/onnx/pull/4001.
gramalingam(2022-02-15 20:59:09):Is that `()` mean an empty shape (rank 0) or no shape?
gramalingam(2022-02-15 21:00:06):Ideally it should be `None` or something like that
jcwchen(2022-02-15 21:21:05):Good question. ~It's tricky here because it seems that both shape (rank 0) and no shape represent in the same way (empty shape in Python)~... Let me investigate more and update the test

Update: Actually they are different:
1. No shape: the tensor type does not have the "shape" field.
2. Rank 0: the tensor type has the "shape" field without any content.
gramalingam(2022-02-15 22:00:02):I think we should supply `None` here when creating the value_info. But the comparison should be fixed at https://github.com/onnx/onnx/blob/d824ce38f935b83f724628a254cb855a95180778/onnx/test/shape_inference_test.py#L76 to check if one has no shape the other has no shape also.
jcwchen(2022-02-15 22:12:59):Yes! I am exactly doing the same thing by adding:
```
assert vi_type.tensor_type.HasField('shape') == inferred_vi_type.tensor_type.HasField('shape')
```
I have tried that rc2 cannot pass this test, but with my fix in this PR, it can pass. Thank you!
gramalingam(2022-02-15 22:13:36):the for-loop below should also be protected by `if (vi_type.tensor_type.HasField('shape'):`
jcwchen(2022-02-15 22:48:29):Although even if a tensor_type doesn't have a shape, tensor_type.shape.dim will just be empty. Still, I think adding protection is safer. Added. Thanks!
gramalingam(2022-02-15 22:52:22):The problem is that `tensor_type.shape` has the side-effect of initializing `shape` to be an empty list (an unfortunate design choice of protobuf) if the object doesn't have a `shape` field. The checker method should not have such a side-effect.
jcwchen(2022-02-15 23:35:57):Originally these 3 tests (which involve Optional-related operators) have a tensor whose shape is empty (has the shape field without any content). We should use `()` to represent this case.

A follow-up question: are rank 0 (having a shape field without any content) and empty shape (not having a shape field) always indifferent? From my understanding, rank 0 means it doesn't have (or need) a shape; empty shape means unknown shape. @gramalingam am I correct? Thank you.
gramalingam(2022-02-16 00:06:09):Let me look at the tests. But "rank 0" means same as "shape is an empty list" same as "empty shape". But "unknown shape" is different and is represented by "no shape field" and it means that the "rank is unknown".
gramalingam(2022-02-16 00:07:18):Using shape `()` or `[]` for "rank 0" makes sense to me. I need to look at the examples to see what is expected in the example.
gramalingam(2022-02-16 00:14:04):I don't think `shape=()` in this example. The then branch produces a `shape=[1]` and else branch produces a `shape=[5]`.  
gramalingam(2022-02-16 00:31:55):I think we should use `shape=(None,)` or `shape=[None]` here
gramalingam(2022-02-16 00:34:04):Yes, here this looks fine, since `output` must be a scalar boolean with empty shape
jcwchen(2022-02-16 00:37:27):Updated since (None, ) is more precise as you suggested. Thank you @gramalingam !
jcwchen(2022-02-16 16:35:16):Is `#include <iostream>` required?
snnn(2022-02-16 18:12:39):Yes, the code below use std::cerr
xadupre(2022-02-18 16:10:34):Sure I'll do that today.
xadupre(2022-02-18 16:47:12):Done.

I started this PR because I was trying to create a documentation for onnx which you can see here: https://xadupre.github.io/draft/onnx_ort/api/onnx_python/index.html. 
gramalingam(2022-02-24 15:51:33):May be worth adding the following lines (from the subsequent section Operator Set):
```
The operator set version is a simple integer value that is monotonically increased as new versions of the
operator set are published.

Operator sets other than the default operator set MUST specify a domain and SHOULD use reverse domain names
based on the responsible organization's identity, the same convention that is used for
[naming Java packages](https://docs.oracle.com/javase/tutorial/java/package/namingpkgs.html).
```
sechkova(2022-02-24 16:19:52):Thanks for the review, it's added with [9600196](https://github.com/onnx/onnx/pull/4039/commits/96001968cf0e9c5569cc308efe9215b0c707ab4d)
jcwchen(2022-03-25 17:14:54):@dnlyun Do you have bandwidth to finish this PR? If not, please allow me to send a duplicate PR of yours and go forward. Thank you for your contribution.
dnlyun(2022-03-25 20:39:12):> @dnlyun Do you have bandwidth to finish this PR? If not, please allow me to send a duplicate PR of yours and go forward. Thank you for your contribution.

Hi @jcwchen, you can go ahead and send the duplicate PR. Thanks
jcwchen(2022-03-30 16:09:40):I created a duplicate PR and this PR was superseded by https://github.com/onnx/onnx/pull/4094. Thus close this one. Thank you for your contribution!
jcwchen(2022-03-10 03:01:11):Where did these updates come from? I didn't see corresponding update from defs.cc/old.cc
jcwchen(2022-03-10 03:04:04):The updates here in TestCoverage.md seem unrelated to this PR.
garymm(2022-03-09 19:17:42):Add a comment to say
# TODO: update once TensorFlow 2.6 is end of life
RobertHenry6bev(2022-03-09 18:51:13):I can not figure out how to git push from my command line with the github
supplied pat.  I've spent 20 minutes on this, which is waaay more than I
should have to do for a 1 word fix from the GUI to github.

I am Robert R. Henry aka ***@***.***

On Wed, Mar 9, 2022 at 10:23 AM G. Ramalingam ***@***.***>
wrote:

> ***@***.**** approved this pull request.
>
> —
> Reply to this email directly, view it on GitHub
> <https://github.com/onnx/onnx/pull/4061#pullrequestreview-904794854>, or
> unsubscribe
> <https://github.com/notifications/unsubscribe-auth/ABBLLY7GYJUQEL2OQRS26QTU7DUA7ANCNFSM5QH2C6QA>
> .
> Triage notifications on the go with GitHub Mobile for iOS
> <https://apps.apple.com/app/apple-store/id1477376905?ct=notification-email&mt=8&pt=524675>
> or Android
> <https://play.google.com/store/apps/details?id=com.github.android&referrer=utm_campaign%3Dnotification-email%26utm_medium%3Demail%26utm_source%3Dgithub>.
>
> You are receiving this because you authored the thread.Message ID:
> ***@***.***>
>

jcwchen(2022-03-09 18:56:12):> I can not figure out how to git push from my command line with the github supplied pat. I've spent 20 minutes on this, which is waaay more than I should have to do for a 1 word fix from the GUI to github. I am Robert R. Henry aka ***@***.***

Since there is only one commit which was not signed-off, you can use the commands below:
```
git commit --amend --signoff    # sign-off previous commit
git push -f  # force push to modify previous commit remotely
```


RobertHenry6bev(2022-03-25 17:39:35):I do not have the bandwidth to jump through all of the hoops to certify
myself as a valid OSS contributor to this project. Too daunting of a
gauntlet for the time and patience I had for a one-off typo fix.

Please file a separate PR for this fix.

Robert Henry
***@***.***

On Fri, Mar 25, 2022 at 10:16 AM Chun-Wei Chen ***@***.***>
wrote:

> ***@***.**** commented on this pull request.
>
> @RobertHenry6bev <https://github.com/RobertHenry6bev> Do you have
> bandwidth to fix the DCO issue? If not, please allow me to send a duplicate
> PR of yours and go forward. Thank you for your contribution.
>
> —
> Reply to this email directly, view it on GitHub
> <https://github.com/onnx/onnx/pull/4061#pullrequestreview-921935660>, or
> unsubscribe
> <https://github.com/notifications/unsubscribe-auth/ABBLLY4YIMEWMBMFDMVLRH3VBXYHHANCNFSM5QH2C6QA>
> .
> You are receiving this because you were mentioned.Message ID:
> ***@***.***>
>

gramalingam(2022-03-25 20:31:00):@RobertHenry6bev : thanks for the fix you suggested. I agree this is a lot of overhead for such a small fix, but unfortunately that's required as of now. For small fixes, please feel free to suggest it as an issue, and we can try to create a PR. Contributions in any form are greatly appreciated, thanks!
jcwchen(2022-03-30 16:10:33):I created a duplicate PR and this PR was superseded by https://github.com/onnx/onnx/pull/4094. Thus close this one. Thank you for your contribution!
jcwchen(2022-03-24 00:45:05):It would be great if we can add generated_shape_data_by_name_in into ShapeInferenceImplBase like [here](https://github.com/onnx/onnx/blob/67f21f2095084f96ca1698ed621a125fe8b53fb0/onnx/shape_inference/implementation.cc#L572) for FunctionProto.
jcwchen(2022-03-24 23:04:02):After discussion, we think it's fine to do it in another PR.
jcwchen(2022-03-25 00:22:43):nit: do we want to keep this for future reference or it can be removed in this PR?
gramalingam(2022-03-29 17:43:27):Removed the comments, thanks!
gramalingam(2022-04-11 17:33:12):Closing this and opening https://github.com/onnx/onnx/pull/4127 (to deal with signoff issues).
chausner(2022-04-06 22:48:29):```suggestion
        all input variables to 32-bit float, perform the computation, and
```
chausner(2022-04-06 22:51:17):```suggestion
                          ? "NumReducedAxes = Sub (Rank, Axis1D)"                               // [rank - axis]: 1D tensor
                          : "NumReducedAxes = Neg (Axis1D)")                                    // [-axis] : 1D tensor
```
chausner(2022-04-06 22:52:14):```suggestion
      The first stage is standardization, which makes the
      normalized elements have zero mean and unit variances.
      The computation required by standardization can be
```
chausner(2022-04-06 22:54:30):git attributes should be set to declare all .pb files as binary, otherwise it can lead to data corruption when git interprets them as text files and messes up line ending characters.
chausner(2022-04-06 22:54:57):```suggestion
    # Standardization step. y_mat is zero-mean and unit-variance.
```
chausner(2022-04-06 22:55:25):```suggestion
        The first stage is standardization, which makes the
        normalized elements have zero mean and unit variances.
        The computation required by standardization can be
```
chausner(2022-04-06 22:56:49):```suggestion
        The first stage is standardization, which makes the
        normalized elements have zero mean and unit variances.
        The computation required by standardization can be
```
wschin(2022-04-12 15:40:01):Is [it](https://github.com/onnx/onnx/blob/2ab133404afce34552aaccd86e7023e1fb9a60d2/.gitattributes#L1) what you're looking for?
chausner(2022-04-12 15:57:57):Not quite. See https://devtut.github.io/git/using-a-gitattributes-file.html#identify-binary-files.
chausner(2022-04-12 16:00:06):For the changes to take effect, it's important that you perform the commit after the .gitattributes file is modified. In this case, you'll have to remove the files and re-add them. GitHub should then also show that the files are binary and not attempt to diff them.
wschin(2022-04-13 03:19:52):I changed `.gitattributes` at this [commit](https://github.com/onnx/onnx/pull/4076/commits/0fc927cc315b1386a2e15054998de5bfa1b606c1). Then, I [removed ](https://github.com/onnx/onnx/pull/4076/commits/73eeee8e90b05078eb2c91183dfdf1615e639aa4)and [added ](https://github.com/onnx/onnx/pull/4076/commits/ebfbde75451c708f13dea14270ca1f5597857a2a)back my `.pb` files. Does it look ok now?
wschin(2022-04-13 04:21:55):I have tried many combinations of `binary`, `-text`, and so on in `.gitattributes` file. Github just keeps showing the diff for `*.pb` files. Since this is a more difficult problem, I'd suggest to address it in another PR.
gramalingam(2022-04-14 23:48:49):Is the second `axes` supposed to be `normalized_axes`?
gramalingam(2022-04-14 23:51:39):Regarding the use of "may": would it be possible to be more precise about what is guaranteed/required (in terms of precision) and what is allowed to be dependent on the implementation?
gramalingam(2022-04-14 23:52:37):Unless there is a good reason, it is better to be specific.
gramalingam(2022-04-14 23:55:46):The definition of BatchNorm went through multiple revisions because there was a need to allow different precisions for different inputs. Is it fine if X/Scale/B are all constrained to be same type?
gramalingam(2022-04-14 23:57:41):Seems redundant given previous line.
gramalingam(2022-04-15 00:02:48):nit: probably simpler to omit the prefix `saved_` in the variable names here: just `mean_shape` and `inv_std_dev_shape`
gramalingam(2022-04-15 00:22:15):Some testcases with negative axis and unspecified axis may be good.
wschin(2022-04-15 03:32:37):Makes sense. I change it to `must`.
wschin(2022-04-15 03:43:59):Layer normalization is very similar to batch normalization (major difference is the normalized axes). As batch normalization have shown the need of using difference precision for `mean` and `variance` tensors, I prefer to keep this as is for layer normalization's `mean` and `variance`. In addition, mix-precision training for language model is popular enough to support extra types. For inference, I guess mix-precision is also helpful to compress the trained models. Does it sound ok?
gramalingam(2022-04-27 19:07:00):Discussed with Wei-Sheng. Summary: the inputs will be of same type for now, as there is no use-case yet of using mixed-precision for inputs.
jcwchen(2022-04-07 00:28:27):Could you please resolve the conflict and let's forward this one?
garymm(2022-04-07 20:44:43):@jcwchen resolved merge conflict.
gramalingam(2022-03-23 17:16:34):I am unclear about the exact invariants of the IR and the design. Would it be possible to add a line or so near https://github.com/onnx/onnx/blob/94e2f64551ded652df53a7e9111031e8aabddaee/onnx/common/ir_pb_converter.cc#L223 to at least document the creation of dummy nodes for valueinfos without a definition?
jcwchen(2022-03-24 01:01:51):After discussion, we think simply skipping such a unused value_info should be fine for now. Then we don't need to create a dummy node for those unused value_infos. 
gramalingam(2022-03-23 17:19:09):What exactly does `kCaptured` mean? Maybe it is relevant to the other two original calls, but not sure it means the same for undefined valueinfos.
jcwchen(2022-03-24 01:08:48):Good question. It seems to me that `kCaptured` is used for an inner (nested) graph that some inputs/outputs might not be visible in value_by_name_of so it creates a dummy node for them with `kCaptured` type.
xuzijian629(2022-03-24 01:02:06):Since the order of `include` in `onnx/common/ir.h` was reordered, `onnx/string_utils.h` was missing in `onnx/common/tensor.h`. I added it manually in `onnx/common/tensor.h.`
jcwchen(2022-04-21 20:38:00):Update: ONNX will have a release between May and June. Applying clang-format right before the code freeze would be a good time. I will let you know once we have finalized the code freeze date. Thank you for waiting!
jcwchen(2022-05-05 17:53:04):Update:
The current plan is -- code freeze on 5/23 and release on 6/5. We can probably do it after 5/23. Thank you!
jcwchen(2022-05-20 19:08:50):The code freeze date is on schedule (5/24) and most of PRs have been merged. If you have bandwidth around next Monday, feel free to update this PR and let's merge it before upcoming ONNX 1.12. Thank you!
xuzijian629(2022-05-23 06:36:38):I merged main branch and re-applied clang-format!
If you have any request before code freeze, please let me know! Thanks!
jcwchen(2022-05-23 19:39:10):Thank you for the quick update! Sorry that there was just a large PR got merged: https://github.com/onnx/onnx/pull/3741 which has a few C++ codes. If no surprise, there shouldn't be any new C++ related PR for ONNX 1.12. Could you please merge the main branch and apply clang-format again. Then, I think this PR is ready to go. Thanks!
xuzijian629(2022-05-24 13:00:26):I merged main and applied clang-format. Thanks for managing this PR for a long time!
fdwr(2022-03-29 05:12:04):FYI @jcwchen and @spandantiwari.
garymm(2022-03-30 16:41:44):issue or PR? If issue, please provide more details on what the issue should contain, tags, etc.
garymm(2022-03-30 16:42:38):turn URL into link for readability
e.g.
`follow [these instructions](https://github.com/microsoft/onnxruntime/blob/master/docs/How_To_Update_ONNX_Dev_Notes.md)`
liqunfu(2022-04-04 18:50:51):It is an issue to be created in ORT. Add a sample issue in ORT and linked here for brevity. Clarified the text so it is clear that the work is on the ORT side to do the integration.   
liqunfu(2022-04-04 18:51:35):fixed
jcwchen(2022-03-31 21:09:10):Not sure whether this invitation link will be expired because it seems quite random... although it's the same URL as a month ago.
prasanthpul(2022-04-06 23:19:53):shouldnt expire. if it does for som reason, we can update
jcwchen(2022-04-14 18:37:53):Here is the summary for different libraries behavior:
- PyTorch
```
import torch
torch.nonzero(torch.tensor(0))
# tensor([], size=(0, 0), dtype=torch.int64)
torch.nonzero(torch.tensor(1))
# tensor([], size=(1, 0), dtype=torch.int64)
```
- TensorFlow
```
import tensorflow
tensorflow.where(tensorflow.constant(0))
# InvalidArgumentError: WhereOp : Unhandled input dimensions: 0 [Op:Where]
tensorflow.where(tensorflow.constant(1))
# InvalidArgumentError: WhereOp : Unhandled input dimensions: 0 [Op:Where]
```
- NumPy
```
import numpy as np
np.array(np.nonzero(0))
# array([], shape=(1, 0), dtype=int64)
np.array(np.nonzero(1))
# array([[0]]), shape (1, 1)
```

To sum up, it seems that old ONNX follows PyTorch's behavior, TensorFlow does not allow scalar input for Where (NonZero-like operator in TensorFlow). ONNXRuntime follows NumPy's behavior, which is what the ONNX document said.
gramalingam(2022-04-14 19:16:15):ONNX itself had an inconsistency (the documentation said "like numpy", but that was vague, but the shape-inference code was more specific and had the pytorch behavior). So, it seems best to fix the "old" op by updating the documentation to clarify the behavior to be aligned with shape-inference. 

For the "new" op: one question is: Does anyone want the numpy behavior?

Assuming some users want it, the best solution seems an upward compatible extension by adding an attribute "treat_0D_as_1D", which if set to 1 (true), will produce the numpy behavior.

Does this sound reasonable?
jcwchen(2022-04-14 19:59:03):Thank you for the conclusion @gramalingam. I agree with your points -- 

> ONNX itself had an inconsistency (the documentation said "like numpy", but that was vague, but the shape-inference code was more specific and had the pytorch behavior). So, it seems best to fix the "old" op by updating the documentation to clarify the behavior to be aligned with shape-inference.

+1 Since ONNX follows PyTorch and TensorFlow even does not allow scalar input. And also NumPy behavior will be deprecated. A proper direction should be following upstream like PyTorch or TensorFlow to prevent users' confusion. We should update the document to resolve ambiguity.

> For the "new" op: one question is: Does anyone want the numpy behavior?

Yes, it's an open question. Personally I think it shouldn't be a popular case since most of developers would probably follow PyTorch and TensorFlow. Still, this will very likely break backwards compatibility in ORT so it's better to include ORT core team to make the decision. 

jcwchen(2022-04-14 21:30:57):@gramalingam before filing an issue in ORT, I just found another issue in ONNX -- it seems to me that current ONNX shape inference for NonZero with scalar input `1` will infer shape as (0, unknown), which is different from PyTorch (1, 0), although with scalar input `0` will infer shape as (0, unknown), which is the same as PyTorch (0, 0). Therefore, current ONNX shape inference does not exactly follow PyTorch's behavior. To fix it, if giving a scalar input, ONNX should infer shape like (unknown, 0) instead of (0, unknown), am I correct?
gramalingam(2022-04-14 21:55:18):@jcwchen : just to clarify: I think you mean that the order of dimensions is different for pytorch (N X D) and numpy (DxN), where D denotes the rank of the input tensor and N denotes the number of non-zero values. Is that correct? It is not specific to scalars. I don't think we should "fix" it, since it changes the spec. I assume that the ORT implementation returns the same as what ONNX shape inference says (DxN) for non-scalars? If so, we should continue to use DxN. We should clarify all this in the op. documentation, but not change the spec or shape-inference, I think. It may be useful to check what the pytorch-to-onnx converter does (it would need to introduce a transpose?). What about tf.where for non-scalars? Does it return NxD or DxN?

jcwchen(2022-04-15 18:09:44):Thank you for the clarification! That's really good to know -- so this difference is a global issue (PyTorch (N X D) v.s. NumPy, ONNX (DxN)) and it's not a specific issue here.

> What about tf.where for non-scalars? Does it return NxD or DxN?

I think TensorFlow behaves like PyTorch -- NxD. For instance
```
import tensorflow as tf
tf.where([2, 4, 0, 5]).shape
# TensorShape([3, 1])
```

> It is not specific to scalars. I don't think we should "fix" it, since it changes the spec. I assume that the ORT implementation returns the same as what ONNX shape inference says (DxN) for non-scalars? If so, we should continue to use DxN. We should clarify all this in the op.

Agree -- if ONNX behaves differently from PyTorch and TensorFlow, it's better to clarify it in the document.
gramalingam(2022-04-28 18:38:06):Hi @jcwchen : would be useful to update the PR description/title, now that this has changed into just adding test-cases
jcwchen(2022-04-28 18:52:52):> Hi @jcwchen : would be useful to update the PR description/title, now that this has changed into just adding test-cases

Thanks for the reminder. I have updated this PR's title, description and content. Please review it again. Thank you!

To summarize, ONNX decides to follow PyTorch's behavior and keep the original shape inference for NonZero. Furthermore, clarify NonZero's behavior for scalar input to prevent confusion.
jcwchen(2022-04-05 18:12:14):Previously if the dim_size of expected value_info is zero, the dim_size of inferred value_info can be anything because the following loop will never run. Add this line here to enhance the dim comparison.
jcwchen(2022-04-05 18:20:32):Random thought: I found this test only works for UnSqueeze with opset_version >= 11 because older version does not support negative axes. To enhance test coverage, I suppose perhaps we can make every test in shape_inference_test.py be tested by every opset_version if the opset_version is not specified in the test.
gramalingam(2022-04-06 16:39:18):As to what happens for a scalar: The numpy spec deprecates this since version 1.17, asking users to explicitly convert it to at-least 1D. This deprecation makes sense, because the output will have one-dimension of size 0 (rank of input tensor), unless it is special-cased to be at-least 1, as in the above fix. I don't think we need to special-case this (unless there is a good reason, that is some model depending on this.) It seems to me that numpy's deprecation is the preferable approach?

gramalingam(2022-04-06 16:50:43):I think the issue (before and after version 11) is that axes was converted from attribute to input (not relating to negative values)? Anyway, on the other question: I don't see this test-case mention any opset_verion? 
jcwchen(2022-04-06 17:55:50):Thanks for the investigation and I understand your concern. Another fact I would like to point out is even today np.nonzero is still behaving like `nonzero(atleast_1d(arr))` although there is a deprecation warning. So if ONNX decides not to behave the same as Current NumPy, the spec should clarify it to prevent confusion. Also, the test in ORT will need to be updated.
jcwchen(2022-04-06 18:00:04):> I think the issue (before and after version 11) is that axes was converted from attribute to input (not relating to negative values)?

It's because opset_version before 11 does not support negative axes.

> Anyway, on the other question: I don't see this test-case mention any opset_verion?

Without specified opset_version means using the latest opset_version. Going forward this test case should always be verified.

Regarding to the proposal here (make every test in shape_inference_test.py be tested by every opset_version if the opset_version is not specified in the test) May I understand your opinion about it @gramalingam ? Thanks.
gramalingam(2022-04-13 17:50:15):This looks like a tricky edge case, since the behavior is not really preserved for the scalar case. If the input is known to be a scalar, we could fail or produce a warning, I suppose. But if the input rank is not known, unclear what we can do.
gramalingam(2022-04-14 17:29:50):Hi Jacky, that's an interesting idea worth discussing (what the default behavior should be if no opset is specified), but I assume you mean that for a different PR, right? No need to mix that with this change for NonZero now, right?
jcwchen(2022-04-14 17:32:49):Yes, that would be a big change and should be done by another PR. We can discuss more details about it in our next meeting. I will keep this PR only focuses on fixing shape inference for NonZero.
gramalingam(2022-04-28 20:30:21):I guess this should not be `static`?
jcwchen(2022-04-28 21:26:47):I suppose it must be static here. Otherwise, it will be defined twice in onnx/defs/tensor/def.cc and onnx/defs/tensor/old.cc and bump into a warning.
gramalingam(2022-04-28 21:32:33):well, but if the goal is to avoid duplicate copies, we need to move it into one of the .cc files, and leave it as a simple extern declaration in the include file? Otherwise, wouldn't you end up with multiple copies anyway? (In any case, there is a CI failure relating to this, that needs some resolution.)
jcwchen(2022-04-28 21:47:21):Thanks for catching this! Yes, I believe extern without static should be the right approach to avoid duplicate copies. Will update it later and let you review.
hariharans29(2022-04-06 22:36:39):Thanks. This certainly does fix #4116.

For #4115, Numpy's clamping logic looks more like this for the `starts` (based on the shown examples in #4115): https://github.com/microsoft/onnxruntime/blob/26fceca90f35fe94f39864ebaec0f2d51e367f10/onnxruntime/core/providers/cpu/tensor/slice_helper.h#L132. So until that is addressed, #4115 won't be resolved by this PR as is.
diyessi(2022-04-06 23:21:33):> Thanks. This certainly does fix #4116.
> 
> For #4115, Numpy's clamping logic looks more like this for the `starts` (based on the shown examples in #4115): https://github.com/microsoft/onnxruntime/blob/26fceca90f35fe94f39864ebaec0f2d51e367f10/onnxruntime/core/providers/cpu/tensor/slice_helper.h#L132. So until that is addressed, #4115 won't be resolved by this PR as is.

Should be okay now. This is also a good time to fix anything that could be worded more clearly.
hariharans29(2022-04-06 23:26:12):> > Thanks. This certainly does fix #4116.
> > For #4115, Numpy's clamping logic looks more like this for the `starts` (based on the shown examples in #4115): https://github.com/microsoft/onnxruntime/blob/26fceca90f35fe94f39864ebaec0f2d51e367f10/onnxruntime/core/providers/cpu/tensor/slice_helper.h#L132. So until that is addressed, #4115 won't be resolved by this PR as is.
> 
> Should be okay now. This is also a good time to fix anything that could be worded more clearly.

Left one minor comment. Thanks for the quick fixes and thanks for taking the time to clarify the spec in the first place ! 
hariharans29(2022-04-08 01:46:29):@gramalingam : Any comments ?
hariharans29(2022-04-06 23:27:04):dim -> dims for consistency
diyessi(2022-04-06 23:37:53):Fixed, along with better wording in the starts change.
CLAassistant(2022-04-11 09:39:17):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=4118) <br/>All committers have signed the CLA.
jcwchen(2022-04-08 01:30:45):```suggestion
            if attr.type == AttributeProto.GRAPHS:
```
Mypy typecheck failed. Using AttributeProto.GRAPHS should help. L194 ditto.
wangyems(2022-04-08 23:40:10):Thanks for reviewing it. I'll add this functionality to attribute tensors as well.
CLAassistant(2022-04-11 09:39:02):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=4124) <br/>All committers have signed the CLA.
CLAassistant(2022-04-11 09:38:55):[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/onnx/onnx?pullRequest=4125) <br/>All committers have signed the CLA.
yaoyaoding(2022-04-11 20:15:54):Hi @jcwchen, signed just now.
p-wysocki(2022-05-25 15:05:23):Is `not_smaller` option for `keep_aspect_ratio_policy` necessary? Neither [PyTorch](https://pytorch.org/vision/main/generated/torchvision.transforms.Resize.html) nor [TensorFlow](https://www.tensorflow.org/api_docs/python/tf/image/resize) has an equivalent.

[Adding new operator docs](https://github.com/onnx/sigs/blob/2750e7eb0878d32bb369d9badd8bcf06ebe92a5f/operators/docs/AddNewOpV3.md?plain=1#L72-L73) states that each operator needs to be tested against an existing backend - are there tests for comparing proposed antialiasing functionality?
jantonguirao(2022-05-25 16:07:28):@p-wysocki 
> Is `not_smaller` option for `keep_aspect_ratio_policy` necessary? Neither [PyTorch](https://pytorch.org/vision/main/generated/torchvision.transforms.Resize.html) nor [TensorFlow](https://www.tensorflow.org/api_docs/python/tf/image/resize) has an equivalent.

`not_smaller` offers a similar behavior as [PyTorch Resize](https://pytorch.org/vision/main/generated/torchvision.transforms.Resize.html) when providing a single number as size. In such case Pytorch takes the shorter dimension and scales it to the desired size, scaling the other spatial dimension by the same factor.
The difference is that this proposal allows you to set different minimum or maximum sizes for each dimension individually.

`not_larger` would be equivalent to resizing the largest dimension to the desired size. This is close enough to [TensorFlow resize_with_pad](https://www.tensorflow.org/api_docs/python/tf/image/resize_with_pad). This TF operator resizes to fit the desired canvas size, padding with zeros those dimensions smaller dimensions instead of distorting the aspect ratio. This would be equivalent to Resize with `not_larger` plus a pad step.

> [Adding new operator docs](https://github.com/onnx/sigs/blob/2750e7eb0878d32bb369d9badd8bcf06ebe92a5f/operators/docs/AddNewOpV3.md?plain=1#L72-L73) states that each operator needs to be tested against an existing backend - are there tests for comparing proposed antialiasing functionality?

I have added tests comparing to a handwritten reference python code. I wrote the python reference code and compared against it in tests.
yuanyao-nv(2022-06-24 18:37:04):Should we be more precise in the ONNX spec about what antialias filter to use for various interpolation methods? For example, by giving the precise mathematical formula in cases of possible ambiguity.
jantonguirao(2022-06-27 09:50:11):> Should we be more precise in the ONNX spec about what antialias filter to use for various interpolation methods? For example, by giving the precise mathematical formula in cases of possible ambiguity.


@yuanyao-nv There is this definition:
```
Antialiasing is achieved by stretching the resampling filter by a factor max(1, 1 / scale), which means that when downsampling, more input pixels contribute to an output pixel.
```
which is basically extending the neighborhood region by the inverse of the scale factor, resampling the interpolation filter weights accordingly. Do you think we should phrase it differently? 

Regarding being mathematically explicit, the formulas are in `resize.py` (cubic_coeffs_antialias, linear_coeffs_antialias). I feel that to express it mathematically in the documentation, we would need to also express all the formulas for the different interpolation methods with all the variants, which might be hard to do concisely
yuanyao-nv(2022-06-29 17:41:38):Thanks for the reply @jantonguirao . 
Do you know if this is the same antialias definition used in [TensorFlow](https://www.tensorflow.org/api_docs/python/tf/image/resize) and [PyTorch](https://pytorch.org/vision/main/generated/torchvision.transforms.Resize.html) ? In the TF definition, they mention some hat/tent filter for bilinear.
jantonguirao(2022-06-29 21:04:16):> Thanks for the reply @jantonguirao . Do you know if this is the same antialias definition used in [TensorFlow](https://www.tensorflow.org/api_docs/python/tf/image/resize) and [PyTorch](https://pytorch.org/vision/main/generated/torchvision.transforms.Resize.html) ? In the TF definition, they mention some hat/tent filter for bilinear.

It seems to be the same filter as those two frameworks.

Regarding Tensorflow:
```becomes a hat/tent filter function with radius 1 when downsampling.```
Yes, this is the same filter. Also known as triangular filter.

Regarding Pytorch:
They mention being close to what PIL does. PIL applies antialiasing with the same approach as I am suggesting here. This is a triangular filter for the case of bilinear interpolation.

I made a demo comparing this proposal against OpenCV (no antialiasing) and Pillow (antialiasing): [Notebook here](https://github.com/onnx/working-groups/blob/main/preprocessing/notebooks/resize-antialias/resize-antialias.ipynb). As you can see, the formulas I am proposing match what Pillow is doing.
mzient(2022-05-04 08:13:18):No boolean?
mzient(2022-05-04 08:19:15):This wording is likely to go stale when a new interpolation method is added. We could say that it is not applicable to "nearest" interpolation method.
jantonguirao(2022-05-04 08:27:52):No, boolean is not available. INT is typically used for boolean flags, from what I can see in the code-base
jantonguirao(2022-05-04 08:29:46):That is true. On the other hand, there is a chance that a new interpolation method is added that doesn't support antialiasing.
gramalingam(2022-06-09 15:53:21):That's correct.
gramalingam(2022-06-09 21:27:47):The shape-inference method should not fail in this situation. Instead, it should leave the output-dims as unknown (in the worst-case).
gramalingam(2022-06-09 21:31:51):I assume that the aspect-ratio-policy applies whether scales or sizes is specified?
jantonguirao(2022-06-14 09:15:52):In my opinion, `aspect-ratio-policy` doesn't make sense for scales. If you intend to keep aspect ratio then you set the scale to the same value for all the relevant dimensions. 
Example: You want to double the height, while keeping aspect ratio between height and width.
- Input (300, 200, 3)
- Scales (2, 2, 1)

I guess that there's nothing stopping us from applying those policies to scales, but I am not sure if there's a use for it. Any thoughts?
jantonguirao(2022-06-14 16:12:25):True. I fixed that now.
gramalingam(2022-06-15 20:02:00):One minor consideration might be to choose whichever makes implementations (of the op) simpler, but it is probably not a significant difference in this case. So, I am fine with your choice, but it would help avoid any confusion/ambiguity if we say this explicitly (see suggestion below).
gramalingam(2022-06-15 20:23:19):Now I see this does answer my question about `scales` input. Maybe we can add the line
```
and it is not applicable when the `scales` input is used.
```
gramalingam(2022-06-15 20:34:31):I think it is incorrect to `continue` here, that will end up with a possibly wrong value for `scale`. Since the scale is unknown, should we return all -1 for `sizes_data`?
jantonguirao(2022-06-17 06:26:06):You are right. I will fix that.
jantonguirao(2022-06-17 06:54:54):Done
p-wysocki(2022-06-17 12:08:26):Is this function necessary? Unless you are going for very explicit and clear code, the same could be acheived by `int(x + 0.5)`, maybe enclosed in a lambda.
p-wysocki(2022-06-17 12:10:48):The user could be notified about a possible undefined behaviour. Maybe throwing an error (like when both `scales` and `sizes` are given in Inputs) or giving a warning would be beneficial?
p-wysocki(2022-06-17 12:57:36):I also agree with @jantonguirao, applying `aspect-ratio-policy` to `scales` seems counter-intuitive to me and could be confusing for the user. An explicit comment about this behaviour is a great idea.
jantonguirao(2022-06-17 15:59:53):Good point. I'll take your suggestion
jantonguirao(2022-06-17 16:24:46):I've added some more error checking
gramalingam(2022-06-21 19:02:44):Unfortunately, this check is not the right one (for the warning message below). This condition checks whether a _constant-value_ is available. Unfortunately, there is no good predefined method/function to check whether an input is specified, but the following should work and be okay:
```cpp
   bool hasScalesInput = (2 < ctx.getNumInputs()) && (ctx.getInputType(2) != nullptr);
```
jantonguirao(2022-06-22 08:52:56):You are right. I have fixed it now. Thanks
jcwchen(2022-07-26 16:08:25):FYI this line causes shape inference failures with some of ONNX Model Zoo models. Please check this issue: https://github.com/onnx/onnx/issues/4380. Thanks!
garymm(2022-04-12 18:52:01):Please add `Fixes #4131` to PR description.
Thanks!
jcwchen(2022-04-11 20:11:50):nit: remove unnecessary spaces
jcwchen(2022-04-11 20:13:24):Remove Q: comments. Ditto L511
jcwchen(2022-04-19 17:02:17):Duplicate function? (L396 `void process(NodeProto& n, std::unordered_map<std::string, const AttributeProto*> attr_map)`). If both of them are required, is it possible to make them common in certain way?
jcwchen(2022-04-19 17:09:44):tiny nit: shall we use `g_in` instead of `gp` to make the naming convention same as others?
gramalingam(2022-04-19 17:25:39):Good catch. I think it is a duplicate from a merge produced by git. I am mystified why the compiler does not complain, since both have same signature?
gramalingam(2022-04-19 18:13:07):I see, they differ in `const` ... 
gramalingam(2022-04-19 18:19:12):Done
jcwchen(2022-04-14 00:06:23):```suggestion
// Forward declarations for ai.onnx version 17
```
Same comment as older opset version above.
jcwchen(2022-04-13 23:43:33):I was wrong. These files are still needed to create protobuf stub even with types-protobuf package. Close here now.
jcwchen(2022-04-30 16:30:58):tiny nit: it seems that seq is the only abbreviation here for other types. Perhaps we can just use sequence here for consistency, but it's up to you.
gramalingam(2022-04-30 18:54:11):Thanks. I am using the abbreviations already used for all these types, in the opschema registrations. See, for example: https://github.com/onnx/onnx/blob/00738d2032ca0a39d9124b02085a90adda96d018/onnx/defs/schema.h#L668

wschin(2022-05-05 02:04:35):Maybe we can extend the test to cover all cases mentioned in
```
            // Grammar:
            // float indicates scalar (rank 0)
            // float [] indicates unknown rank tensor (not a zero rank tensor)
            // float [one-or-more-dimensions] indicates tensor of known rank > 0.
```
?
wschin(2022-05-05 02:17:12):Does this support nested types such as `seq(seq(float))`? Maybe throw if we don't want nested types.
wschin(2022-05-05 02:18:18):Maybe add tests for `seq(optional(float))` and `seq(float)`?
wschin(2022-05-05 02:19:24):Similar to other comments, maybe add more basic and fancy tests for `map`? 
gramalingam(2022-05-05 02:19:32):We want to allow such nested types, and the parser allows it. It is up to operators to restrict allowed types.
gramalingam(2022-05-05 02:23:18):Those are already tested separately. Since the parser is recursive, I think the combinations should work fine. Not sure we need to enumerate all possibilities. However, I did add different forms for the different constructors (like seq, map, optional) so overall there is good coverage, I think.
gramalingam(2022-04-19 17:06:49):Minor nit: I was slightly confused by `in advance`. May be useful to use different wording to clarify that this happens after the release, not before. (I guess this subsection is after-release, but still.) How about changing it to
```
for use by future operator additions and changes
```
jcwchen(2022-04-19 17:20:05):Good catch and yes it looks a little confusing. I have updated it. Please review it again. Thanks!
gramalingam(2022-04-21 22:59:37):Thanks for the fix. Strictly speaking, the issue is that the interface-definition does not clarify what the contract is for these methods like `getInputData` (https://github.com/onnx/onnx/blob/bd849e86cb47e75b765ad120b1a1a05ef5cf8c10/onnx/defs/shape_inference.h#L85 ). I think it is safer for implementations of this interface to return `nullptr` for missing inputs (in cases like the above). This is preferable because the condition `i < ctx.getNumInputs()` is not sufficient to guarantee that input `i` is present in the general case. This particular change is fine, but I think it would help to ensure that implementations of `InferenceContext` are more robust.
jcwchen(2022-04-28 19:01:28):> I think it is safer for implementations of this interface to return nullptr for missing inputs (in cases like the above). This is preferable because the condition i < ctx.getNumInputs() is not sufficient to guarantee that input i is present in the general case. This particular change is fine, but I think it would help to ensure that implementations of InferenceContext are more robust.

I almost had the same thought about this while reviewing this PR. Then, we can prevent global issues like it. Still, definitely this PR is good to go for fixing current Resize issue. I will merge it soon. @gramalingam Would you mind opening an issue to track the work item you mentioned above? Thank you.
xuzijian629(2022-04-15 00:36:23):Hmm… since other shape inference functions don’t specially handle these illegal dims, I think  I should work around somewhere else..
xkszltl(2022-04-16 14:57:48):done
xkszltl(2022-04-20 18:07:30):Rebase.
We have builds depending on the cherry-picking of this patch, additional merge commit may break that.
gramalingam(2022-05-03 21:35:26):I think that the checker may need to be extended to call the checker on the newly added proto fields
gramalingam(2022-05-03 20:45:15):nit: change "identifiers" to "parameters"
gramalingam(2022-05-03 20:45:50):nit: change "attributes" to "attribute parameters"
gramalingam(2022-05-03 20:58:32):nit: The above two lines are duplicated in method above also. I wonder if we should just move this into an else-clause down below.
gramalingam(2022-05-03 21:12:46):minor nit: Not sure why we need this here. It has the effect of allowing "< >" (which is okay), as well as "< foo, >" (which is also okay, but we don't do that consistently everywhere else).
gramalingam(2022-05-03 21:15:02):just wondering: may be the logic could be simplified if we extend the `OnnxParser::Parse(AttributeProto& attr)` method to take the `id` as an optional parameter? 
gramalingam(2022-05-03 21:16:07):nit: may be `PeekIsAttributeValue` would be a better name.
gramalingam(2022-05-03 21:17:22):Why are there 2 values for `gamma`?
garymm(2022-05-10 00:29:14):2022
garymm(2022-05-10 00:29:21):2022
gramalingam(2022-05-10 00:52:01):I believe this is a reference to a previous version (an old TBD that was not updated earlier). But I am not sure what the date should be, April 23, 2021 was not the release date, I guess it was presumably the release date after that?
jcwchen(2022-05-10 01:18:13):I think it should be July 30, 2021. This date is the release date of ONNX 1.10 (the ONNX version which bumped IR version to 8).
liqunfu(2022-05-16 18:17:00):this is for the date when the IR version was changed. The PR is https://github.com/onnx/onnx/pull/3398
It was on 2021/04/23
liqunfu(2022-05-16 18:22:15):That makes more sense!
liqunfu(2022-05-16 18:51:51):changed to May 8th, 2020, (I was using the PR date which is less accurate)
liqunfu(2022-05-16 18:52:15):my mistake, it should be alpha.
liqunfu(2022-05-16 19:28:31):OnnxParser::Parse(AttributeProto& attr) is called from: (1) OnnxParser::Parse(NodeProto& node); (2) OnnxParser::Parse(FunctionProto& fn). For the first case, id is not used. If we want to merge the 2 calls, we can in the first case call OnnxParser::Parse(IdList& idlist, AttrList& attrlist) and ignore the idList. I feel that makes the code less readable. Maybe I understand it wrong? 
jcwchen(2022-04-24 15:56:41):```suggestion
To encourage community participation and wider adoption in the industry, ONNX has introduced [open governance](https://github.com/onnx/onnx/wiki/Expanded-ONNX-Steering-Committee-Announced!) in March 2018. The governance has three defined structures to propel the development of ONNX project forward: [Steering Committee](/community/readme.md#steering-committee), [Special Interest Groups (SIGs)](/community/readme.md#sig---special-interest-groups), and [Working Groups (WGs)](/community/readme.md#wg---working-groups). While SIGs and WGs primarily focus on the technical roadmap of ONNX, the Steering Committee is responsible for setting the vision and governance process of the ONNX community.
```
jcwchen(2022-04-25 23:23:13):```suggestion
dimensions are propagated to specific output dimensions. (See the inference
```
nit
gramalingam(2022-04-26 00:08:40):Added, thanks!
xadupre(2022-04-26 08:18:16):Only one main title or several?
xadupre(2022-04-26 08:20:48):Should we make sure users are using the utility functions to throw exceptions if one shape has an unexpecgted value?
gramalingam(2022-04-26 17:55:58):Made into a sub-section and moved it, thanks!
gramalingam(2022-04-26 17:57:30):Yes, but, unfortunately, we are dependent on reviewers to check for this. Any other suggestions?
lgtm-com[bot](2022-04-26 17:42:56):This pull request **introduces 1 alert** when merging f31152049d8d580768a04bea911aacf5fd06d90e into 8ab1efa4fc979e1748ebd378a71a59a22deabe88 - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-1a800e0e7509e32560f5df4447924477d781c153)

**new alerts:**

* 1 for Unused import
jcwchen(2022-05-02 18:18:24):Try to understand more context about it: you removed the comment here for FLOAT16, because current ParseData for raw_data does not support FLOAT16?
gramalingam(2022-05-04 16:58:57):good point. it should be only removed partially.
liqunfu(2022-05-11 18:21:00):referrred -> referred
liqunfu(2022-05-11 18:22:47):incuded -> included
gramalingam(2022-05-11 20:34:13):Fixed, thanks!
gramalingam(2022-05-11 20:34:23):Fixed, thanks!
gramalingam(2022-05-03 15:44:12):Seems ok to me. But I guess this is not going to be merged into main?
gramalingam(2022-06-21 20:41:19):LGTM, thanks. Have the test-cases been validated with ORT? Unfortunately, there is no other way to validate function definitions.
jantonguirao(2022-06-22 17:44:05):> LGTM, thanks. Have the test-cases been validated with ORT? Unfortunately, there is no other way to validate function definitions.

Yes. I have validated the test cases that don't depend on `Pad-18` having support for `axes`
gramalingam(2022-05-26 15:50:50):Unfortunately, this is going to be a problem. The shape we can get is only a static-shape, not a dynamic-shape. Statically, we may not know the rank of the input.
gramalingam(2022-05-27 00:16:46):Instead, one alternative is to get the (full) shape, convert the axes attribute into a tensor, and use it to index into the full-shape using some form of gather op.
jantonguirao(2022-06-14 07:29:49):I went with a separate code path when axes are not provided. Also, I took your suggestion and used `Gather` instead of extending `Shape`.
gramalingam(2022-06-15 21:50:07):I don't see `axes` being initialized above. Either it needs to be defined, or a better option is to use
```cpp
    builder.Add("axes_input = Constant <value_ints : ints = @axes>()");
```
jantonguirao(2022-06-17 07:04:06):I don't understand this comment. `axes` is declared in line 3617 and the values are read in line 3619.
That being said, your suggestion seems better. I'll change it.
jantonguirao(2022-06-17 07:12:23):Done
p-wysocki(2022-06-23 15:22:35):Is it possible to add a warning/exception when an axis is repeated, like in case of Resize-18?
jantonguirao(2022-06-27 12:56:17):Done
manbearian(2022-05-17 18:02:44):How can we move forward on this? I have some time this week to continue to hack on this if there's more folks would like to see.
gramalingam(2022-05-17 21:31:54):It would be great if we could make this work for both big-endian and small-endian. Is that hard? Is there some reference for the transformation you are using? I am trying to understand why trunction of 32-bit to 16-bit is incorrect.

manbearian(2022-05-20 18:59:21):> It would be great if we could make this work for both big-endian and small-endian. Is that hard? Is there some reference for the transformation you are using? I am trying to understand why trunction of 32-bit to 16-bit is incorrect.

Endianness makes by brain hurt, but i'll see if i can work through the pain and implement it. :)  (i was kind of hoping someone would say it isn't needed ;)

See my [other reply](https://github.com/onnx/onnx/pull/4193#discussion_r878466099) as to why i chose rounding over truncation for converting from fp32 to bf16 and a pointer to the pytorch bfloat16 code which is a correct implementation that i drew inspiration from. 
manbearian(2022-05-20 21:22:18):NaN is now implemented correctly (my previous implementation could potentially convert NaN to infinity)
Big-Endian is now 'supported'... at least i believe i have the right implementation, but i have no way to test.
manbearian(2022-05-23 21:06:48):@gramalingam and @souptc thoughts?
souptc(2022-05-23 22:44:46):i see. i think it is correct. I actually wondering how does our cpu/gpu EP handle it. 
manbearian(2022-05-23 23:06:32):I'm struggling with getting the Python typing correct for one of my new functions. Is there a mpy expert here that might be able to lend a hand?
lgtm-com[bot](2022-05-24 15:39:17):This pull request **introduces 2 alerts** when merging 9e0d8841f6f4a6d11fbb348b12ad69b1cdb67ed0 into 28ed7f5dbf133f446e9249565a804e0c617bb0d9 - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-b3af56df0930e963f70b41a257e592bf5dc864a4)

**new alerts:**

* 1 for Use of the return value of a procedure
* 1 for Unused local variable
gramalingam(2022-05-24 17:13:23):Nit: if we decide that rounding is the right behavior instead of truncating, it would be good to fix the "Cast" implementation in the test-case generator also.
manbearian(2022-05-24 21:13:02):> Nit: if we decide that rounding is the right behavior instead of truncating, it would be good to fix the "Cast" implementation in the test-case generator also.

@gramalingam 
i'm looking at this code now, and suspect i see how it can be changed, but i'm not sure how it gets used. Are there any pointers to how it works?
manbearian(2022-05-24 21:36:59):> i see. i think it is correct. I actually wondering how does our cpu/gpu EP handle it.

do you have a pointer to where the code might be, i'm happy to take a look.
jcwchen(2022-05-24 21:41:38):> > Nit: if we decide that rounding is the right behavior instead of truncating, it would be good to fix the "Cast" implementation in the test-case generator also.
> 
> @gramalingam i'm looking at this code now, and suspect i see how it can be changed, but i'm not sure how it gets used. Are there any pointers to how it works?

https://github.com/onnx/onnx/blob/main/onnx/backend/test/case/node/cast.py This file is used to create the node test model (For instance, https://github.com/onnx/onnx/tree/main/onnx/backend/test/data/node/test_cast_FLOAT_to_BFLOAT16). You can update the code with correct behavior in cast.py first and then use [tools/update_doc.sh](https://github.com/onnx/onnx/blob/main/tools/update_doc.sh) to regenerate related node test models.
jcwchen(2022-05-24 21:44:23):> > i see. i think it is correct. I actually wondering how does our cpu/gpu EP handle it.
> 
> do you have a pointer to where the code might be, i'm happy to take a look.

Probably [here](https://github.com/microsoft/onnxruntime/blob/9707181257391323dfe7170d506650ded04dc59c/include/onnxruntime/core/framework/float16.h#L57). However, different EPs seem to have different behaviors. We are trying to reach out the code owner to understand the inconsistency and which behavior is correct.
manbearian(2022-05-24 23:38:27):@jwchen and @gramalingam i updated cast.py/castlike.py to use rounding for f32->bf16. Let me know if this is what you were looking for.

I was expecting to see some tests fail, but i believe the tests aren't checking for exact values, so the single bit difference isn't detectable.
gramalingam(2022-05-24 23:39:37):I suspect that the checkin for test-data for ops other than cast / cast-like are spurious, I don't think they should change?
jcwchen(2022-05-25 00:04:30):Thank you @manbearian for the quick update! As @gramalingam mentioned, please remove other irrelevant operators' update (like updating output.pb due to different numpy.random behaviors by different machines). tools/update_doc.sh will update every node test data and we should only take Cast/CastLike related tests.

> I was expecting to see some tests fail, but i believe the tests aren't checking for exact values, so the single bit difference isn't detectable.

You are right -- these models/input.pb/output.pb just exactly follow what onnx/backend/test/case/node/[operator_name].py defines. The CI only checks that whether the uploaded model can be reproduced by CI environments.
manbearian(2022-05-25 16:44:32):after talking with @gramalingam  offline we decided to drop the CAST/CASTLIKE changes for now. I have them stored in a branch if we want to bring them back. There was also a suggestion that the f32->bf16 helper support both rounding and truncation modes. I can add support for that with a future PR if there is interest (please open an issue and assign it to me).
gramalingam(2022-05-25 16:52:11):Thanks Ian! The rationale for our decision was the question about what Cast should do (rounding or truncation) is separate (it involves a tradeoff between efficiency and precision) and will require more time to get consensus. However, the helper function make_tensor is not as critical (users can round or truncate as they wish), but it is important to fix the error in make_tensor's current handling for this release.
gramalingam(2022-05-10 21:29:20):FWIW: there is some code that does a cast here:
https://github.com/onnx/onnx/blob/5b1346e30d66e4ec550f6b63c3883b258a2e8e3e/onnx/backend/test/case/node/cast.py#L37
May be worth unifying into some common code (especially if there is some minor difference).
manbearian(2022-05-11 19:07:16):that code is wrong; it's not rounding properly when converting from F32 to BF16. Particularly this:
` np_bfp16 = np_uint16_view[1::2] if little_endisan else np_uint16_view[0::2]`

It looks like this is all tests code though, so its fine for testing as long as the test values are crafted in such a way as to not need rounding.

gramalingam(2022-05-12 20:04:06):yes, I saw the difference. That's why I was suggesting unifying both to use a common *correct* implementation. 
souptc(2022-05-17 22:27:13):why we want this special handle for trancate? from bfloat16 definition in wiki, it seems just truncate the 16 bits from float32.
https://en.wikipedia.org/wiki/Bfloat16_floating-point_format
manbearian(2022-05-20 18:57:19):When converting from FLOAT32 to BF16 truncation can work, since the encodings line up save for the 16 last significant bits, but that means rounding errors. I don't want to get into the exact math, instead let me use a metaphor. let's say i want to convert a 3-digit number to a 2-digit number. Further, let's say the 3-digit number has the value `1.49`. if i convert via truncation i end up with `1.4` if i convert via _rounding_ i end up with `1.5`. The `1.5` value is closer to my original value and is "more correct". The rounding scheme i chose to implement is round-to-nearest-even. Which is the most common rounding scheme. Values below 0.5 are rounded down, values above 0.5 are rounded up, and values exactly equal to 0.5 are rounded to the nearest even value.

The comment i added here around NaN relates to what happens when the value isn't a number. For infinity values the lower 16-bits are 0s, so things work fine. but for NaN the lower mantissa may contain a "NaN payload", extra information that that value is carrying that further describes it beyond just NaN. It's probably not correct to round this value (since its not a number) and it also isn't correct to just truncate it. This is because NaN is defined as having at least 1 bit set in the significand. A truncation risks dropping set bits and creating a significand of all-zeros which becomes infinity. Rounding actually has the same problem. If detecting NaN was easy i would have not rounded it, but because i converted to integer value first, NaN is harder to detect. I will fix this in a future iteration before merging.

Here is the code in pytorch that implements this: 
https://github.com/pytorch/pytorch/blob/e81bfffbe1349c5ba568a36ec911404c6e494ea0/c10/util/BFloat16-inl.h#L14
https://github.com/pytorch/pytorch/blob/e81bfffbe1349c5ba568a36ec911404c6e494ea0/c10/util/BFloat16.h#L51

Notice that pytorch uses round-to-nearest-even for converting from fp32 to bf16 and also it doesn't do rounding or truncating of NaN payloads (it generates a new payload with a fixed pattern).





jcwchen(2022-05-23 23:32:41):```suggestion
import numpy.typing as nptyping  # type: ignore
```
Doesn't have NumPy stub for mypy so simply skip it like above.
jcwchen(2022-05-23 23:53:48):https://github.com/python/mypy/issues/9590 Mypy might have some issues with lambda... I am still investigating. Will keep you posted.
jcwchen(2022-05-24 02:40:02):I am not sure why mypy will complain this lambda. It seems like a false positive to me and it still exists in the latest mypy. You can just simply use `# type: ignore` to ignore it for now. Or, shift the variable without using lambda.
jcwchen(2022-05-24 02:40:29):ditto
manbearian(2022-05-24 16:54:02):i think it was actually `map` that was causing problems. In any case, reading up on numpy, it looks like invoking a lambda on the nparray is the preferred method of doing a map operation, so i'm doing that and marking the lambdas as okay to the linter.
jcwchen(2022-09-15 16:28:43):Close it since the files do not exist after the deprecation of ONNXIFI.
liqunfu(2022-07-21 15:50:47):nip: shall we take into consideration on feed name and check all model's required inputs get mapped to from the feed?
jcwchen(2022-07-21 18:25:27):I don't quite understand -- what kind of feed name you were saying? If you were saying input/output names, then ONNX uses naming like `input_X.pb` and `output_X.pb` for input/output files. If added .pb files do not follow this naming, other CIs will catch this kind of error (during test generation). For here, I believe it will simply fail due to unfound data. I added the size check here, because previously wrong test data size will lead to segmentation fault (access out of range), which is not acceptable.
etiotto(2022-05-20 21:26:32):Can one of the admin trigger the 6 workflows missing ? 
etiotto(2022-05-20 21:31:05):@liqunfu can you please help with the review? 
gramalingam(2022-05-21 00:27:32):@etiotto : it looks like the CI is failing due to some flake8 warnings about blank spaces in the python code.
gramalingam(2022-05-21 00:28:33):===> check flake8
onnx/test/helper_test.py:349:28: W291 trailing whitespace
style check failed

etiotto(2022-05-24 15:08:24):> ===> check flake8 onnx/test/helper_test.py:349:28: W291 trailing whitespace style check failed

I removed the white space now. 
etiotto(2022-05-24 18:01:05):@liqunfu @jcwchen it just occur to me we probably should bump up the release to `1.12.0-rc1` (release candidate 1) to allow testing it with PiPy.  After all validation is done we can push a final PR to change the release version to 1.12.0 - opinions ?
jcwchen(2022-05-24 18:33:57):> @liqunfu @jcwchen it just occur to me we probably should bump up the release to `1.12.0-rc1` (release candidate 1) to allow testing it with PiPy. After all validation is done we can push a final PR to change the release version to 1.12.0 - opinions ?

Yes, we will do it in the release branch after bumping the version to 1.12.0 in the main branch. So this PR is good to go. Thanks!
jcwchen(2022-05-24 19:02:55):FYI: the release CIs failure is irrelevant to this PR. It will be fixed by https://github.com/onnx/onnx/pull/4219.
jcwchen(2022-05-25 21:57:00):It seems all ONNX PRs targeting 1.12 have been merged.
etiotto(2022-05-26 13:18:27):@jcwchen it seems this PR has passed CI but cannot be merged because is missing an approval. Who can approve?
jcwchen(2022-05-26 13:32:10):It needs an approval from @onnx/sig-operators-approvers. @gramalingam Could you please help to approve this? Thank you.
justinchuby(2022-05-20 04:47:06):cc @jcwchen
lgtm-com[bot](2022-05-20 04:52:43):This pull request **introduces 17 alerts** when merging c5d943f7adeefb466bc5edf49e782ac503b563ef into c6497e664fec4f61006b0008f40958352daa2d54 - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-39f4cf773bb063cabd3d9c405f9474d1432c21b3)

**new alerts:**

* 17 for Unused import
jcwchen(2022-05-20 18:12:53):FYI: I just merged it. Please go ahead and update Operators.md and TestCoverage.md. Thanks!
justinchuby(2022-05-20 18:14:20):> FYI: I just merged it. Please go ahead and update Operators.md and TestCoverage.md. Thanks!

Thanks! Will do
lgtm-com[bot](2022-05-20 19:00:48):This pull request **introduces 7 alerts** when merging c8cdbda9e78c41b30ea72553ad81c8b6849c08ae into acaa175e7dd48a4dfdc64f0fd3af7d2461f0b304 - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-f870a067df60b93f4811d6f3b0e5e27a5e527cec)

**new alerts:**

* 7 for Unused import
lgtm-com[bot](2022-05-20 19:15:29):This pull request **introduces 2 alerts** when merging 7e5a6e2882828124e4d3bf386684c8a4cb8ae129 into a4947b1546d538b3c354e61b6eefd49989b5f8cf - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-01a323c8502938dc7db488e88b89f76531349c86)

**new alerts:**

* 2 for Unused import
justinchuby(2022-05-20 19:48:11):@jcwchen I think this is ready for review. Could you help trigger the checks?
justinchuby(2022-05-23 17:02:09):@jcwchen Could you help merging the PR? I don't think I have access to.
jcwchen(2022-05-23 17:12:04):It still needs approval from onnx/sig-operators. Could anyone in @onnx/sig-operators-approvers please review this PR? Thanks!
jcwchen(2022-05-20 22:59:01):Linux-CI failed because of mypy typecheck error here: https://dev.azure.com/onnx-pipelines/onnx/_build/results?buildId=24804&view=logs&j=26ad7b42-9ea7-5ad4-891d-e581c481314e&t=36dc6483-53ad-5f31-0c8e-c9104543cd82
justinchuby(2022-05-20 23:34:53):Looks like the function param is redefining `str`. I can try changing it. Is there a fix you would recommend?
jcwchen(2022-05-21 00:19:48):I don't have a strong preference about the solution as long as mypy typecheck is happy. It's interesting that only L40 complains this error. (I saw similar a change at L29). Actually `with open(cast(str, f), 'wb') as writable:` looks fine to me so it is possible that it is a false alarm? Or, according to the error message, you might try to remove the cast since it says it is unnecessary. (although I think it should be needed)
justinchuby(2022-05-21 01:29:26):Renamed the first param to `content`
snnn(2022-08-03 03:53:54):I setup a new feed to replace the testpypi one: https://aiinfra.visualstudio.com/PublicPackages/_artifacts/feed/ORT-Nightly/connect/pip 
The new one is hosted on ADO. It is fully under ONNX Runtime's control. It doesn't have storage limit. So I do not need to delete old packages every week. 
jcwchen(2022-08-03 15:20:23):Thank @snnn for the effort and update! That's great to know and I will keep in mind for future use case.
etiotto(2022-05-30 13:28:12):@jcwchen @gramalingam should we include this PR into v1.12.0 ? 
jcwchen(2022-05-30 16:40:51):> @jcwchen @gramalingam should we include this PR into v1.12.0 ?

Yes I think we should. Thank you for catching this!
mgoin(2022-05-26 14:12:06):to include the previous restriction as well, so there is a range of good versions
```suggestion
protobuf >= 3.12.2, <= 3.20.1
```
EduMorenoJ(2022-05-26 14:16:37)::+1: 
Yulv-git(2022-05-28 13:40:34):> Thank you for catching these. To fix CI failure, please run
> 
> ```
> python onnx/defs/gen_doc.py
> ONNX_ML=0 python onnx/defs/gen_doc.py
> ```
> 
> to regenerated Operators.md and Operators-ml.md from updated defs.cc and old.cc files.

Thanks for your help!
jcwchen(2022-05-28 14:09:17):No problem! Please note that after updating defs.cc and old.cc, you will need to build ONNX from source (for instance, `pip install -e .`) before running document generation. It seems that you were not using the right installed ONNX in your latest commit so there are still some difference failure in CIs.
Yulv-git(2022-06-03 03:20:52):> Thanks for the quick update! Now the CIs all passed. One last thing: you need to sign-off all your commits by DCO: https://github.com/onnx/onnx/blob/main/CONTRIBUTING.md#dco

Thanks for your guidance. There may be no problem with the current PR.

garymm(2022-05-31 22:33:40):@jcwchen can you please merge this? It's blocking integration testing with ORT.
jcwchen(2022-05-31 22:42:13):license/cla is still not removed entirely... I will try to make it pass
jcwchen(2022-05-31 23:00:50):Try to reopen to this PR for removing unnecessary CLA
jcwchen(2022-05-31 23:50:17):CLA assistant should be removed completely, but somehow it didn't... I will work with @prasanthpul to solve this issue. For now, to quickly unblock this urgent PR, @etiotto could you please sign-off CLA if possible? Thank you.
CLAassistant(2022-06-01 13:46:05):[![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/onnx/onnx?pullRequest=4233) <br/>Thank you for your submission! We really appreciate it. Like many open source projects, we ask that you sign our [Contributor License Agreement](https://cla-assistant.io/onnx/onnx?pullRequest=4233) before we can accept your contribution.<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/onnx/onnx?pullRequest=4233) it.</sub>
gramalingam(2022-05-31 23:16:01):Sorry, can you please clarify what you mean by "used"? Where is it used and for what purpose?
jcwchen(2022-06-01 01:16:51):Sorry it looks confusing and I was about to update the PR description later -- There are two similar maps in mapping:
- TENSOR_TYPE_TO_NP_TYPE: which NumPy type you will use to create such a TensorProto. Take float16 as an example, you can directly use numpy.float16 as values for make_tensor.
- TENSOR_TYPE_TO_STORAGE_TENSOR_TYPE: which ONNX Tensor type wou will use to "store" such a TensorProto. Take float16 as an example again, by model proto definition, you should use "TensorProto.UINT16" to store "TensorProto.FLOAT16".

Based on the float16 example, I think the map for bfloat16 should be
- (TENSOR_TYPE_TO_NP_TYPE) TensorProto.FLOAT16: np.dtype('float32')
- (TENSOR_TYPE_TO_STORAGE_TENSOR_TYPE)  TensorProto.BFLOAT16: TensorProto.UINT16

To make a bfloat16 tensor, ONNX should use NumPy float32 values in the first place and then they will be converted to uint16 later by the helper. Please correct me if I am wrong. Thanks!


gramalingam(2022-06-01 05:14:33):Thanks @jcwchen for the explanation. It would be great if you could add this documentation to the mapping.py file. A quick search indicates it is used perhaps for converting TensorProto to numpy array (and it is used in the other direction only for `raw` data?)
jcwchen(2022-06-01 17:48:42):Actually `TENSOR_TYPE_TO_NP_TYPE` is used by a few places. For instance, cast.py and castlike.py from node test scripts. Basically the type map (TensorProto type to NumPy type) is consistent except TensorProto.BFLOAT16 and I guess the map here for TensorProto.BFLOAT16 is not well tested so we need to be very careful for this case.

Regarding the raw use for bfloat16, I might find a bug [here](https://github.com/onnx/onnx/pull/4234#discussion_r887138570).

If this PR is valid, probably we need to cherry-pick this one into rel-1.12.0 branch to prevent potential issues.
jcwchen(2022-06-01 19:13:13):@manbearian If you have bandwidth, could you please review this PR? Such a mapping issue was uncaught because [helper](https://github.com/onnx/onnx/pull/4234#discussion_r887138570) has an issue to get the right type for raw previously. Thank you for your contribution and time!

cc release manger @etiotto this is a PR we would like to cherry-pick into 1.12 release branch to complete bfloat16 support.
manbearian(2022-06-01 19:48:52):> @manbearian If you have bandwidth, could you please review this PR? Such a mapping issue was uncaught because [helper](https://github.com/onnx/onnx/pull/4234#discussion_r887138570) has an issue to get the right type for raw previously. Thank you for your contribution and time!

No problem, i'm happy to help!

The changes you have here make total sense to me in the abstract. I'm missing some context i think though. What scenario is broken because the mapping from tensorproto->numpy is wrong? Do we have a test case?
jcwchen(2022-06-01 22:08:55):> 
> No problem, i'm happy to help!

Thank you :)
 
> The changes you have here make total sense to me in the abstract. I'm missing some context i think though. What scenario is broken because the mapping from tensorproto->numpy is wrong? Do we have a test case?

Actually it does not break anything internally previously, because TENSOR_TYPE_TO_NP_TYPE is mainly used for make_tensor with raw, but there is a bug [there](https://github.com/onnx/onnx/pull/4234/files#r887138570) After fixing that bug, the map (TENSOR_TYPE_TO_STORAGE_TENSOR_TYPE) should be corrected for the consistent meaning: it is used for converting TensorProto values into Numpy arrays. Externally ONNX users might inspect this map to understand what NumPy type they should use for `vals` while making a tensor.

Currently there is no specific test to verify this map. The test for make_tensor with raw kind of covers partial use case because it relies on `TENSOR_TYPE_TO_NP_TYPE`. To make more tests cover this map, perhaps we can update `make_tensor` [L354-L357](https://github.com/onnx/onnx/blob/main/onnx/helper.py#L354-L357) as:
```
np_dtype = mapping.TENSOR_TYPE_TO_NP_TYPE[data_type]
elif data_type == TensorProto.FLOAT16:
    vals = np.array(vals).astype(np_dtype).view(dtype=np.uint16).flatten().tolist()
elif data_type == TensorProto.BFLOAT16:
    vals = list(map(float32_to_bfloat16, np.array(vals).astype(np_dtype).flatten().tolist()))
```


jcwchen(2022-06-01 17:46:35):The given raw type here should be the "storage type". For instance:
- float16: TensorProto.UINT16
- bfloat16: TensorProto.UINT16

This bug was uncaught by float16 because `np.dtype('float16').itemsize == np.dtype('uint16').itemsize == 2`.
gramalingam(2022-06-01 17:54:56):Right. Only the size is required, and for "raw", size of input data == size of created Tensor data.
gramalingam(2022-06-01 18:17:47):How about
```
This map is used for converting TensorProto values into Numpy arrays.
```
gramalingam(2022-06-01 18:26:39):How about
```
This map indicates what storage-type is used in the protobuf (serialized) representation for TensorProto.
```
jcwchen(2022-06-01 19:06:35):Thank you Rama! I have updated these two comments regarding the maps according to your suggestion.
daquexian(2022-06-11 12:55:18):> One question regarding the 3rd point:
> 
> When converting GraphProto to Graph, add initializers before other nodes to avoid the conflict of unique names.
> 
> I am curious that what circumstance will cause this name conflict? Can't current setUnique design help to avoid this situation?

Thanks for your review! `getNextUnique` will find if the candidate already exists in `initializer_names_`, so it works well when the initializers are already added before any normal ir nodes, but fails otherwise: a normal ir node gets a "unique" name by `getNextUnique` which is not aware of any initializers, and then an initializer with the same name is added.
daquexian(2022-06-13 15:52:40):@jcwchen is this pr ready to merge? Thanks!
daquexian(2022-06-02 03:52:54):After erasing output of `initializer_node_`, the `initializer_to_offset_map_` should be updated accordingly (that is what the `main` branch doesn't do). However, it requires a full traverse to update every element in the map, which makes maintaining a map even worse than a naive search-and-erase.
jcwchen(2022-06-07 16:36:00):tiny nit: could we declare `auto *graph = owningGraph();` in advance before L1257 for later use (L1257-L1261)? Or perhaps directly use `owningGraph()` all the time without having a variable.
daquexian(2022-06-11 12:47:52):sorry for the late response~ updated
etiotto(2022-06-01 15:49:47):@jcwchen I want to change the version number in the rel1.12.0 branch so that we can upload the wheels packages as described in  https://github.com/onnx/onnx/blob/main/docs/OnnxReleases.md
jcwchen(2022-06-01 15:54:44):@etiotto Thanks! It looks good to me. Please fix the DCO failure (perhaps rebase to the latest rel-1.12.0 branch and then update the commit with sign-off) and go ahead.
manbearian(2022-06-01 20:09:33):@gramalingam is there a way i can test big-endian support? I'm not very confident that my implementation is correct.
lgtm-com[bot](2022-06-06 16:40:04):This pull request **introduces 1 alert** when merging ee23979a7665145be1f7f6e7dbd55b3d9cfe641f into 0362e56f2a8582f28b9e631d3388d9bdec6dd509 - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-ed53b5be83584eb43cd41f527e8960e5e0994220)

**new alerts:**

* 1 for Unused import
manbearian(2022-06-06 18:32:48):> @gramalingam is there a way i can test big-endian support? I'm not very confident that my implementation is correct.

@gramalingam (with the help of one of my amazing coworkers) i was able to get an emulation of s390x up and running to verify big-endian support. It turns out that my current implementation was broken and i've added a fix for that to the code.

This fix is to basically disable any special casing for big-endian. I'm not going to say i understand why i don't need to swap the byte order, but i verified that the test passes, a generated ONNX file with BF16 values encodes correctly (same as on my x64 machine), and manual inspection of the in-memory buffer looks correct.

I was able to get the full set of tests running on my emulator and saw this:

```
=============================================== short test summary info ================================================
FAILED onnx/test/helper_test.py::TestHelperTensorFunctions::test_make_bfloat16_tensor_with_raw - AssertionError:
FAILED onnx/test/helper_test.py::TestHelperTensorFunctions::test_make_float16_tensor_with_raw - AssertionError:
FAILED onnx/test/helper_test.py::TestHelperTensorFunctions::test_make_tensor - AssertionError:
FAILED onnx/test/test_external_data.py::TestExternalDataToArray::test_save_model_with_external_data_multiple_times - ...
FAILED onnx/test/test_external_data.py::TestExternalDataToArray::test_to_array_with_external_data - AssertionError: F...
FAILED onnx/test/version_converter_test.py::TestVersionConverter::test_upsample_with_raw_constant_node_9_8 - onnx.onn...
FAILED onnx/test/version_converter_test.py::TestVersionConverter::test_upsample_with_raw_initializer_9_8 - onnx.onnx_...
FAILED onnx/examples/np_array_tensorproto.ipynb::Cell 4
======================== 8 failed, 1904 passed, 1084 skipped, 9 warnings in 1501.56s (0:25:01) =========================
```

i looked into some of these failures, and i have a suspicion that the big-endian support in `numpy_helper.py` is not correct for raw encodings. I also suspect there are likely 1 or 2 more lurking issues as not everything looked related to raw encodings.

manbearian(2022-06-06 19:15:51):> i looked into some of these failures, and i have a suspicion that the big-endian support in numpy_helper.py is not correct for raw encodings. I also suspect there are likely 1 or 2 more lurking issues as not everything looked related to raw encodings.

@gramalingam please let me know if you'd like me to open an issue on this. i don't know if you folks are tracking issues on big-endian platforms and/or would prefer to see a customer reported case before opening an issue.
gramalingam(2022-06-06 19:44:54):> > i looked into some of these failures, and i have a suspicion that the big-endian support in numpy_helper.py is not correct for raw encodings. I also suspect there are likely 1 or 2 more lurking issues as not everything looked related to raw encodings.
> 
> @gramalingam please let me know if you'd like me to open an issue on this. i don't know if you folks are tracking issues on big-endian platforms and/or would prefer to see a customer reported case before opening an issue.

Yes, that would be helpful, thanks! We should fix/resolve these.
manbearian(2022-06-06 20:59:54):> > > i looked into some of these failures, and i have a suspicion that the big-endian support in numpy_helper.py is not correct for raw encodings. I also suspect there are likely 1 or 2 more lurking issues as not everything looked related to raw encodings.
> > 
> > 
> > @gramalingam please let me know if you'd like me to open an issue on this. i don't know if you folks are tracking issues on big-endian platforms and/or would prefer to see a customer reported case before opening an issue.
> 
> Yes, that would be helpful, thanks! We should fix/resolve these.

Opened https://github.com/onnx/onnx/issues/4253
manbearian(2022-06-08 15:03:02):@gramalingam  do you need anything else from me to complete the PR?
gramalingam(2022-06-08 15:05:06):> @gramalingam do you need anything else from me to complete the PR?

No, thanks very much for the explanation and the fix.
garymm(2022-06-08 18:30:14):@etiotto please cherry pick
gramalingam(2022-06-06 18:22:31):I assume that the endian adjustment was removed intentionally (and that it is not required)?
gramalingam(2022-06-06 18:27:15):Out of curiosity: I don't follow the `(ival >> 16) & 1` bit ... we end up adding either 0x7fff or 0x8000 (depending on the 16th bit) ... which seems slightly odd.
manbearian(2022-06-06 18:33:17):See this comment: https://github.com/onnx/onnx/pull/4238#issuecomment-1147757261 regarding the change in big-endian support in this function
gramalingam(2022-06-06 18:37:56):I suspect no special-casing is required for endian because you are using "integer arithmetic operators" (and implementing them correctly is the compiler's responsibility, regardless of endian-ness). I mean, the operators used have a fixed semantics, regardless of the endian-ness.
manbearian(2022-06-06 18:57:17):> Out of curiosity: I don't follow the `(ival >> 16) & 1` bit ... we end up adding either 0x7fff or 0x8000 (depending on the 16th bit) ... which seems slightly odd.

this is the "even" part of the "round-to-nearest-even" rounding. In this rounding mode if the fractional part being dropped is half-way between the two potential target values we round toward the nearest even value.

Specifically, we partition ival into two 16-bit parts [A][B]. If B is less than 0x8000 we round A down. if B is greater than 0x8000 we round A up. if B is exactly equal to 0x8000 we round A up or down based on if A is odd or even.


if A is odd `(ival >> 16) & 1` is 1, so `round` will be `0x8000`
if A is even `(ival >> 16) & 1` is 0, so `round` will be `0x7FFFF`

This lines up with what you noticed above, that `round` is always one of these two values.

Now the key is that we add this to the full 32-bit number.

If B is < 0x8000 and we add either of these values, the result will not overflow into A, so the addition is a no-op. When we drop B, we have effectively "rounded down"

if B is > 08x000 and we add either of these values, the result will *always* overflow a single bit into A. When we drop B, we have effectively "rounded up"

The magic is if B is exactly equal to 0x8000. 
- if we add 0x7FFF to this value, we do *not* overflow into A; the effect is round-down.
- if we add 0x8000 to this value, we do overflow into A; the effect is round-up.

So, if A is even, we round down (to the nearest even) and if A is odd, we round up (to the nearest even).

A useful analogy that might be more familiar, is rounding values in C when converting from FP to INT. In C this conversion "truncates" by dropping the fractional portion effectively implementing "round-toward-zero". However, if you add 0.5 to the value before converting, you get a "round-to-nearest" rounding. The difference between that technique and what i've implemented here is that we don't want "round-to-nearest" we want "round-to-nearest-even" so we need an extra step to force rounding down when the value is already even.


I hope this explanation makes sense!


manbearian(2022-06-06 19:09:26):> I suspect no special-casing is required for endian because you are using "integer arithmetic operators" (and implementing them correctly is the compiler's responsibility, regardless of endian-ness). I mean, the operators used have a fixed semantics, regardless of the endian-ness.

I suspect also that internally python is strong its values in little-endian format regardless of the underlying machine, and that was confusing me. In otherwards i was overcomplicating things by conflating what i would need to do in C vs. what the semantics were of what i wanted. High-level languages FTW win here i suppose.
gramalingam(2022-06-06 19:44:04):Thanks for the detailed explanation about the rounding.
gramalingam(2022-06-14 03:47:06):@liqunfu : could you take a look? This requires approval from archinfra sig too. Thanks!
xadupre(2022-06-06 15:15:52):`": "`?
xadupre(2022-06-06 15:18:33):Maybe it would be worth to have a dictionary value: key to speed it up? I don't know how many times this is called.
xadupre(2022-06-06 15:19:01):`": "`?
xadupre(2022-06-06 15:22:00):escaping `"`?
xadupre(2022-06-06 15:22:25):`": "`?
xadupre(2022-06-06 15:27:08):It seems unnecessary to serialize in python before calling the C function which deserializes the bytes before printing it. pybind11 supports overloading types.
xadupre(2022-06-06 15:28:05):We should have more unit tests.
gramalingam(2022-06-08 23:46:01):I have converted every parser-test (in C++ unit-tests) into also a printer-test. So, we now have better test coverage.
gramalingam(2022-06-08 23:50:50):Done (in all cases below, also).
gramalingam(2022-06-08 23:55:32):I am following the existing conventions. One possible reason for the existing style might be that the protobuf types are generated by the protobuf compiler for both C++ and Python. Not sure how easy it is to automatically convert between these representations using pybind11.
xadupre(2022-06-09 10:13:18):It is better this way. There should be a documentation for this function. I still think the call to SerializeToString() should be avoided.
gramalingam(2022-06-09 16:54:04):How do we avoid the serialization? (Of course, we can reimplement the pretty-printer in Python, but I assume that's not what you mean? That would lead to a potential divergence of behavior between C++ and Python. Since this usage is mostly for debugging and understanding etc., I think the efficiency issue is not that critical.)
xadupre(2022-06-09 17:24:01):The model is serialized here and then deserialized in C++. I think these steps should be avoided. The only change to make that happen is to change the C++ signature to take FunctionProto, ModelProto or a GraphProto and remove the call to ParseFromString. Users may call this function on deep learning models to compare two graphs. Avoiding serialization could make it a lot faster.
gramalingam(2022-06-09 17:28:47):But a Python ModelProto and a C++ ModelProto are different types, with different representations. How do you convert between those?
gramalingam(2022-06-09 17:49:47):Specifically, I don't think that Python serialization and C++ deserialization are inverses of each other (at least, it is not documented to be so, and I doubt it is). So, their composition is not a "no-op", but it serves to convert from the python-representation to C++-representation.
xadupre(2022-06-09 18:26:00):I looked into the code and you are right. It is not trivial. I saw that other functions are using the same trick as you did. I guess the protobuf parser does not copy data and just creates intermediate structures pointing to the raw data. SerializeToString should also avoid copying the data. Let's keep it that way then.
gramalingam(2022-06-09 19:38:36):May be. Doesn't seem critical to me yet. We can do this later, if needed.
gramalingam(2022-06-09 20:35:34):Added support for escape character.
wschin(2022-06-13 21:56:35):This means we scan the same string twice. Maybe consider move this to the while loop in line 282?
```c++
      std::string result;
      bool has_escape = false;
      while ((next_ < end_) && (*next_ != '"')) {
        if (*next_ == '\\') {
          has_escape = true;
          ++next_;
          if (next_ >= end_)
            // Let user knows where is wrong.
            return ParseError("Incomplete string literal: " + result);
        }
        // Generate result on the fly.
        result.push_back(*next_);
        ++next_;
      }```
wschin(2022-06-13 22:09:05):Maybe replace `printQuoted` with `print` and implement a `print` for `std::string`? Then, we can merge these  two `printKeyValuePair`s.
wschin(2022-06-13 22:10:08):```suggestion
  std::ostream& output_;
```
wschin(2022-06-13 22:15:42):Why is this function added given we have all `operator<<`s above?
wschin(2022-06-13 22:35:38):Function requires documentation. Personally, an ideal document should contain
- clear description of each argument and returned value. In this pyi file, what are `bool`, `bytes`, and `bytes`?
- Some input-output examples.

Please consider add them if it makes sense. :)
gramalingam(2022-06-13 23:46:40):True. On the flip-side, we don't know the string-length upfront. So, the string object will keep growing, via re-allocation of memory, as we keep pushing back chars. Asymptotically, that will also cause a doubling of the processing time, plus extra memory-allocation/fragmentation etc. We could reserve some string-length, if we know what's the likely string-sizes, but I decided this wasn't too bad.
gramalingam(2022-06-13 23:53:09):Added, thanks!
gramalingam(2022-06-13 23:53:20):Done, thanks!
gramalingam(2022-06-13 23:54:44):It's just a convenient short-hand. Further, for Python, we just expose this functionality instead of the operator<<.
gramalingam(2022-06-13 23:57:41):That's how I started. But ran into C++ template-specialization issues (template-specialization is allowed for class-members only by some compilers, not all.) Furthermore, we use `print(str)` as well as `printQuoted(str)`, both are needed.
wschin(2022-06-14 00:31:56):Makes sense. It sounds like comparing "reallocating time" and "data loading time". We can do a study if it really becomes a problem.
xadupre(2022-06-14 08:28:50):A Python ModelProto and a C++ ModelProto are different: the Python ModelProto wraps the C++ ModelProto, probably in a capsule. I expect that the Python and C++ SerializeToString to produce the exact same sequence of bytes. That's what pybind11 does. They use swig to do it for ModelProto. It should work in a similar way.
xadupre(2022-06-14 10:32:40):I did some measurements. I was hoping to see that serialization is independant of the size but it does not. So call SerializeToString and then ParseFromString is really inefficient.

![Figure_1](https://user-images.githubusercontent.com/22452781/173557024-2ebff163-2dac-4c33-aeb7-0461daf05587.png)

gramalingam(2022-06-14 15:35:16):> A Python ModelProto and a C++ ModelProto are different: the Python ModelProto wraps the C++ ModelProto, probably in a capsule. I expect that the Python and C++ SerializeToString to produce the exact same sequence of bytes. That's what pybind11 does. They use swig to do it for ModelProto. It should work in a similar way.

Of course, SerializeToString will produce the same sequence of bytes, since that is defined by the Protobuf semantics. But that is not the question. The question is whether the C++ in-memory-representation is available in a Python ModelProto (as you imply). If so, what is the API/call we can use to get the C++ in-memory-representation from a Python ModelProto?
gramalingam(2022-06-14 15:36:22):(The question is about the Deserialize part, that is: creating the in-memory-representation used by the two languages).
xadupre(2022-06-14 16:33:06):I don't see why the in-memory-representation would be different. I found this: https://github.com/pybind/pybind11_protobuf but I have some trouble to link it with onnx.
xadupre(2022-06-14 16:47:16):See https://github.com/onnx/onnx/pull/4274.
jcwchen(2022-06-21 18:56:55):nit: import alphabetically. Justin have an ongoing [PR](https://github.com/onnx/onnx/pull/4297) for it so going forward it would be great if we can always keep the order.
gramalingam(2022-06-21 19:11:05):I can fix this manually, but out of curiosity, what will happen with/after Justins PR? Is there a specific auto-format tool/command recommended for people to apply? Thanks!
gramalingam(2022-06-21 20:30:12):Done!
jcwchen(2022-06-21 22:55:09):Good question. I think all these format tools will be included into style.sh as mentioned [here](https://github.com/onnx/onnx/pull/4296#pullrequestreview-1012856368). Then developer can simply run single script to check whether there is any error before submitting CIs. For auto-apply, perhaps we can also introduce another script if needed.
garymm(2022-06-03 17:47:59):@jcwchen please merge.
gramalingam(2022-06-06 16:42:29):Should this be the parameter `name` instead?
gramalingam(2022-06-06 19:19:53):@etiotto : I believe that we will need to cherrypick this PR for the release, since it fixes the failing tests.
etiotto(2022-06-06 20:06:32):@gramalingam ok I will create a PR for that immediately
etiotto(2022-06-06 19:37:27):@jcwchen FYA - cherry picked your PR #4224 as requested.
garymm(2022-06-08 18:31:28):@etiotto please cherry pick
jcwchen(2022-06-09 17:45:12):Is manual installation for pybind11 really needed? ONNX has its own pybind11 as submodule. Could you please try to build ONNX from source without manual installation for pybind11? Please let me know if you bump into any issues. Thanks!
jcwchen(2022-06-09 17:51:35):Ideally, it would be the best to only have one Protobuf installed in the environment to prevent any possible conflict issues. IMO, perhaps mention something like: If you see Protobuf failures while building Protobuf or ONNX from source, please make sure you only have one installed Protobuf (either static or shared) with recommended version to ensure compatibility.
Yesha-Thakkar(2022-06-09 19:08:43):When I built ONNX from source during my very first installation, I ran into the following issue:

"could not find pybind11 missing: pybind11_dir"

So I assumed the manual installation of pybind11 was necessary. I can try building ONNX from source without this installation again, though!
jcwchen(2022-06-09 19:13:24):Please run `git submodule update --recursive` before building ONNX from source if you didn't include `--recursive` while git clone previously.
Yesha-Thakkar(2022-06-09 21:24:26):It seems to work this time! It appears that there were multiple versions of some packages in my conda environment, which caused some issues. I can change this to say, "If you receive an error, check the following:" instead of suggesting a manual installation. Thanks!
Yesha-Thakkar(2022-06-09 21:24:45):Noted. I can make changes accordingly, thanks!
jcwchen(2022-06-10 18:07:46):Thank you for keeping improving README.md! but the build document should be more generic -- I would suggest:
- No specific Python version (except there is a specific issue for that version) Or perhaps say for example.
- Not assume users use Conda environment

May I understand the intention of this note? What shall we do if pythonxx.lib does not exist?


Yesha-Thakkar(2022-06-14 20:20:51):Building ONNX and running the pip command returns the following error if there are conflicting versions of python. Since I was working with Anaconda 3.9, but my environment was in python 3.8, I ran into issues. If there are no assumptions that the user is working in a Conda environment or a specific Python environment, these changes are not necessary.

Should I close this PR, or are there any changes you recommend I make? Please let me know what you think is the best move. 
jcwchen(2022-06-14 21:17:58):Thank you for providing the context. So it seems like a conflict issue about having multiple installed Python in certain environment. I might encounter similar issues before -- typically cleaning all prebuilt files (possibly there are some existing built files were generated by another Python environment) and rebuilding ONNX from source should help. Besides, if you use `pip install -e .` to build ONNX from source, you need to make sure your `pip` and `python` refer to the same installed Python. For instance, if you have one from native Python and another one from Conda Python with difference Python versions, you will probably see failures. If the issue you bumped into is reproducible, you can try whether the methods above help to resolve it.

> Should I close this PR, or are there any changes you recommend I make? Please let me know what you think is the best move.

Perhaps we can reword the sentences to make them more generic and then put them into [the section of common errors](https://github.com/onnx/onnx#common-errors). It can be something like: if you bump into issues like missing pythonXX.lib while install ONNX from source, please clean all existing build files and rebuild ONNX again. Also please make sure you have consistent Python version for common Python commands (e.g., `python` and `pip`). 


gramalingam(2022-06-14 21:36:54):Yes, the "common errors" seems like a good place for these kind of troubleshooting hints (and FAQs). 
jcwchen(2022-06-15 00:56:00):```suggestion
* If you run into any issues while building ONNX from source, and your error message reads, "Could not find pythonXX.lib", ensure that you have consistent Python versions for common commands, such as `python` and `pip`. Clean all existing build files and rebuild ONNX again.
```
typo
jcwchen(2022-06-10 02:58:22):FYI: #4262 #4263 very likely they are needed as well.
jcwchen(2022-06-10 22:23:40):Superseded by https://github.com/onnx/onnx/pull/4267
garymm(2022-06-10 19:11:17):@jcwchen please merge
garymm(2022-06-10 19:07:55):Oops :-)
I'll send a fix.
wschin(2022-06-10 21:00:28):Thank you! This is fixed in `main` branch but I didn't touch release branch.
wschin(2022-06-10 15:57:43):This PR is replaced by #4266.
lgtm-com[bot](2022-07-14 16:22:43):This pull request **introduces 1 alert** when merging 8df7a7a4d70fd9bb2c3f383a739c408ff41de611 into 0fc92e41e3d66796b65ca67140e80640d834b27e - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-96d2f248f30c11cb9347de6022306f70f942aea6)

**new alerts:**

* 1 for Unused import
lgtm-com[bot](2022-07-20 23:29:56):This pull request **introduces 1 alert** when merging 1195f957c9fabfd5cf6f664d91552d179ea29226 into e5e6c0d965fcd1fd5412a5c66e2182005864c8f9 - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-90ce947f903275041741f0f0e82f3df0fafb0bc9)

**new alerts:**

* 1 for Unused import
jcwchen(2022-07-20 23:35:15):> If there's something else in mapping.py that is truly needed by users, we can expose that in helper.py as well.

> WDYT?

That makes sense to me, but I only have one concern that I believe external users have already started to use variables from mapping.py long time ago... IMO, it's safer to move these variables into private after major version change in ONNX. Still, we can introduce helper functions first. I have added a few in helper.py. Please review it. Thanks!

Even for TENSOR_TYPE_TO_STORAGE_TENSOR_TYPE, it has been used from a few places: https://github.com/search?q=org%3Aonnx+TENSOR_TYPE_TO_STORAGE_TENSOR_TYPE&type=code. Do we still want to deprecate it in next release?
jcwchen(2022-07-22 15:34:11):> That makes sense to me, but I only have one concern that I believe external users have already started to use variables from mapping.py long time ago... IMO, it's safer to move these variables into private after major version change in ONNX. Still, we can introduce helper functions first. I have added a few in helper.py. Please review it. Thanks!

> Even for TENSOR_TYPE_TO_STORAGE_TENSOR_TYPE, it has been used from a few places: https://github.com/search?q=org%3Aonnx+TENSOR_TYPE_TO_STORAGE_TENSOR_TYPE&type=code. Do we still want to deprecate it in next release?

After discussion yesterday, we concluded that at least at this moment we can show deprecated messages for all those we would like to deprecate to warn users in advance. I will add it later.
lgtm-com[bot](2022-08-10 04:12:29):This pull request **introduces 1 alert** when merging 635f6b081fa77b853ce2493458d1d51301ea44f0 into eb634addcbb23ec1baf4d548d826bf3932527533 - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-4d70907e732f8a5a8f62cad8a9b4d521eab44277)

**new alerts:**

* 1 for \`\_\_eq\_\_\` not overridden when adding attributes
lgtm-com[bot](2022-08-16 17:29:04):This pull request **introduces 2 alerts** when merging 35cacc240c888c7105c38bbd6e5fabe74a4dc751 into 1f3cecc6a07527dd132e416f78f5400667fa9e6f - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-0e3346bbf5007cfa2c11b07c5e902a19ee7f06d0)

**new alerts:**

* 1 for \`\_\_eq\_\_\` not overridden when adding attributes
* 1 for Unused import
lgtm-com[bot](2022-08-16 18:13:57):This pull request **introduces 1 alert** when merging 20fc0a7e99b5dd29604642f4c79c0424fcb574f8 into 1f3cecc6a07527dd132e416f78f5400667fa9e6f - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-22973b548dfc7da14aca429435b0c9b1e85dd5aa)

**new alerts:**

* 1 for Unused import
gramalingam(2022-06-13 21:14:20):nit: spelling "storage"
gramalingam(2022-06-13 21:15:43):Are the `str` annotations correct? (Here and several places below.)
gramalingam(2022-06-13 21:20:26):nit: May be useful to include the "from" type also in the name. May be `tensor_dtype_to_string` or something like that?
gramalingam(2022-06-13 21:21:44):Suggesting `tensor_dtype` as `tensor_type` is somewhat overloaded already. We mean the element-type of tensors, right?
gramalingam(2022-06-13 21:23:52):nit: spelling "tensor"
garymm(2022-06-13 22:02:32):use double quotes for consistency with rest of file
garymm(2022-06-13 22:02:50):use double quotes for consistency with rest of file
garymm(2022-06-13 22:03:05):use double quotes
garymm(2022-06-13 22:03:22):use double quotes 
garymm(2022-06-13 22:03:31):ditto, double quotes
garymm(2022-06-17 16:47:12):undo
garymm(2022-06-17 16:47:19):undo
garymm(2022-06-17 16:48:37):I would instead have the function take in a tensor and a data type and returns the field. Because every single caller of this function is going to do the same `getattr(tensor, field)`, so it should just be inside the helper function.
garymm(2022-06-17 16:51:25):Can you figure out a way to have it warn only when the dict is accessed? Maybe something like:

```python
class WarningDict(dict):
   def __getitem__(self, key):
       warnings.warn(...)
       return super().__getitem__(self, key)
```

jcwchen(2022-07-20 23:20:15):I introduced `get_attr_from_sequence_elem_type` and `get_attr_from_optional_elem_type` to achieve that. I will still keep `storage_type_to_field` and `optional_type_to_field` public, because I saw external code has already used them (by `STORAGE_ELEMENT_TYPE_TO_FIELD` and `OPTIONAL_ELEMENT_TYPE_TO_FIELD`). Please let me know if you still think we should hide them as private functions.
jcwchen(2022-07-20 23:20:40):Brilliant! Thanks for sharing. I tried it and it worked well.
jcwchen(2022-07-20 23:29:49):Updated. I used `tensor_dtype_to_XXX` for naming. Thanks!
gramalingam(2022-07-20 23:31:28):I think it will also help to document all of these, given the subtle differences (eg., like storage_tensor_type vs storage_numpy_type, etc.). Given that the functions are inter-related, we could put it into some doc file perhaps,
just to clarify the different types, and when/where they are used or should be used. 
jcwchen(2022-07-20 23:34:07):It seems incorrectly used... We shouldn't use STORAGE_TENSOR_TYPE_TO_FIELD for a TensorProto so I just removed it.
gramalingam(2022-07-20 23:40:42):We could also try to reduce the number of helper functions down below, in case it helps. Not sure if all of these are used to a significant degree?
xadupre(2022-07-21 08:16:59):not sure about the value here, maybe a unit test should cover the consistency of the values
xadupre(2022-07-21 08:21:20):maybe a namedtuple would help to remember the meaning of each element
jcwchen(2022-08-17 16:23:48):Good point. Adding more docs for these functions should be very useful. Just added API comments in the functions and two commonly-used functions (`tensor_dtype_to_np_type` and `tensor_dtype_to_field`) in PythonAPIOverview.md. Please review it. Thanks!

> We could also try to reduce the number of helper functions down below, in case it helps. Not sure if all of these are used to a significant degree?

I think perhaps functions like `sequence_type_to_field`, `optional_type_to_field`, `tensor_dtype_to_storage_tensor_type` can be private functions, because ONNX code can directly use outer functions like `get_attr_from_sequence_elem_type`, `get_attr_from_optional_elem_type`, `tensor_dtype_to_storage_numpy_type` correspondingly. I am not very sure whether those functions are helpful to external users though. @gramalingam WDYT?
jcwchen(2022-08-17 16:25:22):Great idea. I have introduced `TensorDtypeMap = NamedTuple(
    "TensorDtypeMap", [("np_type", np.dtype), ("storage_type", int), ("name", str)]
)` to better manage this data structure.
jcwchen(2022-08-17 17:50:49):I merged previous `TENSOR_TYPE_TO_NP_TYPE` and `TENSOR_TYPE_TO_STORAGE_TENSOR_TYPE` with the new naming map into the single map `TENSOR_TYPE_MAP`, because they have the same set of keys (TensorProto data_type). They were the ground truth here and not tested previously either. If the unit test is really needed, I am inclined to have another PR for it since this PR is already a big one and actually I didn't change the original content here.
gramalingam(2022-09-14 21:12:02):Are we definitely planning to remove in release 1.14? If we are not sure, we can change the message to "will be removed soon" or "will be removed in the next release or so."
gramalingam(2022-09-14 21:12:55):Looks like there is some typo or copy-paste error above?
xadupre(2022-09-15 08:01:07):why dtype sometimes and only type some other types?
xadupre(2022-09-15 08:05:14):I would have done this change in a separate PR. It would be easier to read.
xadupre(2022-09-15 08:08:14):In PR #4510, I remove the use of dictionary `TENSOR_TYPE_TO_STORAGE_TENSOR_TYPE`. It think use int8 or uint8 to store bool should not be allowed.
jcwchen(2022-09-21 23:24:12):Sounds good. I have created a specific PR for it: https://github.com/onnx/onnx/pull/4530.
jcwchen(2022-09-21 23:24:57):Good catch. Updated by always using dtype.
jcwchen(2022-09-21 23:27:06):To be flexible, I changed it into: `XXX is now deprecated and will be removed in the next release or so.`
jcwchen(2022-09-21 23:27:33):Thanks for catching this! Corrected.
gramalingam(2022-09-22 05:35:14):IMHO, I think it would be simpler to rewrite the code as below:
```py
   if elem_type == SequenceProto.TENSOR:
      return [to_array(v) for v in sequence.tensor_values]
   elif elem_type == SequenceProto.SPARSE_TENSOR:
      return [to_array(v) for v in sequence.sparse_tensor_values]
   elif elem_type == SequenceProto.SEQUENCE:
      return [to_list(v) for v in sequence.sequence_values]
   etc.
```
I don't think we need to define the helper methods like `get_attr_from_seqience_elem_type`.
gramalingam(2022-09-22 05:38:56):I don't think we need these utility/helper methods. Please see my comment in `numpy_helper.py` on rewriting the code there. I think splitting the code into these two files actually makes the logic of either one harder to understand. (I understand this is a problem with the pre-existing code introduced by someone else previously, and not this PR.)
gramalingam(2022-09-22 05:40:46):Change to `return None`
gramalingam(2022-09-22 05:41:56):Change to `return to_list(optional.tensor_value)`
gramalingam(2022-09-22 05:42:56):Likewise for all the cases. We don't need the helper utility method, whose purpose is only to do this switch/case anyway.
gramalingam(2022-09-22 05:55:04):I wonder if we can omit this function. Users can call the two functions themselves. It is not easy to document/describe, and too many such functions in the API can confuse a reader.
github-code-scanning[bot](2022-09-24 01:16:28):## Unnecessary lambda

This 'lambda' is just a simple wrapper around a callable object. Use that object directly.

[Show more details](https://github.com/onnx/onnx/security/code-scanning/132)
github-code-scanning[bot](2022-09-24 01:16:28):## Unnecessary lambda

This 'lambda' is just a simple wrapper around a callable object. Use that object directly.

[Show more details](https://github.com/onnx/onnx/security/code-scanning/133)
gramalingam(2022-09-26 19:25:29):Nit: suggest moving this to end of the existing sub-section on `Manipulating TensorProto and Numpy Array` instead of this new sub-section
jcwchen(2022-09-26 21:01:52):Moved.
jcwchen(2022-09-26 21:02:08):Makes sense to me. Just omitted.
jcwchen(2022-09-26 21:02:30):Done. Thanks!
jcwchen(2022-09-26 21:17:21):For readability, yes I think keeping if-else statement makes more sense, but for maintainability, having a common function for the same mapping seems not too bad (we don't need to change the mapping in several places if mapping is changed). However, if the attribute mapping for protos is quite stable, I am fine with having if-else statements. I have removed attribute map for SequenceProto and OptionalProto and used if-else statements instead.

Not sure whether it is a concern -- that also means that previously the attribute mappings for SequenceProto and OptionalProto are public used functions and in the future they will be removed.
daquexian(2022-06-21 04:45:58):It is the feature I've been looking forward to for a long time. Serializing and deserializing between c++ and python waste much time in onnx optimizer. 

I have another idea to discuss: Why do we stick to protobuf and not switch to another more flexible format like flatbuffers? It is so strange that onnx, an op standard, compromises because of a replaceable storage format. But the fact is we even create a specific mechanism (external data) just to solve the problem of protobuf (cannot handle >2GB data).

cc @onnx/steering-committee @onnx/sig-archinfra 
xadupre(2022-06-21 08:11:48):Thanks for the feedback. The other implementation I tried is #4281. However, depending on how protobuf 4 is integrated into onnx, this option may not be available anymore, see https://developers.google.com/protocol-buffers/docs/news/2022-05-06.
justinchuby(2022-06-15 17:51:15):@jcwchen we prob need a newer version of typing_extensions: https://github.com/python/typing/pull/724
justinchuby(2022-06-15 19:25:55):Could you update the rc so that it can be tested in pytorch?
jcwchen(2022-06-15 17:53:14):Try this:
```suggestion
def bfloat16_to_float32(data: np.ndarray, dims: Union[int, Sequence[int]) -> np.ndarray:
```
jcwchen(2022-06-15 18:13:12):Sorry, my typo:
```suggestion
def bfloat16_to_float32(data: np.ndarray, dims: Union[int, Sequence[int]]) -> np.ndarray:
```
justinchuby(2022-06-15 18:14:54):Done
HSQ79815(2022-06-20 10:15:54):@daquexian  thanks for approval
lzu-zhanghr(2022-06-17 09:44:18):found synax error
WilliamTambellini(2022-06-17 15:53:50):Hi @lzu-zhanghr  
Have you thought about first adding a DPA op and then a MHA:
![DPA-MHA](https://user-images.githubusercontent.com/109458/174333544-4af4301e-c3f6-49ee-889b-d5f945be14ef.png)
?
lzu-zhanghr(2022-06-27 01:20:56):@WilliamTambellini I have added a [scaled dot-product attention operator](https://github.com/onnx/onnx/pull/4311).
justinchuby(2022-06-21 01:37:04):@jcwchen that sounds like a solid plan to me. In particular, we can (1) split lint checks from the tests so that they can still run in spite of formatting errors. (2) add the mypy checks in an other pr (maybe https://github.com/tsuyoshicho/action-mypy)

The reference workflow is from onnx runtime https://github.com/microsoft/onnxruntime/blob/master/.github/workflows/lint.yml, where we have inline annotations in the `lint-python` job. These are all reviewdog actions. They provided annotations in the PR UI and are non-blocking. So we are still fine even if reviewdog fails.

The blocking checks are defined in `lint-python-format`, where we use the official `psf/black@stable` and `isort/isort-action@master` which should be more reliable. I think we can do the same in onnx, adding flake8 and mypy to the list.

style.sh sounds good
justinchuby(2022-06-21 16:14:46):I can update style.sh in the same pr
justinchuby(2022-06-21 19:10:20):@jcwchen there are new mypy errors. Were they expected?
justinchuby(2022-06-21 19:12:30):ohh looks like excluding is not working
justinchuby(2022-06-21 19:14:30):Locally mypy yields

```
onnx/onnx_ml_pb2.pyi:10:1: error: Cannot find implementation or library stub for module named "google.protobuf.message"  [import]
onnx/onnx_ml_pb2.pyi:14:1: error: Cannot find implementation or library stub for module named "google.protobuf.internal.containers"  [import]
tools/protoc-gen-mypy.py:33:1: error: Cannot find implementation or library stub for module named "google.protobuf.descriptor_pb2"  [import]
tools/protoc-gen-mypy.py:33:1: error: Cannot find implementation or library stub for module named "google"  [import]
tools/protoc-gen-mypy.py:33:1: error: Cannot find implementation or library stub for module named "google.protobuf"  [import]
tools/protoc-gen-mypy.py:34:1: error: Cannot find implementation or library stub for module named "google.protobuf.compiler"  [import]
onnx/onnx_operators_ml_pb2.pyi:2:1: error: Cannot find implementation or library stub for module named "google.protobuf.message"  [import]
onnx/onnx_operators_ml_pb2.pyi:16:1: error: Cannot find implementation or library stub for module named "google.protobuf.internal.containers"  [import]
onnx/onnx_data_pb2.pyi:2:1: error: Cannot find implementation or library stub for module named "google.protobuf.message"  [import]
onnx/onnx_data_pb2.pyi:14:1: error: Cannot find implementation or library stub for module named "google.protobuf.internal.containers"  [import]
onnx/__init__.py:19:1: error: Cannot find implementation or library stub for module named "google.protobuf.message"  [import]
onnx/__init__.py:19:1: note: See https://mypy.readthedocs.io/en/stable/running_mypy.html#missing-imports
onnx/__init__.py:19:1: error: Cannot find implementation or library stub for module named "google"  [import]
onnx/__init__.py:19:1: error: Cannot find implementation or library stub for module named "google.protobuf"  [import]
onnx/__init__.py:67:9: error: Returning Any from function declared to return "bytes"  [no-any-return]
onnx/checker.py:20:1: error: Cannot find implementation or library stub for module named "google.protobuf.message"  [import]
onnx/helper.py:8:1: error: Cannot find implementation or library stub for module named "google.protobuf.message"  [import]
onnx/helper.py:8:1: error: Cannot find implementation or library stub for module named "google"  [import]
onnx/helper.py:8:1: error: Cannot find implementation or library stub for module named "google.protobuf"  [import]
workflow_scripts/test_model_zoo.py:96:9: error: Incompatible types in assignment (expression has type "str", variable has type "ModelProto")  [assignment]
onnx/test/symbolic_shape_test.py:59:21: error: Returning Any from function declared to return "Optional[TensorShapeProto]"  [no-any-return]
onnx/test/symbolic_shape_test.py:61:21: error: Returning Any from function declared to return "Optional[TensorShapeProto]"  [no-any-return]
onnx/test/shape_inference_test.py:59:64: error: Returning Any from function declared to return "Union[SupportsDunderLT, SupportsDunderGT]"  [no-any-return]
onnx/backend/test/runner/__init__.py:154:21: error: Value of type variable "SupportsRichComparisonT" of "sorted" cannot be "Type[TestCase]"  [type-var]
onnx/backend/test/runner/__init__.py:167:26: error: Value of type variable "SupportsRichComparisonT" of "sorted" cannot be "Dict[str, TestItem]"  [type-var]
```

which is less than in CI. Do we need to update the mypy version for it to read pyproject?
jcwchen(2022-06-21 23:16:45):> which is less than in CI. Do we need to update the mypy version for it to read pyproject?

There is an open issue to track this work item: https://github.com/onnx/onnx/issues/3964. I am happy to see mypy upgrade, but it can be a large PR. You can decouple it to another PR if needed. Thanks!
justinchuby(2022-06-23 19:16:12):@jcwchen I moved the mypy config back and added exclude. Tests should now pass
justinchuby(2022-06-21 16:54:09):What was `git diff --exit-code` for?
justinchuby(2022-06-21 16:56:12):mypy fails locally with `third_party/pybind11/setup.py: error: Duplicate module named "setup" (also at "./third_party/benchmark/setup.py")`. Do we need to exclude third_party?
jcwchen(2022-06-21 17:40:11):> Do we need to exclude third_party?

Makes sense to me. Please exclude third_party. Thanks!
jcwchen(2022-06-21 17:45:09):IIUC, previous command is applying clang-format changes so if there is any undone change related to clang-format before running style.sh, `git diff --exit-code` will remind.
justinchuby(2022-06-21 18:20:24):I will keep it then!
justinchuby(2022-06-21 18:50:13):Updated to pyproject
jcwchen(2022-06-23 22:09:50):Please include rel-* branch as well (Or perhaps all branches).
jcwchen(2022-06-23 22:29:30):May I understand why it is now turned-off?
justinchuby(2022-06-27 16:33:34):I unset it so that it will always run all checks in the script. The `$err` will set the exit code if any of them fails
jcwchen(2022-06-29 17:23:03):![image](https://user-images.githubusercontent.com/14194980/176497844-516964ca-4abb-4333-b1c0-de2fc14dcee9.png)
@justinchuby I am seeing this in the CIs and my local. Is it not supported by older mypy? (ONNX uses 0.782)


justinchuby(2022-06-29 17:25:06):Hmm, let me test that with 0.782
justinchuby(2022-06-21 00:58:17):@jcwchen 
justinchuby(2022-07-18 18:13:24):Closed in favor of #4364
sechkova(2022-06-21 16:23:33):I see that the CI is failing with the 'import' from another test file. Is there anything against adding `__init__.py` to the test directory and making it a python module?
jcwchen(2022-06-21 18:35:24):Instead of making these functions as public functions, perhaps let TestDataPropagation inherit TestShapeInference to use those functions?
jcwchen(2022-06-21 18:45:15):How about directly import shape_inference_test?
sechkova(2022-06-23 12:57:18):When TestDataPropagation directly inherits TestShapeInference , the test discovery executes all test cases in TestShapeInference twice: once as part of TestShapeInference and another time as the base class of TestDataPropagation.
 I ended up grouping the common functions in a helper class and making both TestShapeInference  and TestDataPropagation inherit it. What do you think?
jcwchen(2022-06-23 17:48:00):You are right. Current refactor code you have looks good to me. Thanks! 
jcwchen(2022-06-23 17:48:37):What if we do not ignore here? Is there any error by mypy typecheck?
jcwchen(2022-06-23 17:50:12):There are some unused import in data_propagation_test.py and shape_inference_test.py. Could you please remove them? You can check the error log here: https://lgtm.com/projects/g/onnx/onnx/rev/pr-cd1b0be1b5c98bff4a169b869ee7ad5ce1bee56a.
sechkova(2022-06-23 19:13:49):Yes, at least in my local environment, mypy couldn't resolve it.
```
python setup.py typecheck
...
running typecheck
onnx/test/data_propagation_test.py:8: error: Cannot find implementation or library stub for module named 'shape_inference_test'
onnx/test/data_propagation_test.py:8: note: See https://mypy.readthedocs.io/en/latest/running_mypy.html#missing-imports
```
sechkova(2022-06-23 19:20:52):Sorry about that, should be ok now: [4660bb2](https://github.com/onnx/onnx/pull/4302/commits/4660bb2307370414bc6465f250cbdbef5e2bff9f)
jcwchen(2022-06-23 22:16:12):Thanks for providing the error. Sorry for changing -- could you please still use `from onnx.test.shape_inference_test import TestShapeInferenceHelper` instead? I did a quick experiment and it seems it can pass mypy typeheck (0.782).
sechkova(2022-06-24 15:25:29):This will indeed pass the mypy check but it leads back to the errors in the CI test run (I still pushed the change to show the error). From what I can tell, in a local dev environment, onnx is installed in an editable mode (`pip install -e .`), meaning the onnx main directory is added to the sys.path and this issue does not appear.

```
 from onnx.test.shape_inference_test import TestShapeInferenceHelper
E   ModuleNotFoundError: No module named 'onnx.test.shape_inference_test'
```

I believe that this is the only case where one python test tries to import from another.
Potential solutions I could think of:
- Using editable install in the CI
- Running the tests with `python -m pytest ` which adds the current directory to the sys.path https://docs.pytest.org/en/7.1.x/how-to/usage.html#calling-pytest-through-python-m-pytest
- Adding `__init__.py` to the test directory and making it a package helps with the import path resolution. Not sure if this won't have any unwanted side efects, though
- Don't use the full dotted path. This will trigger mypy as before

I don't know the inital reasoning when buiding the test suit, so there could be a better answer?
jcwchen(2022-06-24 21:57:30):Thank you so much for the investigation -- they are really helpful. It's interesting that I cannot reproduce the same error in my local with either `pip install -e .` or `python setup.py`, but the CI failures are there.

I don't have a confident answer about whether adding `__init__.py` will have any side effects or not...  Another possible solution would be using mypy with `--namespace-packages`, which would probably default in future mypy by: https://github.com/python/mypy/issues/8584. However, I did try to run mypy with this config, but it's still throwing the same error (I guess there is an issue about old mypy that `--namespace-packages` does not work for some cases: https://github.com/python/mypy/issues/8584). ONNX is using mypy==0.782, which is at little old.

Since I don't want to block you due to an old mypy issue in ONNX, to be safe without adding `__init__.py`, could you please still use `from shape_inference_test import TestShapeInferenceHelper  # type: ignore` (Sorry for changing my opinion again and again) and please add a TODO comment above saying something like `TODO: remove the following ignore after mypy upgrade in ONNX`. I think we are close to merge this PR. Thank you for the contribution.
sechkova(2022-06-27 14:45:27):I couldn't find a way to make `--namespace-packages` work either. I reverted back to a local import as you suggested.
AlexandreEichenberger(2022-06-21 20:02:33):You need to override the DCO. See here how you can avoid this situation in the future.
https://github.com/onnx/onnx-mlir/blob/main/docs/Workflow.md#step-7-commit--push
jcwchen(2022-06-21 22:01:05):Close it because there is no new things between the main branch and rel-1.12.0 branch.
lzu-zhanghr(2022-06-23 09:45:19):@jcwchen Could you review this pull-request?


lzu-zhanghr(2022-06-28 12:17:13):> 
thanks for approval， I check main branch and find latest opset version is 17. 
lzu-zhanghr(2022-06-29 02:24:59):@gramalingam  Could you review this pull-request?
gramalingam(2022-06-29 19:03:15):Hi, can you please provide some more context? Eg., point to example models that can be expressed using this op? And the relationship to the MultiHeadAttention in the other PR (4312)?
lzu-zhanghr(2022-06-30 01:10:42):> > thanks for approval， I check main branch and find latest opset version is 17.
> 
> To clarify: I haven't approved it and I suggested you get review/approval from onnx/sig-operators. Please note that opset 17 has been released with recent ONNX 1.12 so please use opset version 18 for new introduced operators. Thanks!

Thanks for your explanation
lzu-zhanghr(2022-06-30 01:23:40):> Hi, can you please provide some more context? Eg., point to example models that can be expressed using this op? And the relationship to the MultiHeadAttention in the other PR (4312)?

Scaled Dot-Product Attention is a key part of MultiHeadAttention, and it is simpler.
In my view, scaled dot-product attention is real attention.
Example models could be BETR, DETR, VIT, and so on.
WilliamTambellini(2022-06-30 02:49:26):@lzu-zhanghr  have you checked the computation matches the one from pytorch:
https://github.com/pytorch/pytorch/blob/c71886e04827f9fa87a3a4a0ee4e76f3b1dc22e1/torch/nn/functional.py#L4814
?
lzu-zhanghr(2022-06-30 02:54:42):> @lzu-zhanghr have you checked the computation matches the one from pytorch: https://github.com/pytorch/pytorch/blob/c71886e04827f9fa87a3a4a0ee4e76f3b1dc22e1/torch/nn/functional.py#L4814 ?
It perfectly  fits the pytorch's one.

WilliamTambellini(2022-06-30 02:55:23):@lzu-zhanghr  be aware that TF/t2t offers an option to upcast:
https://github.com/tensorflow/tensor2tensor/blob/ef1fccebe8d2c0cf482f41f9d940e2938c816c78/tensor2tensor/layers/common_attention.py#L1602
"activation_dtype: Used to define function activation dtype when using mixed precision."
lzu-zhanghr(2022-06-30 03:01:39):> @lzu-zhanghr be aware that TF/t2t offers an option to upcast: https://github.com/tensorflow/tensor2tensor/blob/ef1fccebe8d2c0cf482f41f9d940e2938c816c78/tensor2tensor/layers/common_attention.py#L1602 "activation_dtype: Used to define function activation dtype when using mixed precision."
Thanks. 
The operator just offers options from the original paper.

WilliamTambellini(2022-06-29 14:56:16):any way to also output attn_output_weights ?
lzu-zhanghr(2022-06-30 01:12:06):There is no conflict between using attn_mask and outputting attention weight. These are just some test cases for different situations. And the test case has been added.

garymm(2022-06-24 18:25:29):please remove this line and the next and just use the default values
justinchuby(2022-06-29 17:28:09):An inline comment on the ordering is going to be helpful imo
gramalingam(2022-06-30 15:31:06):Why deserialize and serialize back?
gramalingam(2022-06-30 15:33:10):Should this be `lib_version` ?
gramalingam(2022-06-30 15:36:51):Adding comments just like in `ModelProto` would be helpful.
gramalingam(2022-06-30 15:38:46):Don't understand this. Is it here by mistake?
liqunfu(2022-07-21 19:42:00):Good point. TODO add so that I can make the codel change when working with LibProto - I need to merge this branch in order to build ort with my optional input branch.
jcwchen(2022-06-30 18:10:23):cc @justinchuby 
justinchuby(2022-07-15 23:39:10):```suggestion
  [Lint / Lint Python](/.github/workflows/lint.yaml) | Every PR |<ul>ubuntu-latest</ul>| <ul><li>Not required -- it shows lint warnings for suggestions in PR</li><li>flake8 (but required by style.h in another CI)</li><li>pyflakes</li><li>misspell</li><li>shellcheck</li><li>pylint</li><li>mypy (but required by `style.sh` in another CI)</li></ul> |
  [Lint / Enforce style](/.github/workflows/lint.yaml) | Every PR |<ul>ubuntu-latest</ul>| <ul><li>flake8</li><li>isort</li><li>black</li><li>mypy</li><li>clang-format</li><li>unix line endings</li><li>c++ namespace rules</li><li>Auto-generated files are up to date</li></ul> |
```
gramalingam(2022-07-20 23:48:36):Does this produce warnings about pre-existing code issues? Will it continue to do so after the other pending PRs to fix existing python code? If so, it would be worth explaining that here (something like "This may produce warnings about pre-existing code issues that are not required to be fixed in the PR. We plan to fix them in upcoming PRs".)
xadupre(2022-07-21 07:56:39):There should be a way to apply it on git diff only.
jcwchen(2022-07-21 15:57:44):For now some checks will produce warnings about pre-existing code issues in "changed" files, which is sometimes confusing for PR review.

I assume setting `filter_mode: diff_context` will only applying it on git diff only. If that is true, we should set it for every check in https://github.com/onnx/onnx/blob/main/.github/workflows/lint.yaml. (like reviewdog/action-flake8, reviewdog/action-pyflakes, reviewdog/action-shellcheck) However, it seems to me that not every check supports filter_mode. For instance, [action-pyflakes](https://github.com/reviewdog/action-pyflakes). @justinchuby May I understand do you know how to only apply reviewdog/action-pyflakes on git diff only?
justinchuby(2022-07-21 16:01:34):That is precise. For flake8, since we enforce 0 errors in style.sh, it helps to expose all errors in the annotations. For shellcheck, since we don't usually run shellcheck locally, it helps to surface potential errors. pyflakes doesn't have the option. Other than that, I think the rest are diff_context 
justinchuby(2022-07-21 20:24:00):For py flakes we may be able to either fork and update the check or create a pr to the check to enable this option 
justinchuby(2022-07-21 21:43:40):We could also use "added", which will only show lints that are introduced
jcwchen(2022-07-22 17:54:06):After offline discussion, we concluded that for now we should use `filter_mode: diff_context` for every check. Since pyflake8 does not support it, we decided to remove it. For now the Python Lint CI should only review content from git diff. Thank @justinchuby for the quick update: https://github.com/onnx/onnx/pull/4371. Please review this PR again. Thank you all!
jcwchen(2022-06-30 20:49:44):cc @garymm explicitly mentioning "ONNX should use rc for release candidates" in the release document should make your ORT PR https://github.com/microsoft/onnxruntime/pull/12039 more reliable.
linkerzhang(2022-07-14 08:09:15):Please use target VERSION_NUMBER with `rc` (e.g., `1.x.0rc1`) to test TestPyPI in advance before using it for final release.
gramalingam(2022-07-12 17:18:05):Re:
```
I wonder if it would be useful / helpful to create a small documentation with technical details.
"How to implement a new operator or function". How to deal with defs.cc ? How to create attributes.
```
Yes, it would be very useful.

Just to clarify: there is already a doc file here: https://github.com/onnx/onnx/blob/main/docs/AddNewOp.md ... if you have suggestions for what new stuff to add, that would be welcome, thanks!
gramalingam(2022-07-12 17:29:59):You need to generate the test-case data and include it in the PR. Eg., see https://github.com/onnx/onnx/blob/main/tools/update_doc.sh for the steps to generate the updated files.
andife(2022-07-13 21:26:30):> You need to generate the test-case data and include it in the PR. Eg., see https://github.com/onnx/onnx/blob/main/tools/update_doc.sh for the steps to generate the updated files.
I generated the files and pushed the mish related ones. 

I wonder why also 

	geändert:       onnx/backend/test/data/node/test_acos/test_data_set_0/output_0.pb
	geändert:       onnx/backend/test/data/node/test_acosh/test_data_set_0/output_0.pb
	geändert:       onnx/backend/test/data/node/test_asin/test_data_set_0/output_0.pb
	geändert:       onnx/backend/test/data/node/test_asinh/test_data_set_0/output_0.pb
	geändert:       onnx/backend/test/data/node/test_atan/test_data_set_0/output_0.pb
	geändert:       onnx/backend/test/data/node/test_atanh/test_data_set_0/output_0.pb
	geändert:       onnx/backend/test/data/node/test_cosh/test_data_set_0/output_0.pb
	geändert:       onnx/backend/test/data/node/test_cosh_example/test_data_set_0/output_0.pb
	geändert:       onnx/backend/test/data/node/test_lstm_batchwise/test_data_set_0/output_0.pb
	geändert:       onnx/backend/test/data/node/test_lstm_batchwise/test_data_set_0/output_1.pb
	geändert:       onnx/backend/test/data/node/test_lstm_defaults/test_data_set_0/output_0.pb
	geändert:       onnx/backend/test/data/node/test_lstm_with_initial_bias/test_data_set_0/output_0.pb
	geändert:       onnx/backend/test/data/node/test_lstm_with_peepholes/test_data_set_0/output_0.pb
	geändert:       onnx/backend/test/data/node/test_reduce_log_sum_exp_do_not_keepdims_example/test_data_set_0/output_0.pb
	geändert:       onnx/backend/test/data/node/test_reduce_log_sum_exp_do_not_keepdims_random/test_data_set_0/output_0.pb
	geändert:       onnx/backend/test/data/node/test_reduce_log_sum_exp_keepdims_example/test_data_set_0/output_0.pb
	geändert:       onnx/backend/test/data/node/test_reduce_log_sum_exp_keepdims_random/test_data_set_0/output_0.pb
	geändert:       onnx/backend/test/data/node/test_reduce_log_sum_exp_negative_axes_keepdims_example/test_data_set_0/output_0.pb
	geändert:       onnx/backend/test/data/node/test_reduce_log_sum_exp_negative_axes_keepdims_random/test_data_set_0/output_0.pb
	geändert:       onnx/backend/test/data/node/test_rnn_seq_length/test_data_set_0/output_0.pb
	geändert:       onnx/backend/test/data/node/test_simple_rnn_batchwise/test_data_set_0/output_0.pb
	geändert:       onnx/backend/test/data/node/test_simple_rnn_batchwise/test_data_set_0/output_1.pb
	geändert:       onnx/backend/test/data/node/test_simple_rnn_defaults/test_data_set_0/output_0.pb
	geändert:       onnx/backend/test/data/node/test_sinh/test_data_set_0/output_0.pb
	geändert:       onnx/backend/test/data/node/test_tan/test_data_set_0/output_0.pb
	geändert:       onnx/backend/test/data/node/test_tanh/test_data_set_0/output_0.pb

Do I've to commit than, too?
jcwchen(2022-07-19 00:15:56):> Do I've to commit than, too?

No need to commit other unrelated input.pb/output.pb. They were updated because different platforms might behave differently for input/output generation. Please just ignore them. Thanks!
gramalingam(2022-07-12 17:20:33):Now, it needs to be in opset 18.
gramalingam(2022-07-12 17:25:41):Looks like no change in this file? Can we drop this redundant blank line change?
gramalingam(2022-07-12 17:26:08):Should be in opset 18 now.
andife(2022-07-13 02:47:41):I added this line because of the flake8 error with tools/style.sh, but I agree it could/should maybe a separate PR (like mentioned here https://github.com/onnx/onnx/issues/4328)
gramalingam(2022-07-19 00:24:50):Wondering whether we should also include bfloat16 as an allowed type. Any reasons/concerns for or against this? Thanks!
gramalingam(2022-07-19 00:28:08):Never mind. It looks like Softplus doesn't support bfloat16. Better to add bfloat16 support to both ops at same time.
p-wysocki(2022-07-20 09:14:17):Some negative values could be included in the test, just in case. It's up to you, I don't think it's absolutely necessary.
gramalingam(2022-07-12 16:31:02):Thanks for adding the details. It looks good to me ... the only unclear part is the int/uint to int/uint part: I don't fully understand what truncation means with regards to the sign bit, for example. In the case where the value is IN range, it is not a pure truncation because of the sign bit, right? Can the behavior wrt sign bit be clarified? Thanks!
yuanyao-nv(2022-07-18 21:37:32):> Thanks for adding the details. It looks good to me ... the only unclear part is the int/uint to int/uint part: I don't fully understand what truncation means with regards to the sign bit, for example. In the case where the value is IN range, it is not a pure truncation because of the sign bit, right? Can the behavior wrt sign bit be clarified? Thanks!

@gramalingam For in-range conversions, there should be no ambiguity because the integer should be exactly representable in the new dtype. 
Are you referring to out-of-range? There, a non-sign bit in a longer int can be interpreted as the sign bit of a shorter int during truncation. For example, 200 (int16) -> -56 (int8)
gramalingam(2022-07-19 00:12:58):@yuanyao-nv : thanks for the clarification, it looks fine. Maybe it is worth adding a line clarifying that the truncation semantics is with respect to a two's complement representation for negative numbers?
yuanyao-nv(2022-07-21 02:52:12):@gramalingam I've added a sentence to specify two's complement. Please review again. Thanks.
yuanyao-nv(2022-08-05 18:15:24):> Hi @yuanyao-nv, I can repro the git diff failure from the CI. Did you use `sh tools\update_doc.sh` to update `Operators.md` and "Changelog.md" instead of updating those docs manually?

@jcwchen Thanks. Is it requirement to use `update_doc.sh`? For small changes I just edited manually. Let me try the script.
jcwchen(2022-08-05 18:20:25):> Is it requirement to use update_doc.sh?

Yes. See [here](https://github.com/onnx/onnx/blob/main/docs/CONTRIBUTING.md#generated-operator-documentation). And the CIs will run the same script to check whether those documents are updated properly according to the updated C++ code under onnx/defs.
p-wysocki(2022-07-21 16:48:47):It would be great to specify that it's about a case of casting to a lower-bit float, it may look confusing at first.
yuanyao-nv(2022-07-21 17:00:22):I think OOR is a more accurate statement than lower bits since float16 and bfloat16 have the same number of bits but can still result in OOR conversion.
gramalingam(2022-07-21 17:04:15):I think @p-wysocki means replacing `float` by something like `{double, float, float16, bfloat16}` ... otherwise, a float-to-float conversion sounds confusing (if interpreted as float32).
yuanyao-nv(2022-07-21 17:20:37):@gramalingam I see. How about we say "floating point" instead of "float". This should make it clear we are referring to all floating point formats.
gramalingam(2022-07-21 18:03:12):Fine with me. If so, we could use fixed point instead of int, uint. 
gramalingam(2022-07-21 18:04:57):Thanks for the change. Minor nit: may be add "for signed types" after "two's complement representation", since it is not meant for unsigned.
yuanyao-nv(2022-07-22 17:57:55):I changed them to "floating point" and "fixed point" as suggested.
yuanyao-nv(2022-07-22 17:58:11):Updated.
jantonguirao(2022-07-13 15:53:17):@jcwchen Do you know what is the minimum version of protobuf required by ONNX. I believe it'd be good to document that.
jantonguirao(2022-07-14 07:09:09):@jcwchen Do you know what is the minimum version of proto

> > @jcwchen Do you know what is the minimum version of protobuf required by ONNX. I believe it'd be good to document that.
> 
> Now the minimum version of Protobuf required by ONNX is 3.12.2. See [requirements.txt](https://github.com/onnx/onnx/blob/0fc92e41e3d66796b65ca67140e80640d834b27e/requirements.txt#L2).

In such case, [the documentation](https://github.com/onnx/onnx#linux) might be a bit misleading:
```
Ubuntu users: the quickest way to install protobuf is to run
apt-get install python3-pip python3-dev libprotobuf-dev protobuf-compiler
```
For instance, Ubuntu 18.04 has libprotobuf-dev on version 3.0.0, and Ubuntu 20.04 on version 3.6.1. Only Ubuntu 22.04 has a compatible version (3.12.4). I think we should clarify the documentation and recommend installation from source, at least for older versions of Ubuntu.
jantonguirao(2022-07-14 07:33:26):@jcwchen @gramalingam I created https://github.com/onnx/onnx/pull/4360 to clarify about the required version in the documentation. Thanks

jcwchen(2022-07-15 01:12:41):Sorry I wasn't clear about the required Protobuf version. Actually the minimum version of "Python" Protobuf required by ONNX is 3.12.2. I did see issues using Python Protobuf<=3.11: https://github.com/onnx/onnx/issues/3656.

By contrast, regarding proto/Protobuf compiler (different from Python Protobuf), IIUC, current ONNX has more compatibility with older Protobuf versions (3<=version<4 should be all fine).

A question: did you see issues from older Protobuf compiler version with the latest ONNX? Is it the reason why you propose this PR to make ONNX more compatible? If yes, I am thinking perhaps we can have new CIs to catch such kind of issues. Currently every CI uses Protobuf 3.16.0 as protoc.
jantonguirao(2022-07-15 14:43:03):> Sorry I wasn't clear about the required Protobuf version. Actually the minimum version of "Python" Protobuf required by ONNX is 3.12.2. I did see issues using Python Protobuf<=3.11: #3656.
> 
> By contrast, regarding proto/Protobuf compiler (different from Python Protobuf), IIUC, current ONNX has more compatibility with older Protobuf versions (3<=version<4 should be all fine).
> 
> A question: did you see issues from older Protobuf compiler version with the latest ONNX? Is it the reason why you propose this PR to make ONNX more compatible? If yes, I am thinking perhaps we can have new CIs to catch such kind of issues. Currently every CI uses Protobuf 3.16.0 as protoc.

Yes. Someone reported in the ONNX operator slack channel about a compiler error when installing protobuf from the ubuntu repositories:
```
/workspace/onnx_yuanyao/onnx/defs/sequence/defs.cc: In function 'bool onnx::BuildSequenceMapBodyFunc(const onnx::FunctionBodyBuildContext&, const onnx::OpSchema&, onnx::FunctionProto&)':
    /workspace/onnx_yuanyao/onnx/defs/sequence/defs.cc:675:40: error: no match for 'operator[]' (operand types are 'google::protobuf::RepeatedPtrField<onnx::ValueInfoProto>' and 'int')
             seq_at_node.add_output(g_inputs[inputIndex].name());
                                            ^
    /workspace/onnx_yuanyao/onnx/defs/sequence/defs.cc:683:37: error: no match for 'operator[]' (operand types are 'google::protobuf::RepeatedPtrField<onnx::ValueInfoProto>' and 'int')
             identity.add_output(g_inputs[inputIndex].name());
                                         ^
    /workspace/onnx_yuanyao/onnx/defs/sequence/defs.cc:741:30: error: no match for 'operator[]' (operand types are 'google::protobuf::RepeatedPtrField<onnx::ValueInfoProto>' and 'int')
         int64_t dtype = g_outputs[outputIndex].type().tensor_type().elem_type();
 ```
 
 I believe that CI should test with the minimum version (3.0?). I will update the other PR to reflect that 3.0 is the minimum required version.
jcwchen(2022-07-18 17:04:13):Thanks for the info. I have added a PR to make one of ONNX Linux CI use protoc from apt-get: https://github.com/onnx/onnx/pull/4365. It does catch this error. Let's forward your PR and then see whether there is other issue.
gramalingam(2022-07-27 20:45:33):Do you need to rebase? Or, something is off, it is showing other recently merged PRs in the diff here, which is a bit confusing.
p-wysocki(2022-07-28 07:37:20):> Do you need to rebase? Or, something is off, it is showing other recently merged PRs in the diff here, which is a bit confusing.

I had an issue with a non-signed off commit, and then broke something trying to fix it. I'll get to fixing it today.
p-wysocki(2022-07-28 11:15:13):I moved the PR to https://github.com/onnx/onnx/pull/4389, my attempts at fixing non-signed-off commits led me to breaking my git, creating a new PR was a quicker and less messy solution.
gramalingam(2022-07-19 16:50:01):I suggest:
```
Either input 'split' or the attribute 'num_outputs' should be specified, but not both.
If the attribute 'num_outputs' is specified, then the tensor is split into equal sized parts.
If the input 'split' is specified, it indicates the sizes of each output in the split.
```
gramalingam(2022-07-19 16:50:52):I think the attribute should be optional.
gramalingam(2022-07-19 16:54:09):Would be good to check that the attribute is specified, and produce an error message otherwise. Similarly, it may be useful to check in the then-branch that the attribute is *not* specified.
gramalingam(2022-07-19 16:57:35):Unfortunately, the formatting changes makes it hard to find which lines of code changed. Is it possible to undo the formatting changes?
gramalingam(2022-07-19 16:59:02):Can we also check that the input `split` is not specified?
p-wysocki(2022-07-20 14:46:38):Changed it to optional.
p-wysocki(2022-07-20 14:47:31):Your version is much better. I applied it, together with `split` and `num_outputs` being optional and an error raised if both are given.
p-wysocki(2022-07-20 14:48:59):Done, please let me know if my changes address your comment.
p-wysocki(2022-07-20 14:53:34):I must have had "format on save" option on. I reverted the changes, will modify/add shape inference tests soon.
p-wysocki(2022-07-20 14:53:48):Good catch, done.
gramalingam(2022-07-21 15:52:57):Omit above line. It is one or the other, so not a question of "overriding"
p-wysocki(2022-07-27 19:19:08):Done.
gramalingam(2022-07-27 20:42:13):Is that # in the second line a typo or intentional?
gramalingam(2022-07-27 20:50:45):Suggest moving the above line out of the else-branch to before the if-statement, and adding a check in the then-branch that `num_outputs_attr == nullptr` (to catch the case where both are specified, which is also erroneous).
p-wysocki(2022-07-28 09:25:32):It was a result of my failed attempt at fixing non-signed-off commit issue. My git got messed up, I'm going to open a new, clean PR. Sorry for inconvenience. 
p-wysocki(2022-07-28 11:15:51):I added a shape inference error in case both are specified. The PR has been moved to https://github.com/onnx/onnx/pull/4389.
jcwchen(2022-07-20 20:54:26):@daquexian @houseroad please let us know if you still have other concerns. Otherwise, I will forward this PR tomorrow. Thank you!
daquexian(2022-07-21 02:44:11):@jcwchen I have no other concerns about this PR. Thanks!
houseroad(2022-07-18 04:12:46):why do we remove the following instructions?
jantonguirao(2022-07-18 07:13:48):I removed it from here because it was duplicated information. the ONNX build steps are repeated in line 177 (167 after the changes in this PR).

Here there are 3 different ways to install protobuf (from ubuntu repos, from source in debian/ubuntu, and from source in CentOS/RHEL/Fedora). The steps on how to build ONNX from source are the next step.
daquexian(2022-07-19 07:50:22):Since the minimum required version is 3.0.0, should this line be
```suggestion
Ubuntu 18.04 (and newer) users may choose to install protobuf via
```
daquexian(2022-07-19 07:59:04):Frankly speaking, I think the old build instructions are poorly organized (so this PR is very necessary), for most of the instructions are describing how to install the third-party protobuf instead of onnx itself, which confuses users. How about removing the line 139-150 (because protobuf > 3.0.0 can be installed by apt in debian/ubuntu), and [folding](https://gist.github.com/pierrejoubert73/902cc94d79424356a8d20be2b382e1ab) the line 152-163?
jcwchen(2022-07-20 01:23:40):```suggestion
First, you need to install protobuf. The minimum Protobuf compiler (protoc) version required by ONNX is 3.0.0.
```
Shall we use Protobuf compiler (protoc) instead to prevent confusion with Python Protobuf version?
jcwchen(2022-07-20 01:24:56):```suggestion
In this case, it is required to add `-DONNX_USE_PROTOBUF_SHARED_LIBS=ON` to CMAKE_ARGS in the ONNX build step.
```
jcwchen(2022-07-14 21:42:35):Reopen this PR to trigger CI
justinchuby(2022-07-15 17:36:17):@jcwchen mypy is happy now! Have you seen anything like this when checking the proto?

```
generating /home/runner/work/onnx/onnx/onnx/onnx_data_pb.py
Traceback (most recent call last):
  File "/home/runner/work/onnx/onnx/onnx/backend/test/stat_coverage.py", line 8, in <module>
    from onnx import defs, load, AttributeProto
  File "/home/runner/work/onnx/onnx/onnx/__init__.py", line 6, in <module>
    from onnx.external_data_helper import load_external_data_for_model, write_external_data_tensors, convert_model_to_external_data
  File "/home/runner/work/onnx/onnx/onnx/external_data_helper.py", line 9, in <module>
    from .onnx_pb import TensorProto, ModelProto, AttributeProto, GraphProto
  File "/home/runner/work/onnx/onnx/onnx/onnx_pb.py", line 4, in <module>
    from .onnx_ml_pb2 import *  # noqa
ModuleNotFoundError: No module named 'onnx.onnx_ml_pb2'
```
jcwchen(2022-07-15 18:09:56):Thanks for making mypy happy.

> ModuleNotFoundError: No module named 'onnx.onnx_ml_pb2'

I haven't seen this error before. It seems happened during `python onnx/backend/test/stat_coverage.py`. This issue should be caused by somehow ONNX was not installed correctly. The only difference from Linux-CI on AzurePipelines I can see is `python setup.py develop`.
jcwchen(2022-07-15 18:38:20):Shall we add a comment saying that the formatter versions here are corresponding to the versions in setup.py?
jcwchen(2022-07-15 18:41:09):Just out of curiosity: is the target-version for Python here required? I just would like to reduce the duplicate work to add/remove Python versions in a lot of places if possible.
justinchuby(2022-07-15 18:48:25):If not specified, black will assume the python version it is executed in, which may affect compatibility (and consistency).
justinchuby(2022-07-15 18:48:31):Done
jcwchen(2022-07-15 18:17:45):Close because it is not an issue anymore.
jcwchen(2022-07-19 23:32:21):Reopen to trigger release CIs
gramalingam(2022-07-20 18:47:32):Why did the generated documentation files change?
gramalingam(2022-07-20 18:50:56):What format check was added to CI (as mentioned in the PR description)? I couldn't find any. Thanks!
justinchuby(2022-07-20 20:19:45):@gramalingam Doc changed because they contain source (test etc) that are formatted. black and isort were added (tools/style.sh)
justinchuby(2022-07-20 20:30:03):@jcwchen I think it is easier to resolve conflicts the other way around, since git is very good at recognizing where small functions changes should go (in the formatted files). But if we wait for those changes to go in first, the whole reformatting process needs to be run again, essentially the work of creating a new PR.
jcwchen(2022-07-20 20:51:00):> @jcwchen I think it is easier to resolve conflicts the other way around, since git is very good at recognizing where small functions changes should go (in the formatted files). But if we wait for those changes to go in first, the whole reformatting process needs to be run again, essentially the work of creating a new PR.

I remembered I often resolved code conflict about Python import when merging PRs, but perhaps git is good at pure import reordering. Then I am fine with either way. If we decide to forward this PR first, then I will suggest we forward this ASAP, because this PR brings more required code check and we should accomplish it before more new PRs are proposed to prevent confusion (i.e., previously their PRs pass required CI, but after merging this PR they possibly fail).
justinchuby(2022-07-20 21:48:39):Let us know if this looks good to you @gramalingam ? If so we can move forward with this pr and coordinate with the ones waiting 
justinchuby(2022-08-10 15:06:04):Closed in favor of #4427
jcwchen(2022-07-20 00:53:22):May I understand why this file needs to be skipped?
jcwchen(2022-07-20 01:11:55):So after this PR, isort and black check will be "required" in ONNX CIs. I am good with this and I just want to mention that we need to reflect this in the document. I will take care of this by https://github.com/onnx/onnx/pull/4332.
justinchuby(2022-07-20 01:30:45):thanks!
justinchuby(2022-07-20 01:32:55):A sorted import list will cause import errors in internal modules (onnx partially initialized, because import order matters here). To avoid errors like this, torch.onnx requires developers to only import modules: https://github.com/pytorch/pytorch/wiki/torch.onnx-Namespacing#naming-and-usage-convention
jcwchen(2022-07-22 15:58:55):One important change I would like to highlight is that this PR makes released ONNX wheel for Linux and Mac start to use `ONNX_USE_LITE_PROTO=ON`. The released wheel for Windows has already used it for quite a while (since ONNX 1.7.0) and so I think it's good to sync the option settings for every released wheel to prevent confusions. Also it can save some binary size (~0.5 MB).

It's possible that Linux users use older protoc versions (e.g., 3.0.0) from apt-get. Although using prebuilt wheel should be a fine case, to be safe, I tested the ONNX Linux wheel, which was built with `ONNX_USE_LITE_PROTO=ON` from this PR, on an Ubuntu 18.04 with protoc 3.0.0 and it did work well without any issue.
justinchuby(2022-07-19 23:31:57):I would catch a more specific exception
justinchuby(2022-07-19 23:33:09):Possible to do `raise ... from ...`?
justinchuby(2022-07-20 00:02:16):```suggestion
```

Fine to not have this branch since errors not caught are raised
jcwchen(2022-07-20 00:45:59):@justinchuby I found we have different mypy version between required check (from style.sh) and Python Lint. That's why this line only passed by style.sh (mypy 0.782) but failed with Python Lint (mypy 0.971). To prevent confusion, is it possible to use mypy 0.782 from reviewdog for now? I believe many of existing ONNX code cannot pass by the latest mypy.

If it is not feasible, do you think we should disable no-any-return in Python Lint's mypy? It seems to complain here due to missing type specified from Protobuf's SerializeToString, which will be represented as Any type.
jcwchen(2022-07-20 00:46:22):All updated. Thanks for the reviews. Please review it again.
justinchuby(2022-07-20 01:45:49):It doesn't seem the check supports this at the moment. disabling no-any-return sounds like a good idea.
gramalingam(2022-07-20 18:55:17):Suggest
```
The proto size is larger than the 2 GB limit. Please use save_as_external_data to save tensors
separately from the model file.
```
gramalingam(2022-07-20 18:59:42):You could try changing `result = proto.SerializeToString()` to `return proto.SerializeToString()` and delete the `return result` to see if it helps.
gramalingam(2022-07-20 19:01:12):Or, assign an empty string to `result` at the beginning. I wonder if it is just an imprecision in the analysis producing a false positive.
justinchuby(2022-07-20 20:24:21):If you really want, I would do `cast(bytes, result)` to explicitly make mypy happy. https://docs.python.org/3/library/typing.html#typing.cast
justinchuby(2022-07-20 20:25:47):Mypy doesn't know about SerializeToString. It may if we compile the lib first and install all dependencies in the github action
jcwchen(2022-07-20 21:27:13):Thank you both for all the suggestions, but unfortunately none of these can resolve the mypy error in two CIs. `cast(bytes, result)` will bring Redundant cast to "bytes" if mypy knows SerializeToString. 

> Mypy doesn't know about SerializeToString. It may if we compile the lib first and install all dependencies in the github action

Good idea. I think we should do that before mypy check to solve this issue globally. Especially I also saw mypy errors in other PRs like (from [this PR](https://github.com/onnx/onnx/pull/4270/files)):
```
[mypy] reported by reviewdog 🐶 Library stubs not installed for "google.protobuf.message" (or incompatible with Python 3.8) [import] Raw Output: /home/runner/work/onnx/onnx/onnx/helper.py:8:1: error: Library stubs not installed for "google.protobuf.message" (or incompatible with Python 3.8) [import]
```
and
```
[mypy] reported by reviewdog 🐶 Module "onnx" has no attribute "OptionalProto"; maybe "Optional"? [attr-defined] Raw Output: /home/runner/work/onnx/onnx/onnx/mapping.py:5:1: error: Module "onnx" has no attribute "OptionalProto"; maybe "Optional"? [attr-defined]
```

@justinchuby do you have bandwidth to take care of this in added mypy check from Python Lint? Definitely it can help to resolve many unneccessary mypy errors in every PR. Thanks!
jcwchen(2022-07-20 21:27:33):Updated. Please review it again. Thanks!
justinchuby(2022-07-20 23:52:57):I may be able to pick this up early next week if that works. 
andife(2022-07-22 11:25:33):Thank you for your contribution. The answer is not related to TopK in special.

For your information:
The Operators.md is generated by executing https://github.com/onnx/onnx/blob/main/tools/update_doc.sh after changing https://github.com/onnx/onnx/blob/main/onnx/defs/math/defs.cc 


justinchuby(2022-07-22 17:15:15):@jcwchen 
justinchuby(2022-07-22 17:18:56):We prob don't need to reenable pyflakes because flake8 is supposed to include pyflakes. We just need to configure flake8 properly (to enable those checks)
jcwchen(2022-07-22 17:25:28):> We prob don't need to reenable pyflakes because flake8 is supposed to include pyflakes. We just need to configure flake8 properly (to enable those checks)

That's good to know. ~I thought pyflakes involves other linter as well (https://github.com/reviewdog/action-pyflakes#lint---reviewdog-integration), but actually ONNX Python Lint has covered some of them already: action-shellcheck and action-misspell.~ Then yes we don't need to reenable it. Updated my old comment.
justinchuby(2022-07-22 17:27:19):I think that's just a templated readme that's not updated


jcwchen(2022-07-22 17:36:00):> I think that's just a templated readme that's not updated

You are right 👍 
daquexian(2022-07-23 07:32:14):"ai.onnx" and "" both mean the default domain. Should ops with domain "ai.onnx" have consistent behavior with those with "" domain?
gramalingam(2022-07-25 00:42:14):yes, that's right. (Does it have any implication to this PR?)
daquexian(2022-07-25 02:06:19):> Does it have any implication to this PR?

Yes, maybe the line 323 can be changed to `if (node.domain() != "" && node.domain() != "ai.onnx")` if we want consistent behavior here
gramalingam(2022-07-25 17:23:45):I think it is better in the current form. Both forms are legal and mean the same thing. The parser/printer are just convenience utilities to convert the proto format to/from a readable textual form. It is useful to preserve the distinction between the two different, but equivalent, forms in the parser/printer.
AlexandreEichenberger(2022-07-28 15:18:14):Can an approved accept this PR as it is a potential vulnerability. Thanks
jcwchen(2022-07-28 15:20:12):cc @onnx/sig-archinfra-approvers PTAL. Thanks!
jcwchen(2022-07-28 15:56:16):> LGTM, not sure if the default `LOAD_LIBRARY_SEARCH_DEFAULT_DIRS` is still needed as a fully qualified path is used. But maybe its used for something else... your call.

Thanks for catching this. `LOAD_WITH_ALTERED_SEARCH_PATH` seems more accurate for absolute path and so I will use it instead.
mszhanyi(2022-07-27 03:29:51):@jcwchen Could you please help me to merge it? It looks I have no permission.
jcwchen(2022-07-27 15:59:14):It seems some of Mac Release CIs start failing now: https://github.com/onnx/onnx/commits/main. @onnx/sig-archinfra-approvers PTAL at this PR. Thanks!
jcwchen(2022-07-27 22:54:46):To be consistent with previous behavior: data_prop was not enabled for full_check.
```suggestion
    ShapeInferenceOptions options{true, 1, false};
```
daquexian(2022-07-28 02:44:37):`void check_model(const ModelProto& model)` -> `void check_model(ModelProto& model)` brings a breaking change for C++ users and also unnecessarily refuses const models even when full checking is not needed. Maybe it's better to use the following code: 

```suggestion
void check_model(const ModelProto& model);
void check_model(ModelProto& model, bool full_check);
```
sechkova(2022-08-03 13:13:40):Thanks for the suggestion, not breaking the API is better in any case. I added new commits addressing the comments.
jcwchen(2022-08-03 21:37:58):I have a concern: if users call checker with full_check, did they expect the ONNX model will get updated (shape_inference will bring inferred shapes in value_info)?

Current behavior:
- check_model+full_check with model path (Python and C++ API): the original model will be overrode by the inferred model thru model path (it's by default from onnx.shape_inference_path)
- check_model+full_check with ModelProto (Python and C++ API): the original model won't be updated

After this PR:
- check_model+full_check with model path (Python and C++ API): same as current behavior
- check_model+full_check with ModelProto (Python API): the original model won't be updated
- check_model+full_check with ModelProto (C++ API): the original ModelProto will be overrode by the inferred model

IMO, ideally all APIs behaviors for check_model should be consistent to prevent confusion. It seems weird to me that the ONNX model will be updated after onnx.checker with full_check. Still, it's an existing issue (check_model does not update the model, but check_model_path does) so it should not be the blocker to this PR. If both of you also think it's a valid concern, probably we can improve it in another PR.
gramalingam(2022-08-04 14:05:25):Yes, it would be helpful to ensure a consistent and orthogonal API. Do we need full-check and const-ness of the model interact or can they be orthogonal/independent? It would also be a good idea to add some comment/documentation for all of this.
p-wysocki(2022-07-27 14:29:21):Good change, LTGM.
postrational(2022-07-28 16:12:23):Previously discussed in https://github.com/onnx/onnx/pull/4358
jcwchen(2022-08-18 18:09:10):For unknown reason (might be a GitHub server issue), this PR hasn't been merged into the main branch even after 1 hrs... Let's see whether it will be merged toady. 
gramalingam(2022-08-18 18:11:15):As per @p-wysocki , it is unclear why this (empty) merge even happened. He says he did nothing to trigger it. And it seems to be an empty merge with no commits. What's happening? Seems mysterious to me.
p-wysocki(2022-08-19 07:49:23):I was 1 commit ahead of `onnx:main`, it contained all my changes, squashed with a sign off. Unfortunately some other commits got in and there were ~350 files changed. Long story short, I reverted this one, single commit from `p-wysocki:add_split18` and pushed it to my fork, essentially syncing `onnx:main` with `p-wysocki:add_split18`, what can be seen as "Files changed: 0". I think this may be just a weird behavior from GitHub, I did not enable merging (merge should not be possible at all anyway), my changes are not on master and it would be just a "visual" glitch. 

We had this happen in OpenVINO too, where an external contributor seemingly merged his changes, but they were not on master and the PR showed 0 changed files.
postrational(2022-07-28 15:35:43):This sentence should not be removed:

> Split a tensor into a list of tensors, along the specified 'axis'.
postrational(2022-07-28 15:36:42):CONTRIBUTING.md should not change in this PR.
p-wysocki(2022-07-28 16:21:11):Good catch, I restored it.
p-wysocki(2022-07-28 16:21:22):I reverted the changes.
daquexian(2022-07-29 07:11:52):Suggestion:
```suggestion
              split.resize(ctx.getNumOutputs(), chunk_size);
```
gramalingam(2022-08-02 15:15:22):I think we should perhaps have test-cases for both the earlier version and newer version? @jcwchen ?
jcwchen(2022-08-02 15:36:49):After this PR: https://github.com/onnx/onnx/pull/3855, going forward all of node test cases for the same operator will be updated to the latest opset version if there is an opset bump. If people would like to keep some old test cases for old opset versions, they need to specify the opset_version while creating node test cases. Take Loop op as an example, see this [line](https://github.com/onnx/onnx/blob/ea7c5204713cce87e8c22e5382833c68483f329b/onnx/backend/test/case/node/loop.py#L127).
gramalingam(2022-08-02 15:45:34):I believe that we will need to copy the old test-case, but add the opset version to be the old one (and the new one can be as in the current PR)?
jcwchen(2022-08-02 15:50:27):Correct. In that case we can keep the same test cases for both newer and older opset versions.
p-wysocki(2022-08-04 12:54:59):I applied the changes you suggested. Opset13 tests were reverted, new (opset18) tests stayed the same.
daquexian(2022-07-29 04:18:43):Critical bug: In C++, built-in types like `is_int` and `dim` will not be initialized unless we initialize them explicitly, so `is_int` and `dim` are in an uninitialized state even after calling `Dimension()`, and then accessing these two members is undefined behavior. It causes bugs in IR users like onnx optimizer and onnxsim (and also version converter, I think)
daquexian(2022-07-29 04:19:14):Performance improvement: copying std::string may be expensive
daquexian(2022-07-29 04:19:21):Performance improvement: copying std::function may be expensive
daquexian(2022-07-29 04:20:00):Performance improvement: avoid unnecessary copy of ValueInfoProto
daquexian(2022-07-29 04:21:29):Performance improvement: move constructor and move assignment operator are implicit deleted if copy constructor or copy assignment operator is explicitly declared, so they should be defined explicitly to enable moving.
daquexian(2022-07-29 04:22:24):Bug: ResourceGuard should not be copy-able, for the `destructor` function can only be executed once.
daquexian(2022-07-29 04:31:26):Performance improvement: move constructor and move assignment operator are implicit deleted if copy constructor or copy assignment operator is explicitly declared, so move constructor should be defined explicitly to enable moving.
daquexian(2022-07-29 04:32:13):Define destructor explicitly to meet the rule of five
daquexian(2022-07-29 04:33:32):Performance improvement: `emplace_back(...)` is more effective than `push_back(Dimension(...))` because it constructs the `Dimension` object in-place so no copy occurs.
jcwchen(2022-08-01 23:08:04):It looks useful. Do you think we should add this check into CI as well?
daquexian(2022-08-05 07:42:10):@jcwchen sorry for the late response. Yes, it is definitely better if we have this in CI. Maybe we can add it into CI in another PR.
daquexian(2022-08-01 15:24:54):@jcwchen thanks for your review! I just found flake8 released 5.0.2 two hours ago, so I upgraded flake8 from 5.0.1 to 5.0.2 in case 5.0.2 contains some bugfixes.
jcwchen(2022-08-01 15:43:20):> @jcwchen thanks for your review! I just found flake8 released 5.0.2 two hours ago, so I upgraded flake8 from 5.0.1 to 5.0.2 in case 5.0.2 contains some bugfixes.

Thanks for the update! Please allow me to retitle the flake8 version as `5.0.2` for this PR. I will merge it now.
jcwchen(2022-08-04 00:17:53):nit: could we also have another test for Windows-path like "..\\..\\..\\file.bin"?
jcwchen(2022-08-04 00:26:28):Just speak aloud: is it possible this PR breaks some tests in ORT? If so, we need to keep this in mind when upadating ONNX in ORT.

Even if it does break some tests there, probably we should fix it from ORT side (making them use inner directory instead), because loading files from outside directory seems like a bad practice in that situation.
jnovikov(2022-08-04 21:58:03):It might. Speaking about the vulnerability, the vulnerability affects the ORT(I tested the POC using onnxruntime) and this new check should fix it. 

It might break some tests, but I'm not sure how to test it now. 
I do not know how to build ORT from scratch to check it with patched ONNX library, so it would be great if someone experienced will do it or just provide some guide to me how to build ORT and build it with patched onnx lib. 
jnovikov(2022-08-04 22:13:44):Done
jcwchen(2022-08-08 20:50:28):Yes, testing it in onnxruntime in advance will be super great. Provide some context about how to build onnxruntime with updated ONNX:
- Build onnxruntime from souce: https://onnxruntime.ai/docs/build/inferencing.html#linux
- Update ONNX in onnxruntime: https://github.com/microsoft/onnxruntime/blob/master/docs/How_To_Update_ONNX_Dev_Notes.md
jcwchen(2022-08-08 20:56:01):Unused pathlib?
jcwchen(2022-08-09 00:21:16):I have discussed it with onnxruntime team. It seems that they have similar check already so ideally this PR shouldn't bring test failures when updating ONNX there.
jnovikov(2022-08-10 00:29:34):Good if so. As I mentioned I didn't found the checks inside the onnxruntime. At least my poc works for the 
onnxruntime==1.10.0 (pypy) version. 
williamberman(2022-08-01 21:51:07):Not sure if this is right for windows. Don't have a local windows dev environment set up to test
jcwchen(2022-08-01 23:19:20):From my local Windows, I can run `.setuptools-cmake-build\Debug\onnx_gtests.exe` directly without setting additional PATH.
jcwchen(2022-08-01 23:20:30):```suggestion
# If you set DEBUG=1, use `.setuptools-cmake-build\Debug\onnx_gtests.exe` instead
.setuptools-cmake-build\Release\onnx_gtests.exe
```
williamberman(2022-08-01 23:41:29):Added!
jcwchen(2022-08-02 15:11:40):```suggestion
export LD_LIBRARY_PATH="./.setuptools-cmake-build/:$LD_LIBRARY_PATH"
```
philipwan(2022-08-02 07:53:57):@jcwchen 
hi，Due to this known issue: https://github.com/onnx/onnx/issues/4367, and according to your suggestion, I made a pull request, there is no problem in the cherry-pick process, please review this PR, thank you。
By the way，will version 1.7.1 be released if it is merged?
jbachurski(2022-08-09 08:58:30):@gramalingam Would you mind viewing the proposal if this sort of interface would work? In the future it could be extended to support the other arguments, they could be optional so it's backward compatible.
gramalingam(2022-08-09 21:42:28):Updating the pyi definition for [OpSchema](https://github.com/onnx/onnx/blob/main/onnx/onnx_cpp2py_export/defs.pyi#L7) would be useful.
jbachurski(2022-08-10 10:11:10):> I think it might be useful to promote an API like this as a wrapper around the raw C++ method (that is, make if part of the public API instead of something in just this test-file). But, for that, I don't think we would want `node` to be optional. And am not sure about `num_outputs` either.

When it comes to the Python-side interface: A node has input/output names, operator name & domain, attributes. Own name and docstring don't matter. We are missing basically only the operator version it seems, and the input types. I see some variations of the possible signature:

- Inputs & outputs
  - `list[TypeProto]` - we already have an ordering of fields inside the node. Node inputs/outputs are positional anyway, so making it explicit with ordering in a `list` seems okay.
  - `dict[str, TypeProto]` - since the node names the fields in definition. On the other hand since the node has to get created explicitly first anyway in this approach we might as well use the names that are within the node. I think this shouldn't mess with generics etc. since if we use a name multiple times is has to have the same type - we get some very simple consistency check.
- Operator version
  - `version: int` - get the current operator schema from a list at this current version based on what is in the node.
  - `OpSchema`- make the user explicitly provide the schema themselves. Is more redundant, since the Node already has the domain and identifier defined. 


Also, where would we put this function? `shape_inference` would be fine, but really it should have been called `inference` (as it also infers types and other stuff?). Could also put it in `helper`. Any ideas?

Let me know your thoughts and I'll get started on some improvements in the Python-interface area.

jbachurski(2022-08-10 14:31:58):@gramalingam I pushed some changes:

```cpp
std::unordered_map<std::string, py::bytes> CallNodeInferenceFunction(
    OpSchema* schema,
    const py::bytes& nodeBytes,
    std::unordered_map<std::string, py::bytes> valueTypesByNameBytes,
    std::unordered_map<std::string, py::bytes> inputDataByNameBytes,
    std::unordered_map<std::string, py::bytes> inputSparseDataByNameBytes) {
```

- Now returns a map, since the inputs are a map already. Doesn't return entries were the output `TypeProto` was not initialised (inference failed implicitly?).
- Added `OpSchema::Verify` before the call to the inference (checks if the count of inputs/outputs is correct, etc.), and `OpSchema::CheckInputOutputType` afterwards (checks if the types are consistent with the signature). Both should throw `ValidationError`.
- At the same time, inference itself may throw `InferenceError`. Interestingly, this may also throw errors related to attributes (so things like `Cast` will check here if exactly one of the right attributes is set).
- Let me know if I should handle exceptions there in some more involved way. I think what is thrown already reflects most of what `check_model` and `infer_shapes` do.

```py
# class OpSchema:
    def infer_node_outputs(self, node_proto: bytes, value_types: Dict[str, bytes],
                           input_data: Dict[str, bytes], input_sparse_data: Dict[str, bytes]
                           ) -> Dict[str, bytes]: ...
```

- This is just a binding for the C++ function. I added it to `defs.pyi` as above. 
- My general feeling is that the attributes after `value_types` should have empty defaults so that in the future `generatedShapeData` and `graphInferenceContext` may be supported as well (but this involves, I think, significant work).

```py
# shape_inference.py
def infer_node_outputs(
    schema: onnx.defs.OpSchema, node: onnx.NodeProto, input_types: Dict[str, onnx.TypeProto],
    input_data: Optional[Dict[str, onnx.TensorProto]] = None,
    input_sparse_data: Optional[Dict[str, onnx.SparseTensorProto]] = None
) -> Dict[str, onnx.TypeProto]: ...
```

- The public-facing Python function does all of the conversions and expects proper types.
- Already has default arguments, which we may change in the future. Reflect the actual underlying implementation.
- Does some checks if all the expected keys are present and only copies over what is actually required on C++ side for some performance.


Let me know what you think of these changes! I also added some more tests.
jbachurski(2022-08-15 09:08:43):Thanks! 
Should be ready for merge, then?
gramalingam(2022-08-09 21:37:10):I think this getter function already exists: see `GetTypeAndShapeInferenceFunction`.
gramalingam(2022-08-09 21:51:21):I think it might be useful to promote an API like this as a wrapper around the raw C++ method (that is, make if part of the public API instead of something in just this test-file). But, for that, I don't think we would want `node` to be optional. And am not sure about `num_outputs` either.
gramalingam(2022-08-09 21:53:02):nit: for various functions it might be useful to explicitly indicate it is a node-level inference (vs. graph-level inference) in the name, eg: `CallNodeInferenceFunction` and `infer_types_for_node`.
jbachurski(2022-08-10 10:04:33):Sure, that definitely makes sense. We could always require a `NodeProto`, `num_outputs` was a temporary idea.
jbachurski(2022-08-10 10:11:32):That's a fair idea, I'll adapt to this in the next iteration.
jbachurski(2022-08-10 10:29:58):You're right, I think I must have thought it was private for some reason. Fixed!
gramalingam(2022-08-10 21:32:05):nit: I assume the 1 is the type encoding of float. If so, better to use the symbolic names like `TensorProto.FLOAT` instead.
gramalingam(2022-08-10 21:38:44):A question: wondering what happens in builds that don't support exceptions. I assume that the C++ code with abort. What does pybind return in this case? (I realize that this is a pre-existing issue, but just wondering whether we should mark/exclude tests in some special way for this case.)
jbachurski(2022-08-10 21:54:07):Looks like this isn't really going to be an issue, since `cmake_build` under Python requires the bindings to be built with exceptions:

https://github.com/onnx/onnx/blob/f6cb158986cf65feae64774d9a851e2649b94073/setup.py#L207-L208
jbachurski(2022-08-10 21:56:17):I'll fix this (and the Windows CI, looks like it's failing due to a different default int dtype for a numpy array?) tomorrow.
gramalingam(2022-08-10 22:00:02):I heard that the Windows CI are failing across PRs, may be unrelated to this PR. Thanks!
xadupre(2022-08-11 16:26:36):None is better than an empty container as a default value.
jbachurski(2022-08-11 21:00:28):I don't think I know what you mean here. This is C++ side, so there's no None. Do you mean to make it a pointer and use nullptr/use std::optional? 
I think that this wouldn't be a lot better as the map isn't mutated and also in this it makes sense that the default is "no additional information" (for the inference context we have to pass in _some_ map, and it makes sense to make it empty as there's no input data we know).
Could you provide a code example for the change?
justinchuby(2022-08-18 05:02:39):Do we explicitly require a dictionary for these params? If not prefer a more generic type like Mapping. 

——-

Use more generic types for input parameters and specific types for return values. For example, a function may take a `Sequence` and return a `List`.

Diagram for reference: https://gist.github.com/justinchuby/4021cebe9e093f636759a88de325c85f
p-wysocki(2022-08-18 14:51:25):LGTM, after @gramalingam suggestions. These are all the reduction functions PyTorch uses, but I also found more in [Apple's BNNS](https://developer.apple.com/documentation/accelerate/bnns/reductionfunction). We could try figuring out if any of them could be useful and added together with `max` and `min`. Just a thought, I believe it's okay as is.
philass(2022-08-19 00:01:08):@gramalingam, Thanks for the review and the great suggestions. I have updated the PR with your recommendations. Sorry for the delay!

@p-wysocki thats a really good point. There is this delicate balance between trying to not add every possible `reduction` op and adequately expressing Pytorch and Tensorflow ops in ONNX.

There are more Pytorch Scatter ops than just `Add`, `Mul`, `Max` and `Min`. We found that the Pytorch scatter package is something that is quite common. It consists of the following Scatters.

- [Scatter Add](https://pytorch-scatter.readthedocs.io/en/1.3.0/functions/add.html)
- [Scatter Div](https://pytorch-scatter.readthedocs.io/en/1.3.0/functions/div.html)
- [Scatter Max](https://pytorch-scatter.readthedocs.io/en/1.3.0/functions/max.html)
- [Scatter Mean](https://pytorch-scatter.readthedocs.io/en/1.3.0/functions/mean.html)
- [Scatter Min](https://pytorch-scatter.readthedocs.io/en/1.3.0/functions/min.html)
- [Scatter Mul](https://pytorch-scatter.readthedocs.io/en/1.3.0/functions/mul.html)
- [Scatter Std](https://pytorch-scatter.readthedocs.io/en/1.3.0/functions/std.html)
- [Scatter Sub](https://pytorch-scatter.readthedocs.io/en/1.3.0/functions/sub.html#)

The reason that I only added `Max` and `Min` is that we have actually seen models that need `ScatterMax` and `ScatterMin`. Namely 
- [GATConv](https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#torch_geometric.nn.conv.GATConv) 
- [GATv2Conv](https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#torch_geometric.nn.conv.GATv2Conv) 
- [GENConv](https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#torch_geometric.nn.conv.GENConv) 
- [TransformerConv](https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#torch_geometric.nn.conv.TransformerConv) 
- [SuperGATConv](https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#torch_geometric.nn.conv.SuperGATConv) 
- [AGNNConv](https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#torch_geometric.nn.conv.AGNNConv)

Furthermore `Sub`, `Std`, `Mean`, `Div` can likely be represented by a combination of `ScatterAdd`, and `ScatterMul`, and other ops to get the required behavior. 

I think a true general solution is to use some of the ideas of `If` and `Loop` where some subgraph can be given as an attribute. This subgraph would be the reduction operation. That is more involved, and I don't have the knowledge of ONNX to drive that at the moment.

In the short term I think `Max` and `Min` are a good start.
p-wysocki(2022-08-19 07:52:55):@philass Thank you for such a detailed answer, I agree that `Max` and `Min` are sufficient for now.
gramalingam(2022-08-30 23:12:29):Hi @philass : thanks again for this PR. It seems to require some merge conflict (hopefully just the minor one in operator_sets.h). Once that is addressed, we can merge the PR.
philass(2022-09-01 09:58:31):@gramalingam thanks for pointing that out.

I've resolved the conflict, and I am ready to get this merged :)
gramalingam(2022-08-11 15:52:39):nit: may be we can combine all these 4 descriptions into one, as:
```
    output = np.copy(data)
    update_indices = indices.shape[:-1]
    for idx in np.ndindex(update_indices):
        output[indices[idx]] = reduction-op (output[indices[idx]], updates[idx])
where the reduction-op is +/*/max/min as specified.
```
gramalingam(2022-08-11 15:54:54):As above: it would help to consolidate all the cases into one.
gramalingam(2022-08-11 15:58:37):I suggest adding the following, as the last part of the documentation:
```

(Opset 18 change): Adds max/min to the set of allowed reduction ops.
```
philass(2022-08-18 22:25:20):Thats a great suggestion
jbachurski(2022-08-07 17:05:36):```suggestion
    package_data={"onnx": ["py.typed", "*.pyi"]},
```
gramalingam(2022-08-10 21:46:12):nit: Do we need to add an end-of-line?
jcwchen(2022-08-10 22:12:08):Sounds good. Added.
jcwchen(2022-08-10 22:54:29):One nit update: I removed adding `py.typed` in MANIFEST.in, because I think this line L343 will do the same thing here. I have tested that the updated ONNX wheel can still produce correct type information.
daquexian(2022-08-08 12:15:25):Could you please explain a bit more about why checking the length of fmt resolves potential vulnerability issues? Thanks!

BTW, should `<` be used instead of `<=`? Because `strlen(fmt)` doesn't include the terminating null but the `n` of `vsnprintf` includes it.
AlexandreEichenberger(2022-08-08 14:03:23):We did the same fixes in onnx-mlir for the same type of vulnerability:

from https://cplusplus.com/reference/cstdio/vsnprintf/
```
Write formatted data from variable argument list to sized buffer
Composes a string with the same text that would be printed if format was used on [printf](https://cplusplus.com/printf), but using the elements in the variable argument list identified by arg instead of additional function arguments and storing the resulting content as a C string in the buffer pointed by s (taking n as the maximum buffer capacity to fill).

If the resulting string would be longer than n-1 characters, the remaining characters are discarded and not stored, but counted for the value returned by the function.

Internally, the function retrieves arguments from the list identified by arg as if [va_arg](https://cplusplus.com/va_arg) was used on it, and thus the state of arg is likely to be altered by the call.

In any case, arg should have been initialized by [va_start](https://cplusplus.com/va_start) at some point before the call, and it is expected to be released by [va_end](https://cplusplus.com/va_end) at some point after the call.

Parameters
s
Pointer to a buffer where the resulting C-string is stored.
The buffer should have a size of at least n characters.
n
Maximum number of bytes to be used in the buffer.
The generated string has a length of at most n-1, leaving space for the additional terminating null character.
[size_t](https://cplusplus.com/cstdio:size_t) is an unsigned integral type.
format
C string that contains a format string that follows the same specifications as format in [printf](https://cplusplus.com/printf) (see [printf](https://cplusplus.com/printf) for details).
arg
A value identifying a variable arguments list initialized with [va_start](https://cplusplus.com/va_start).
[va_list](https://cplusplus.com/va_list) is a special type defined in [<cstdarg>](https://cplusplus.com/cstdarg).
```

The key phrase is "If the resulting string would be longer than n-1 characters, the remaining characters are discarded and not stored, but counted for the value returned by the function."

So in your case, the code is already safe as the buffer is sized `BUFFER_MAX_SIZE` and you limit the writing in the buffer by `BUFFER_MAX_SIZE` bytes -1.

The format string is kind of irrelevant, as any `%f` or similar input will be expanded in multi-characters. And the `vsnprintf` already will truncate the output string written to the buffer to `BUFFER_MAX_SIZE`.

So you can safely remove the assert, which does not protect anything, and I would just write a lengthy comment explaining why this is safe.
jcwchen(2022-08-08 23:18:19):Thank you @AlexandreEichenberger for the elaboration! You suggestion makes sense to me. I just reverted the unnecessary check and add some comments about why the current vsnprintf is fine.
gramalingam(2022-08-09 19:12:37):Another test-case where `inputs=[]` or `input=['']` would be useful.

How about test-cases for optional_get_element?
gramalingam(2022-08-11 15:46:12):The input should be made into an optional input.
gramalingam(2022-08-12 15:59:02):Trying to understand this: this file is about *model* inputs/outputs, right, and not about *node/op* inputs/outputs. For model inputs/outputs there is no possibility of optional-input/output of kind 1 (static), right? I don't understand why this is being triggered. (May be some fix in the model-generator is required if it is adding an unnecessary model-input?)
gramalingam(2022-08-12 15:59:48):I am worried things will break if a model has an input X, and there is no corresponding input-file in the test-folder.
gramalingam(2022-08-12 16:09:31):I think the combination of `input_type_proto == tensor_type_proto` and above (`'empty': 'optional_input'`) is not a valid combination to test? That might also be triggering the change to cmd_tools.py below?
gramalingam(2022-08-15 18:55:16):Is this change still required (after eliminating that test-case)? Wondering why this change is needed.
daquexian(2022-08-10 11:02:04):Please sign the DCO following the instructions in https://github.com/onnx/onnx/pull/4426/checks?check_run_id=7760879314. Thanks!
justinchuby(2022-08-10 15:07:55):@jcwchen I created this pr to set up the config and CI for the formatters. Please let me know when the approved changes are merged and I can run the actual formatting process
justinchuby(2022-08-10 17:14:40):Do we check for output_X.pb in the CI? I ran tools/update_doc.sh and got those. I will revert the changes
justinchuby(2022-08-10 17:25:58):Done
justinchuby(2022-08-10 20:16:03):@gramalingam ready for your approval
jcwchen(2022-08-10 17:05:19):One thing irrelevant to this PR: is it possible to silent this kind of warning (Undefined variables from ONNX) from pylint? If not, shall we build ONNX first in this CI to let pylint understand ONNX? Building ONNX might be helpful for the next mypy check as well.
justinchuby(2022-08-10 17:21:52):#4428
jcwchen(2022-08-10 22:17:12):nit: @justinchuby do you think it is good to include other commits about formatting as well? e.g., https://github.com/onnx/onnx/pull/4212, https://github.com/onnx/onnx/pull/4084, https://github.com/onnx/onnx/pull/3982.
justinchuby(2022-08-10 22:18:52):Sure
justinchuby(2022-08-10 22:23:11):Done
daquexian(2022-08-12 04:43:26):LGTM, thanks for your contribution!
justinchuby(2022-08-18 04:58:25):There’s issue forms which look very nice. The PyTorch repo is using it: https://docs.github.com/en/communities/using-templates-to-encourage-useful-issues-and-pull-requests/configuring-issue-templates-for-your-repository#creating-issue-forms
daquexian(2022-08-17 02:18:47):> scale can be nullptr but hasScaleInput is still true

Could you please elaborate a bit on it? Thanks!
liqunfu(2022-08-17 16:42:22):> 


liqunfu(2022-08-17 17:04:01):> > scale can be nullptr but hasScaleInput is still true
> 
> Could you please elaborate a bit on it? Thanks!

Thanks @daquexian for your input. I am integrating ONNX main into ORT. One issue I am facing is a test break related to https://github.com/onnx/onnx/pull/4388. 4388 is actually relax the checking so I think the break is even before 4388. 

The test being broke is https://github.com/microsoft/onnxruntime/blob/ce01ed02da48cf25317eb2d4dfa4841012bf122c/onnxruntime/test/providers/cpu/tensor/resize_op_test.cc#L860

It fails because both scales and size are provided. If I add empty string for scales input, it fails because with resize opset11, 'scales" is not optional. I am trying to limit my fix in resizeShapeInference. However, I am afraid InferenceContext does not provide enough information for what I need. For example, I need to know if input "scales' is an empty tensor. 

I think a general solution to this type of issues will be to use different shape inference functions if there is a difference between op spec. I will update this PR for this. 
gramalingam(2022-08-17 18:18:59):hi @liqunfu : I don't understand the problem. A quick question first: is [this line](https://github.com/microsoft/onnxruntime/blob/ce01ed02da48cf25317eb2d4dfa4841012bf122c/onnxruntime/test/providers/cpu/tensor/resize_op_test.cc#L878) supposed to be `{}` instead of `{0}` ?
gramalingam(2022-08-17 18:21:44):In general, I am a bit concerned about the previous fix in https://github.com/onnx/onnx/pull/4388 also: I think it is important to keep the spec (the description of the op) and the code in sync. If such fixes are required, we should update the op's spec to clarify it too.
liqunfu(2022-08-17 20:04:53):> hi @liqunfu : I don't understand the problem. A quick question first: is [this line](https://github.com/microsoft/onnxruntime/blob/ce01ed02da48cf25317eb2d4dfa4841012bf122c/onnxruntime/test/providers/cpu/tensor/resize_op_test.cc#L878) supposed to be `{}` instead of `{0}` ?

It is not this. I added one test case which may explain the issue better.
liqunfu(2022-08-17 20:07:04):> In general, I am a bit concerned about the previous fix in #4388 also: I think it is important to keep the spec (the description of the op) and the code in sync. If such fixes are required, we should update the op's spec to clarify it too.

The fix #4388 did fix an issue in ModelZoo. The issue this PR is to fix is specific to opset11 where we cannot use empty string for scales because it is not optional. 
daquexian(2022-08-17 02:14:07):Is it changed by mistake?
daquexian(2022-08-17 02:15:49):I think it's better to always write {} to avoid unexpected results.
gramalingam(2022-08-17 20:06:29):May be we should add an extra `int opset` parameter to the method, so that we can customize the behavior for each opset as needed.
gramalingam(2022-08-17 20:09:34):Is the empty tensor input supported only for opset11? Or, also later?
liqunfu(2022-08-17 21:10:47):empty string is only supported since opset13. with opset11, 'scales' is not optional and thus cannot use empty string.
According to text description of scales in opset13+, I think empty tensor cannot be used to express "no scales input", only empty sting can be used for the purpose. 
Given that, I will use 'int opset' parameter for the method.
 
gramalingam(2022-08-18 18:46:16):This can be deleted now, right?
gramalingam(2022-08-18 20:46:20):nit: I think it is better to pass in the value 11, because the op is defined in opset version 11.
gramalingam(2022-08-18 20:50:01):nit: may be better to pass in value 13 instead of 18. 

Also the naming is a bit challenging for the last version, because it may apply to future versions also (like version 19). So, I wonder if it is better to just call them `resizeShapeInference_opset13` and `resizeShapeInference_opset11` ? The upper end is implicit and omitted from name.
gramalingam(2022-08-18 17:36:43):@daquexian can you please take a look? This requires the approval from someone in operator SIG. Thanks!
gramalingam(2022-08-18 19:57:48):> Great improvement! Shall we also apply `hasInput` in
> 
> 1. https://github.com/onnx/onnx/blob/main/onnx/defs/tensor/utils.cc#L95-L99
> 2. https://github.com/onnx/onnx/blob/main/onnx/defs/quantization/old.cc#L41
> 
> Besides, do you think it is also useful if we can have another common function for `ctx.getNumInputs() > x && hasInputShape(ctx, x)`? For example: https://github.com/onnx/onnx/blob/main/onnx/defs/nn/old.cc#L70-L77 It can be done by another PR if needed.

Added the first two, thanks! As for the next suggestion, `hasInputShape` already works this way, so we don't need a new function I think. (The extra condition `ctx.getNumInputs() > x` is not needed.)
gramalingam(2022-08-30 21:05:26):Looks good to me. The only open question is whether we need to bump the opset-version number for some of these ops, or can we avoid that?
gramalingam(2022-09-07 23:50:21):LGTM, just had a nit-comment about indentation.
liqunfu(2022-09-08 00:34:14):> LGTM, just had a nit-comment about indentation.

thanks @gramalingam, I just made the correction.
gramalingam(2022-08-20 00:04:59):One general question / comment: I think, in the long term, it would be better to move towards simple function (instead of context-dependent function). I think we could do that by using attribute-references for such attributes.

The main issue, I think, is the use of default-values for these attributes. I think it may be a good idea to make it the responsibility of the function-inliner to use the default-values when inlining a call where the attribute is not explicitly provided.

Does that make sense? I realize that the existing implementation of inliner will need to be updated to do this, but just want to discuss the idea first.
gramalingam(2022-08-20 00:05:50):nit: I guess NegLsmbda => NegLambda (typo from original probably).
gramalingam(2022-08-20 00:07:32):`slop` => `slope` ?
linkerzhang(2022-08-25 09:24:20):+1 on moving towards to simple function.
gramalingam(2022-08-30 20:49:59):I suggest using `ctx.hasInput(1)` instead. Same for line below.
gramalingam(2022-08-30 20:53:29):Why change this? Seems unrelated to the main PR. Anyway: we can decide on this after the decision about opsets in function-definitions.
liqunfu(2022-08-30 21:35:57):I discussed with the original author, this AddOpset("", 16) was added due to a typo. It was detected by the change that use the function's opset version if specified in test case generation. 
justinchuby(2022-08-31 22:10:56):```suggestion
        # replace opset versions with what are specified in function proto
```
gramalingam(2022-09-07 23:48:12):Nit: indentation seems off in above line, and similarly in other defs below.
gramalingam(2022-09-08 03:28:46):Wouldn't it be simpler to have the if-condition outside the for-loop and change the else-statement to update `kwargs["opset_imports"] = func_opset_import` in 1 step? It looks a bit confusing, even though I guess it works.
liqunfu(2022-09-08 16:00:36):The purpose is to append if "opset_imports" does not exist, otherwise update. So it cannot be outside of the loop.

liqunfu(2022-09-08 17:30:25):took a second look, you are right. I made the correction. Thanks!
lgtm-com[bot](2022-08-24 21:15:04):This pull request **fixes 1 alert** when merging 037359b069dc07236320673963382da122491bba into 9b8c8d2d35d91aba17203ee5ee93d6c552bfb86e - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-c423288c64618c5eaa8a81d13e40d263b4830d5c)

**fixed alerts:**

* 1 for Module is imported with &#39;import&#39; and &#39;import from&#39;
lgtm-com[bot](2022-08-24 22:46:20):This pull request **fixes 1 alert** when merging 6150b2d4eca1d71bb909b8c303604048cbbcb501 into 9b8c8d2d35d91aba17203ee5ee93d6c552bfb86e - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-506ee7bbcccc62ef3620bd3ae9cc8ec5b536764c)

**fixed alerts:**

* 1 for Inconsistent definition of copy constructor and assignment \(&#39;Rule of Two&#39;\)
lgtm-com[bot](2022-08-24 23:28:36):This pull request **fixes 1 alert** when merging f19594a5cdaa45c57d35059cc6f310083afc5bde into 9b8c8d2d35d91aba17203ee5ee93d6c552bfb86e - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-edec32b7f5000bb6b75832d69c02d4ddd4c80df4)

**fixed alerts:**

* 1 for Inconsistent definition of copy constructor and assignment \(&#39;Rule of Two&#39;\)
lgtm-com[bot](2022-09-14 21:20:36):This pull request **fixes 1 alert** when merging 6625dc08147e3e90a8c7d3f5ad24e5d924db00cb into 5fbb7b507e8c908d210179181c9f22ffc01e80fc - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-8db75b6fdfebb35c1bfbf9597ab697dd71765c7f)

**fixed alerts:**

* 1 for Inconsistent definition of copy constructor and assignment \(&#39;Rule of Two&#39;\)
jcwchen(2022-08-26 16:34:13):cc @onnx/sig-archinfra-approvers, @snnn: this PR is ready for review. Thanks!
jcwchen(2022-09-09 18:50:26):@snnn Thank you for the reviews. I think I should solve all of them. Please take another round of review. Meanwhile, I will try to validate this PR and https://github.com/onnx/onnx/pull/4400 in ORT and see whether there is any issue in advance.

cc @postrational IIRC, you mentioned you are interested in this topic in our last operator-sig meeting. Feel free to review. Thank you!
snnn(2022-08-26 22:01:56):This would not work. 
Please manually use https://docs.microsoft.com/en-us/windows/win32/api/stringapiset/nf-stringapiset-multibytetowidechar function to convert entry.value() to a wide char string, and during the conversion you need to set code page to CP_UTF8. Because the ONNX standard expects all strings are UTF-8. 
snnn(2022-08-26 22:02:34):You could try if it works for paths with Chinese characters. 
snnn(2022-08-26 22:04:50):Why do you use rfind? Why search from end to start while the comment says you want to check if the string starts with something? 

And please replace == 0 with == std::wstring::npos.

snnn(2022-08-26 22:06:40):Would you please add a comment of what does this function do?
snnn(2022-08-26 22:12:01):This part looks very confusing. Initially I thought the second one was specialization of the first one, Then I realized it missed "template" keyword. 
https://docs.microsoft.com/en-us/cpp/cpp/explicit-specialization-of-function-templates?view=msvc-170 

So, why for std::string you use template but for C style strings you use overloading? Why not using the same approach for both? 

snnn(2022-08-26 22:13:08):Please use L'\0'
snnn(2022-08-26 22:15:14):utf8str.size() doesn't include the nul-terminator of the string. 
So does `size_required`.
When you construct ws_str with length=size_required, will the underlaying implementation automatically add a '\0' at the end? I'm not sure. 
jcwchen(2022-08-31 20:36:06):Good catch. I should further use `utf8str_to_wstring` (including `MultiByteToWideChar`) to convert the std::string into std::wstring first.
jcwchen(2022-08-31 20:36:18):I followed the [original implementation](https://github.com/onnx/onnx/blob/8133ea176ca6aeb28a26a6f3e8e0687ed3c99a8e/onnx/checker.cc#L132), but I agreed it looks confusing. To prevent confusion, the code should reflect what the comment said. Thus, I changed it into:
```
if (relative_path.find(L".." + w_k_preferred_path_separator, 0) != std::string::npos) {
...
```
jcwchen(2022-08-31 20:36:23):Sounds good. Just added in path.h.
jcwchen(2022-08-31 20:45:02):Thanks for the pointer. I added it because we need to explicitly define a function for char[] input (directly giving raw string in double quote)

To prevent confusion, I also templatized this function and added comments. Now I added it as:
```
// std::string in template cannot be recognized as char[]
// Therefore explicitly define char[] to handle char[] input
template <typename CHAR, typename std::size_t N>
std::basic_string<CHAR> clean_relative_path(const CHAR (&path)[N]) {
  return clean_relative_path(std::basic_string<CHAR>(path));
}
```
jcwchen(2022-08-31 21:43:42):Good point. I checked the document and it does not explicitly say we need to handle end of string on our own so I tried to rum some experiments as you suggested. I initialize some Chinese+English characters as std::wstring and then use `std::codecvt_utf8` to convert them from std::wstring into std::string. Next, I used the `utf8str_to_wstring` here to convert these std::string back to std::wstring. In result, the twice converted std::wstring is identical as the one before conversion. Therefore, I think the original method should be good. Please let me know if you still have concern. Thanks!
snnn(2022-09-07 17:25:53):On Windows, please use 
```
path canonical(const path& pval, const path& base = current_path());
path canonical(const path& pval, error_code& ec);
path canonical(const path& pval, const path& base, error_code& ec);
```
from std::filesystem
snnn(2022-09-07 17:28:36):What is `const CHAR (&path)[N]` ? I'm not familiar with this grammar .
snnn(2022-09-07 17:31:49):Please try to not implement such things by yourself. Use the functions in std::filesystem instead. Because it is very very difficult to handle all the cases.  Path names on Windows have many different styles. 
snnn(2022-09-07 17:33:29):Please use "constexpr const char*" or "constexpr const wchar_t*" so that they can be initialized at compile time. We should avoid having dynamic initializers whenever possible. 
snnn(2022-09-07 17:36:58):And on Windows you can use "path::make_preferred" function
snnn(2022-09-07 17:37:35):On Windows you can use `path::operator/=`
snnn(2022-09-07 17:39:57):You should not need to have this function for Windows. Instead, you should iterate `path` as a container. 

Like this:
```C++
// filesystem_path_example.cpp
// compile by using: /EHsc /W4 /permissive- /std:c++17 (or later)
#include <string>
#include <iostream>
#include <sstream>
#include <filesystem>

using namespace std;
using namespace std::filesystem;

wstring DisplayPathInfo()
{
    // This path may or may not refer to an existing file. We are
    // examining this path string, not file system objects.
    path pathToDisplay(L"C:/FileSystemTest/SubDir3/SubDirLevel2/File2.txt ");

    wostringstream wos;
    int i = 0;
    wos << L"Displaying path info for: " << pathToDisplay << endl;
    for (path::iterator itr = pathToDisplay.begin(); itr != pathToDisplay.end(); ++itr)
    {
        wos << L"path part: " << i++ << L" = " << *itr << endl;
    }

    wos << L"root_name() = " << pathToDisplay.root_name() << endl
        << L"root_path() = " << pathToDisplay.root_path() << endl
        << L"relative_path() = " << pathToDisplay.relative_path() << endl
        << L"parent_path() = " << pathToDisplay.parent_path() << endl
        << L"filename() = " << pathToDisplay.filename() << endl
        << L"stem() = " << pathToDisplay.stem() << endl
        << L"extension() = " << pathToDisplay.extension() << endl;

    return wos.str();
}

int main()
{
    wcout << DisplayPathInfo() << endl;
    // wcout << ComparePaths() << endl; // see following example
    wcout << endl << L"Press Enter to exit" << endl;
    wstring input;
    getline(wcin, input);
}
```
snnn(2022-09-07 17:40:19):LGTM.
snnn(2022-09-07 17:41:21):Please check the range of utf8str.size() before forcing converting it to int. size_t could be bigger than int. 
jcwchen(2022-09-08 00:50:10):This is the way to create template for array of T I believe: https://stackoverflow.com/questions/33234979/how-to-write-a-template-function-that-takes-an-array-and-an-int-specifying-array
jcwchen(2022-09-08 00:51:38):To confirm, are you suggesting that for Windows we should directly use `canonical` instead of using `clean_relative_path` to clean relative path on other platforms?
snnn(2022-09-08 01:07:34):Why don't you use std::array instead?
And why this function takes an array instead of a C-string pointer?
jcwchen(2022-09-08 15:52:28):I think existing lexically_normal+make_prefered from filesystem should cover `clean_relative_path` and `normalize_separator` for Windows. I will update this PR soon (remove self implementation and utilize existing functions whenever possible) and let you know. Thanks for the pointer!
snnn(2022-09-08 16:29:57):> To confirm, are you suggesting that for Windows we should directly use `canonical` instead of using `clean_relative_path` to clean relative path on other platforms?

Yes.
snnn(2022-09-09 03:08:28):You missed a const. 
constexpr const char*
snnn(2022-09-09 03:08:55):And  why it is a string instead of a single char?
jcwchen(2022-09-09 18:47:02):Previously I tried C-string pointer and it didn't work. It seems required to be defined like it for using template with char[] string (i.e., "this is a string"). Still, now Windows and POSIX have different implementation so this PR doesn't need template anymore.
jcwchen(2022-09-09 18:47:38):Thanks for catching this unintended change. Just updated.
snnn(2022-09-09 22:59:39):The comment doesn't match the code. You are checking if the string contains "..", not "starts"
snnn(2022-09-09 23:03:40):I don't understand this test case. It seems that we expect this function would force converting every abs path to relative path? But why "/abc" becomes "abc"? If the current working directory is "/", it makes sense. But, what if we are not in the root dir? 

And on Windows abs path **may** start with things like "C:" , not "/". Then in that case what this function `clean_relative_path` would do?
jcwchen(2022-09-10 01:41:40):Updated. Thanks!
jcwchen(2022-09-10 01:46:13):Good question... I also found it is confusing when involving absolute path. Actually ONNX IR only allows using relative path to represent the location for external tensors: https://github.com/onnx/onnx/blob/main/docs/IR.md#external-tensor-data.

Therefore, I further add absolute path check in checker, add comment for clean_relative_path that it cannot work with absolute path, and remove absolute path tests in `common_path_test.cc` to prevent confusions.
jcwchen(2022-08-26 17:15:54):Note: I only put double quotes around $<TARGET_FILE:onnx> for Mac. Other platforms like Windows and Ubuntu originally work fine from my experiments and further putting double quotes will bring other issues. Therefore I didn't put double quotes for other platforms.
daquexian(2022-08-29 13:42:05):LGTM. Could you please also write the note above this line in CMakeLists.txt so that future readers can know the context?
jcwchen(2022-08-30 04:17:26):Good idea. Just added. Thanks!
gramalingam(2022-09-07 18:12:29):LGTM, thanks! Just one minor comment above to clarify shapes for the zero-size tensor.
p-wysocki(2022-09-22 10:00:46):@jcwchen I ran the commands, they did not show any files. However, after removing `test_driver.cc` changes the CI started passing. I have no idea why it's passing now, maybe the issue was sporadic or has been fixed on master in the meantime. Thanks for the help!
jcwchen(2022-09-28 16:42:27):FYI: to detect similar issue (introducing new node test directories without corresponding test script) I mentioned above, I added a CI check for it by this PR: https://github.com/onnx/onnx/pull/4514.
p-wysocki(2022-09-28 18:38:26):@jcwchen I applied your suggestions, thanks! For some reason I assumed that script that generates data also removes the old files. I also approved your PR introducing a CI check for that, it sounds helpful. Test names also have been aligned.
gramalingam(2022-09-29 15:29:53):> I might figure out why previous CI has failed: some added test model files are not needed. I don't see where they are generated. For instance, `test_uneven_split` and `test_uneven_split_opset18`. I would suggest you clean all of added test files and then use `python onnx/backend/test/cmd_tools.py generate-data` to generated them again. Since related test was just removed due to recent ONNXIFI deprecation, I will add a similar check in CI to prevent future issues like it.
> 
> Another suggestion would be making test files naming convention consistent. For names of Split related tests, please use `test_split_XXX_XXX`. IMO, it will be helpful for users to search related test files.

I think it would be helpful to update the `AddNewOp` document (may be as a helpful Troubleshooting tip at the end) to remind people to delete the old folder before generating test-cases. (If the CI can catch it, that would be great too.)
gramalingam(2022-09-29 15:31:22):> > I might figure out why previous CI has failed: some added test model files are not needed. I don't see where they are generated. For instance, `test_uneven_split` and `test_uneven_split_opset18`. I would suggest you clean all of added test files and then use `python onnx/backend/test/cmd_tools.py generate-data` to generated them again. Since related test was just removed due to recent ONNXIFI deprecation, I will add a similar check in CI to prevent future issues like it.
> > Another suggestion would be making test files naming convention consistent. For names of Split related tests, please use `test_split_XXX_XXX`. IMO, it will be helpful for users to search related test files.
> 
> I think it would be helpful to update the `AddNewOp` document (may be as a helpful Troubleshooting tip at the end) to remind people to delete the old folder before generating test-cases. (If the CI can catch it, that would be great too.)

Or, may be the test-case generator should delete the folder? (Of course, it could be a bit risky in case the user has some other file there ... may be warn the user if there is some extra files there perhaps.)
jcwchen(2022-09-29 15:35:59):> I think it would be helpful to update the AddNewOp document (may be as a helpful Troubleshooting tip at the end) to remind people to delete the old folder before generating test-cases. (If the CI can catch it, that would be great too.)

Sounds good. I will add sentence about it in AddNewOp.md in this PR as well: https://github.com/onnx/onnx/pull/4514. Also note that that PR will let CI to catch this kind of issue. Perhaps we can forward that PR first and have that verify this PR.

> Or, may be the test-case generator should delete the folder? (Of course, it could be a bit risky in case the user has some other file there ... may be warn the user if there is some extra files there perhaps.)

Yes, but it seems possible that users would like to keep existing ones for some testing purposes? Therefore, to keep behavior consistent, I introduced --clean flag for generate_data for CI use (by default it is False so it won't clean original node directory automatically).
gramalingam(2022-09-07 18:08:42):Can you clarify the input/output shapes explicitly to help the reader? I assume that the shapes are [0] (a 1-dimensional tensor with dimension-size=0).
p-wysocki(2022-09-08 09:47:41):Good point, the notation can be a bit confusing. I added a comment explaining what this array is.
gramalingam(2022-09-29 15:33:29):Need to update above line by adding the line from below ``If the tensor is not evenly splittable, the last chunk will be smaller.``
p-wysocki(2022-09-30 12:23:05):Good catch, done.
lgtm-com[bot](2022-09-02 13:32:40):This pull request **introduces 8 alerts** when merging fce8badf793ba02b3e45c4ba81f8813939742dca into f2f140805c109490f4a5d09381ab131178e54023 - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-367b05f945ec6749f1fb14877e9a4a4daf2b37d6)

**new alerts:**

* 7 for Unused import
* 1 for Unused local variable
lgtm-com[bot](2022-09-02 14:23:17):This pull request **introduces 7 alerts** when merging 5018923cb59909231bb49c6722d6063eaca88168 into f2f140805c109490f4a5d09381ab131178e54023 - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-069ad443c9d2d1b2b813ef8859e6c36eef38f792)

**new alerts:**

* 6 for Unused import
* 1 for Unused local variable
lgtm-com[bot](2022-09-02 14:35:35):This pull request **introduces 7 alerts** when merging 4e247d75c657d55ccfc1607e6ffa582ee4439eff into f2f140805c109490f4a5d09381ab131178e54023 - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-30dc1676599644bfc838d47ea0982726edfd65e7)

**new alerts:**

* 6 for Unused import
* 1 for Unused local variable
lgtm-com[bot](2022-09-02 14:44:41):This pull request **introduces 7 alerts** when merging 3682e11dc69cea76869361cab60b7a0bb4935483 into f2f140805c109490f4a5d09381ab131178e54023 - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-e44eec3e6c93b168745f707a632d39679ab7698f)

**new alerts:**

* 6 for Unused import
* 1 for Unused local variable
lgtm-com[bot](2022-09-02 16:11:41):This pull request **introduces 7 alerts** when merging 7b53b81ce4151774fec6d699bc5e4ef319e51e73 into f2f140805c109490f4a5d09381ab131178e54023 - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-fc2b58c008f19ccaf2ca6f8bb12d371bb0c03711)

**new alerts:**

* 6 for Unused import
* 1 for Unused local variable
lgtm-com[bot](2022-09-02 17:06:33):This pull request **introduces 7 alerts** when merging 452253bffb101c9e072b9939c49e763638d09dc4 into f2f140805c109490f4a5d09381ab131178e54023 - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-1f006897ffe3874d057996b42084c1248a02e007)

**new alerts:**

* 6 for Unused import
* 1 for Unused local variable
lgtm-com[bot](2022-09-02 17:41:50):This pull request **introduces 7 alerts** when merging 4800f3db558e546c5884a54e10b4cd3f68cdeb0f into f2f140805c109490f4a5d09381ab131178e54023 - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-be273e7b9f1a76619027dae7409cc0e9d6ba33d4)

**new alerts:**

* 6 for Unused import
* 1 for Unused local variable
lgtm-com[bot](2022-09-02 17:57:40):This pull request **introduces 7 alerts** when merging 4a9218a5ebe58fdd6bb2d02db8019d47ff151c86 into f2f140805c109490f4a5d09381ab131178e54023 - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-685597135e2e0d4888be138c468392d45a5fb276)

**new alerts:**

* 6 for Unused import
* 1 for Unused local variable
lgtm-com[bot](2022-09-02 18:41:30):This pull request **introduces 7 alerts** when merging fd14c7c37d2262c85e04524613757c113014c511 into f2f140805c109490f4a5d09381ab131178e54023 - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-d3cb24fb4907be59b9d1ee265f7631d939af2518)

**new alerts:**

* 6 for Unused import
* 1 for Unused local variable
lgtm-com[bot](2022-09-02 19:49:02):This pull request **introduces 7 alerts** when merging d5dd0efeb1f7fa52e46704801bf1286e5cea3a06 into f2f140805c109490f4a5d09381ab131178e54023 - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-b60d796f53e31d9dd28f29d5117ed4a7355a4395)

**new alerts:**

* 6 for Unused import
* 1 for Unused local variable
lgtm-com[bot](2022-09-02 21:26:27):This pull request **introduces 7 alerts** when merging 8707d2449f0fd0410587dc13c3b3008a3f651835 into f2f140805c109490f4a5d09381ab131178e54023 - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-7fa69e4b12e112f2dcc6959fcda6c06f3be8a643)

**new alerts:**

* 6 for Unused import
* 1 for Unused local variable
lgtm-com[bot](2022-09-03 08:31:24):This pull request **introduces 7 alerts** when merging b181f443aa3371d474c1d286f39716c8032789cb into f2f140805c109490f4a5d09381ab131178e54023 - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-becf0b1c38472325f8f55d5f43a0ed05fce7da97)

**new alerts:**

* 6 for Unused import
* 1 for Unused local variable
lgtm-com[bot](2022-09-03 09:05:21):This pull request **introduces 7 alerts** when merging 7a68ae8b2d277871c7788ae55ef881d4c0ba1b41 into f2f140805c109490f4a5d09381ab131178e54023 - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-4bd63b61f3f881ce64a89c66a2cdeeedd85d7528)

**new alerts:**

* 6 for Unused import
* 1 for Unused local variable
lgtm-com[bot](2022-09-03 09:20:54):This pull request **introduces 7 alerts** when merging 176cbd0f7a9eee3649ab25246ab8d3b1625599fc into f2f140805c109490f4a5d09381ab131178e54023 - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-3b1667c4138a0d33d17f714421ed9b33880ee44d)

**new alerts:**

* 6 for Unused import
* 1 for Unused local variable
lgtm-com[bot](2022-09-03 09:38:39):This pull request **introduces 6 alerts** when merging 504aedd27ba80fd8fcf453342791b977e9b53acf into f2f140805c109490f4a5d09381ab131178e54023 - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-f8cfb3a5c296dc4d3c4894f076c9ff8a7c037f52)

**new alerts:**

* 6 for Unused import
lgtm-com[bot](2022-09-03 10:44:13):This pull request **introduces 7 alerts** when merging 4498dcb954c26a50d8a510abd40f5ecf69626e6d into f2f140805c109490f4a5d09381ab131178e54023 - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-d5592daf158bd67ae038721dd3726e3588220a7a)

**new alerts:**

* 7 for Unused import
lgtm-com[bot](2022-09-03 15:58:45):This pull request **introduces 6 alerts** when merging 8a1343c0cf8f03e93b535dfa32d1caef639398b0 into f2f140805c109490f4a5d09381ab131178e54023 - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-1899a83a082a7471877392c06c43072e71dd23db)

**new alerts:**

* 6 for Unused import
lgtm-com[bot](2022-09-03 17:35:38):This pull request **introduces 6 alerts** when merging 4677bc4972d89bec8e54f4791d9a397eea5a2ee0 into f2f140805c109490f4a5d09381ab131178e54023 - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-6218ff0c673dae8716d719e622bbc3a53676e4d3)

**new alerts:**

* 6 for Unused import
lgtm-com[bot](2022-09-03 19:05:55):This pull request **introduces 6 alerts** when merging 6273ab94bf90fb99ca2169255e4df7dc37569307 into f2f140805c109490f4a5d09381ab131178e54023 - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-154acce0f59c5f6396aba4b50fcde40398727fe4)

**new alerts:**

* 6 for Unused import
lgtm-com[bot](2022-09-03 21:35:11):This pull request **introduces 6 alerts** when merging 1548e7a36f1ea411484431d00d5d6f7626ea4aec into f2f140805c109490f4a5d09381ab131178e54023 - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-379a3668334b06c3cfbf8c81e6a349f91533cd21)

**new alerts:**

* 6 for Unused import
lgtm-com[bot](2022-09-03 21:45:22):This pull request **introduces 6 alerts** when merging acd69edb29dd6b44bbfe211d9948b75375795502 into f2f140805c109490f4a5d09381ab131178e54023 - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-b6e05276b5f0b38e4a0c7da360547c73d6f6e40b)

**new alerts:**

* 6 for Unused import
lgtm-com[bot](2022-09-04 00:37:22):This pull request **introduces 6 alerts** when merging 6534f545edaa27b839d21d0bdeebec553487a8c8 into f2f140805c109490f4a5d09381ab131178e54023 - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-093d025cce5dfc919a0a43553250f08aee17df5e)

**new alerts:**

* 6 for Unused import
lgtm-com[bot](2022-09-04 09:20:42):This pull request **introduces 9 alerts** when merging cc8393fff6344201a5b7d5ead77c22dc09528aa7 into f2f140805c109490f4a5d09381ab131178e54023 - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-165b97fa4f0387c82d5a4365a7cf083dae406470)

**new alerts:**

* 8 for Unused import
* 1 for Unused local variable
lgtm-com[bot](2022-09-05 07:07:34):This pull request **introduces 9 alerts** when merging fc68aa50dd30f8857d43ec9b39ec8fd4e45e84c8 into f2f140805c109490f4a5d09381ab131178e54023 - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-ba0827a77c5d2941cfb7f779e09d5a65da4832a9)

**new alerts:**

* 8 for Unused import
* 1 for Unused local variable
lgtm-com[bot](2022-09-05 07:35:04):This pull request **introduces 9 alerts** when merging 5315f33f353d0d10bc286843a6aafe06ce134ae3 into f2f140805c109490f4a5d09381ab131178e54023 - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-98b48a7fafb110b31c7c7a12e7ba5782c8fab80e)

**new alerts:**

* 8 for Unused import
* 1 for Unused local variable
lgtm-com[bot](2022-09-05 08:57:58):This pull request **introduces 11 alerts** when merging 83163e06a7d35cfeabab9c08ad69b6f797857633 into f2f140805c109490f4a5d09381ab131178e54023 - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-a1b17bd875affb4095ce13d21c0d637047cd5095)

**new alerts:**

* 10 for Unused import
* 1 for Unused local variable
lgtm-com[bot](2022-09-05 10:52:31):This pull request **introduces 15 alerts** when merging d2827cd79d1313ae39b832362e11168f131f5490 into f2f140805c109490f4a5d09381ab131178e54023 - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-33a17c164462712abfc7be4e23276d23757d5b96)

**new alerts:**

* 11 for Unused import
* 3 for Conflicting attributes in base classes
* 1 for Unused local variable
lgtm-com[bot](2022-09-05 12:17:27):This pull request **introduces 15 alerts** when merging a4fd14e94255c023cd5a0b45487954af37819c24 into f2f140805c109490f4a5d09381ab131178e54023 - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-a432ed54ef3a675dfba4c73859ed1f0394825e68)

**new alerts:**

* 11 for Unused import
* 3 for Conflicting attributes in base classes
* 1 for Unused local variable
lgtm-com[bot](2022-09-05 13:22:26):This pull request **introduces 15 alerts** when merging c31556f681acc8f4b0c0d146572fdadc8974912d into f2f140805c109490f4a5d09381ab131178e54023 - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-cbfe0fa3d82a6659941a8a8ecb148adea54d93fd)

**new alerts:**

* 11 for Unused import
* 3 for Conflicting attributes in base classes
* 1 for Unused local variable
lgtm-com[bot](2022-09-05 13:57:56):This pull request **introduces 15 alerts** when merging 403f122a8e058aa0ed9ddbba4feda53fd5ff6cc3 into f2f140805c109490f4a5d09381ab131178e54023 - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-fd1d2b8da1e95a977015f48262227a9a3312036d)

**new alerts:**

* 11 for Unused import
* 3 for Conflicting attributes in base classes
* 1 for Unused local variable
lgtm-com[bot](2022-09-05 14:27:43):This pull request **introduces 15 alerts** when merging 71a374c02a4080908e590f57f2e354ab6b70de8f into f2f140805c109490f4a5d09381ab131178e54023 - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-a5d8d9023e7ac60b1e70b2fb1f7d2d0b6cf2878e)

**new alerts:**

* 11 for Unused import
* 3 for Conflicting attributes in base classes
* 1 for Unused local variable
lgtm-com[bot](2022-09-05 14:50:54):This pull request **introduces 15 alerts** when merging 53aed5a4e211118b511d7115e0ac16bb2b4c2c65 into f2f140805c109490f4a5d09381ab131178e54023 - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-c1d94a645f5e6c2384d646562a5bb274e1816494)

**new alerts:**

* 11 for Unused import
* 3 for Conflicting attributes in base classes
* 1 for Unused local variable
lgtm-com[bot](2022-09-05 17:53:49):This pull request **introduces 15 alerts** when merging e4156c936c8ffc7e145c88702a40b9d3d9bf6dde into f2f140805c109490f4a5d09381ab131178e54023 - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-7af072123200b07de1e5d053294479d11eeecb6a)

**new alerts:**

* 11 for Unused import
* 3 for Conflicting attributes in base classes
* 1 for Unused local variable
lgtm-com[bot](2022-09-05 18:05:35):This pull request **introduces 15 alerts** when merging 91321eb0da9f164da2e89d0123868060801dc044 into f2f140805c109490f4a5d09381ab131178e54023 - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-3fb0861bde71f8e9d373f78fb30e1a48a2df7f84)

**new alerts:**

* 11 for Unused import
* 3 for Conflicting attributes in base classes
* 1 for Unused local variable
lgtm-com[bot](2022-09-06 08:13:16):This pull request **introduces 15 alerts** when merging 1a5ecd6e4ff9ebe20ff59e645c0483a616464ef4 into f2f140805c109490f4a5d09381ab131178e54023 - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-36ab6d2b1875de6ae65e3ce0ca76a450e36cd5d8)

**new alerts:**

* 11 for Unused import
* 3 for Conflicting attributes in base classes
* 1 for Unused local variable
lgtm-com[bot](2022-09-06 08:50:51):This pull request **introduces 32 alerts** when merging f20ceb4b435f079ba6a8d7c575081cb136f63a60 into f2f140805c109490f4a5d09381ab131178e54023 - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-6dd8390e43ad1e7ceab85d0da8b63800879cebe9)

**new alerts:**

* 28 for Unused import
* 3 for Conflicting attributes in base classes
* 1 for Unused local variable
lgtm-com[bot](2022-09-06 09:36:53):This pull request **introduces 32 alerts** when merging b2b5d492e71d702f45f276404481d5df912607ef into f2f140805c109490f4a5d09381ab131178e54023 - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-c30cfa888f1031f05638b97b9f682dc1365482e7)

**new alerts:**

* 28 for Unused import
* 3 for Conflicting attributes in base classes
* 1 for Unused local variable
lgtm-com[bot](2022-09-06 10:13:05):This pull request **introduces 32 alerts** when merging 76064d1b60d9ac7b64a9c1c6b466233c72138cac into f2f140805c109490f4a5d09381ab131178e54023 - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-488916394036c1487c6e6f7b608694ce0781405a)

**new alerts:**

* 28 for Unused import
* 3 for Conflicting attributes in base classes
* 1 for Unused local variable
lgtm-com[bot](2022-09-06 10:22:45):This pull request **introduces 32 alerts** when merging 9b34611692854545b640339eba83aacfe40d96e5 into f2f140805c109490f4a5d09381ab131178e54023 - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-4db630bcc6130631375ef3756545387b1e0f421d)

**new alerts:**

* 28 for Unused import
* 3 for Conflicting attributes in base classes
* 1 for Unused local variable
lgtm-com[bot](2022-09-06 10:36:03):This pull request **introduces 32 alerts** when merging 20fb556ca50557ae66241542530c352f1b528771 into f2f140805c109490f4a5d09381ab131178e54023 - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-4b2fc7ca6da178c878edf6a03eb76c2e4af3edba)

**new alerts:**

* 28 for Unused import
* 3 for Conflicting attributes in base classes
* 1 for Unused local variable
lgtm-com[bot](2022-09-06 10:50:28):This pull request **introduces 32 alerts** when merging 69d2399fc88a6c2f921d21b74e11639ebd1b6d79 into f2f140805c109490f4a5d09381ab131178e54023 - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-fb1d4a3c970a9ad06f97dc4a51b3510327027a4b)

**new alerts:**

* 28 for Unused import
* 3 for Conflicting attributes in base classes
* 1 for Unused local variable
lgtm-com[bot](2022-09-06 11:05:55):This pull request **introduces 32 alerts** when merging 3fbd1957ef36f4250704cd338da497c64bfc46cc into f2f140805c109490f4a5d09381ab131178e54023 - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-9c7887dc251e79e9f9d4e60edc3785aa6e711b10)

**new alerts:**

* 28 for Unused import
* 3 for Conflicting attributes in base classes
* 1 for Unused local variable
lgtm-com[bot](2022-09-06 13:13:48):This pull request **introduces 32 alerts** when merging 79a6f6dd9a94d5524348e613bf8650102e94be0d into f2f140805c109490f4a5d09381ab131178e54023 - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-b165af879a17dcf687ca09c055857029201bd4c3)

**new alerts:**

* 28 for Unused import
* 3 for Conflicting attributes in base classes
* 1 for Unused local variable
lgtm-com[bot](2022-09-06 13:26:51):This pull request **introduces 28 alerts** when merging 5712ec535ebe734b55d12670793cf3dd9c24b9a0 into f2f140805c109490f4a5d09381ab131178e54023 - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-86075f4aef0426f854f43bf3f71004ec720259c6)

**new alerts:**

* 28 for Unused import
lgtm-com[bot](2022-09-06 13:58:20):This pull request **introduces 28 alerts** when merging 71134c078f85b5a0480cdd344715c456d5a4386e into f2f140805c109490f4a5d09381ab131178e54023 - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-c302f758646fdaa90ad1dd212889565c85bd8ba5)

**new alerts:**

* 28 for Unused import
lgtm-com[bot](2022-09-06 16:07:18):This pull request **introduces 28 alerts** when merging b080bc07a5432c5082f8b266c0d4567d72cdc879 into f2f140805c109490f4a5d09381ab131178e54023 - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-9334b6fd109aa78242829cbe32bab35166115b1c)

**new alerts:**

* 28 for Unused import
lgtm-com[bot](2022-09-07 13:27:49):This pull request **introduces 28 alerts** when merging 37b2c4e1350a2687cf2b2e2f184eaad5840d4e44 into f2f140805c109490f4a5d09381ab131178e54023 - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-12e189637c7c0a173765d5d45217eda9b9b89523)

**new alerts:**

* 28 for Unused import
lgtm-com[bot](2022-09-07 13:58:32):This pull request **introduces 33 alerts** when merging 72ec8f762dff8896fc8eee66da4a301108670df2 into f2f140805c109490f4a5d09381ab131178e54023 - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-0b4a12b755b9b2f40ade5431a2fb145afd81abfa)

**new alerts:**

* 33 for Unused import
lgtm-com[bot](2022-09-07 16:30:07):This pull request **introduces 40 alerts** when merging 9aaf853c863c21d4cd726566bf700de0cde34eb8 into f2f140805c109490f4a5d09381ab131178e54023 - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-913c642235c13878498822a50bc1ace5a0c54c1c)

**new alerts:**

* 40 for Unused import
lgtm-com[bot](2022-09-07 17:07:39):This pull request **introduces 40 alerts** when merging 349e642a27f9948822ff0ce3dadebb91fb322f01 into f2f140805c109490f4a5d09381ab131178e54023 - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-209476d2dcc0e905a7e4bba08fa65b6f7cb4e8e4)

**new alerts:**

* 40 for Unused import
lgtm-com[bot](2022-09-08 08:41:27):This pull request **introduces 40 alerts** when merging 71a64f41a2f8e0c4a369f5084f2417a33cec7e59 into 421516583bf692e0c616a5e988ae09a57a75a238 - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-a397443525d7ef464d07425a9ad0c659e33a07f7)

**new alerts:**

* 40 for Unused import
lgtm-com[bot](2022-09-08 10:58:17):This pull request **introduces 49 alerts** when merging 32f45fceeaad5bc4dc9e97fb2371947a1280655d into 421516583bf692e0c616a5e988ae09a57a75a238 - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-90677b15e407861620ca90536d6d40b83b908722)

**new alerts:**

* 49 for Unused import
lgtm-com[bot](2022-09-08 14:27:48):This pull request **introduces 57 alerts** when merging 6e8bb9ad4fd24a4432660f38777cb74919055e3a into 421516583bf692e0c616a5e988ae09a57a75a238 - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-1390fee6246a9cc7cfe606d32e9c9f68a9d5d9bd)

**new alerts:**

* 56 for Unused import
* 1 for Unused local variable
lgtm-com[bot](2022-09-08 14:53:15):This pull request **introduces 70 alerts** when merging 28e6bced8f4388af4ff1c26d6f047844d895228c into 421516583bf692e0c616a5e988ae09a57a75a238 - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-e58599ad04fefea07299d47e676f88dd6672002e)

**new alerts:**

* 69 for Unused import
* 1 for Unused local variable
lgtm-com[bot](2022-09-08 15:54:42):This pull request **introduces 70 alerts** when merging eb7bbd75a51abdc2687ec5651c1a07317576aff5 into 421516583bf692e0c616a5e988ae09a57a75a238 - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-86202d2923d5c63eed9eb2340d8163513e288946)

**new alerts:**

* 69 for Unused import
* 1 for Unused local variable
lgtm-com[bot](2022-09-08 17:52:43):This pull request **introduces 83 alerts** when merging 473e52e233512b724d999c2d029930a9f683dd40 into 421516583bf692e0c616a5e988ae09a57a75a238 - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-8ef1c9b15e28e22816a0ddbab532350bec9b857a)

**new alerts:**

* 81 for Unused import
* 2 for Unused local variable
lgtm-com[bot](2022-09-08 18:23:17):This pull request **introduces 86 alerts** when merging c29a30ddbc0cf9dc9580aabd0249d369b0b1394e into f4263d8035f7f7a6af314c5f625e824b98083df9 - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-7721b9c2ef3b75caf5d293803b17335f600baf53)

**new alerts:**

* 83 for Unused import
* 2 for Unused local variable
* 1 for Signature mismatch in overriding method
lgtm-com[bot](2022-09-08 22:29:43):This pull request **introduces 89 alerts** when merging d9b058cf95f2c17fa4d2b654184dfc3e962779e2 into f4263d8035f7f7a6af314c5f625e824b98083df9 - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-ed8046bb56ca9c76ae17e50bb6044fcd1cdcdb9e)

**new alerts:**

* 86 for Unused import
* 2 for Unused local variable
* 1 for Signature mismatch in overriding method
lgtm-com[bot](2022-09-08 23:45:20):This pull request **introduces 89 alerts** when merging 854247949fd6b334dc24f12e9c3f6693bd792b13 into f4263d8035f7f7a6af314c5f625e824b98083df9 - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-529bd554a717fbd1bdd5136a8fda895c1893335b)

**new alerts:**

* 86 for Unused import
* 2 for Unused local variable
* 1 for Signature mismatch in overriding method
lgtm-com[bot](2022-09-08 23:53:42):This pull request **introduces 90 alerts** when merging 96ff6ce3b15ad66043a0765e6cc697c4992c6480 into f4263d8035f7f7a6af314c5f625e824b98083df9 - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-709e090ec179be05ec2ae51a14089e4631d7715c)

**new alerts:**

* 87 for Unused import
* 2 for Unused local variable
* 1 for Signature mismatch in overriding method
lgtm-com[bot](2022-09-09 09:51:36):This pull request **introduces 107 alerts** when merging e80b54c07d72166addf6ff2a83fe58edd85b9427 into f4263d8035f7f7a6af314c5f625e824b98083df9 - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-58eb80217b5e3ef4a4122c8731a8b91020c88417)

**new alerts:**

* 104 for Unused import
* 2 for Unused local variable
* 1 for Signature mismatch in overriding method
lgtm-com[bot](2022-09-09 11:16:28):This pull request **introduces 104 alerts** when merging 528ecc5f86e87016b4ab29e863fa7156fcc8ed34 into f4263d8035f7f7a6af314c5f625e824b98083df9 - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-120171a51b0500d575210f68d8f25c9c96b103f8)

**new alerts:**

* 101 for Unused import
* 2 for Unused local variable
* 1 for Signature mismatch in overriding method
lgtm-com[bot](2022-09-09 11:26:48):This pull request **introduces 113 alerts** when merging 3336e0c7c8adb3a30c7aee34419c23392c9cf3f6 into f4263d8035f7f7a6af314c5f625e824b98083df9 - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-42b83641da90674e4dc765c584dae7cc7bd7c37b)

**new alerts:**

* 110 for Unused import
* 2 for Unused local variable
* 1 for Signature mismatch in overriding method
lgtm-com[bot](2022-09-09 12:40:26):This pull request **introduces 134 alerts** when merging 5c5a6063f69bc277e51ff18f9b85d4d59cbfb94d into f4263d8035f7f7a6af314c5f625e824b98083df9 - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-28d743a0c71e8a1252a505b52650d012bc93c34e)

**new alerts:**

* 131 for Unused import
* 2 for Unused local variable
* 1 for Signature mismatch in overriding method
lgtm-com[bot](2022-09-09 13:19:37):This pull request **introduces 135 alerts** when merging b56402c2c43fc33cbc1323bf833b73029b6f79b2 into f4263d8035f7f7a6af314c5f625e824b98083df9 - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-c7947ac7000766ac8168167cbf401cd62ed4c312)

**new alerts:**

* 132 for Unused import
* 2 for Unused local variable
* 1 for Signature mismatch in overriding method
lgtm-com[bot](2022-09-09 13:34:35):This pull request **introduces 137 alerts** when merging cc78dc31b3fe91bbb5a7fcce55fe0620d2e98ddb into f4263d8035f7f7a6af314c5f625e824b98083df9 - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-a1e98b2ccb049dafe919491fadb0960d65a4d8eb)

**new alerts:**

* 134 for Unused import
* 2 for Unused local variable
* 1 for Signature mismatch in overriding method
lgtm-com[bot](2022-09-09 14:26:35):This pull request **introduces 147 alerts** when merging 3a184ae82101871d9fdd6d48f190bb961b9ea519 into f4263d8035f7f7a6af314c5f625e824b98083df9 - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-0f5a8d9a2f220929a1eb47f4594611d2be11d528)

**new alerts:**

* 144 for Unused import
* 2 for Unused local variable
* 1 for Signature mismatch in overriding method
lgtm-com[bot](2022-09-09 14:57:28):This pull request **introduces 151 alerts** when merging a20e553c6729865902ece9c10f7bd1f3e66847a0 into f4263d8035f7f7a6af314c5f625e824b98083df9 - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-7d119adf78afb1bca0fa7a702a0deb4f79af7a51)

**new alerts:**

* 148 for Unused import
* 2 for Unused local variable
* 1 for Signature mismatch in overriding method
lgtm-com[bot](2022-09-09 15:34:39):This pull request **introduces 151 alerts** when merging 7ec73b28b16f79a9ca1ce340afebd91847fb091f into f4263d8035f7f7a6af314c5f625e824b98083df9 - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-9b3590703b3659b7425a650c661c58ed5bea853f)

**new alerts:**

* 146 for Unused import
* 3 for Unused local variable
* 1 for Signature mismatch in overriding method
* 1 for Syntax error
lgtm-com[bot](2022-09-09 16:26:32):This pull request **introduces 155 alerts** when merging 4712f7a216476927a5a894d2aa75908566bc7ac9 into f4263d8035f7f7a6af314c5f625e824b98083df9 - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-41729290a8568903b2d11cbbe501865da03ef37e)

**new alerts:**

* 151 for Unused import
* 3 for Unused local variable
* 1 for Signature mismatch in overriding method
lgtm-com[bot](2022-09-09 17:04:52):This pull request **introduces 152 alerts** when merging a009c9f41f8e632276c9b98b69e7ed03be24df7d into f4263d8035f7f7a6af314c5f625e824b98083df9 - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-db3ac628434cc83ede340bdb6d061849fee02d14)

**new alerts:**

* 148 for Unused import
* 3 for Unused local variable
* 1 for Signature mismatch in overriding method
lgtm-com[bot](2022-09-09 17:16:59):This pull request **introduces 152 alerts** when merging 80afe23b8558a852b41f4e5c68c66b45c120b030 into f4263d8035f7f7a6af314c5f625e824b98083df9 - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-5aabb9a6b04e83f4cd446bab4e398297ae0f33a7)

**new alerts:**

* 148 for Unused import
* 3 for Unused local variable
* 1 for Signature mismatch in overriding method
lgtm-com[bot](2022-09-13 13:06:32):This pull request **introduces 156 alerts** when merging 7a20366a03111c584c7bf81075097435a76dd0be into 5fbb7b507e8c908d210179181c9f22ffc01e80fc - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-402c61b68b9bdd2bf5010b92fda014dc90b8dca8)

**new alerts:**

* 152 for Unused import
* 3 for Unused local variable
* 1 for Signature mismatch in overriding method
lgtm-com[bot](2022-09-13 13:42:56):This pull request **introduces 156 alerts** when merging 344ca8448d8fa123a9479d7605306f743295b3c7 into 5fbb7b507e8c908d210179181c9f22ffc01e80fc - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-8d45f4a6cba1dacbb9e10ec8a43830c29e6049a4)

**new alerts:**

* 152 for Unused import
* 3 for Unused local variable
* 1 for Signature mismatch in overriding method
lgtm-com[bot](2022-09-13 15:13:38):This pull request **introduces 156 alerts** when merging 2557a3b099fb67cab2fc0e7c52923b75890f1637 into 5fbb7b507e8c908d210179181c9f22ffc01e80fc - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-c103cc62757ba6e4d28fcf34a1ea83c7bc6a7f0a)

**new alerts:**

* 152 for Unused import
* 3 for Unused local variable
* 1 for Signature mismatch in overriding method
lgtm-com[bot](2022-09-13 15:52:34):This pull request **introduces 156 alerts** when merging ad545a83b840d2613a6d1ff94070e4aeaf8b3f37 into 5fbb7b507e8c908d210179181c9f22ffc01e80fc - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-faadb3e36e5352d0fef5dcecb12e250f412b3b06)

**new alerts:**

* 152 for Unused import
* 3 for Unused local variable
* 1 for Signature mismatch in overriding method
lgtm-com[bot](2022-09-13 16:36:10):This pull request **introduces 158 alerts** when merging 66b0f649e59b063622a4c8196f643c32674c70a0 into 5fbb7b507e8c908d210179181c9f22ffc01e80fc - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-048b6b6e0a2f62ce6efd80b124943f810555dcc9)

**new alerts:**

* 154 for Unused import
* 3 for Unused local variable
* 1 for Signature mismatch in overriding method
lgtm-com[bot](2022-09-14 14:05:59):This pull request **introduces 158 alerts** when merging b0b98241004b52c3817ee8e588f137004f3e6e01 into 5fbb7b507e8c908d210179181c9f22ffc01e80fc - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-9178c16f5370d8db0535ea4c2e208735d415d98d)

**new alerts:**

* 154 for Unused import
* 3 for Unused local variable
* 1 for Signature mismatch in overriding method
lgtm-com[bot](2022-09-15 10:57:51):This pull request **introduces 162 alerts** when merging 3b3a14ab6fa6a24a4ad2940f37ec2646e3083646 into a252e651fda8e73935827505a148146459c2e06d - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-c52a9ec7d8f24f9aeebe7d62578530ec4599c8f3)

**new alerts:**

* 158 for Unused import
* 3 for Unused local variable
* 1 for Signature mismatch in overriding method
lgtm-com[bot](2022-09-15 13:43:04):This pull request **introduces 163 alerts** when merging 83dfdafa7180cd46f73866dc5921c9a9d46d19ff into a252e651fda8e73935827505a148146459c2e06d - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-ad9a1d26f95d42066d92c44bde166a6793c2f490)

**new alerts:**

* 159 for Unused import
* 3 for Unused local variable
* 1 for Signature mismatch in overriding method
lgtm-com[bot](2022-09-15 15:12:39):This pull request **introduces 165 alerts** when merging abc533b18e0a2c1638c7c10f0f66f283de6f9bc9 into a252e651fda8e73935827505a148146459c2e06d - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-faef289a3c8c8bb69a2aa1ff089a9dbf544dbcc9)

**new alerts:**

* 161 for Unused import
* 3 for Unused local variable
* 1 for Signature mismatch in overriding method
lgtm-com[bot](2022-09-15 15:42:21):This pull request **introduces 161 alerts** when merging 383eb6c90adb1cc216f43ba8b643d46b847c3512 into a252e651fda8e73935827505a148146459c2e06d - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-d78ba4a9f3295598d16274ea261bf89179bba8d2)

**new alerts:**

* 157 for Unused import
* 3 for Unused local variable
* 1 for Signature mismatch in overriding method
lgtm-com[bot](2022-09-16 14:30:03):This pull request **introduces 162 alerts** when merging 0f4b38d854f660d7a0afa8c6241466934c2b7539 into d15ba967cfd2b7804634e0ffb03bdba8c1bc4527 - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-c941454f18c141eadf3fc6c121339f6c904c560c)

**new alerts:**

* 158 for Unused import
* 3 for Unused local variable
* 1 for Signature mismatch in overriding method
lgtm-com[bot](2022-09-16 16:43:24):This pull request **introduces 165 alerts** when merging f335fa6576bb972a937a2a3538001cbe4a8716ef into d15ba967cfd2b7804634e0ffb03bdba8c1bc4527 - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-b07bba5702c782cc517746f414bfb0609623ac95)

**new alerts:**

* 161 for Unused import
* 3 for Unused local variable
* 1 for Signature mismatch in overriding method
lgtm-com[bot](2022-09-16 17:30:24):This pull request **introduces 165 alerts** when merging f465352d0fd145be5b33104242551e33d8e3ebb3 into d15ba967cfd2b7804634e0ffb03bdba8c1bc4527 - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-1d083eb7ec4d5625eb0a21812d076218f606cad8)

**new alerts:**

* 161 for Unused import
* 3 for Unused local variable
* 1 for Signature mismatch in overriding method
gramalingam(2022-09-16 23:53:30):A couple of questions.

(a) Can we enable users to do something like below:
```py
from ... import Celu
x = np.array(...)
y = Celu.eval(x, alpha=0.5)
```

The idea is to enable users to execute/evaluate single-op on inputs+attributes, by exposing a simple python-function interface not involving any "proto" stuff. I think it can be done without much effort. I use the name `eval` since various names like `run` or `_run` are already in use in the Op interfaces.

(b) I suspect that a simple implementation of the above will construct a NodeProto / AttributeProto to construct an instance of `Celu` and run it. Ideally, however, it should be the other way around: namely, the `run` methods should call the `eval` method. But I don't know if this will be much of an effort to do. But I can see that some ops already have such a structure in place, for example `_ArgMax._run` calls `_argmax` ... this seems the preferable form. Here, `_argmax` exposes a simple python-function interface (without any protos involved).
xadupre(2022-09-17 10:13:45):(a) Method eval was not implemented but it was easy to do. It is now done. See below.
(b) In most cases, a method `run` which calls `eval` or a method `eval` which creates an onnx node and then method `run` can be implemented. With the current design, second option is a lot easier to implement. I chose this way because some operators are more efficient if precomputation is done. It is the case for operators using subgraphs but also for operators defined as functions.

```python
import numpy as np
from onnx.runtime.aionnx._op_list import Celu

x = np.array([[0, 1], [-1, 2]], dtype=np.float32)
y = Celu.eval(x, alpha=0.5)
```

Or:

```python
import numpy as np
from onnx.runtime.aionnx import load_op

Celu = load_op("", "Celu")  # domain is ""
x = np.array([[0, 1], [-1, 2]], dtype=np.float32)
y = Celu.eval(x, alpha=0.5)
```

And:

```python
import numpy as np
from onnx.runtime.aionnx._op_list import Celu

onnx_node = Celu.make_node(alpha=0.5)
```


lgtm-com[bot](2022-09-17 10:13:55):This pull request **introduces 165 alerts** when merging 66de94f8a3bc7a4504bd65b76fda00fccc5e5166 into d15ba967cfd2b7804634e0ffb03bdba8c1bc4527 - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-03e0ce2841849b555c06bb6ad20687bdbdbc45bf)

**new alerts:**

* 161 for Unused import
* 3 for Unused local variable
* 1 for Signature mismatch in overriding method
lgtm-com[bot](2022-09-17 10:29:37):This pull request **introduces 165 alerts** when merging 631c215393a9e4daa942a6fc3adc221349fc756a into d15ba967cfd2b7804634e0ffb03bdba8c1bc4527 - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-64417ab5c2603359412dadb576868c22c8f89a0e)

**new alerts:**

* 161 for Unused import
* 3 for Unused local variable
* 1 for Signature mismatch in overriding method
lgtm-com[bot](2022-09-17 21:28:40):This pull request **introduces 164 alerts** when merging b3853ea669eda0806e30ce728d5c3d4b5f904716 into d15ba967cfd2b7804634e0ffb03bdba8c1bc4527 - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-c060b809781fdafe3efa806e65b8272926d0b705)

**new alerts:**

* 160 for Unused import
* 3 for Unused local variable
* 1 for Signature mismatch in overriding method
lgtm-com[bot](2022-09-18 10:48:01):This pull request **introduces 165 alerts** when merging 9071c859cd85a94db8c3e0a44a38f6accb8dbf67 into d15ba967cfd2b7804634e0ffb03bdba8c1bc4527 - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-bfd837fbd7f6bfe3b00a843d6fa5b943fa9c7376)

**new alerts:**

* 161 for Unused import
* 3 for Unused local variable
* 1 for Signature mismatch in overriding method
lgtm-com[bot](2022-09-18 11:15:57):This pull request **introduces 165 alerts** when merging 83d40458ac14944dc372213c317a1c4ecca5cf2f into d15ba967cfd2b7804634e0ffb03bdba8c1bc4527 - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-50687b09d997d3c54a3d8efb82b7312c63354dbd)

**new alerts:**

* 161 for Unused import
* 3 for Unused local variable
* 1 for Signature mismatch in overriding method
lgtm-com[bot](2022-09-18 11:28:42):This pull request **introduces 165 alerts** when merging 57a293cb0bfb714b4a0117442847f11b5415913b into d15ba967cfd2b7804634e0ffb03bdba8c1bc4527 - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-bbf4e99bae3bc470c4f3b55ae3f97d9508830eaf)

**new alerts:**

* 161 for Unused import
* 3 for Unused local variable
* 1 for Signature mismatch in overriding method
lgtm-com[bot](2022-09-19 12:57:59):This pull request **introduces 169 alerts** when merging 8a8365dc23d273ec614ca16deb84b8fd90b6b7f4 into d15ba967cfd2b7804634e0ffb03bdba8c1bc4527 - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-b39aeae7ae5220f0c42c5554ba9c0b87e7b5e5eb)

**new alerts:**

* 162 for Unused import
* 6 for Unused local variable
* 1 for Signature mismatch in overriding method
lgtm-com[bot](2022-09-19 14:43:10):This pull request **introduces 169 alerts** when merging bec062a3938275b28f38094664ce743f0c254359 into d15ba967cfd2b7804634e0ffb03bdba8c1bc4527 - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-3249b211f88500dcaeb14d65aa743e48f6d98fc5)

**new alerts:**

* 163 for Unused import
* 5 for Unused local variable
* 1 for Signature mismatch in overriding method
lgtm-com[bot](2022-09-19 15:14:30):This pull request **introduces 167 alerts** when merging ca8d10b9f89985ba6e9d8d597f1239b2172e8c1a into d15ba967cfd2b7804634e0ffb03bdba8c1bc4527 - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-f0cc75823e8e2f8bb260819d3a3d7ed7219433ce)

**new alerts:**

* 163 for Unused import
* 3 for Unused local variable
* 1 for Signature mismatch in overriding method
lgtm-com[bot](2022-09-20 10:30:29):This pull request **introduces 169 alerts** when merging e62bd676088c830ef9469177cdf8a096b8d44031 into d15ba967cfd2b7804634e0ffb03bdba8c1bc4527 - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-d6c4d07be364800c54fdbc2d94a7c41bb45787b9)

**new alerts:**

* 165 for Unused import
* 3 for Unused local variable
* 1 for Signature mismatch in overriding method
lgtm-com[bot](2022-09-21 17:23:20):This pull request **introduces 173 alerts** when merging 8ac8b74ee8a3df08e0136c3362b1143e1d772772 into 895593af6ac17d4fe2749091659083d3b1246cae - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-79c44d36e6d016ee21db79978b4cd932d5ce0d73)

**new alerts:**

* 166 for Unused import
* 6 for Unused local variable
* 1 for Signature mismatch in overriding method
lgtm-com[bot](2022-09-21 18:31:50):This pull request **introduces 170 alerts** when merging da9993a712c6bff232d691adea565e868b6f6809 into 895593af6ac17d4fe2749091659083d3b1246cae - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-7eac4685da539c7bf1a76984a0ebddab92bbd049)

**new alerts:**

* 166 for Unused import
* 3 for Unused local variable
* 1 for Signature mismatch in overriding method
lgtm-com[bot](2022-09-21 19:57:43):This pull request **introduces 170 alerts** when merging e012fc2c21d9d0be77873f06608a9566e42d1c43 into 895593af6ac17d4fe2749091659083d3b1246cae - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-76935c278956855ae3b5bf310d9c1d33c55a26a2)

**new alerts:**

* 166 for Unused import
* 3 for Unused local variable
* 1 for Signature mismatch in overriding method
gramalingam(2022-09-21 23:42:42):A generic comment: I just realized that `onnx.runtime` is going to cause confusion vs. `onnxruntime`. I think we may be better off calling this a `reference implementation` ... of course, we may need a suitable abbreviation for that. Perhaps `ri` ? `refimpl` ? May be we can discuss a name over tomorrow's call.
lgtm-com[bot](2022-09-22 06:48:32):This pull request **introduces 170 alerts** when merging dd0af888b606870f31349e3faa20f154ea292284 into 895593af6ac17d4fe2749091659083d3b1246cae - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-4b2d89e4ddaeec987607897888ee1074d14e6adc)

**new alerts:**

* 166 for Unused import
* 3 for Unused local variable
* 1 for Signature mismatch in overriding method
lgtm-com[bot](2022-09-22 07:16:18):This pull request **introduces 172 alerts** when merging aa258bcf6eb5b853bec45cf23dee7f71ecca3c26 into 895593af6ac17d4fe2749091659083d3b1246cae - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-eb4ee2f6c4f7baea168e262a404e312b9cc43148)

**new alerts:**

* 167 for Unused import
* 4 for Unused local variable
* 1 for Signature mismatch in overriding method
lgtm-com[bot](2022-09-22 13:30:49):This pull request **introduces 173 alerts** when merging d45724d2e0f83b6dc3bd9bc88c5ddd052efb1f82 into 895593af6ac17d4fe2749091659083d3b1246cae - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-c5ed93643edc7fbcc9005989667a00f54d6eb1ef)

**new alerts:**

* 167 for Unused import
* 5 for Unused local variable
* 1 for Signature mismatch in overriding method
lgtm-com[bot](2022-09-22 14:27:44):This pull request **introduces 171 alerts** when merging 26b701e1a7e10bcde557100b47d2faf490342a65 into 895593af6ac17d4fe2749091659083d3b1246cae - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-294d0ecc9ddfb236ddbc03c867e9dbc335089979)

**new alerts:**

* 167 for Unused import
* 3 for Unused local variable
* 1 for Signature mismatch in overriding method
lgtm-com[bot](2022-09-23 10:27:28):This pull request **introduces 171 alerts** when merging 255bfa759dd6129d5e023725bd45530ca52f4de8 into 895593af6ac17d4fe2749091659083d3b1246cae - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-ca55f2a228d4aeda0c45dd3ff06f1263d6c04069)

**new alerts:**

* 167 for Unused import
* 3 for Unused local variable
* 1 for Signature mismatch in overriding method
lgtm-com[bot](2022-09-23 10:50:20):This pull request **introduces 175 alerts** when merging bcef4c2955d9669f70147517b9cad5d6ec8ed162 into 895593af6ac17d4fe2749091659083d3b1246cae - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-89febe301492b5c350af25706d7f35f69c9eb8bb)

**new alerts:**

* 168 for Unused import
* 6 for Unused local variable
* 1 for Signature mismatch in overriding method
lgtm-com[bot](2022-09-23 14:00:22):This pull request **introduces 172 alerts** when merging a8e23efd804f3aa29dd7408e376660bc21cf0bb7 into 895593af6ac17d4fe2749091659083d3b1246cae - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-cc947cd0b839ce9b7fd2bf16659490023abf2e83)

**new alerts:**

* 168 for Unused import
* 3 for Unused local variable
* 1 for Signature mismatch in overriding method
lgtm-com[bot](2022-09-24 10:51:30):This pull request **introduces 174 alerts** when merging 0486bddb358de5ecb0906af7ca572741304676a1 into 0a18e17e8079896cd88855de368180a806d8abb7 - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-328abc8e22ac9bbfc654a6ef93a34b5418e24b72)

**new alerts:**

* 169 for Unused import
* 4 for Unused local variable
* 1 for Signature mismatch in overriding method
lgtm-com[bot](2022-09-24 11:45:52):This pull request **introduces 180 alerts** when merging 91b22b98ec8f567b80778ee46dccbaf92964dec0 into 0a18e17e8079896cd88855de368180a806d8abb7 - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-4a69627bed0d12e9529a079b873639b61cf3ec32)

**new alerts:**

* 175 for Unused import
* 4 for Unused local variable
* 1 for Signature mismatch in overriding method
lgtm-com[bot](2022-09-26 13:14:56):This pull request **introduces 179 alerts** when merging 50b379407db95f96a1189704ea0808537fb69200 into 0a18e17e8079896cd88855de368180a806d8abb7 - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-d11cacea54ff1d58c5575a29b2ac7858458ff837)

**new alerts:**

* 173 for Unused import
* 5 for Unused local variable
* 1 for Signature mismatch in overriding method
lgtm-com[bot](2022-09-26 13:56:49):This pull request **introduces 179 alerts** when merging 773bef8a59ef349a734b6e06ed8bae6aea094d87 into 0a18e17e8079896cd88855de368180a806d8abb7 - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-651db0dbff999b0ffe4a27bef3e30aaad653cf8d)

**new alerts:**

* 174 for Unused import
* 4 for Unused local variable
* 1 for Signature mismatch in overriding method
lgtm-com[bot](2022-09-26 14:35:04):This pull request **introduces 176 alerts** when merging 0aeae9b9488c3dc05d202e47e9368bb98bd523a5 into 0a18e17e8079896cd88855de368180a806d8abb7 - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-bb02835d0d500f9dbe95f2afd5c07e6d9f1af26d)

**new alerts:**

* 171 for Unused import
* 4 for Unused local variable
* 1 for Signature mismatch in overriding method
lgtm-com[bot](2022-09-26 16:13:11):This pull request **introduces 178 alerts** when merging 8b587e3534db061b3355940d5754acbe0e2d6d3b into 0a18e17e8079896cd88855de368180a806d8abb7 - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-1377952355ecce60a26370c0dbb2c0c65628705d)

**new alerts:**

* 172 for Unused import
* 4 for Unused local variable
* 1 for Signature mismatch in overriding method
* 1 for Incomplete ordering
lgtm-com[bot](2022-09-27 15:22:35):This pull request **introduces 178 alerts** when merging 2bf2d333556068e6a2fa0994fa58bc00807c8afe into f0d607478424ab9a8d3751a6e15d48c817350f8b - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-e562ad98a87561445bf6a8d12e644f9b6c050b94)

**new alerts:**

* 172 for Unused import
* 4 for Unused local variable
* 1 for Signature mismatch in overriding method
* 1 for Incomplete ordering
lgtm-com[bot](2022-09-27 16:11:51):This pull request **introduces 179 alerts** when merging 437cb41b1d20fd1c8971251b5c96d89eb0322381 into f0d607478424ab9a8d3751a6e15d48c817350f8b - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-680fb386841ad63be502ca40b05c0a4a90913623)

**new alerts:**

* 173 for Unused import
* 4 for Unused local variable
* 1 for Signature mismatch in overriding method
* 1 for Incomplete ordering
lgtm-com[bot](2022-09-27 17:20:30):This pull request **introduces 179 alerts** when merging 21252dc5745fa1cbce3b459ea852c2dfaa8edc1f into f0d607478424ab9a8d3751a6e15d48c817350f8b - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-4f3bc26a6ee85ec0309d3a4fd9798a78a2339607)

**new alerts:**

* 173 for Unused import
* 4 for Unused local variable
* 1 for Signature mismatch in overriding method
* 1 for Incomplete ordering
lgtm-com[bot](2022-09-27 18:29:40):This pull request **introduces 179 alerts** when merging 75f5e2b0f59a875a38ec0cc8c074f7083fac727d into f0d607478424ab9a8d3751a6e15d48c817350f8b - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-3a7a64617fe4c47fe0203fd8dc69193fbe2a3e81)

**new alerts:**

* 173 for Unused import
* 4 for Unused local variable
* 1 for Signature mismatch in overriding method
* 1 for Incomplete ordering
lgtm-com[bot](2022-09-28 17:29:02):This pull request **introduces 179 alerts** when merging 32580c521bd06cfb2c3ebe279f857970e3eff73e into de2459972d3f834dba24a586982d1ffce5532e0b - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-bd23fbfc4f592d19877a2f41e93e55280cd76eb8)

**new alerts:**

* 173 for Unused import
* 4 for Unused local variable
* 1 for Signature mismatch in overriding method
* 1 for Incomplete ordering
lgtm-com[bot](2022-09-29 15:34:40):This pull request **introduces 180 alerts** when merging f29b7550f1d7576bae7f529e79430271d47b9f31 into de2459972d3f834dba24a586982d1ffce5532e0b - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-c512245678846987dae4453f0f962bfe1354ff66)

**new alerts:**

* 174 for Unused import
* 4 for Unused local variable
* 1 for Signature mismatch in overriding method
* 1 for Incomplete ordering
lgtm-com[bot](2022-09-29 18:06:25):This pull request **introduces 180 alerts** when merging 2417c6271fa3e699f0b33a533ba8983f0f0bed85 into de2459972d3f834dba24a586982d1ffce5532e0b - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-72b90f91fd9c3b7ae655c53157ec23753e1c4c30)

**new alerts:**

* 174 for Unused import
* 4 for Unused local variable
* 1 for Signature mismatch in overriding method
* 1 for Incomplete ordering
lgtm-com[bot](2022-09-30 13:17:39):This pull request **introduces 180 alerts** when merging dd3d618e3fa86b54033ce4f9a3e27ac637062ecd into de2459972d3f834dba24a586982d1ffce5532e0b - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-cc69c324a94739aaee429155150211ec67958bb8)

**new alerts:**

* 174 for Unused import
* 4 for Unused local variable
* 1 for Signature mismatch in overriding method
* 1 for Incomplete ordering
lgtm-com[bot](2022-09-30 16:29:06):This pull request **introduces 180 alerts** when merging 564d142ff7fa0aa540f9e40c333150449fff0159 into de2459972d3f834dba24a586982d1ffce5532e0b - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-9df0e271318feae1ebd88448ff952229c8fbdb70)

**new alerts:**

* 175 for Unused import
* 4 for Unused local variable
* 1 for Incomplete ordering
lgtm-com[bot](2022-09-30 16:44:16):This pull request **introduces 180 alerts** when merging c3e5b19bc16e385809e919950e4164dfcd46e29f into de2459972d3f834dba24a586982d1ffce5532e0b - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-c85adf12bc5797936c979c01d233fef94b8a7660)

**new alerts:**

* 175 for Unused import
* 4 for Unused local variable
* 1 for Incomplete ordering
lgtm-com[bot](2022-09-30 17:37:14):This pull request **introduces 180 alerts** when merging 2dc2119f5415174cb623c99295a3641db8f6daff into de2459972d3f834dba24a586982d1ffce5532e0b - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-4306c03c0b17072522b3e990962312b954186c37)

**new alerts:**

* 175 for Unused import
* 4 for Unused local variable
* 1 for Incomplete ordering
lgtm-com[bot](2022-09-30 18:22:15):This pull request **introduces 181 alerts** when merging f4564fe815c9d663cc26d951ddddd2516d2e9944 into de2459972d3f834dba24a586982d1ffce5532e0b - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-d6c4e3b964783b333b2e8e79a75f7d10f98b2019)

**new alerts:**

* 175 for Unused import
* 4 for Unused local variable
* 1 for Syntax error
* 1 for Incomplete ordering
lgtm-com[bot](2022-10-03 10:44:28):This pull request **introduces 182 alerts** when merging 099d3995553f86e171c69b8cbc5a345c743e8e97 into 025b3bea78bc0170889b02a7086399883917e2e2 - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-20563fd5432057e46ee93e995b45c4cea0e6e704)

**new alerts:**

* 176 for Unused import
* 4 for Unused local variable
* 1 for \`\_\_eq\_\_\` not overridden when adding attributes
* 1 for Incomplete ordering
lgtm-com[bot](2022-10-03 14:22:26):This pull request **introduces 182 alerts** when merging 36b642dc81194fd544b82baf5c2d9098cda9cc26 into 025b3bea78bc0170889b02a7086399883917e2e2 - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-3d7387b7b92845616625a217a288761dec4f4f5e)

**new alerts:**

* 176 for Unused import
* 4 for Unused local variable
* 1 for \`\_\_eq\_\_\` not overridden when adding attributes
* 1 for Incomplete ordering
lgtm-com[bot](2022-10-03 16:44:21):This pull request **introduces 182 alerts** when merging a15824cd536cdc8440f2ab6a6fdbebd6127a0e3c into 4ccc72b6e5c9a6c58d357c1e9442039c3e674e1d - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-1900175b6c859565debe0426a217a400774800e3)

**new alerts:**

* 176 for Unused import
* 4 for Unused local variable
* 1 for \`\_\_eq\_\_\` not overridden when adding attributes
* 1 for Incomplete ordering
gramalingam(2022-10-03 18:32:23):> 1. Add onnxruntime and torch into https://github.com/onnx/onnx/blob/main/requirements-dev.txt and test those related tests in at least one CI.

I don't think that's a good idea. We don't want to create cyclic dependencies. It's preferable if onnx doesn't depend on those repos. I had a separate question elsewhere in this PR why we have test-cases that invoke onnxruntime. This PR should have nothing to do with onnxruntime, IMHO.


lgtm-com[bot](2022-10-04 09:08:48):This pull request **introduces 182 alerts** when merging ab3a737584c2577e7e707861253ad791002f7425 into bc0377ebe27d8d82727a4649102dbe011eb0b782 - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-cf59d48d068bff1db62df61b158deae8e2bc0820)

**new alerts:**

* 176 for Unused import
* 4 for Unused local variable
* 1 for \`\_\_eq\_\_\` not overridden when adding attributes
* 1 for Incomplete ordering
lgtm-com[bot](2022-10-04 11:30:22):This pull request **introduces 183 alerts** when merging 3d376bbfdf60eb2458fc2aaf942a468207946349 into bc0377ebe27d8d82727a4649102dbe011eb0b782 - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-fabaccf9aaaef4321b7c9c9b68ec113fa5dfaf6b)

**new alerts:**

* 177 for Unused import
* 4 for Unused local variable
* 1 for \`\_\_eq\_\_\` not overridden when adding attributes
* 1 for Incomplete ordering
lgtm-com[bot](2022-10-04 12:37:12):This pull request **introduces 181 alerts** when merging 03f128209b562ab917614314867d2b491e8104c4 into bc0377ebe27d8d82727a4649102dbe011eb0b782 - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-550fd1d253b5b2812f93908c1cca8dc4f9496b5c)

**new alerts:**

* 175 for Unused import
* 4 for Unused local variable
* 1 for \`\_\_eq\_\_\` not overridden when adding attributes
* 1 for Incomplete ordering
lgtm-com[bot](2022-10-04 13:58:56):This pull request **introduces 182 alerts** when merging 5a1a885d7c2cf8a6c2ae17c78a601876b25581a1 into bc0377ebe27d8d82727a4649102dbe011eb0b782 - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-f0330303fe6f2c68057a7641d27a8dfa487d7213)

**new alerts:**

* 176 for Unused import
* 4 for Unused local variable
* 1 for \`\_\_eq\_\_\` not overridden when adding attributes
* 1 for Incomplete ordering
xadupre(2022-10-04 16:06:44):> Thank you @xadupre for the huge effort! Some preliminary thoughts (since this PR is still WIP, please ignore my point if you are already aware of them):
> 
> 1. Add onnxruntime and torch into https://github.com/onnx/onnx/blob/main/requirements-dev.txt and test those related tests in at least one CI.
> 2. Use new helper functions instead of old variables from mapping.py
> 3. Have a document under docs/ to explain what these files are for and how to use them.
> 
> All above points can be done by another PR if they make sense.

1. I would prefer to avoid adding more dependencies. I left the unit test using onnxruntime or torch to debug future implementation. It does not reduce the code coverage if thet are removed. 

2. I'll do it. I started this PR before this change was merged.

3. The documentation is written in class ProtoRun with a couple of examples. It can be extended.
lgtm-com[bot](2022-10-04 16:16:05):This pull request **introduces 183 alerts** when merging a7cb78f3e62707b8e59d215f0dab70b00f6daba2 into 136a2c0e0757e3a71cfce7e8aa06e08a61c8ee5b - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-ce2fa9214dce851e8e71b1219e761019874dc0ee)

**new alerts:**

* 177 for Unused import
* 4 for Unused local variable
* 1 for \`\_\_eq\_\_\` not overridden when adding attributes
* 1 for Incomplete ordering
lgtm-com[bot](2022-10-04 16:43:56):This pull request **introduces 183 alerts** when merging d933342f12c872a8e376d84ac6b15ed053483b06 into 136a2c0e0757e3a71cfce7e8aa06e08a61c8ee5b - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-9e1d8012efb23aecaeb354b10a4d500a9b1a8da1)

**new alerts:**

* 177 for Unused import
* 4 for Unused local variable
* 1 for \`\_\_eq\_\_\` not overridden when adding attributes
* 1 for Incomplete ordering
lgtm-com[bot](2022-10-04 17:53:07):This pull request **introduces 183 alerts** when merging ee0181dfe60da118f2786076f17d7e7f5641a605 into ad31171d44c480424551e404ba94f1435d3b3a54 - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-a363f5fc59bace9c5a83238521b1adc464b5c736)

**new alerts:**

* 177 for Unused import
* 4 for Unused local variable
* 1 for \`\_\_eq\_\_\` not overridden when adding attributes
* 1 for Incomplete ordering
lgtm-com[bot](2022-10-05 16:24:32):This pull request **introduces 186 alerts** when merging ae4392edba3eab3f96fba1f6f9868cb3042a9255 into 98f6110963ee5e7226226dcc2f2e664c20de5993 - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-713e5e74c91f5d14b2675e99c4ca8238f90a3e64)

**new alerts:**

* 180 for Unused import
* 4 for Unused local variable
* 1 for \`\_\_eq\_\_\` not overridden when adding attributes
* 1 for Incomplete ordering
lgtm-com[bot](2022-10-05 17:31:14):This pull request **introduces 186 alerts** when merging aee0616c03e2ae7f64bae834f6ec15b17b5ca648 into 98f6110963ee5e7226226dcc2f2e664c20de5993 - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-308f00e17c48c96ee9f23b0d1257ee6876fb2be3)

**new alerts:**

* 180 for Unused import
* 4 for Unused local variable
* 1 for \`\_\_eq\_\_\` not overridden when adding attributes
* 1 for Incomplete ordering
lgtm-com[bot](2022-10-06 12:37:19):This pull request **introduces 187 alerts** when merging eb8f2a4f3a9eba8063574da452e70eacdecdf4ad into 0ba78ff8708c08a6a0aa1c2f130e0b24a4333694 - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-b6ba216987a0336eab56f74ec5dc6db0b3691de7)

**new alerts:**

* 181 for Unused import
* 4 for Unused local variable
* 1 for \`\_\_eq\_\_\` not overridden when adding attributes
* 1 for Incomplete ordering
lgtm-com[bot](2022-10-06 13:36:42):This pull request **introduces 187 alerts** when merging b5d40c73568fa7db7f64f9bfda42c8c3e7857bdc into 0ba78ff8708c08a6a0aa1c2f130e0b24a4333694 - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-3c5ff52882463979bc0072a36b9474ffc837aefb)

**new alerts:**

* 181 for Unused import
* 4 for Unused local variable
* 1 for \`\_\_eq\_\_\` not overridden when adding attributes
* 1 for Incomplete ordering
lgtm-com[bot](2022-10-06 14:28:38):This pull request **introduces 187 alerts** when merging 91578f23bbf538067b5236303a135f00e6eadddf into 0ba78ff8708c08a6a0aa1c2f130e0b24a4333694 - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-8724f737cddab11578a197ed865ab873487380d1)

**new alerts:**

* 181 for Unused import
* 4 for Unused local variable
* 1 for \`\_\_eq\_\_\` not overridden when adding attributes
* 1 for Incomplete ordering
lgtm-com[bot](2022-10-07 09:59:23):This pull request **introduces 189 alerts** when merging e39b0eda0d6274f8df0c2aa67679f65425c33c7a into 0ba78ff8708c08a6a0aa1c2f130e0b24a4333694 - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-8c3e99d44a2d6e0bac674a7d87116491cbd1d9a7)

**new alerts:**

* 183 for Unused import
* 4 for Unused local variable
* 1 for \`\_\_eq\_\_\` not overridden when adding attributes
* 1 for Incomplete ordering
lgtm-com[bot](2022-10-07 12:51:57):This pull request **introduces 192 alerts** when merging ea921e9b4012aae56b8741edf3e66043b93269ec into 0ba78ff8708c08a6a0aa1c2f130e0b24a4333694 - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-b4658cf7605a60524368a56490d806b2ce1bcd9f)

**new alerts:**

* 186 for Unused import
* 4 for Unused local variable
* 1 for \`\_\_eq\_\_\` not overridden when adding attributes
* 1 for Incomplete ordering
lgtm-com[bot](2022-10-07 13:07:10):This pull request **introduces 192 alerts** when merging 34a8cad2cfc04f144dd6c57738ed2ecfd0aa34ec into 0ba78ff8708c08a6a0aa1c2f130e0b24a4333694 - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-1589667d576cd5737f61b45ca6469063fb14d235)

**new alerts:**

* 186 for Unused import
* 4 for Unused local variable
* 1 for \`\_\_eq\_\_\` not overridden when adding attributes
* 1 for Incomplete ordering
lgtm-com[bot](2022-10-07 13:35:53):This pull request **introduces 191 alerts** when merging e55c0a1239c934e96910c84e545f62ef0a169ec8 into 0ba78ff8708c08a6a0aa1c2f130e0b24a4333694 - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-98d66acbfc504de775713096d9d7af2dd689224e)

**new alerts:**

* 185 for Unused import
* 4 for Unused local variable
* 1 for \`\_\_eq\_\_\` not overridden when adding attributes
* 1 for Incomplete ordering
lgtm-com[bot](2022-10-07 14:52:56):This pull request **introduces 188 alerts** when merging 821588c298ea8978f0cdd12b27af3b34360700a3 into 0ba78ff8708c08a6a0aa1c2f130e0b24a4333694 - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-c2bb2c5d44422db9a2d26888e50a57d9bce647e2)

**new alerts:**

* 182 for Unused import
* 4 for Unused local variable
* 1 for \`\_\_eq\_\_\` not overridden when adding attributes
* 1 for Incomplete ordering
lgtm-com[bot](2022-10-07 15:43:41):This pull request **introduces 188 alerts** when merging 6577cb26fd6e8f0964e47110dfb430f8378789f3 into f87cd99cb434bd9a72a7fc6fe5dba8447166bf8c - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-6c235516fb16a0271587de053073f4765cb69209)

**new alerts:**

* 182 for Unused import
* 4 for Unused local variable
* 1 for \`\_\_eq\_\_\` not overridden when adding attributes
* 1 for Incomplete ordering
lgtm-com[bot](2022-10-10 13:22:08):This pull request **introduces 188 alerts** when merging 9712d80a655890058e91c37fd67d7803f1590066 into 9d55e19409f7022b4d4ac3ea600d1d1371f8ac4b - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-b8b7eaf5c3bb60fe89debaccb2808afe563c4b59)

**new alerts:**

* 182 for Unused import
* 4 for Unused local variable
* 1 for \`\_\_eq\_\_\` not overridden when adding attributes
* 1 for Incomplete ordering
lgtm-com[bot](2022-10-10 13:58:11):This pull request **introduces 188 alerts** when merging 89a98a18f2cb7167ad0010251339bd2143f80c3f into 9d55e19409f7022b4d4ac3ea600d1d1371f8ac4b - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-623307b2f7c209d3baafa0fe767975ab3e46c557)

**new alerts:**

* 182 for Unused import
* 4 for Unused local variable
* 1 for \`\_\_eq\_\_\` not overridden when adding attributes
* 1 for Incomplete ordering
lgtm-com[bot](2022-10-12 17:14:25):This pull request **introduces 188 alerts** when merging 8d056906cd150f4129750190e16c3a90a68bf366 into 9d55e19409f7022b4d4ac3ea600d1d1371f8ac4b - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-4aaec9594fb8753aa33516a7a708fd630826aae1)

**new alerts:**

* 182 for Unused import
* 4 for Unused local variable
* 1 for \`\_\_eq\_\_\` not overridden when adding attributes
* 1 for Incomplete ordering
gramalingam(2022-10-18 22:13:55):It would be a good idea to update the AddNewOp.md document, especially [Step 2.2](https://github.com/onnx/onnx/blob/main/docs/AddNewOp.md#step-2-submit-pr-) so that future new ops update the op implementation here.

For this, it would also be useful to point users to a model sample op implementation as a template. For the template, I think it would be preferable to follow a pattern like below:
```py
def new_op (x, attr1, attr2):
   ... actual computation ...

NewOp = make_op_class(new_op)
```

I think this should be doable. I understand higher-order ops like Loop are more involved/complicated, but they are rare. For ops without Graph attributes, a simple functional model as above would be easier for adding new ops.
gramalingam(2022-10-18 22:26:50):I think that adding a `readme.md` file in the root directory `funconnx` would be helpful. In particular, to start with, we should add a disclaimer in the readme file that this is a new addition to the existing ONNX spec, and that there is potential for mismatch between the official spec and the implementation here, and that in the case of such a mismatch, the official spec overrides this implementation.

I think this is a practical approach to adding such a large PR, since I don't think we can get a reasonable review of all op implementations in a short-time, so we can continue reviewing and correcting the op implementations gradually.
abock(2022-10-20 16:58:02):Regarding naming:

|Current| → |Proposed|
|-|-|-|
| onnx.funconnx | → | onnx.reference |
| onnx.funconnx.proto_run.ProtoRun | → | onnx.reference.runtime.Runtime<sup>1</sup> |
| onnx.funconnx.aionnx | → | onnx.reference.ops |
| onnx.funconnx.experimental | → |onnx.reference.ops.experimental |
| onnx.funconnx.aionnx_preview_training | → | onnx.reference.ops.experimental.training |

With potential new op domains organize under submodules, like `experimental[.training]` above:
  * `onnx.reference.ops.<somedomain>`

Reframing the example in the PR description, we end up with:

```python
from onnx.reference import Runtime
sess = Runtime(onnx_model)
results = sess.run(None, {"X": a, "A": a, "B": b})
print(results[0])  # display the first result
```

<sup>1.</sup> Should the `Runtime` type itself be called `ReferenceRuntime` to be even more explicit?

```python
from onnx.reference import ReferenceRuntime
sess = ReferenceRuntime(onnx_model)
```

gramalingam(2022-10-20 20:39:50):> Should the `Runtime` type itself be called `ReferenceRuntime` to be even more explicit?

Sounds good to me.
jcwchen(2022-11-11 14:42:44):It seems that some tests in reference_evaluator_backend_test.py and reference_evaluator_test.py failed (see [CI](https://github.com/onnx/onnx/actions/runs/3439012930/jobs/5735799774)). One failed Windows CI uses x86 machine and another CI uses old NumPy version 1.16.6. Since other CIs are fine, is it because NumPy behaves slightly differently in Windows x86 machine and old NumPy version 1.16.6?
gramalingam(2022-09-16 23:30:34):Don't understand this. Unary operators have only one input, right?
xadupre(2022-09-17 09:05:35):The comment was wrong. I updated it. It checks that input and output types are the same.
gramalingam(2022-09-20 23:18:38):nit: may be useful to give it a name like `__n_outputs` to avoid conflicting with any ONNX op attribute-name. Same for `verbose`.
gramalingam(2022-09-21 23:34:35):I am a bit unclear why we have tests of ort here?
gramalingam(2022-09-21 23:48:35):A few lines documenting the scope of this file would help. My understanding is that this is for the backend\test\case\node tests? Does it also include backend\test\case\model tests?
justinchuby(2022-09-22 04:14:03):`import numpy as np` to follow its convention?
justinchuby(2022-09-22 04:17:12):```suggestion
    sequence: Union[np.ndarray, Sequence], tensor: np.ndarray, position: np.ndarray = None
```

may be this since it can be np.ndarray?
justinchuby(2022-09-22 04:24:40):Is it possible to pass in a dict for attributes instead?
justinchuby(2022-09-22 04:29:04):Looks like `assert_allclose` is recommended: https://numpy.org/doc/stable/reference/generated/numpy.testing.assert_almost_equal.html#numpy.testing.assert_almost_equal.

I remember these functions do broadcasting. Do we want to wrap them and check shape and dtype as well?
justinchuby(2022-09-22 04:30:02):Use longer names for parameters?
justinchuby(2022-09-22 04:32:17):Doesn't have to be this PR - Looks like much of these tests can be parameterized: https://pypi.org/project/parameterized/
justinchuby(2022-09-22 04:35:23):We can skip this test if ort is not installed. Example https://github.com/pytorch/pytorch/blob/c7c2578f93fbfad5f769543848642a16b6071756/torch/testing/_internal/common_utils.py#L1204-L1211
xadupre(2022-09-22 06:29:26):They can be removed. I'm just checking that the runtime I write and onnxruntime have the same results. Backend tests are not enough to understand what an implementation does wrong.
xadupre(2022-09-22 06:30:31):Yes, it runs all the backend test with the python runtime and checks that it produces the expected outputs.
xadupre(2022-09-22 06:31:45):I used many lines of codes I wrote a while ago. I usually use numpy for my teachings because it is less confusing for students. Done.
xadupre(2022-09-22 06:42:36):Done
xadupre(2022-09-22 06:46:07):If possible, I'd like to keep verbose as it is. I doubt an operator would use it as this parameter means the same thing accross all packages. Same goes for n_outputs. The meaning of the parameter is quite clear. If an operator uses it in its definition, it should have the same meaning. I don't mind using an attribute instead of `**kwargs`. Any particular reason? `make_node` is also using `**kwargs`.
xadupre(2022-09-22 06:48:06):done
xadupre(2022-09-22 06:55:06):parametized is not a dependency. I suggest another PR.
xadupre(2022-09-22 07:04:46):done
justinchuby(2022-09-22 13:38:16):> I don't mind using an attribute instead of `**kwargs`. Any particular reason? `make_node` is also using `**kwargs`.

Just to avoid naming conflicts if there is a need. 


liqunfu(2022-09-26 21:14:55):not to local from file system, utilize onnx.backend.test.case.node.collect_testcases()
liqunfu(2022-09-26 21:17:46):verbose use an enum type
github-code-scanning[bot](2022-09-27 16:11:05):## Signature mismatch in overriding method

Overriding method '_run' has signature mismatch with [overridden method](1).

[Show more details](https://github.com/onnx/onnx/security/code-scanning/143)
github-code-scanning[bot](2022-09-27 16:11:05):## Overwriting attribute in super-class or sub-class

Assignment overwrites attribute numpy_fct, which was previously defined in superclass [OpRunBinaryNumpy](1).
Assignment overwrites attribute numpy_fct, which was previously defined in superclass [OpRunBinaryNumpy](2).

[Show more details](https://github.com/onnx/onnx/security/code-scanning/495)
github-code-scanning[bot](2022-09-27 16:11:06):## Incomplete ordering

Class BoxInfo implements [__lt__](1), but does not implement __le__ or __gt__ or __ge__.

[Show more details](https://github.com/onnx/onnx/security/code-scanning/545)
github-code-scanning[bot](2022-09-27 16:11:06):## Mismatch between signature and use of an overridden method

Overridden method signature does not match [call](1), where it is passed an argument named 'axes'. Overriding method [method ReduceSum_13._run](2) matches the call.
Overridden method signature does not match [call](3), where it is passed an argument named 'axes'. Overriding method [method ReduceSum_13._run](4) matches the call.

[Show more details](https://github.com/onnx/onnx/security/code-scanning/139)
github-code-scanning[bot](2022-09-27 16:11:06):## Returning tuples with varying lengths

softmaxcrossentropy returns [tuple of size 1](1) and [tuple of size 2](2).

[Show more details](https://github.com/onnx/onnx/security/code-scanning/141)
github-code-scanning[bot](2022-09-27 16:11:06):## Module is imported with 'import' and 'import from'

Module 'numpy' is imported with both 'import' and 'import from'

[Show more details](https://github.com/onnx/onnx/security/code-scanning/145)
github-code-scanning[bot](2022-09-27 16:11:07):## Unused import

Import of 'Im2Col' is not used.

[Show more details](https://github.com/onnx/onnx/security/code-scanning/146)
github-code-scanning[bot](2022-09-27 16:11:07):## Unused import

Import of 'Adagrad' is not used.

[Show more details](https://github.com/onnx/onnx/security/code-scanning/148)
github-code-scanning[bot](2022-09-27 16:11:07):## Unused import

Import of 'Adam' is not used.

[Show more details](https://github.com/onnx/onnx/security/code-scanning/149)
github-code-scanning[bot](2022-09-27 16:11:07):## Unused import

Import of 'Momentum' is not used.

[Show more details](https://github.com/onnx/onnx/security/code-scanning/150)
github-code-scanning[bot](2022-09-30 18:33:38):## Syntax error

Syntax Error (in Python 3).

[Show more details](https://github.com/onnx/onnx/security/code-scanning/601)
github-code-scanning[bot](2022-10-03 10:53:44):## `__eq__` not overridden when adding attributes

The class 'IntMap' does not override ['__eq__'](1), but adds the new attribute [added_keys](2).

[Show more details](https://github.com/onnx/onnx/security/code-scanning/613)
jcwchen(2022-10-03 17:10:47):```suggestion
            raise unittest.SkipTest("torch not installed")
```
jcwchen(2022-10-03 17:29:02):nit: perhaps use:
```
data = np.zeros([6, 28])
data[0][0] = 1.
```
or wrap these long lines somehow.
xadupre(2022-10-04 15:59:56):done
xadupre(2022-10-04 16:02:15):done
gramalingam(2022-10-05 19:34:58):Seems to ignore schema.domain? Should either use (domain, opname) or filter out only ops belonging to standard domain. Not sure which is intended.
gramalingam(2022-10-05 19:36:01):Not sure I understand what the above comment means.
gramalingam(2022-10-05 19:37:22):I see some duplication of code across the above folder and the op implementations in this PR, which I guess is related. But not sure what the plan/suggestion above is.
gramalingam(2022-10-05 19:47:57):Minor nit: may be `implicit_inputs` would be a better name? I find `local` confusing since these are *not* local (but belong to outer scope).
gramalingam(2022-10-05 19:48:46):Currently, this does not handle nested graph-attributes, which could also contain references to outer-outer-scope.
gramalingam(2022-10-05 19:52:45):I think `name` should be replaced by `value.name` here, if my understanding is correct.
gramalingam(2022-10-05 19:57:28):Seems like a duplicate of the check in line 327 above
gramalingam(2022-10-05 20:11:51):Can you clarify the role of `kwargs` here? It looks like they can represent attribute-values based on the lines below (355 etc.), and will over-ride those specified in the object-constructor (Node) when constructing kwargs for the call to _run. But I remember seeing that _run implementations of specific ops directly access attribute values using `self.attrname`. So, it is unclear what is intended here.
gramalingam(2022-10-05 20:16:46):Bit unclear what happens if a name appears in both overridden_attributes and kwargs. May be easier to understand it if it is made explicit in line 355-356 by choosing the correct one there.
gramalingam(2022-10-05 20:19:57):May be this should read `must not return nested-tuple`? Otherwise, looks odd in comparison to line 376.
gramalingam(2022-10-05 20:22:27):Seems to be missing from parameter list above. Is it supposed to be part of kwargs?
gramalingam(2022-10-05 20:23:27):I guess a copy-paste error?
gramalingam(2022-10-05 20:59:53):nit: The above few lines (113-122) seem irrelevant here and can be omitted. In particular, it is an implementation detail of `eval`, and I think we can change `eval` to work without constructing a node, so not worth describing those details here.
gramalingam(2022-10-05 21:04:58):For a reference-implementation, it is better not to add "relaxations" such as the above ... it is an error if opset_imports does not include all used opsets.
gramalingam(2022-10-05 21:13:41):Specifically, it is important to fail with an error in any situation that the standard/spec says is an error.
gramalingam(2022-10-05 21:14:17):Suggest omitting lines 178-179
gramalingam(2022-10-05 21:19:53):What's the type supposed to be for functions? I am a bit unclear about the call to this in line 185.
gramalingam(2022-10-05 21:22:43):Don't understand this line. Do the `values` also encode the domain/function-name already? Unclear to me why passing the value, without the name is sufficient for executing f.
gramalingam(2022-10-05 21:23:55):Ok, never mind. I guess the `proto_` field includes this info.
gramalingam(2022-10-05 21:37:41):nit: "namme" to "name"
gramalingam(2022-10-06 05:42:49):Do all op implementations do this kind of type-checking? Or, is it based on case-by-case basis?
gramalingam(2022-10-06 05:51:20):Trying to understand this: is the `or self.axis` part necessary? At least, the calls to _run I saw in some part of the code I saw (in op_run.py or proto_run.py) already passed in self.axis.
xadupre(2022-10-06 11:05:30):This no collision of operator name accross domains but I should fix it.
xadupre(2022-10-06 11:09:20):I updated the comment. I forgot to change it.
xadupre(2022-10-06 11:10:16):renamed
xadupre(2022-10-06 11:13:28):this function is not used anymore, I'll remove it
xadupre(2022-10-06 11:34:28):Yes. I changed the error message.
xadupre(2022-10-06 11:36:30):The operator name and domain is already defined by class cls (the runtime). It can only execute a specific node. So it is not needed.
xadupre(2022-10-06 11:57:33):moved into the docstring of make_node.
xadupre(2022-10-06 11:58:31):done
xadupre(2022-10-06 11:59:47):added one annotation
xadupre(2022-10-06 12:01:31):yes
xadupre(2022-10-06 12:19:17):I clarified. kwargs only contain argument `context`, a dictionary with the local result when running a node with a subgraph.
xadupre(2022-10-06 12:19:35):I clarified.
xadupre(2022-10-07 12:54:08):case by case, I assume the model was verified with onnx.checker.
xadupre(2022-10-07 12:54:30):Remove, it is not necessary. This code was moved in OpRun.
gramalingam(2022-10-10 01:27:44):I don't understand. I meant that the description about the parameter `verbose` seems like it is here by mistake?
xadupre(2022-10-10 09:22:31):fixed
gramalingam(2022-10-12 20:06:04):Where is `_run_body` defined? I couldn't find it in my search in my visual-code. Thanks!
xadupre(2022-10-13 08:46:05):In OpRun._load_attributes (https://github.com/onnx/onnx/pull/4483/files#diff-ea8c1377a6b06240f3181211d2d4bfecdf64493cd6cf602f1245016d7a9496c3R215). It is done automatically for every graph attribute.
gramalingam(2022-10-18 21:51:44):Wonder if this should be removed now? How are optional values (like optional tensors) represented in this implementation? ONNX Loops were extended to support optional-types also at some point.
gramalingam(2022-10-18 22:00:43):I guess these are scan outputs? But they should be tensors. It looks like the values are sequences in this implementation? If so, they need to be concatenated together into a tensor. Not sure if I misunderstood.
jcwchen(2022-10-18 23:28:07):```suggestion
import numpy as np
```
Also other same imports in this PR
jcwchen(2022-10-18 23:54:21):Is it possible that `obj.input_types[i].tensor_type.elem_type` equals to `TensorProto.FLOAT16`?
jcwchen(2022-10-19 00:14:20):https://github.com/onnx/onnx/blob/8669fad0247799f4d8683550eec749974b4f5338/onnx/numpy_helper.py#L16
Are they different?
xadupre(2022-10-20 17:46:04):done
xadupre(2022-10-20 18:04:32):I removed this test. It is never triggered.
xadupre(2022-10-20 18:06:59):You are right. There is no backend test covering this case.
justinchuby(2022-10-20 18:08:57):👀 a print
justinchuby(2022-10-20 22:15:09):Possible to use absolute module paths?
xadupre(2022-10-21 09:53:57):thanks
xadupre(2022-10-21 09:54:23):why?
justinchuby(2022-10-21 15:28:25):[PEP8](https://peps.python.org/pep-0008/#imports) recommends: 

> Absolute imports are recommended, as they are usually more readable and tend to be better behaved (or at least give better error messages) if the import system is incorrectly configured (such as when a directory inside a package ends up on sys.path):

```
import mypkg.sibling
from mypkg import sibling
from mypkg.sibling import example
```

imo it improves readability (tells reader where the module comes from without having to go back to the dir structure) and makes the code easier to maintain when we move files around. Things start to get complicated when we have lines like `...foo.bar` or `....foo` and we may quickly lose count of the dots.
xadupre(2022-10-21 16:01:49):I did not know. I usually use relative imports because when I work on a package and the package is also installed in site-package, absolute imports are ambiguous. I was searching for speeds but did not find anything about importing time. I assumed that relative import are faster since there is only one possible location instead of having mulitple (sys.path). But maybe I'm wrong. onnx.reference.ops._op_list imports more than 100 files, maybe that's worth measuring before changing everything.
justinchuby(2022-10-21 16:24:05):Without having done the measurement myself, imported modules are cached so I think any speed difference would only matter the first time (and the difference should be small anyways)?

For site-package ambiguity, does developing in a venv help?
xadupre(2022-10-21 18:01:13):No worries, it is just a question of habits. I've been using relative imports since I discovered them. onnx is using absolute import in all other places, so I should use absolute too.
xadupre(2022-10-24 12:50:32):done
justinchuby(2022-10-27 16:03:30):Use Sequence when possible to allow more valid inputs?
justinchuby(2022-10-27 16:05:32):Looks like pprint is unused (file top, couldn't comment there so)
xadupre(2022-10-27 16:35:58):I removed the one I added and modified the existing one by adding a default value to dims.
xadupre(2022-10-27 16:47:47):done
xadupre(2022-10-27 16:47:51):done
gramalingam(2022-11-02 22:29:58):This is a bit confusing. Is `data` not an `ndarray` in this case (as the type annotation above indicates)? Is it meant to be a scalar? (I notice that the scalar-transformation function has disappeared from the helper.py.) Clarify in the type-annotation or comments as appropriate.
xadupre(2022-11-03 07:35:08):I removed the version I added in helper.py to handle scalar. It seemed confusing to have two versions. I extended this one. I fixed the annotation.
gramalingam(2022-11-09 23:36:44):nit: "specifications" => "specification"
xadupre(2022-11-10 16:28:14):done
xadupre(2022-09-02 15:33:27):The overlap seems small. It should be ok either way.
xadupre(2022-09-02 18:27:54):@jcwchen the PR is ready to reviewed. black should prevent any merging issue.
justinchuby(2022-09-08 15:52:21):action-mypy uses the local mypy if found. We will simply need to install mypy at the version of our choice before running the check.
gramalingam(2022-09-23 15:55:30):Thanks for fixing these!
justinchuby(2022-09-08 15:51:28):We should avoid ignores without error nums (too board)
jcwchen(2022-09-12 15:54:59):May I understand which kind of error does it ignore? mypy 0.971 produces: `error: Incompatible types in assignment (expression has type "str", variable has type "ModelProto")  [assignment]`
jcwchen(2022-09-12 15:56:22):```suggestion
import pytest
```
jcwchen(2022-09-12 15:58:47):May I understand which kind of error does it ignore? mypy 0.971 produces: ` Returning Any from function declared to return "bytes"  [no-any-return]`
jcwchen(2022-09-12 16:00:33):May I understand which kind of error does it ignore? mypy 0.971 produces: `error: Value of type variable "SupportsRichComparisonT" of "sorted" cannot be "Type[TestCase]"  [type-var]`
jcwchen(2022-09-12 16:00:39):May I understand which kind of error does it ignore? mypy 0.971 produces: `error: Value of type variable "SupportsRichComparisonT" of "sorted" cannot be "Dict[str, TestItem]"  [type-var]`
jcwchen(2022-09-12 16:01:01):May I understand which kind of error does it ignore? mypy 0.971 produces: `error: Returning Any from function declared to return "Union[SupportsDunderLT, SupportsDunderGT]"  [no-any-return]`
jcwchen(2022-09-12 16:01:27):May I understand which kind of error does it ignore? mypy 0.971 produces: `error: Returning Any from function declared to return "Optional[TensorShapeProto]"  [no-any-return]`
xadupre(2022-09-13 13:04:51):mypy returns `Returning Any from function declared to return "bytes"` in this case. I assume SerializeToString() is annotated as Any when this function returns bytes. mypy detects the inconsistency. I don't think we can easliy remove it. I don't know if onnx is the source of the wrong annotation or if it is protobuf.
xadupre(2022-09-13 13:06:06):Because it was not detected before, I assume a couple of annotations are inconsistent. It is a unit test so I did not look into it.
xadupre(2022-09-13 13:06:38):thanks, I missed it.
xadupre(2022-09-13 13:08:00):mypy returns `Returning Any from function declared to return "bytes"` in this case. So function returns a less generic type than the underlying function. I assume SerializeToString() is annotated as `Any` and this function returns `bytes`. mypy detects the inconsistency. I don't think we can easliy remove it. I don't know if onnx is the source of the wrong annotation or if it is protobuf.
xadupre(2022-09-13 13:10:10):I assume TestCase does not really support sort. I'll check.
xadupre(2022-09-13 13:13:49):I don't think mypy is seeing `v.type.sparse_tensor_type.shape` as a `TensorShapeProto`. I did not check why since it is coming from protobuf. I suggest fixing it in another PR. Right now, it explicitely ignores what was implicitely ignored.
justinchuby(2022-09-13 14:37:14):I don’t think we needed this change because it should read the mypy config
justinchuby(2022-09-13 14:38:17):I would still suggest adding the error code in square brackets to narrow the ignore for clarity. 
xadupre(2022-09-13 15:07:59):I was not sure. I removed this flag since I also added it in setup.cfg.
xadupre(2022-09-13 15:23:44):From what I understood, you can ignore this error by forcing the type:  `type: expected type`. But mypy checks the `expected type` exists and for that it must have been imported. But if I do that, it would raise the warning *unused import*. So I think `# type: ignore` is not bad here.
jcwchen(2022-09-13 18:01:24):```suggestion
        for model_name, error in failed_messages:
```
This should fix it. Thanks for catching.
xadupre(2022-09-14 17:25:45):Changed.
jcwchen(2022-09-14 18:17:57):```suggestion
        for model_name, error in failed_messages:
            print(f"{model_name} failed because: {error}")
```
This hasn't been resolved yet?
jcwchen(2022-09-14 18:23:03):Shall we bump the used mypy version in [setup.py](https://github.com/onnx/onnx/blob/5fbb7b507e8c908d210179181c9f22ffc01e80fc/setup.py#L333) since this PR enable mypy check with mypy 0.971?

And I think this PR can close this issue: https://github.com/onnx/onnx/issues/3964.
xadupre(2022-09-14 18:36:16):done
xadupre(2022-09-14 18:36:25):done
gramalingam(2022-09-28 00:08:51):LGTM, thanks!
philass(2022-10-03 22:38:15):@p-wysocki thanks for the review!

I checked other unary and binary ops. And it seems that most times 0d tensors aren't checked.

If you feel it would add more coverage, then I am happy to add tests.

And thanks for the compliment. I copied the code patterns in the codebase. I can't take too much credit :) 
gramalingam(2022-10-03 23:05:45):I think some minor conflict-resolution is required since the other PR (NOT op) touches the same lines in the opset file.
philass(2022-10-04 10:32:04):> I think some minor conflict-resolution is required since the other PR (NOT op) touches the same lines in the opset file.

@gramalingam Thanks for merging in `BitwiseNot`. I have fixed the merge conflicts. And these ops should be ready to merge.
gramalingam(2022-09-28 00:07:26):Nit:
(i) change `resulted` to `resulting`. 
(ii) exchange the order of `{name}` and `bitwise`.
philass(2022-09-28 17:33:59):Good catch! That is definitely better wording

I've made the changes to the PR
gramalingam(2022-09-28 00:13:53):LGTM, thanks! The only question is whether some of the ops should be defined as functions in terms of others.
philass(2022-09-28 17:54:57):Thanks for the review!

> LGTM, thanks! The only question is whether some of the ops should be defined as functions in terms of others.

I think bitwise ops are sufficiently primitive where it may be overkill to make some of these functions. From my understanding most chips and hardware accelerators have first class support for these bitwise operations.

However if you feel that this is best done with functions, I can try and figure out how to do that.

gramalingam(2022-09-28 20:09:05):> Thanks for the review!
> 
> > LGTM, thanks! The only question is whether some of the ops should be defined as functions in terms of others.
> 
> I think bitwise ops are sufficiently primitive where it may be overkill to make some of these functions. From my understanding most chips and hardware accelerators have first class support for these bitwise operations.
> 
> However if you feel that this is best done with functions, I can try and figure out how to do that.

Not necessary. I think this is fine. I agree that in practice, it won't make a big difference.
philass(2022-10-03 22:37:26):@p-wysocki thanks for the review!

I checked other unary and binary ops. And it seems that most times 0d tensors aren't checked.

If you feel it would add more coverage, then I am happy to add tests.
jcwchen(2022-09-08 17:29:50):I chose requirements-dev.txt instead of `pip install mypy==0.782` so we don't have to update a lot of hardcoded versions when upgrading in the future.
xadupre(2022-09-20 18:37:50):This PR is not necessary but i think it is better this way and more consistent. I'm curious to know which runtime cannig read the raw data. It should be able to cast the pointer raw_data into the proper type to read it. If a runtime cannot read int16 format, then the converting library should modify the type of an initializer. I don't think we should keep this kind of weird behavior just to accomodate a runtime.
gramalingam(2022-09-20 22:40:36):I suggest something in-between: we generalize the helper function `make_tensor` to generate the more efficient encoding, but the inefficient encoding should still be supported.

Specifically, I think we need to support backward-compatibility for pre-existing models, so we should not make proto changes that break backward-compatibility.

As long as there is an efficient encoding, and we help users create the efficient encoding, that should serve the main purpose. If someone wants to explicitly create the less efficient encoding, we should not prevent it.
gramalingam(2022-09-20 22:54:45):Note that protobuf uses variable-length encoding for integers. So, a boolean value encoded as int32 is encoded using a single byte in the proto format. In fact, I believe values 1 to 127 are encoded using 1 byte (one bit is used as the continuation-bit). Similarly, values encodable using 14 bits take 2 bytes. Otherwise, it would be 3 bytes. Or, at least, that's my understanding based on a quick look at [this description](https://developers.google.com/protocol-buffers/docs/encoding#varints)
gramalingam(2022-09-20 22:55:56):In contrast, the raw format uses a fixed-length encoding.
lgtm-com[bot](2022-09-21 12:47:45):This pull request **introduces 1 alert** when merging c78c905a00650f3292b4d043c28826cbd27adbaa into 895593af6ac17d4fe2749091659083d3b1246cae - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-f5a7dfbc6b999b11ca5566a4331b40e75963afa5)

**new alerts:**

* 1 for Unused local variable
xadupre(2022-09-21 20:17:10):I updated the code to keep the former make_tensor. I read the documentation in onnx.proto and onnx implements what it says it should do. Maybe this PR is not needed. However, it still looks counterintuitive to me. We should probably do a benchmark to compare the loading / writing time when users use `make_tensor(..., raw=True or False)`. That would help making the right decision.
jcwchen(2022-09-29 15:52:37):Not sure whether we can directly add --clean in the script: https://github.com/onnx/onnx/blob/de2459972d3f834dba24a586982d1ffce5532e0b/tools/update_doc.sh#L25
Or even set --clean=True by default.
gramalingam(2022-09-29 15:58:43):> Not sure whether we can directly add --clean in the script:
> 
> https://github.com/onnx/onnx/blob/de2459972d3f834dba24a586982d1ffce5532e0b/tools/update_doc.sh#L25
> 
> 
> Or even set --clean=True by default.

Would it be worthwhile for the doc generator to track files it generated and finally issue a warning message if there were old files (not generated in this run)?
jcwchen(2022-09-29 17:17:04):> Would it be worthwhile for the doc generator to track files it generated and finally issue a warning message if there were old files (not generated in this run)?

Makes sense to me. I have added such a check in cmd_tools and if there are additional files, it will throw a warning to let users understand. Please review it. Thanks!
jcwchen(2022-10-03 14:57:36):@gramalingam if you have time, could you please review this PR? (It needs an approval from SIG-operators-approvers as well). Thank you!
gramalingam(2022-10-03 15:16:04):So, all the deleted *.pb files were spurious (out-dated) files created by this issue? Good to catch them and remove them!
jcwchen(2022-10-03 15:18:02):> So, all the deleted *.pb files were spurious (out-dated) files created by this issue? Good to catch them and remove them!

Yes, they were caught by the enhanced CI check (with --clean) brought by this PR because they cannot be regenerated by any existing test script. Therefore I removed them.
jcwchen(2022-09-15 18:24:44):Added, deleted, renamed for `*output_*.pb` and `*input_*.pb` will be detected.
JackBoosY(2022-09-22 09:49:14):I will open a new PR to sign DCO. Sorry.
xadupre(2022-09-19 07:29:17):What if a node is Undefined from a domain != from "" or "ai.onnx"?
jcwchen(2022-09-20 02:45:01):Good question. Did you mean an operator named "Undefined" but from domain "" or "ai.onnx"? If yes, current behavior is do nothing without any error message.
justinchuby(2022-09-22 04:11:34):May want to ignore third_party
justinchuby(2022-09-22 04:08:29):```suggestion
        queries: security-extended,security-and-quality
```

Enables more checks
lgtm-com[bot](2022-09-23 03:31:27):This pull request **introduces 1 alert** when merging 5e56af2b672560c558ff5dac5fd294044ecc7808 into 895593af6ac17d4fe2749091659083d3b1246cae - [view on LGTM.com](https://lgtm.com/projects/g/onnx/onnx/rev/pr-29dfbf8fe593ec445aa7154f46a88d6d386a2878)

**new alerts:**

* 1 for Unused import
gramalingam(2022-09-20 21:44:47):Change to "Returns true iff at least one of the attribute-value is specified."
gramalingam(2022-09-20 21:46:13):The checks above are unnecessary and can be omitted. The default-checker will already check this from the opschema information.
gramalingam(2022-09-20 21:48:25):I guess you will add a check for all other attributes as well?
gramalingam(2022-09-20 21:49:35):I think we should omit the second check on ints_size?
gramalingam(2022-09-20 21:55:12):May be something like:
```cpp
   for (const char *name : { "int", "ints", ... })
      if ctx.getAttribute(name) != nullptr {
         has_attr = true;
         break;
      }
```
gramalingam(2022-09-20 22:02:48):I think something like
```cpp
   builder.Add("output0 = Constant < value = bool {1} > ()");
```
may work?
gramalingam(2022-09-20 22:07:29):May be replace `True` by `bool(kwargs)` ... then you can use the same test-case with no-attributes as well as one-or-more attributes.
liqunfu(2022-09-22 00:20:59):Yes I will find a way to iterate all types defined in proto. For now, I just want to covert ints so that I can validate it in converting reduce ops to function ops.
liqunfu(2022-09-22 00:21:52):I think I added the check due to test failure. Let me double check.
liqunfu(2022-09-22 00:22:22):will try
jcwchen(2022-09-22 16:23:23):Merge this condition (has_int) with the condition of has_ints?
gramalingam(2022-09-23 17:52:43):May be change to `test_attribute_has_{name}_false` ? Or, omit the `value_` part in the call-site. Otherwise, we will get `value_value_int`.
JackBoosY(2022-09-22 09:52:16):cc @jcwchen 
JackBoosY(2022-09-23 02:41:53):Are all the commits should have sign-off info?
jcwchen(2022-09-22 17:07:32):Provide the complete steps:
```suggestion
git clone https://github.com/microsoft/vcpkg.git
cd vcpkg
```
p-wysocki(2022-10-20 16:11:19):After an internal discussion it has been decided that `storage_order` should be kept in order to maintain backwards compatibility.
kraiskil(2022-09-22 14:23:27):Reading `the output spatial size match the input` (in my broken English) sounds a bit like "input size is equal to output size". Is the original: `output_shape[i] = ceil(input_shape[i] / strides[i])` for each axis `i` no longer valid?
p-wysocki(2022-09-22 14:33:47):I do not actually remember changing that at all, I am not sure why it appeared in the changelog. I'll look into it.
gramalingam(2022-09-27 23:42:05):You can used the predefined parametrized adapter `RemoveAttribute` ... no need to define a new adapter class.
```cpp
   registerAdapter("MaxPool", 17, 18, RemoveAttribute(kstorage_order));
```
gramalingam(2022-09-27 23:42:32):No need for a specialized adapter. Please see comment below.
gramalingam(2022-09-27 23:45:55):Does the `storage_order` attribute affect the output-value? I assume it has no effect on the output, is that correct? Just trying to make sure I understand. If it has an effect, then the adapter needs to be different.
kraiskil(2022-09-28 13:19:21):I don't see how the storage order should affect on the python code level. Also looking at the `.pb` tensors files generated out of this test (both input and output), the data seems to be plain-old-row-major. (which makes sense - grepping for `storage_order` in the onnx sources shows it is nowhere implemented).

But indeed, the output *is* affected - the optional output-indices (tensor z) calculated in this test seem to have indices calculated in a column-major fashion, and `z` needs transposing for the test to be correct.
p-wysocki(2022-10-04 10:58:33):Good point, done.
p-wysocki(2022-10-04 10:58:42):Done.
p-wysocki(2022-10-06 15:21:02):Fixed, there was a wrong reference to a docstring in `old.cc`. Thanks for letting me know!
p-wysocki(2022-10-14 13:52:34):Should `z` be corrected if this test is for `opset12` only? The operator behavior has not been changed, so as long as `opset_imports=[onnx.helper.make_opsetid("", 12)],` is present, the test should pass just like it did before. I did not reintroduce this test to `opset18` because `storage_order` is being removed.
kraiskil(2022-10-15 11:28:52):No I don't think `z` is wrong in the old opset tests where `storage_order` is set.
Only when storage_order is removed does `z` become incorrect - I just didn't notice the test was being removed. Removing it is a good fix for this :)
gramalingam(2022-10-18 23:17:36):My question was motivated by the adapter, rather than this test-case. If I understand correctly, simply omitting the storage_order attribute is not correct for converting a model with the old op to the new op ... do we need to insert some transpose to get the desired behavior of the original op (when the attribute is used)?
gramalingam(2022-10-18 23:32:43):In other words: it looks like the storage_order is a meaningful attribute ... it is used to convert the n-tuple of indices into a single index. It makes sense for MaxPool, since the value comes from a single index (ignoring duplicate max values), unlike the other pooling operations, and it is not relevant for other pooling ops, since they don't have such an index output. So, I wonder whether we should just the op be as is, instead of removing the attribute? (I agree that this is probably irrelevant for inference, since it is likely used for backprop in training, but we can't assume what users are using it for.)
snnn(2022-10-29 05:12:03):But I found an interesting thing: ORT's Windows CI build pipelines build ONNX python package from source with protobuf 3.21.8. Then when ORT tries to run its python tests(which depends on protobuf and onnx python packages), then if the protobuf version is 3.18.3, the tests fail. However, if we uninstall protobuf and install onnx again, because onnx says the max supported protobuf version is 3.20.1, then pip will get that version, and it works! 
jcwchen(2022-10-29 12:35:22):Thanks for bringing this up. I bumped into similar issue before... I was considering to aggressively bump Protobuf version as 3.20 for building ONNX, but there were some test failures if the built ONNX played with Python Protobuf<=3.19. Therefore, if ONNX decides to bump its Protobuf version for building ONNX as 3.20, probably ONNX needs to upgrade its required Python Protobuf as `protobuf >= 3.20, < 4` in https://github.com/onnx/onnx/blob/main/requirements.txt. The compatibility is kind of limited in that case.

Thus, I took a step back in next release: I decided to bump Protobuf version as 3.18.3 for building ONNX instead and so ONNX can preserve much broader compatibility for Python Protobuf version: https://github.com/onnx/onnx/pull/4544. Still, in the future while ONNX needs higher Protobuf version, this issue will hit us again anyway. We need to keep in mind about the limited compatibility issue for Protobuf>=3.20 or Protobuf>=3.21.
snnn(2022-10-29 15:42:18):But they say 3.18.x releases are end of support? Should you use a newer one?
jcwchen(2022-10-29 16:24:13):Good question. Protobuf 3.19.5 or 3.19.6 has another issue about fixed build_type in their build config. So the options we have here are 3.18.3 or 3.20.2+. I slightly prefer 3.18.3, because other ONNX related tools might have Protobuf version requirement and using Protobuf 3.20.2+ for building ONNX might break something, as your example above, if they really need lower Python Protobuf version (<3.20) somehow. In addition, we can add an announcement in next release and say in the following release ONNX will upgrade its Protobuf version to 3.20 for build and the minimum required Python Protobuf version will be 3.20, to let users be aware of it in advance.

IIRC, ONNX Runtime is using Protobuf 3.18.3 for build also. @snnn Are you planning to upgrade ORT's Protobuf version to 3.20+ soon? If yes, definitely we should re-evaluate if ONNX should upgrade its Protobuf version to 3.20, to prevent some conflict issue in the near future.
jcwchen(2022-10-29 16:36:08):> But they say 3.18.x releases are end of support?

Forgot to answer your question -- yes Protobuf people said 3.18.x are out of their support window, but 3.18.3 is a quite fresh patch they have released last month.
snnn(2022-10-29 23:08:52):> Are you planning to upgrade ORT's Protobuf version to 3.20+ soon?

yes. For this issue: https://github.com/microsoft/onnxruntime/issues/13335

But the change won't be as trivial as before. It needs some code changes. 


dependabot[bot](2022-10-20 17:18:53):Looks like protobuf is up-to-date now, so this is no longer needed.
justinchuby(2022-09-23 22:50:45):We can use vscode's format document to standardize format on this file
justinchuby(2022-09-23 22:52:34):fyi this can also go into pyproject I think
jcwchen(2022-09-23 23:30:28):Formatted and moved pylint. Thanks!
AlexandreEichenberger(2022-09-28 20:45:32):Suggestion: the text in the diff is in a low contrast grey that is hard to read. Please keep it in easy to read black:

![image](https://user-images.githubusercontent.com/7581397/192885191-b8afc18b-a45b-4307-a71a-88b49dcf56a8.png)

AlexandreEichenberger(2022-09-28 20:46:32):Minor nit: when we click on a link that goes to the end of a page (e.g. Table on the onnx operator), the title is not at the top of the page , probably because it is the end of the page.
Adding blank carriage return may be a simple fix
prasanthpul(2022-09-28 20:46:43):* some of the ops have "or" as a subheading. seems like a formatting error?

* "expect" as a heading on the main operator page is odd. it's the function used, but doesn't make sense as a header.

* can the op tables be used as the operator TOC? its basically the same info, except for the direct links to older versions
zhenhuaw-me(2022-09-29 09:09:00):This is awesome!

~A quick question, why title "Python Documentation of ONNX" at landing page (similar issue of `onnx_python` for the path `docs/docsgen/source/onnx_python/`) while it doesn't seem to be python specific?~ Okay, it seems to be python specific.

For the operator docs, do we plan to redirect (url redirect or notes) the user from the current doc https://github.com/onnx/onnx/blob/main/docs/Operators.md to this new doc?

prasanthpul(2022-09-30 17:32:42):superceded by #4561 
github-code-scanning[bot](2022-09-23 23:44:44):## Jinja2 templating with autoescape=False

Using jinja2 templates with autoescape=False can potentially allow XSS attacks.

[Show more details](https://github.com/onnx/onnx/security/code-scanning/124)
github-code-scanning[bot](2022-09-23 23:44:45):## Jinja2 templating with autoescape=False

Using jinja2 templates with autoescape=False can potentially allow XSS attacks.

[Show more details](https://github.com/onnx/onnx/security/code-scanning/125)
github-code-scanning[bot](2022-09-23 23:44:45):## Implicit string concatenation in a list

Implicit string concatenation. Maybe missing a comma?

[Show more details](https://github.com/onnx/onnx/security/code-scanning/126)
github-code-scanning[bot](2022-09-23 23:44:45):## Implicit string concatenation in a list

Implicit string concatenation. Maybe missing a comma?

[Show more details](https://github.com/onnx/onnx/security/code-scanning/127)
github-code-scanning[bot](2022-09-23 23:44:45):## Variable defined multiple times

This assignment to 'language' is unnecessary as it is redefined [here](1) before this value is used.

[Show more details](https://github.com/onnx/onnx/security/code-scanning/130)
github-code-scanning[bot](2022-09-23 23:44:45):## Variable defined multiple times

This assignment to 'pygments_style' is unnecessary as it is redefined [here](1) before this value is used.

[Show more details](https://github.com/onnx/onnx/security/code-scanning/131)
github-code-scanning[bot](2022-09-23 23:44:45):## Module is imported with 'import' and 'import from'

Module 'onnx.defs' is imported with both 'import' and 'import from'

[Show more details](https://github.com/onnx/onnx/security/code-scanning/128)
github-code-scanning[bot](2022-09-23 23:44:45):## Unused import

Import of 'pydata_sphinx_theme' is not used.

[Show more details](https://github.com/onnx/onnx/security/code-scanning/129)
natke(2022-09-25 19:54:02):@xadupre Can you please have a look at these warnings? 
github-code-scanning[bot](2022-09-29 19:45:15):## Variable defined multiple times

This assignment to 'html_theme_options' is unnecessary as it is redefined [here](1) before this value is used.

[Show more details](https://github.com/onnx/onnx/security/code-scanning/575)
cjvolzka(2022-09-28 16:03:11):When this is ready, is it something that would be back ported to the current 1.12 onnx release or would it wait for some future release (ie 1.13)?
jcwchen(2022-09-28 16:15:17):> When this is ready, is it something that would be back ported to the current 1.12 onnx release or would it wait for some future release (ie 1.13)?

Good question. ONNX 1.13 release will probably happen this Nov. or Dec so I would say it will only be included in future release instead of a patch release for current 1.12. However, for now I am even not sure whether this PR will be included in next 1.13 since this PR will upgrade ONNX's minimum supported version of Python Protobuf (3.12.2 -> 3.20.0). I will need more time to investigate whether it is OK for other ONNX related tools. May I understand your demand? (Is it also because of this issue ~https://github.com/onnx/onnx/security/dependabot/4~ [GHSA-8gq9-2x98-w8hf](https://github.com/protocolbuffers/protobuf/security/advisories/GHSA-8gq9-2x98-w8hf)?) 
cjvolzka(2022-09-30 01:02:37):@jcwchen I can't seem to access the link you provided. It's related to https://github.com/onnx/onnx/issues/4545 which references https://github.com/protocolbuffers/protobuf/security/advisories/GHSA-8gq9-2x98-w8hf. 

Basically anyone who pulls in onnx and runs a dependency scanner is going to trip up on this. For our case onnx-mlir -> onnx -> protobuf. 
jcwchen(2022-09-30 01:11:25):> @jcwchen I can't seem to access the link you provided. It's related to https://github.com/onnx/onnx/issues/4545 which references [GHSA-8gq9-2x98-w8hf](https://github.com/protocolbuffers/protobuf/security/advisories/GHSA-8gq9-2x98-w8hf).

Yes, thank you for providing the valid reference and that is the one I was talking about. I think it's good to have, but let me announce it for a while and see whether anyone has a concern about the upgrade. I will let you know when the decision is made. Thank you for waiting.

jcwchen(2022-10-20 17:20:35):@cjvolzka FYI ONNX has bumped its used Protobuf version from 3.16.0 to 3.18.3. Thanks for waiting.
AlexandreEichenberger(2022-10-20 18:25:42):@p-wysocki can we make sure that this PR will come in ONNX 1.13 release? Since it's merged, it appears that this will be the case, just wanting to make sure. Thanks.
github-code-scanning[bot](2022-09-29 19:19:36):## Variable defined multiple times

This assignment to 'html_theme_options' is unnecessary as it is redefined [here](1) before this value is used.

[Show more details](https://github.com/onnx/onnx/security/code-scanning/575)
xadupre(2022-09-29 07:32:49):It is possible to add a unit test to check the warning is raised when using the dictionary and not raised when using the function?
jcwchen(2022-09-29 18:12:00):> It is possible to add a unit test to check the warning is raised when using the dictionary and not raised when using the function?

Good idea. I just added tests by `TestHelperMappingFunctions` and actually `np_dtype_to_tensor_dtype` needs to be fixed as well. With the added tests, now I am confident that these new introduced helpers should not unexpectedly throw a deprecation warnings. Please review it. Thank you!
xadupre(2022-09-29 18:19:50):Misspelling deprecation
xadupre(2022-09-29 18:25:53):I wonder if it is worth checking what happens in case of bfloat16.
jcwchen(2022-09-29 20:50:36):Corrected. Thanks!
jcwchen(2022-09-29 21:00:31):Tests for bfloat16 looks useful since the mapping for bfloat16 is quite confusing. Adding these tests can prevent future regression. Just added them. PTAL. Thanks!
jcwchen(2022-09-29 21:01:30):Please note that I think BFLOAT16 is missing here.
xadupre(2022-09-30 07:42:15):I can be done in another PR but I would check all the types:

```python
    def test_numeric_types(self):  # type: ignore
        dtypes = [
            np.float16,
            np.float32,
            np.float64,
            np.int8,
            np.int16,
            np.int32,
            np.int64,
            np.uint8,
            np.uint16,
            np.uint32,
            np.uint64,
            np.complex64,
            np.complex128,
        ]
        for dt in dtypes:
            with self.subTest(dtype=dt):
                t = np.array([0, 1, 2], dtype=dt)
                ot = from_array(t)
                u = to_array(ot)
                self.assertEqual(t.dtype, u.dtype)
                assert_almost_equal(t, u)

    def test_make_tensor(self):  # type: ignore
        for pt, dt in TENSOR_TYPE_TO_NP_TYPE.items():
            if pt == TensorProto.BFLOAT16:
                continue
            with self.subTest(dt=dt, pt=pt, raw=False):
                if pt == TensorProto.STRING:
                    t = np.array([["i0", "i1", "i2"], ["i6", "i7", "i8"]], dtype=dt)
                else:
                    t = np.array([[0, 1, 2], [6, 7, 8]], dtype=dt)
                ot = make_tensor("test", pt, t.shape, t, raw=False)
                self.assertFalse(ot is None)
                u = to_array(ot)
                self.assertEqual(t.dtype, u.dtype)
                self.assertEqual(t.tolist(), u.tolist())
            with self.subTest(dt=dt, pt=pt, raw=True):
                t = np.array([[0, 1, 2], [6, 7, 8]], dtype=dt)
                if pt == TensorProto.STRING:
                    with self.assertRaises(TypeError):
                        make_tensor("test", pt, t.shape, t.tobytes(), raw=True)
                else:
                    ot = make_tensor("test", pt, t.shape, t.tobytes(), raw=True)
                    self.assertFalse(ot is None)
                    u = to_array(ot)
                    self.assertEqual(t.dtype, u.dtype)
                    assert_almost_equal(t, u)
```
jcwchen(2022-09-30 13:46:40):Thank you @xadupre for the suggestion. It looks good to me, but I am inclined to have it in another PR (this PR more focuses on bug fixes) Feel free to let me know if you still have other concern.
AllenTiTaiWang(2022-09-29 16:05:59):> Do you know why it didn't fail previously in ONNX CIs? Is it simply because colorama might be missing in some environments?

I think it gives the warning, but not sure why it didn't make tests flaky. @justinchuby Do you know what happened?
AllenTiTaiWang(2022-09-29 16:06:34):BTW, I don't have write access on this repo ...
justinchuby(2022-09-29 16:13:25):Not completely sure. Could it be a glitch in pip? It only happens for py37 from what I have seen
jcwchen(2022-09-30 13:40:10):For simplicity, I am inclined to take this PR only if there is an actual issue happening from onnx side. Now I will close this one. Thank you @AllenTiTaiWang for proposing this PR and it will still be helpful if we do see similar issues in the future. 
jcwchen(2022-09-29 01:09:07):Is it OK to use `>=` instead? 
```suggestion
colorama>=0.4.5
```
AllenTiTaiWang(2022-09-29 01:22:01):Yes, >= makes more sense here, but before making change. @justinchuby I see you are setting up fixed version for lint in onnx-script. Is there any concerns that we use >= ?
justinchuby(2022-09-29 01:32:09):Not really. whatever works

Update: I changed to using >=
jcwchen(2022-09-29 03:05:17):Ditto.
```suggestion
    "colorama>=0.4.5",
```
jcwchen(2022-09-29 16:21:32):To make `requirements-dev.txt`compact, if it only fails with Python 3.7 somehow while using isort, could we add a TODO here saying colorama can be removed after Python 3.7 is deprecated?

It seems to me at least the latest isort does set colorama as a required dependency: https://github.com/PyCQA/isort/blob/12cc5fbd67eebf92eb2213b03c07b138ae1fb448/pyproject.toml#L47.
justinchuby(2022-09-29 16:34:39):looking at the pyproject, colorama is set as an optional dependency. usually installing `isort[colors]` will work (which downloads colorama). In this case something's wrong with the version resolution.

Do we actually see prolonged test runs in onnx for py37? It could also be that onnx-script didn't have `wheels` in the environment before.

If we don't see any issues I don't think we need to change anything.
AllenTiTaiWang(2022-09-29 17:53:54):Even though we don't really see the prolonged test runs for py37, it's potentially a risk proved in another repo, and now that we already have this PR, I think it's no harm to merge? But I am fine either way.
jcwchen(2022-09-29 15:20:43):cc @askhade Thank you for bringing this up to make checker more succinct.
gramalingam(2022-10-05 15:31:06):Minor nit: `used_experimental_ops` would be a slightly better name than `existed_experimental_ops`.
gramalingam(2022-10-05 15:32:18):Nit: `Warning: Model contains experimental ops:` may be a simpler error message.
gramalingam(2022-10-05 15:35:34):I think we should check that the domain is the standard domain also. If it is some other domain, we should not treat it as experimental. (I realize that this is a pre-existing issue, but would be good to fix it.)
jcwchen(2022-10-05 16:20:19):Updated. 
jcwchen(2022-10-05 16:20:29):Sounds good. Done.
jcwchen(2022-10-05 16:25:25):Good point. I have further considered domain. Please note that I also changed the public function `check_is_experimental_op`'s interface from
`bool check_is_experimental_op(std::string node_op_type)`
to
`bool check_is_experimental_op(const NodeProto& node)`. I think right now check_is_experimental_op is only used internally in onnx repo (shape_inference's implementation.cc) and passing the node itself is simpler. Let me know if you have concern with it. Thanks!
gramalingam(2022-10-05 16:54:14):Looks good, thanks!

Another nit (about the old code): I think the ` ? true : false` at the end is unnecessary and can be omitted?
jcwchen(2022-10-05 16:56:51):Good catch.. Will update it soon. Thanks!
justinchuby(2022-10-06 23:36:49):@jcwchen 
justinchuby(2022-10-07 00:42:13):All checks passed
justinchuby(2022-10-12 16:36:46):Do we need another reviewer for this?
jcwchen(2022-10-12 16:42:05):Could @onnx/sig-operators-approvers please review/approve this PR? Thanks!

> Do we need another reviewer for this?

Files under ONNX are owned by sig-operators-approvers, sig-archinfra-approvers, onnx/steering-committee. See https://github.com/onnx/onnx/blob/main/CODEOWNERS. If your PR involves files with multiple owners, it will need approvals from all code owners.
gramalingam(2022-10-12 17:42:43):Not sure about this ... may be this was intentional, to include the expected output in the documentation?
jcwchen(2022-10-12 17:49:23):> to include the expected output in the documentation

Yes that is the most likely reason... If it is really useful, we can still keep it on demand.
gramalingam(2022-09-30 17:58:07):Why not something like
```
    Input: float[M,K] x, float[K,N] a, float[N] c
    Output: float[M, N] axc
    ax = onnx.MatMul(a, x)
    axc = onnx.Add(ax, c)
```
It's just pseudo-code. And the above is much closer to ONNX proto.
gramalingam(2022-09-30 18:03:55):I suggest omitting above line. Or, clarify that inputs/outputs represent the input parameters to the inference-function and the prediction produced by the inference-function. 
gramalingam(2022-09-30 18:07:53):May be change title to `Documentation of ONNX Python Package` ?
gramalingam(2022-09-30 18:14:57):The part starting from this line seem out of place for this file. May be a copy-paste error? Feels like it should be in another file.
xadupre(2022-09-30 18:19:14):True, I split the files and forgot to remove it.
gramalingam(2022-09-30 18:19:46):This section feels out of place in the "operator" documentation. I am trying to understand it's purpose.

The code itself seems useful as a utility, but not relevant to this "operator" chapter (or whatever it is called).

Of course, it is useful to explain to the reader how to interpret the `expect(...)` line found in the documentation. That explanation seems appropriate for this section, but would be better explained in text on what the `expect` API means.

xadupre(2022-09-30 18:20:03):done
xadupre(2022-09-30 18:20:11):I tried to improve.
xadupre(2022-09-30 18:20:15):done
xadupre(2022-09-30 18:20:20):done
xadupre(2022-09-30 18:23:24):This comes from a comment from Marjan. He was telling that none of the examples in the documentation of every operator can be run. Two reasons, missing imports (added) and a function expect difficult to find. So I suggested to give an implementation of this function based on onnxruntime as a way for the user to check the output against the expected outputs.
gramalingam(2022-09-30 18:32:18):I see. Ok. I think the fundamental issue is the code is being used for two purposes: to generate test-cases in proto format, but also to serve as example (input, output) pairs to be included in the document. IMHO, what is most useful in the documentation is something that can be *read*: that is, sample input and sample output. But we currently don't do a great job in the documentation part (since we just copy the test-generation code). Anyway: that is a separate issue, maybe a future potential improvement.
jcwchen(2022-09-30 18:58:16):Do we still need "add-docs" branch now?
jcwchen(2022-09-30 21:56:29):So is this builds doc in this CI more like for check/test purpose? If yes, shall we use a more precise displayName?
xadupre(2022-09-30 21:58:10):Test documentation?
jcwchen(2022-09-30 21:58:49):Sounds good!
jcwchen(2022-10-01 17:36:42):It is not allowed to deploy from a external PR
jcwchen(2022-10-05 15:58:27):@onnx/sig-archinfra-approvers please review/approve this PR and let's make this CI green first.
jcwchen(2022-10-05 17:43:22):Now the [pages.yml](https://github.com/onnx/onnx/blob/main/.github/workflows/pages.yml) can build ONNX successfully, but it still fails later while building docs: `cd docs/docsgen && make html -j auto`. I saw a similar failure in AzurePipelines (it shows warning instead of error [there](https://dev.azure.com/onnx-pipelines/onnx/_build/results?buildId=31544&view=logs&j=983f5ddd-a6d2-5453-804f-c4202009b4f1&t=23721ae6-d1a8-5bda-b271-f882ff462c68)). It is a warning so the AzurePipeline CI doesn't catch this failure. @xadupre do you know what's the exact failure is? Did you bump into similar issue from your local?
xadupre(2022-10-05 18:41:37):This is expected. As you can see, the build shows build succeeded. The errors tells that sphinx cannot compile some part of the documentation and if you look at the errors, it comes from the documentation for every operator written in c++ files. It does not stop the html pages to be generated. Azure devops probably interprets the errord on stderr as failures. The regular ci is set to continue even there failure. For the job generating the pages, maybe we set ask azure devops to look for string "build succeeded" and declare the task as a success if it is found. What do you think? I'll work later to improve and the rendering and fix the errors. Right now, the rendering is on par with thr markdown pages or better even with errors.
jcwchen(2022-10-05 23:58:31):Thanks for the details. It's a weird behavior that it says build succeed, but it also throws stderr... If the original API for doc generation cannot silent those warnings somehow, yes probably we need to handle that from our own for now. 

> maybe we set ask azure devops to look for string "build succeeded" and declare the task as a success if it is found. What do you think?

Sounds like a plan. Or, I was thinking whether we can directly add `always()` (see [reference](https://docs.github.com/en/actions/learn-github-actions/expressions#always)) in the section for building doc to make it always continue to run, and then further check the generated doc before deploying. The problem is it seems hard to know whether the build is successful without the output message from the make command. I think yours is more feasible.

gramalingam(2022-10-12 21:31:30):Thanks for the contribution!
cbourjau(2022-10-14 07:18:37):Thank you very much for your feedback @gramalingam . I incorporated it and expanded the tests accordingly.  
gramalingam(2022-10-12 21:33:00):This looks correct. I think we could make it more precise if the second tensor's dimension-size is statically known, I suppose.
gramalingam(2022-10-12 22:30:34):Specifically, I think we could do something like
```cpp
   Dim dim;
   if (hasInputShape(ctx, 1)) {
      const auto& indices_shape = getInputShape(ctx, 1);
      if (indices_shape.dim_size() > 0) {
          dim = indices_shape.dim(0)
      }
   }
   *output_shape->add_dim() = dim;
```
gramalingam(2022-10-12 22:32:19):However, I notice that the op's description doesn't mention the order of axes for the output. This looks like a missing piece of information.
gramalingam(2022-10-12 22:47:30):onnxruntime, however, seems to be consistent with the above ordering.
cbourjau(2022-10-14 07:18:28):While missing in the specification, this appears to me to be the behavior people would expect. I added your suggestion to the shape inference. Thanks!
wschin(2022-10-07 16:40:33):Why is it removed? It looks like a behavior change and needs a new op version.

[Update]
I found the new "reduction" attribute actually allows duplicate entries.
ilya-lavrenov(2022-10-13 22:50:12):Hi team, 

Anything else I can help to get it merged?
jcwchen(2022-10-11 15:55:20):cc @justinchuby I have tested this PR with your provided model and it can infer Resize's shape as [1, 1, 2, 2].
justinchuby(2022-10-11 17:34:42):Thank you!
gramalingam(2022-10-11 18:16:20):Thanks Jacky for this. I wonder, however, if we can use [getShapeInput](https://github.com/onnx/onnx/blob/a525f98d5ed31660b629bab90c680a39658a5c08/onnx/defs/shape_inference.cc#L428) ... I think it was abstracted as a function for this purpose.
jcwchen(2022-10-12 00:21:54):> Thanks Jacky for this. I wonder, however, if we can use [getShapeInput](https://github.com/onnx/onnx/blob/a525f98d5ed31660b629bab90c680a39658a5c08/onnx/defs/shape_inference.cc#L428) ... I think it was abstracted as a function for this purpose.

Good catch. Yes, I should use getShapeInput instead. Updated. Please review it. Thanks!
jcwchen(2022-10-12 00:26:12):Actually I was confused about this check. I saw the document also mentioned this, but existing test models like `test_resize_tf_crop_and_resize_axes_2_3` and `test_resize_tf_crop_and_resize_axes_3_2` do not follow this. Take `test_resize_tf_crop_and_resize_axes_2_3` as an example, the model is (input rank 4 != sizes 2):

![image](https://user-images.githubusercontent.com/14194980/195221692-ae4b451d-f84d-47df-a7db-d4183624bc67.png)

@gramalingam do you think they are invalid ONNX models?
gramalingam(2022-10-12 17:59:15):Hi, I think this may be a bug introduced when the `axes` attribute was added to Resize. If the `axes` attribute is specified, we need to account for it (and the PR that added the attribute may have failed to account for it both in code and the documentation).
gramalingam(2022-10-12 18:01:32):This PR: https://github.com/onnx/onnx/pull/4126
gramalingam(2022-10-12 18:02:25):Good catch!
gramalingam(2022-10-12 18:20:41):Can you clarify why? Specifically: is my understanding correct that this is an error-situation? Or, is this supposed to be a valid situation? (I guess it could be valid if `axes` is empty ... except for that edge-case.)
gramalingam(2022-10-12 18:23:19):Oh ... I remember, is this the old test-case fix where empty-tensor was used (before input was made optional)?
jcwchen(2022-10-12 18:24:29):https://github.com/onnx/onnx/pull/4388 I think it just provides some flexibility to let ONNX handle old models.

>  I remember, is this the old test-case fix where empty-tensor was used (before input was made optional)?

Yes! Some existing models in ONNX Model Zoo have this kind of usage.
jcwchen(2022-10-12 18:30:34):Thank you for the helpful context! Sorry that I think I provided the wrong model above. Here is the right model for `test_resize_tf_crop_and_resize_axes_2_3`.

![image](https://user-images.githubusercontent.com/14194980/195420523-9898babf-69e6-4790-b04e-bdae26b1f3d3.png)


Based on the naming of this test model, does it mean that this model should have axis attribute?

gramalingam(2022-10-12 18:32:59):So, it is difficult to see what was in the old-version vs. new-version here ... the check is fine if axes attribute wasn't specified, right? The new-version seems to have this check only if axes attribute is not specified?
gramalingam(2022-10-12 18:36:39):> Based on the naming of this test model, does it mean that this model should have axis attribute?

Yes, the name suggests that. (Excuse the previous comment, it is appearing out-of-order here, it was for previous thread.)
jcwchen(2022-10-12 18:46:32):IIUC, the only difference about this issue is the update from this PR enables "sizes" input to have the shape information even if the input tensor is unknown. Previously the tensor (sizes) will be nullptr anyway so it did not check the rank for input x and input sizes... IMO, a possible fix might be making these 2 models have attribute axes (2, 3) and axes (3, 2) and then they should pass the existing check. Does it look good to you?
gramalingam(2022-10-12 18:55:19):I agree the test-cases seem to be missing the axes attribute. We should add them. I guess you are saying that the checker wasn't catching the error previously, and we are able to catch it now (because of the improvement due to data-propagation)?
jcwchen(2022-10-12 19:04:38):> I guess you are saying that the checker wasn't catching the error previously, and we are able to catch it now (because of the improvement due to data-propagation)?

Yes. data-propagation will at least keep the shape information (the shape of "sizes" in this case) so it can do more check. I will update these models then. Thanks!
jcwchen(2022-10-12 21:01:29):I have updated these 2 models with attribute axes. Now both of them have passed onnx.shape_inference. PTAL.
jcwchen(2022-10-13 16:24:17):After internal discussion, close it since it is not really needed for now.
jcwchen(2022-10-12 17:35:51):tiny nit: will `https://github.com/onnx/onnx/tree/main/docs` be better?
jcwchen(2022-10-12 17:38:28):I thought the content in https://onnx.ai/onnx/ will be updated after every update in the main branch?
xadupre(2022-10-12 17:48:18):I think the update is manual. So it is really up to us.
gramalingam(2022-10-12 17:52:39):Someone else was asking me a similar question (about `docs/Operators.md`) ... It is a good idea to have a latest-release documentation. (We could add a development version also, if it is feasible to have both ... I mean in `onnx.ai`.)
jcwchen(2022-10-12 17:53:58):The [document from website](https://onnx.ai/onnx/onnx_doc_folder/text_diff_ScatterND_16_18.html) seems to include Rama's recent update https://github.com/onnx/onnx/pull/4575. (which is merged after [this PR for page creating](https://github.com/onnx/onnx/pull/4576)) Unless someone is manually triggering it after Rama's PR?
xadupre(2022-10-12 17:54:43):For new users of onnx, markdown pages are very difficult to use. They is no title, no way to search them, markdown is very slow to render... 
jcwchen(2022-10-12 17:56:41):I see. I was just confused that why onnx's Readme has a link to redirect to itself (https://github.com/onnx/onnx).
jcwchen(2022-10-12 17:57:33):To make it only trigger manually, we should remove this event https://github.com/onnx/onnx/blob/main/.github/workflows/pages.yml#L7.
jcwchen(2022-10-13 14:32:23):> To make it only trigger manually, we should remove this event

I have a fresh PR to do that: https://github.com/onnx/onnx/pull/4597
andife(2022-10-13 16:49:58):> LGTM. Thank you for catching these! I am surprise that reviewdog/action-misspell
> 
> https://github.com/onnx/onnx/blob/4637098e881d0d2f63caa5e945a9fdc620e11e06/.github/workflows/lint.yaml#L48
> did not catch these in advance..

Maybe we should check it with different mispelt words? ... I can create a pull request for testing that topic within the next days
gramalingam(2022-10-17 16:47:29):Thanks for the PR! 
gramalingam(2022-10-17 16:56:55):IMO, the code can be rewritten as:
```cc
checkInputRank(ctx, 0, 2);
Dim N, E;
unifyInputDim(ctx, 0, 0, N);
std::vector<int64_t> class_ids;
auto has_ids = getRepeatedAttribute(ctx, "class_ids", class_ids);
if (has_ids) {
   unifyDim(E, class_ids.size()); // or, just: E.set_dim_value(class_ids.size());
}
updateOutputShape(ctx, 0, {N});
updateOutputShape(ctx, 1, {N, E});
```
jbachurski(2022-10-17 23:10:51):Great suggestion! I wasn't aware of these higher-level functions. I applied it & refactored in the cases of the tree ensemble operators.
gramalingam(2022-10-19 03:59:22):Hi, Thanks for finding the typo, and the PR. Unfortunately, operators.md is generated documentation. So, you will need to fix the source code [here](https://github.com/onnx/onnx/blob/1010de3a3bfd77d753860f9981f07cb3424ca202/onnx/defs/nn/defs.cc#L2406) and generate the documentation (after installing the changes) by running `python onnx\defs\gen_doc.py` ... I realize that's a lot for a single line fix :(
gramalingam(2022-10-19 16:53:24):As the CI indicates, there are other places that need to be fixed as well: `docs/Changelog.md`. (Running the document-generator generates all these automatically.)
gramalingam(2022-11-02 16:52:00):LGTM. I think there may be a minor interaction between this PR and https://github.com/onnx/onnx/pull/4512 (which is rationalizing all Reduce* functions to take axes as an input instead of attribute) which will need to be resolved when both get merged.
yuanyao-nv(2022-11-02 21:22:54):@gramalingam Thanks for the suggestions. I've made the updates. Please review again. Thanks!
gramalingam(2022-11-03 20:16:52):LGTM, thanks! I was wondering whether you had a chance to test the correctness of the function-body definition (eg., by running onnxruntime, or any other backend, on the generated expanded test-cases)? You may need to manually patch the opset of the generated test-cases to be the previous opset, and then it should be a valid test-case for the previous opset, I think, runnable by onnxruntime etc.
yuanyao-nv(2022-11-04 05:17:25):> LGTM, thanks! I was wondering whether you had a chance to test the correctness of the function-body definition (eg., by running onnxruntime, or any other backend, on the generated expanded test-cases)? You may need to manually patch the opset of the generated test-cases to be the previous opset, and then it should be a valid test-case for the previous opset, I think, runnable by onnxruntime etc.

Thanks for pointing this out. I just tested and actually found a small error in the function body. Now fixed.
github-code-scanning[bot](2022-10-25 22:28:55):## Unused local variable

Variable dims_x is not used

[Show more details](https://github.com/onnx/onnx/security/code-scanning/1462)
gramalingam(2022-11-01 23:07:54):Is this `RankPlusOne` used? I can't see it, may be I missed it.
gramalingam(2022-11-01 23:11:50):We could replace these multiple statements with just `C = Shape <start = 1, end = 2> (X)`.
gramalingam(2022-11-01 23:13:00):Similarly, `N = Shape <start=0, end=1> (X)` should work here.
gramalingam(2022-11-01 23:14:55):And `HW = Shape <start=2>(X)` should work here. Also renaming `HW` to `Rest` will probably more accurately describe the non-image cases.
gramalingam(2022-11-01 23:30:46):Ok, this could be a problem. For better or worse, ONNX tries to allow dynamic shapes and tensors of statically unknown rank (eg., arising out of imprecision in shape-inference). We can't assume that the rank is known above.

I think the function body would be simplified if we flatten the input tensor into a 3D tensor first, eg. via
```
   Shape3D = Constant <value_ints = [0, 0, -1]>()
   X3D = Reshape (X, Shape3D)
``` 

gramalingam(2022-11-01 23:34:20):Because the function doesn't really care about the distinctions between 3rd dimension onwards. 
gramalingam(2022-11-02 16:45:18):Nit: Suggest making this a top-level function (move it outside the class), as a "reference implementation". Also, suggest dropping the suffix `_test_mode` (feels like a copy-paste artifact not relevant to this op, since it doesn't have a separate test-mode).
gramalingam(2022-11-02 16:49:24):May be add `.astype(np.float32)` to bias and scale as well? Not sure if that is the default, but I notice x has it above.
natke(2022-10-26 17:44:48):> LGTM. Thank you for the improvement! Please signoff DCO: https://github.com/onnx/onnx/blob/main/CONTRIBUTING.md#dco.

Done
jcwchen(2022-10-26 14:04:45):More context about why manually check the error log: https://github.com/onnx/onnx/pull/4570#issuecomment-1268809300. To confirm: have those build errors resolved by this PR? If not, probably we still need to keep this code.
natke(2022-10-26 17:18:20):It only fails when you use `make -j auto`, which I removed.

When I run the original workflow the docs were not generated but the build still passed.

Hence to make this workflow more reliable, I removed the hack and now the build takes a little longer (~5mins). I considered this a worthwhile tradeoff  
natke(2022-10-26 17:20:54):And yes, you can see the passing workflow here: https://github.com/natke/onnx/actions/runs/3325549665
natke(2022-10-26 17:21:34):Which publishes to the URLs in the PR description
snnn(2022-11-05 16:34:17):LGTM.
take-cheeze(2022-11-05 13:36:37):@jcwchen Thank you for reviewing! DCO and lint should be fixed now!
jcwchen(2022-11-09 16:04:50):Thank you for the contribution! Please allow me to reopen this PR to trigger release CIs
jcwchen(2022-11-09 22:03:58):nit: not sure whether we want to use `macosx_10_12_` for universal2 as well. Probably we can assume only 10.15+ users will try universal2 wheel.
daquexian(2022-11-10 09:49:30):Thanks for your review! It is what I ignored.

What's the reason for choosing 10.15? The minimum macos version that can run on apple silicon chips seems 11. Maybe we can use `macosx_11_0_`.
jcwchen(2022-11-10 19:01:35):Good point. `macosx_11_0_` looks better. Thanks!
daquexian(2022-11-11 13:38:51):I updated the code and it's a little strange that CI fails. I'll fix it tomorrow.
daquexian(2022-11-13 05:43:04):@jcwchen I found that python3.7 is incompatible with `macosx_11_0_`.

```bash
$ python3.7 -m pip install onnx-1.12.0-cp37-cp37m-macosx_11_0_x86_64.whl
ERROR: onnx-1.12.0-cp37-cp37m-macosx_11_0_x86_64.whl is not a supported wheel on this platform.
$ python3.7 -m pip install onnx-1.12.0-cp37-cp37m-macosx_10_15_x86_64.whl
Processing ...
$ python3.8 -m pip install onnx-1.12.0-cp38-cp38-macosx_11_0_x86_64.whl
Processing ...
```

So I prefer keeping the versions of x86_64 and universal2 wheels the same. It is also what [numpy](https://pypi.org/project/numpy/1.21.0/#files) does:

<img width="564" alt="image" src="https://user-images.githubusercontent.com/11607199/201507644-78e890ab-49d8-49b5-9829-f5d0c99ef84d.png">

